{"data": [{"paragraphs": [{"qas": [{"question": "Build an installer for .NET app that can run on Windows and OS X?", "id": 1045, "answers": [{"answer_id": 1040, "document_id": 626, "question_id": 1045, "text": "For Windows, consider Windows Installer XML (WiX). For OSX, you need to generate a .app bundle. Here is an example using the nant tasks included with the Monobjc project.", "answer_start": 347, "answer_category": null}], "is_impossible": false}], "context": "I am surprised I could not find this question already asked, so if I simply missed it please notify promptly.\nI need to write a very small, fairly simple application in .NET that will be downloaded by end-consumers and installed on their system. Silverlight's sandbox model will not work - it has to be a full-on downloaded, installed executable.\nFor Windows, consider Windows Installer XML (WiX). For OSX, you need to generate a .app bundle. Here is an example using the nant tasks included with the Monobjc project.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Using WiX to create an IIS virtual directory", "id": 1801, "answers": [{"answer_id": 1787, "document_id": 1373, "question_id": 1801, "text": "You should use iis:WebVirtualDir and iis:WebApplication from http://schemas.microsoft.com/wix/IIsExtension namespace.", "answer_start": 637, "answer_category": null}], "is_impossible": false}], "context": "I'd ask this on the WiX mailing list, but it seems to be down.\nI have an application which is both a desktop app and a web app which runs locally. I've created a couple of basic WiX installers, but haven't yet used the IIS extension to create a virtual directory under IIS. I haven't been able to find a simple example of how to do this. All I need to do is create the virtual directory, set its port, and point it at a real directory which I'm creating with the rest of the installer.\nA bonus would be enabling IIS on the machine if it's not already enabled, but I'm guessing that's not possible, and isn't a dealbreaker for me anyway.\nYou should use iis:WebVirtualDir and iis:WebApplication from http://schemas.microsoft.com/wix/IIsExtension namespace.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Wheel file installation", "id": 1629, "answers": [{"answer_id": 1617, "document_id": 1203, "question_id": 1629, "text": "You normally use a tool like pip to install wheels. Leave it to the tool to discover and download the file if this is for a project hosted on PyPI.", "answer_start": 176, "answer_category": null}], "is_impossible": false}], "context": "How do I install a .whl file? I have the Wheel library but I don't know how to use it to install those files. I have the .whl file but I don't know how to run it. Please help.\nYou normally use a tool like pip to install wheels. Leave it to the tool to discover and download the file if this is for a project hosted on PyPI.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How can i change the application name in xamarin android?", "id": 482, "answers": [{"answer_id": 489, "document_id": 213, "question_id": 482, "text": "Remove the Label of the MainActivity, set the application in the Manifest.xml via UI or code:\n<manifest ...>\n    <application android:label=\"@string/app_name\" />\n</manifest>", "answer_start": 509, "answer_category": null}], "is_impossible": false}], "context": "The icon label is being overwritten by the Label specified on my Main Activity Label.\nWhen I email the apk to the phone, gmail asks me to install 'MyApp', which appears to be coming from the manifest or the Assembly. However, the icon on the App Launch screen shows 'Demo App', which is coming from thI want the Label from the Activity to be appearing as the title of the app(Demo App), whereas I want the App launch name to be 'MyApp'. Can I override the title somehow to get this working?e MainActivity.cs.\nRemove the Label of the MainActivity, set the application in the Manifest.xml via UI or code:\n<manifest ...>\n    <application android:label=\"@string/app_name\" />\n</manifest>\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "how do i install a c library so i can use it", "id": 1469, "answers": [{"answer_id": 1458, "document_id": 1042, "question_id": 1469, "text": "Put the header files in a location which your compiler is aware of (typically IDE allows to set so-called include directories, otherwise you specify a flag like -I&lt;path-to-headers&gt; when invoking the compiler)\nPut the dll files in a location which your linker is aware of (surely your IDE will allow that, otherwise you speficy a flag like -L&lt;path-to-libraries&gt; -l&lt;name-of-libraries&gt;\n\nLast but not least, since I see that BASS library is a commercial product, probably they will have made available some installa", "answer_start": 1017, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI have this library called BASS which is an audio library which I'm going to use to record with the microphone. I have all the files needed to use it, but I don't know how to install the library. I tried taking the example files and putting them in the same directory as the bass.h file. But I got a bunch of errors saying there are function calls that doesn't exist. \n\nSo my question is, how do I install it to be able to use it? \n    \n\nInstalling a C++ library means specifying to interested software (eg. a compiler) the location of two kinds of files: headers (typical extensions *.h or .hpp) and compiled objects (.dll or *.lib for instance).\nThe headers will contain the declarations exposed to the developer by the library authors, and your program will #include them in its source code, the dll will contain the compiled code which will be or linked together and used by your program, and they will be found by the linker (or loaded dynamically, but this is another step).\nSo you need to\n\nPut the header files in a location which your compiler is aware of (typically IDE allows to set so-called include directories, otherwise you specify a flag like -I&lt;path-to-headers&gt; when invoking the compiler)\nPut the dll files in a location which your linker is aware of (surely your IDE will allow that, otherwise you speficy a flag like -L&lt;path-to-libraries&gt; -l&lt;name-of-libraries&gt;\n\nLast but not least, since I see that BASS library is a commercial product, probably they will have made available some installation instructions?\n    \n\nRun this command in a terminal or console.\n\ncpp -v\n\n\nNotice at the end of the output, you'll see a line like this:\n\n#include&lt;...&gt; search starts here:\n\n\nThere will be a list of directories below that line.\nMove the package folder to one of those directories.\nThen try importing the module with &lt;&gt;.\n    \n\nSee the code below code and don not forget to put bass.dll in the directory of your exe file and include the file bass.lib with your project and don not forget also to include the path to bass.h and bass.lib in the default include and lib path of your project.\n\n#include &lt;iostream&gt;\n#include \"bass.h\"\n\nusing namespace std;\n\nint main(int argc, const char **argv)\n{\n   if (!BASS_Init(-1, 44100, 0, NULL ,NULL)) \n   {\n   cout&lt;&lt;\"Can't initialize device\";\n   return -1;\n   }\n\n            int stream = BASS_StreamCreateFile(false, \"D:\\\\mypro\\\\Trans_Langs\\\\germ\\\\quran_amma\\\\Translations\\\\Sound_aya\\\\Sora1\\\\Hafs\\\\basfar\\\\a7.mp3\", 0L, 0L, 0);\n            if (stream != 0)\n            {\n                // play the stream channel\n                BASS_ChannelPlay(stream, false);\n            }\n            else\n            {\n                // error creating the stream\n                cout&lt;&lt;\"Stream error: {0}\", BASS_ErrorGetCode();\n            }\n\n   getchar();\n\n            BASS_StreamFree(stream);\n            // free BASS\n            BASS_Free();\n\n return 0;\n}\n\n    \n\nIf there are files named configure, Makefile or install you can try running them in that order. After that, any program that wants to link with this library must use a command like this:  \n\nc++ &lt;your_program.cpp&gt; -l&lt;library_name&gt; -L&lt;path_where_library_is_installed&gt;\n\n\nThe library path is usually the original library folder itself, unless you explicitly change it or the library itself puts its files in global locations like /usr/local or something like that.\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How do I deploy Node.js applications as a single executable file?", "id": 142, "answers": [{"answer_id": 150, "document_id": 88, "question_id": 142, "text": " Meanwhile I have found the (for me) perfect solution: nexe, which creates a single executable from a Node.js application including all of its modules.It's the next best thing to an ideal solution.", "answer_start": 229, "answer_category": null}], "is_impossible": false}], "context": "Supposed I have written a Node.js application, and I now would like to distribute it. Of course, I want to make it easy for the user, hence I do not want him to install Node.js, run npm install and then manually type node app.js. Meanwhile I have found the (for me) perfect solution: nexe, which creates a single executable from a Node.js application including all of its modules.It's the next best thing to an ideal solution.", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "What is the differences between Client's account and Admin account?", "id": 200651, "answers": [{"answer_id": 239426, "document_id": 357720, "question_id": 200651, "text": " they will not be able to access the Settings section or administrate your Admin account.", "answer_start": 1536, "answer_category": null}], "is_impossible": false}, {"question": "Which privilege can create user group?", "id": 200652, "answers": [{"answer_id": 239428, "document_id": 357720, "question_id": 200652, "text": "Craft Pro", "answer_start": 1922, "answer_category": null}], "is_impossible": false}, {"question": "What is the default account type when installing Craft?", "id": 200649, "answers": [{"answer_id": 239417, "document_id": 357720, "question_id": 200649, "text": "an admin", "answer_start": 800, "answer_category": null}], "is_impossible": false}, {"question": "What can Craft Pro do?", "id": 200653, "answers": [{"answer_id": 239429, "document_id": 357720, "question_id": 200653, "text": "Craft Pro allows you to set permissions on users and groups, such as the ability to access the control panel, edit content within certain sections, etc.", "answer_start": 2390, "answer_category": null}], "is_impossible": false}, {"question": "How to create Client's account in Craft?", "id": 200650, "answers": [{"answer_id": 239420, "document_id": 357720, "question_id": 200650, "text": "via the Account menu in the bottom left of the Control Panel", "answer_start": 1170, "answer_category": null}], "is_impossible": false}], "context": "Users\n\n\nCraft 3 Documentation\n\n\nCraft 2 Documentation\nCraft 3 Documentation\nCraft 2 Class Reference\nCraft 3 Class Reference\n\n\n\n\nUsers\nCraft calls all member accounts of the system \u201cusers\u201d.\nCraft creates the first user account during installation. If you stick with Craft Personal, this is the only account you will be able to create. If you need more, you will need to upgrade to either Craft Client or Craft Pro, which offer additional user accounts.\nAdmin Accounts #\nAdmin accounts are special accounts that can do absolutely everything within Craft, including some things that there aren\u2019t even explicit permissions for:\n\nEverything within the Settings section\nMake other users Admins (Craft Pro only)\nAdministrate other Admins (Craft Pro only)\n\nThe user account you create during installation is an admin by default.\n\nConsidering the amount of damage an admin can do, it\u2019s strongly recommended that you be conservative with creating new admin accounts. Only do it if you trust that they know what they\u2019re doing.\n\nClient\u2019s Account #\nCraft Client allows you to add one additional user account to the system: the \u201cClient\u2019s Account\u201d. You can create the Client\u2019s Account via the Account menu in the bottom left of the Control Panel.\n\nThe first time you click that option, you need to complete the registration page where you can create the Client\u2019s Account. Once you create a Client Account, you will see an account settings page if you visit the \u201cClient\u2019s Account\u201d page again.\nThe Client\u2019s Account is not an Admin account like yours, so they will not be able to access the Settings section or administrate your Admin account. They\u2019ll be able to do everything else, though.\nIf you ever upgrade from Craft Client to Craft Pro, note that the Client\u2019s Account will become just like any other user account. You will need to manually assign any permissions you want that account to have at that point.\nUser Groups #\nIf you have Craft Pro, you can create User Groups to help organize your site\u2019s user accounts, as well as batch-set permissions on them.\nTo create a new User Group, go to Settings \u2192 Users and click the \u201cNew Group\u201d button. You can give your group a Name and Handle, plus any permissions you want every user within the group to have.\nAfter you create your groups, you can assign users to groups by going into their account settings and clicking on the Permissions tab.\nPermissions #\nCraft Pro allows you to set permissions on users and groups, such as the ability to access the control panel, edit content within certain sections, etc. You can apply these permissions directly to user accounts as well as to user groups. When you apply permissions to a user group, all users that belong to that group will inherit them.\nThe permissions Craft comes with are:\n\n\n\nPermission\nHandle\n\n\n\n\nAccess the site when the system is off\naccessSiteWhenSystemIsOff\n\n\nAccess the CP\naccessCp\n\n\n\u21b3\u00a0\u00a0Access the CP when the system is off\naccessCpWhenSystemIsOff\n\n\n\u21b3\u00a0\u00a0Perform Craft and plugin updates\nperformUpdates\n\n\n\u21b3\u00a0\u00a0Access [Plugin Name]\naccessPlugin-[PluginHandle]\n\n\nEdit users\neditUsers\n\n\n\u21b3\u00a0\u00a0Register users\nregisterUsers\n\n\n\u21b3\u00a0\u00a0Assign permissions\nassignUserPermissions\n\n\n\u21b3\u00a0\u00a0Administrate users\nadministrateUsers\n\n\nDelete users\ndeleteUsers\n\n\nEdit [Locale Name]\neditLocale:[LocaleID]\n\n\nEdit entries\neditEntries:[SectionID]\n\n\n\u21b3\u00a0\u00a0Create entries\ncreateEntries:[SectionID]\n\n\n\u21b3\u00a0\u00a0Publish entries\npublishEntries:[SectionID]\n\n\n\u21b3\u00a0\u00a0Delete entries\ndeleteEntries:[SectionID]\n\n\n\u21b3\u00a0\u00a0Edit other authors\u2019 entries\neditPeerEntries:[SectionID]\n\n\n\u21b3\u00a0\u00a0Publish other authors\u2019 entries\npublishPeerEntries:[SectionID]\n\n\n\u21b3\u00a0\u00a0Delete other authors\u2019 entries\ndeletePeerEntries:[SectionID]\n\n\n\u21b3\u00a0\u00a0Edit other authors\u2019 drafts\neditPeerEntryDrafts:[SectionID]\n\n\n\u21b3\u00a0\u00a0Publish other authors\u2019 drafts\npublishPeerEntryDrafts:[SectionID]\n\n\n\u21b3\u00a0\u00a0Delete other authors\u2019 drafts\ndeletePeerEntryDrafts:[SectionID]\n\n\nEdit [Global Set Name]\neditGlobalSet:[GlobalSetID]\n\n\nEdit [Category Group Name]\neditCategories:[CategoryGroupID]\n\n\nView [Asset Source Name]\nviewAssetSource:[SourceID]\n\n\n\u21b3\u00a0\u00a0Upload files\nuploadToAssetSource:[SourceID]\n\n\n\u21b3\u00a0\u00a0Create subfolders\ncreateSubfoldersInAssetSource:[SourceID]\n\n\n\u21b3\u00a0\u00a0Remove files\nremoveFromAssetSource:[SourceID]\n\n\n\nPublic Registration #\nCraft Pro has the option of allowing public user registration, which is disabled by default.\nTo enable public registration, go to Settings \u2192 Users \u2192 Settings, and check the \u201cAllow public registration?\u201d setting. With that checked, you will also have the ability to choose a default user group to which Craft will assign the publicly-registered users.\nOnce you set up your site to allow public user registration, the last step is to create a user registration form on your site\u2019s front end.\n\n\n\nCraft 3 Documentation\nIntroduction\n\nAbout Craft CMS\nCode of Conduct\nHow to Use the Documentation\n\nInstalling Craft\n\nServer Requirements\nInstallation\n\nUpgrading & Updating Craft\n\nUpgrading from Craft 2\nUpdating Craft 3\nChanges in Craft 3\n\nGetting Started\n\nThe Pieces of Craft\nDirectory Structure\n\nCore Concepts\n\nSections and Entries\nFields\nTemplates\n\nTwig Primer\n\nCategories\nAssets\nUsers\nGlobals\nTags\nRelations\nRouting\nSearching\nSites\nLocalization\nElement Queries\nContent Migrations\nConfiguration\n\nTemplating\n\nGlobal Variables\nFunctions\nFilters\nTags\nQuerying Elements\nElements\nCommon Examples\n\nPlugin Development\n\nIntro to Plugin Dev\nCoding Guidelines\nUpdating Plugins for Craft 3\nChangelogs and Updates\nPlugin Settings\nControl Panel Section\nAsset Bundles\nServices\nExtending Twig\nWidget Types\nField Types\nVolume Types\nUtility Types\nElement Types\nElement Action Types\nPlugin Migrations\nPublishing to the Plugin Store\n\n\n\u00a9 Pixel & Tonic\n\n\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "installing oracle instantclient on mac os x without setting environment variable", "id": 1405, "answers": [{"answer_id": 1394, "document_id": 978, "question_id": 1405, "text": "Download the instantclient distribution from oracle.com.  For doing non-java software development, you will need (assuming Oracle 10.2):\n\ninstantclient-basic-macosx-10.2.0.4.0.zip\ninstantclient-sdk-macosx-10.2.0.4.0.zip\ninstantclient-sqlplus-macosx-10.2.0.4.0.zip\n\n\nUnzip the three files.  This will give you a directory\n\ninstantclient_10_2/\n\n\nCopy the files to /usr, which is one of the default places the dynamic loader searches.\n\nsudo cp instantclient_10_2/sdk/include/*.h /usr/include\nsudo cp instantclient_10_2/sqlplus         /usr/bin\nsudo cp instantclient_10_2/*.dylib         /usr/lib", "answer_start": 661, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nOracle's instructions specify setting DYLD_LIBRARY_PATH.  This makes my application dependent on random users' configuration and is very troublesome to set up.\n\nHow can I avoid having to set any environment variables?\n\nhttp://www.oracle.com/technology/software/tech/oci/instantclient/htdocs/intel_macsoft.html\n\nrelated note for linux: installing Oracle Instantclient on Linux without setting environment variables?\n    \n\nOracle's instantclient installation instructions specify that the user set DYLD_LIBRARY_PATH.  This is very troublesome to manage for multiple users.\n\nTo use the instantclient without setting any environment variables:\n\nDownload the instantclient distribution from oracle.com.  For doing non-java software development, you will need (assuming Oracle 10.2):\n\ninstantclient-basic-macosx-10.2.0.4.0.zip\ninstantclient-sdk-macosx-10.2.0.4.0.zip\ninstantclient-sqlplus-macosx-10.2.0.4.0.zip\n\n\nUnzip the three files.  This will give you a directory\n\ninstantclient_10_2/\n\n\nCopy the files to /usr, which is one of the default places the dynamic loader searches.\n\nsudo cp instantclient_10_2/sdk/include/*.h /usr/include\nsudo cp instantclient_10_2/sqlplus         /usr/bin\nsudo cp instantclient_10_2/*.dylib         /usr/lib\n\n\nIf you use tnsnames.ora, copy it to /etc, which is the default global place the oracle runtime searches.\n\nsudo cp tnsnames.ora /etc\n\n\nTest with\n\n/usr/bin/sqlplus scott/tiger@myoracle\n\n    \n\nIf your goal is simply to run sqlplus on your MacBook, this might work for you. Remove the DYLD_LIBRARY_PATH environment variable from ~/.bashrc and replace it with an alias:\n\nalias sqlplus=\"DYLD_LIBRARY_PATH=/Applications/instantclient_11_2 sqlplus\"\n\nBlog entry: sqlplus and DYLD_LIBRARY_PATH on Mac OS/X\n    \n\nAfter much research I found an sustainable solution to this error, that involves editing Apache's launchd configuration .plist file to specify the required DYLD_LIBRARY_PATH, ORACLE_HOME, and LD_LIBRARY_PATH environment variables.\n\nYou can view the full answer here: https://stackoverflow.com/a/20670810/1914455\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Installing git on a cPanel server", "id": 1782, "answers": [{"answer_id": 1768, "document_id": 1353, "question_id": 1782, "text": "You have just to create a Git's account (be sure to add remote ssh access) then in your .bashrc you add just this alias\nalias git=\"/usr/local/cpanel/3rdparty/bin/git\"", "answer_start": 611, "answer_category": null}], "is_impossible": false}], "context": "I need to install git on cPanel but I am finding a lot of information indicating that this is not a simple process.\nI have a cPanel/WHM instance on a CentOS distribution and am trying to install git without breaking cPanel. The information I have found says that installing git can break cPanel because of the Perl dependencies.\nDoes anyone have advice on how best to install git on cPanel such that all cPanel user accounts will be able to use git, make sure that the installation doesn't break Perl, and what is missing in the RPM provided by cPanel based on the statement in the last paragraph quoted above?\nYou have just to create a Git's account (be sure to add remote ssh access) then in your .bashrc you add just this alias\nalias git=\"/usr/local/cpanel/3rdparty/bin/git\"\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Error when installing mpi4py", "id": 1183, "answers": [{"answer_id": 1176, "document_id": 759, "question_id": 1183, "text": " I solved it by installing libopenmpi-dev package\nsudo apt install libopenmpi-dev\nand then installed the mpi4py using pip\nsudo pip install mpi4py", "answer_start": 47, "answer_category": null}], "is_impossible": false}], "context": "As the error says, there are libraries missing. I solved it by installing libopenmpi-dev package\nsudo apt install libopenmpi-dev\nand then installed the mpi4py using pip\nsudo pip install mpi4py\nI got the error message that mpi.h was missing during the installation. The path was set correctly, but only for my user, not for root. So if you run into trouble with missing libraries/headers during any installation, make sure the correct environment is also set up for root.\nAs I use mpi-selector to select which mpi implementation you use, I just had to run mpi-selector as root to set up everything correctly, and the installation succeeded.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Visual Studio 2010 Publish Web feature not including all DLLs", "id": 590, "answers": [{"answer_id": 596, "document_id": 315, "question_id": 590, "text": " None of these answers are sufficient in my mind. This does seem to be a genuine bug. I will update this response if I ever find a non-hack solution, or Microsoft fixes the bug.", "answer_start": 221, "answer_category": null}], "is_impossible": false}], "context": "When I perform a local build everything is cool. All DLLs are included in the bin\\debug folder. The problem is that when I use the Publish Web command in Visual Studio 2010, it deploys everything except ExternalAssembly2. None of these answers are sufficient in my mind. This does seem to be a genuine bug. I will update this response if I ever find a non-hack solution, or Microsoft fixes the bug.", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "python cannot find dateutil relativedelta", "id": 1467, "answers": [{"answer_id": 1456, "document_id": 1039, "question_id": 1467, "text": "Verify you installed the package.\nIf installed, verify that the files have been stored in the right directory (a directory accessible from your python interpreter (= in the PYTHONPATH, useful article here).\nVerify permission on those files.\nRestart your shell if you tried the import there.\nReboot your computer (ouch... it's 10 years since I started using GNU/Linux, but I still suffer from the bad memories of Windows! ;)", "answer_start": 2031, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI am trying to run a program using paster serve, but I keep getting the error:\n\n\n  ImportError: No module named dateutil.relativedelta\n\n\nI am running Python version 2.6.7 and dateutil version 1.5, so it should be installed.\n\nHas anyone got any ideas as to why this would happen?\n\nI am importing using\n\nfrom dateutil.relativedelta import *\n\n\nI can even see the package when I search:\n\n/usr/lib/python2.7/site-packages/dateutil/relativedelta.pyc\n/usr/lib/python2.7/site-packages/dateutil/relativedelta.py\n/usr/lib/python2.7/site-packages/dateutil/relativedelta.pyo\n\n\n\n\nUPDATE\n\nImmediately I look at this and see that dateutil is only installed for Python\u00a02.7, and I bet what I was doing was this:\n\nsudo yum install python-dateutil\n\n\nTo which sudo would have switch to the default Python version (i.e., Python\u00a02.7 instead of 2.6.4).\n\nSolving this would have been as simple as:\n\nsu\n(switch to virtual environment)\nyum install python-dateutil\n\n\nUsing su and then switching to the virtual environment will give root access and install to the virtual Python directory. Using sudo will install libraries to the default directory, not the virtual environments site-packages.\n    \n\nI also ran into this issue. The simple solution I ended up using was to add --upgrade to the end of the command. This forced it to install it even though Python thought it was installed. This resolved the issue. \n\nSo if you have this issue, try the following:\n\nsudo pip install python-dateutil --upgrade\n\n\nIt can't possibly hurt anything, so there is no harm in just forcing it to be reinstalled.\n    \n\nI had a similar issue but for a simpler reason. My fresh virtualenv simply didn't have dateutil installed and I didn't know the Python package name. I tried pip install dateutil, which obviously didn't work since the package name was incorrect. Running pip install python-dateutil instead worked (without resorting to sudo).\n    \n\nThis looks like a problem of package installation to me. A troubleshooting list that comes to my mind:\n\n\nVerify you installed the package.\nIf installed, verify that the files have been stored in the right directory (a directory accessible from your python interpreter (= in the PYTHONPATH, useful article here).\nVerify permission on those files.\nRestart your shell if you tried the import there.\nReboot your computer (ouch... it's 10 years since I started using GNU/Linux, but I still suffer from the bad memories of Windows! ;)\n\n    \n\n(The previous comment about installing python-dateutil helped me, so perhaps my comment helps someone else).\n\nFor those on Mac OS (v10.6 (Snow Leopard); I am not sure about other versions), the dateutils package is located by default at:\n\n/System/Library/Frameworks/Python.framework/Versions/2.6/Extras/lib/python/dateutil\n\n\nwhereas pip install writes the package out to:\n\n/Library/Python/2.6/site-packages\n\n\nand does not update the /Library/Python/2.6/site-packages/easy-install.pth file. As a result, when you import dateutil, you will still point to the old location, you can verify this by \"import dateutil; dateutil.__file__\".\n\nSo what I did (probably better methods are available) was to rename the old directory (/System/Library/.../dateutil) to dateutil.obsolete and restarted Python, then ran the same set of commands again. This doesn't do anything to the path file or sys.path, but skips the old dateutils package so you can get to the new one.\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to Deploy my Open Source Projects using Maven's Central Repository?", "id": 530, "answers": [{"answer_id": 532, "document_id": 255, "question_id": 530, "text": "Have you taken a look at the Guide to uploading artifacts to the Central Repository?", "answer_start": 538, "answer_category": null}], "is_impossible": false}], "context": "Is there anything I could do to get my own open source stuff into Maven's Central repository?\n\nI've wondered many times how I could get my own projects into Maven's Central repository. I was asking this myself, especially as I've seen some well known projects hosting their own repository, requiring users to add dependency and repository. At the same time, it's getting difficult for other projects to depend on those projects. As I neither want others to add an additional repository nor to host one myself, I'm looking for other ways.\nHave you taken a look at the Guide to uploading artifacts to the Central Repository?\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Silent install Qt run installer on ubuntu server", "id": 799, "answers": [{"answer_id": 795, "document_id": 482, "question_id": 799, "text": "To make it more simple, I have packed a generic script to extract Qt from an offline/online installer.\nThe script: qtci/extract-qt-installer at master \u00b7 benlau/qtci\nExample Usage:\nextract-qt-installer qt-opensource-linux-x64-android-5.5.1.run ~/Qt", "answer_start": 277, "answer_category": null}], "is_impossible": false}], "context": "I wanted to know if there is a way to do a silent install of the Qt run installer on Ubuntu Server?\nI mean by-pass the options of the installer and do a default install? There have few minor different in answering the questions of wizard for a different kind of version of Qt. To make it more simple, I have packed a generic script to extract Qt from an offline/online installer.\nThe script: qtci/extract-qt-installer at master \u00b7 benlau/qtci\nExample Usage:\nextract-qt-installer qt-opensource-linux-x64-android-5.5.1.run ~/Qt\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Capistrano deploy fails after I changed the repository URL", "id": 1689, "answers": [{"answer_id": 1677, "document_id": 1262, "question_id": 1689, "text": "cap deploy:cleanup -s keep_releases=0", "answer_start": 87, "answer_category": null}], "is_impossible": false}], "context": "I gotta say I\u2019m not sure, since I haven\u2019t been able to test this but this should work:\ncap deploy:cleanup -s keep_releases=0\nSince it wipes every release (cache) from the server.\nApparently you will also need to remove shared/cached-copy, because this doesn\u2019t seem to be cleaned by the Capistrano call above according to the comment below.\nI have a simple deployment via capistrano from a Git repository. At first I was deploying form GitHub, everything worked just fine. But then I moved my repository to BitBucket and now I'm getting\nfatal: Could not parse object '9cfb...'.\nThe problem goes away once I change\nset :deploy_via, :remote_cache\nto\nset :deploy_via, :copy\nbut that doesn't fix the problem, it only bypasses it. Is there any way I can tell capistrano to just drop the old cache?\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "how to create a robust minimal installer for windows", "id": 1511, "answers": [{"answer_id": 1500, "document_id": 1088, "question_id": 1511, "text": "\n\n\nTo have an entry in the Add/Remove Programs, you need to have a\nsetup project.\nThe setup project may compile into 2 files: *.msi and\nsetup.exe\nTo create a self-extracting package, you can use the\nIExpress t", "answer_start": 4524, "answer_category": null}], "is_impossible": false}, {"question": "how to create a robust minimal installer for windows using QT?", "id": 1512, "answers": [{"answer_id": 1501, "document_id": 1088, "question_id": 1512, "text": ".\n\nInstall the cqtdeployer tool. I am recommended use the online installer, because it has the latest version of cqtdeployer with minor bug fixes.\nOpen the cmd with your framework envirement.\nRun the cqtdeployer for your application on the cmd.\n\ncqtdeployer -bin path\\to\\myApp.exe qif \n\nIf you use Qt then add the qmake option.\ncqtdeployer -bin path\\to\\myApp.exe -qmake path\\to\\qmake.exe q", "answer_start": 6300, "answer_category": null}], "is_impossible": false}], "context": "\n\n                    \n\n            \n\n        \n\n            \n\n                \n\n                    \n\n                        Closed. This question needs to be more focused. It is not currently accepting answers.\n\n                        \n\n                    \n\n                \n\n            \n\n        \n\n            \n\n        \n\n                \n\n                    \n\n                \n\n            \n\n                \n\n                    Want to improve this question? Update the question so it focuses on one problem only by editing this post.\n\n                \n\n                    Closed last year.\n\n\n\n            \n\n        \n\n            \n\n                    \n\n                        Improve this question\n\n                    \n\n            \n\n\n\n\n\n    \n\n\n\nI want to create an installer EXE with some specific properties:\n\n\nit should be a single exe file\nit should be robust (i.e. the technique should be known to work well on any Windows system)\nit should create only a single file (a .scn screensaver file) in %systemroot%\nit should add the option to uninstall that particular file in \"Control Panel -&gt; Add or Remove Programs\" but without creating any new entry C:\\Program Files or copying an uninstaller.exe somewhere.\n\n\nI've seen installers that work like that - so how to do this?\n    \n\nTake a look at  NSIS it is quite simple and it is used to create installers for Windows.\n    \n\nAs you've said it can be an MSI, I'd suggest going down that route.\n\nYou could play around with installer projects in Visual Studio (if you have it; I'm not sure if the Express editions support making installers), as they'll quite happily produce an MSI that will do what you're asking. It's worth noting they'll produce an EXE and an MSI, you only need to distribute the MSI though.\n\nFailing that, take a look at Windows Installer XML (WiX) as that will let you hand sculpt an MSI that does exactly what you want it to do and will cater for all five of your points above.\n\nNote: Using an MSI will mean that you'll need to have Windows Installer on the machine you're installing to - It's in-box from Windows 2000/ME upwards.\n    \n\nI've heard good things about the free installer Innosetup, and the website says it can create single EXE installs.\n    \n\nTry Advanced Installer, I think it can do all that you ask. I use it for slightly more complex installers but the free version is easy to use and powerful.\n\nThere can be an issue with MSI files, older PCs don't have the latest Windows Installer, 3.1, so you need to install that or prompt the user to, first! Have you looked into using IExpress if you just want to copy a file across?\n    \n\nBoth NSIS and Inno Setup will cope with all the tasks specified. Inno Setup uses a Pascal-like language for its scripts, NSIS uses its own script language.\n    \n\nIt come to my attention that this answer is still being read. This answer is over 5 years old and much of the information here may not be relevant anymore. Using IExpress also comes with its risks as well, which you can learn more about here.\nIf you wish to proceed anyways, here is the old answer:\nIExpress\nYou can use Microsoft Windows's built in installer wizard called \"Iexpress\". To access that, open \"Run\", which is done by hitting Windows Key and R at the same time(WindowsKey+R). When you get in \"Run\", type in the little box \"iexpress.exe\". Then hit enter. You are there!\nNotes: Only available on Microsoft Windows (Legit, and NonLegit)\nSetup Explained:\nPage 1\n\nCreate a Self Extraction Directive file.\nMakes a new installation file\nCreate a Self Extraction Directive file.\nEdits a .SED file\nPage 2 (Of Create a Self Extraction Directive file.)\nLook in the \"Description\" at the bottom\nPage 3\nPackage Title\nThe name of your installer, as displayed at the top\nPage 4\nConfirmation Prompt\nWhat to tell the user before installing\nPage 5\nLicense Agreement\nSkip if you don't know what this is\nPage 6\nPackaged Files\nWhat files you need to install\nPage 7\nShow Window\nHow the window is displayed\nPage 8\nDisplayed At Top\nPage 9\nPackage Names and Options\nWhere to save your File\nHide File Extracting Progress From User\nHides the animation\nStore File using Long Name inside installer\nIgnore if you don't know what this is\nPage 10\nDisplayed At Top\nYou are done!\nClick on the Title to download the file, without doing those instructions. If it is a virus, got infected or you think it is a virus, feel free to delete the file.\n\n\n    \n\nYou can create a single installer file using just Visual Studio 2017 Pro and Windows in 3 steps:\n\n\nTo have an entry in the Add/Remove Programs, you need to have a\nsetup project.\nThe setup project may compile into 2 files: *.msi and\nsetup.exe\nTo create a self-extracting package, you can use the\nIExpress tool\n\n    \n\nNSIS, MSI any installer can do that for you. \n\nNSIS is pretty small and compact.\n    \n\nNSIS is the way to go, very simple to learn, just write a simple file specifying which files you want to install over where. You can also add QuickLaunch options and so on. Then run the NSIS compiler and you get the exe.\n    \n\nInstallShield has a release option called \"Single .EXE\".  It can bundle the payload inside, plug register w/Add-Remove Programs as you specified.\n    \n\nThe not copying an installer somewhere and adding an entry into Add/Remove seem to be counteractive.  My understanding is that add-remove programs neccesarily references a copy of the uninstaller that resides in a Windows directory (so it doesn't lose access to it).\n\nPerhaps you could have the screensaver double as an uninstaller if you pass it some sort of command-line option. Then simply tell the msi that the uninstaller IS the screensaver (hence no unnecessary coping to some other directory.)\n    \n\nIf you want to use standard deployment methods with your installer you probably should stay away from NSIS.  See http://unattended.sourceforge.net/installers.php for an overview.\n    \n\nI recommend using the Microsoft Visual Studio Installer Project. It is pretty easy to use.\n\nhttps://marketplace.visualstudio.com/items?itemName=VisualStudioClient.MicrosoftVisualStudio2017InstallerProjects\n\nYou can install it as an extension in visual studio. \n    \n\nTry the CQtDeployer tool. This tool using the Qt Installer Framework for create installers and supports Windows and Linux platforms.\n\nInstall the cqtdeployer tool. I am recommended use the online installer, because it has the latest version of cqtdeployer with minor bug fixes.\nOpen the cmd with your framework envirement.\nRun the cqtdeployer for your application on the cmd.\n\ncqtdeployer -bin path\\to\\myApp.exe qif \n\nIf you use Qt then add the qmake option.\ncqtdeployer -bin path\\to\\myApp.exe -qmake path\\to\\qmake.exe qif \n\nFor more examples of using this util see the CQtDeployer Wiki\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Depend on a branch or tag using a git URL in a package.json?", "id": 1824, "answers": [{"answer_id": 1810, "document_id": 1395, "question_id": 1824, "text": "As of NPM version 1.1.65, you can do this:\n<user>/<project>#<branch>", "answer_start": 231, "answer_category": null}], "is_impossible": false}], "context": "Say I've forked a node module with a bugfix and I want to use my fixed version, on a feature branch of course, until the bugfix is merged and released.\nHow would I reference my fixed version in the dependencies of my package.json?\nAs of NPM version 1.1.65, you can do this:\n<user>/<project>#<branch>\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Downloading Eclipse plug-in update sites for offline installation", "id": 753, "answers": [{"answer_id": 753, "document_id": 440, "question_id": 753, "text": "Most Eclipse plug-ins can be installed without the Eclipse updater, by copying the required JARs available at the update site, into the plugins and features directories of the Eclipse installation.", "answer_start": 306, "answer_category": null}], "is_impossible": false}], "context": "A plug-in that I want to install provides an update site for installation. However, the Eclipse installation that I want to install it to is on a machine that is not connected to the Internet. Is there a way for me to access the site (HTTP, FTP, etc.) to download the files in it for offline installation? Most Eclipse plug-ins can be installed without the Eclipse updater, by copying the required JARs available at the update site, into the plugins and features directories of the Eclipse installation.", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "IntelliJ web application context path", "id": 1304, "answers": [{"answer_id": 1295, "document_id": 874, "question_id": 1304, "text": "Overriding CATALINA_BASE in Tomcat startup script may break IDEA deployment as it works by supplying custom CATALINA_BASE location where the modified configuration is placed so that Tomcat loads artifacts directly from the location specified as the artifact output directory.", "answer_start": 311, "answer_category": null}], "is_impossible": false}], "context": "When I set the Application context, it doesn't seem to have any effect. I was expecting the app to be deployed to myapp, or at least the generated context.xml file to set my app's context to myapp. Any thoughts on why this doesn't work (does it have something to do with the fact that this is an exploded war)?\nOverriding CATALINA_BASE in Tomcat startup script may break IDEA deployment as it works by supplying custom CATALINA_BASE location where the modified configuration is placed so that Tomcat loads artifacts directly from the location specified as the artifact output directory.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "bundle install returns \"Could not locate Gemfile\"", "id": 1536, "answers": [{"answer_id": 1525, "document_id": 1113, "question_id": 1536, "text": "You just need to change directories to your app, THEN run bundle install", "answer_start": 365, "answer_category": null}], "is_impossible": false}], "context": "I followed the steps even adding on gem 'rb-readline' to the Gemfile, but apparently the file can't be found and when I go to my text editor I do see the Gemfile itself. I noticed that they made me put gem 'rails', 3.2.3 and my version of Rails is 3.2.1 so I tried changing it to 3.2.1 but that didn't work either.\nAny thoughts or advice would be much appreciated.\nYou just need to change directories to your app, THEN run bundle install\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Source unreachable when using the NuGet Package Manager Console", "id": 1615, "answers": [{"answer_id": 1602, "document_id": 1189, "question_id": 1615, "text": "This may solve your problem :Install-Package Akka.net -Source nuget.org", "answer_start": 745, "answer_category": null}], "is_impossible": false}], "context": "We are moving our package management from manually updating files to NuGet. I am trying to install older versions of packages to match the one we already have in source control. There is no way to do this from the UI so I use the command line to get the proper version.\nThis happens only in one of our solutions. If I create a new solution or use another one in the same repository, packages will install as expected. I can even install the packages in a dummy solution to fill the local cache and then install them successfully in the faulty solution. I have tried to Invoke-WebRequest on the url from the NuGet Console and I get an HTTP 200.\nWhat can cause this? Is there a way to override the package sources on a per solution/project basis?\nThis may solve your problem :Install-Package Akka.net -Source nuget.org\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How do I find a list of Homebrew's installable packages?", "id": 1555, "answers": [{"answer_id": 1544, "document_id": 1132, "question_id": 1555, "text": "You can use Homebrew Formulae page to see the list of installable packages. https://formulae.brew.sh/formula/", "answer_start": 92, "answer_category": null}], "is_impossible": false}], "context": "Recently I installed Brew. How can I retrieve a list of available brew packages to install?\nYou can use Homebrew Formulae page to see the list of installable packages. https://formulae.brew.sh/formula/\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "installing pwntools on macos", "id": 1527, "answers": [{"answer_id": 1516, "document_id": 1104, "question_id": 1527, "text": "a few weeks now so that seems unlikely.\n\nThanks if anyone can help.\n    \n\ncapstone==3.0.5 still tries to build for both i386 and x86_64, this is already fixed on master and will be released with the next version. Looking at the Makefile, there are two possibilities:\n\n\nTurn off M", "answer_start": 24841, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI am trying to install pwntools on macOS.\n\nWhenever I try pip3 install pwntools, it pauses for a while on Running setup.py bdist_wheel for capstone ... and then prints Failed building wheel for capstone (I assume \u2018capstone\u2019 is some dependency) followed by a long error message. Here is the full output:\n\nCollecting pwntools\nCollecting tox&gt;=1.8.1 (from pwntools)\n  Using cached https://files.pythonhosted.org/packages/8f/c3/64bade66e6188a0dc02689392d5c782a9eb6648b54ddc2db034aa495b6a2/tox-3.5.2-py2.py3-none-any.whl\nRequirement already satisfied: pyelftools&gt;=0.2.4 in /usr/local/lib/python3.7/site-packages (from pwntools) (0.25)\nCollecting capstone&gt;=3.0.5rc2 (from pwntools)\n  Using cached https://files.pythonhosted.org/packages/2b/61/d66abbf007f8cd2643095131c7dcb8186cc42603aea6d5a18cddcbf390a5/capstone-3.0.5.post1.tar.gz\nRequirement already satisfied: python-dateutil in /usr/local/lib/python3.7/site-packages (from pwntools) (2.7.3)\nRequirement already satisfied: psutil&gt;=3.3.0 in /usr/local/lib/python3.7/site-packages (from pwntools) (5.4.7)\nCollecting pyserial&gt;=2.7 (from pwntools)\n  Using cached https://files.pythonhosted.org/packages/0d/e4/2a744dd9e3be04a0c0907414e2a01a7c88bb3915cbe3c8cc06e209f59c30/pyserial-3.4-py2.py3-none-any.whl\nRequirement already satisfied: sortedcontainers&lt;2.0 in /usr/local/lib/python3.7/site-packages (from pwntools) (1.5.10)\nRequirement already satisfied: intervaltree in /usr/local/lib/python3.7/site-packages (from pwntools) (2.1.0)\nCollecting unicorn (from pwntools)\n  Using cached https://files.pythonhosted.org/packages/7d/7f/47fe864fe967e91de2d57677618cffc91bee3918f0a3cdbaa6500b36855e/unicorn-1.0.1.tar.gz\nRequirement already satisfied: mako&gt;=1.0.0 in /usr/local/lib/python3.7/site-packages (from pwntools) (1.0.7)\nRequirement already satisfied: pypandoc in /usr/local/lib/python3.7/site-packages (from pwntools) (1.4)\nRequirement already satisfied: packaging in /usr/local/lib/python3.7/site-packages (from pwntools) (18.0)\nRequirement already satisfied: paramiko&gt;=1.15.2 in /usr/local/lib/python3.7/site-packages (from pwntools) (2.4.2)\nRequirement already satisfied: pip&gt;=6.0.8 in /usr/local/lib/python3.7/site-packages (from pwntools) (18.0)\nCollecting ropgadget&gt;=5.3 (from pwntools)\nRequirement already satisfied: requests&gt;=2.0 in /usr/local/lib/python3.7/site-packages (from pwntools) (2.19.1)\nRequirement already satisfied: pygments&gt;=2.0 in /usr/local/lib/python3.7/site-packages (from pwntools) (2.2.0)\nRequirement already satisfied: pysocks in /usr/local/lib/python3.7/site-packages (from pwntools) (1.6.8)\nRequirement already satisfied: setuptools&gt;=30.0.0 in /usr/local/lib/python3.7/site-packages (from tox&gt;=1.8.1-&gt;pwntools) (40.4.1)\nCollecting filelock&lt;4,&gt;=3.0.0 (from tox&gt;=1.8.1-&gt;pwntools)\n  Using cached https://files.pythonhosted.org/packages/b4/fe/5ca16d167849b980925d3bd706cda266c7435dcf21675c546374da207654/filelock-3.0.9-py3-none-any.whl\nCollecting toml&gt;=0.9.4 (from tox&gt;=1.8.1-&gt;pwntools)\n  Using cached https://files.pythonhosted.org/packages/a2/12/ced7105d2de62fa7c8fb5fce92cc4ce66b57c95fb875e9318dba7f8c5db0/toml-0.10.0-py2.py3-none-any.whl\nCollecting virtualenv&gt;=1.11.2 (from tox&gt;=1.8.1-&gt;pwntools)\n  Using cached https://files.pythonhosted.org/packages/b6/30/96a02b2287098b23b875bc8c2f58071c35d2efe84f747b64d523721dc2b5/virtualenv-16.0.0-py2.py3-none-any.whl\nCollecting pluggy&lt;1,&gt;=0.3.0 (from tox&gt;=1.8.1-&gt;pwntools)\n  Using cached https://files.pythonhosted.org/packages/1c/e7/017c262070af41fe251401cb0d0e1b7c38f656da634cd0c15604f1f30864/pluggy-0.8.0-py2.py3-none-any.whl\nRequirement already satisfied: six&lt;2,&gt;=1.0.0 in /usr/local/lib/python3.7/site-packages (from tox&gt;=1.8.1-&gt;pwntools) (1.11.0)\nCollecting py&lt;2,&gt;=1.4.17 (from tox&gt;=1.8.1-&gt;pwntools)\n  Using cached https://files.pythonhosted.org/packages/3e/c7/3da685ef117d42ac8d71af525208759742dd235f8094221fdaafcd3dba8f/py-1.7.0-py2.py3-none-any.whl\nRequirement already satisfied: MarkupSafe&gt;=0.9.2 in /usr/local/lib/python3.7/site-packages (from mako&gt;=1.0.0-&gt;pwntools) (1.0)\nRequirement already satisfied: wheel&gt;=0.25.0 in /usr/local/lib/python3.7/site-packages (from pypandoc-&gt;pwntools) (0.31.1)\nRequirement already satisfied: pyparsing&gt;=2.0.2 in /usr/local/lib/python3.7/site-packages (from packaging-&gt;pwntools) (2.2.1)\nRequirement already satisfied: pyasn1&gt;=0.1.7 in /usr/local/lib/python3.7/site-packages (from paramiko&gt;=1.15.2-&gt;pwntools) (0.4.4)\nRequirement already satisfied: cryptography&gt;=1.5 in /usr/local/lib/python3.7/site-packages (from paramiko&gt;=1.15.2-&gt;pwntools) (2.3.1)\nRequirement already satisfied: bcrypt&gt;=3.1.3 in /usr/local/lib/python3.7/site-packages (from paramiko&gt;=1.15.2-&gt;pwntools) (3.1.4)\nRequirement already satisfied: pynacl&gt;=1.0.1 in /usr/local/lib/python3.7/site-packages (from paramiko&gt;=1.15.2-&gt;pwntools) (1.3.0)\nRequirement already satisfied: idna&lt;2.8,&gt;=2.5 in /usr/local/lib/python3.7/site-packages (from requests&gt;=2.0-&gt;pwntools) (2.7)\nRequirement already satisfied: urllib3&lt;1.24,&gt;=1.21.1 in /usr/local/lib/python3.7/site-packages (from requests&gt;=2.0-&gt;pwntools) (1.23)\nRequirement already satisfied: chardet&lt;3.1.0,&gt;=3.0.2 in /usr/local/lib/python3.7/site-packages (from requests&gt;=2.0-&gt;pwntools) (3.0.4)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/site-packages (from requests&gt;=2.0-&gt;pwntools) (2018.8.24)\nRequirement already satisfied: cffi!=1.11.3,&gt;=1.7 in /usr/local/lib/python3.7/site-packages (from cryptography&gt;=1.5-&gt;paramiko&gt;=1.15.2-&gt;pwntools) (1.11.5)\nRequirement already satisfied: asn1crypto&gt;=0.21.0 in /usr/local/lib/python3.7/site-packages (from cryptography&gt;=1.5-&gt;paramiko&gt;=1.15.2-&gt;pwntools) (0.24.0)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.7/site-packages (from cffi!=1.11.3,&gt;=1.7-&gt;cryptography&gt;=1.5-&gt;paramiko&gt;=1.15.2-&gt;pwntools) (2.19)\nBuilding wheels for collected packages: capstone, unicorn\n  Running setup.py bdist_wheel for capstone ... error\n  Complete output from command /usr/local/opt/python/bin/python3.7 -u -c \"import setuptools, tokenize;__file__='/private/var/folders/gg/v_3n0tps1zsbm17vmkmshp640000gn/T/pip-install-hfg12cnd/capstone/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" bdist_wheel -d /private/var/folders/gg/v_3n0tps1zsbm17vmkmshp640000gn/T/pip-wheel-ijgw4f7z --python-tag cp37:\n  running bdist_wheel\n  running build\n  Building C extensions\n    CC      cs.o\n    CC      utils.o\n    CC      SStream.o\n    CC      MCInstrDesc.o\n    CC      MCRegisterInfo.o\n    CC      arch/ARM/ARMDisassembler.o\n    CC      arch/ARM/ARMInstPrinter.o\n    CC      arch/ARM/ARMMapping.o\n    CC      arch/ARM/ARMModule.o\n    CC      arch/AArch64/AArch64BaseInfo.o\n    CC      arch/AArch64/AArch64Disassembler.o\n    CC      arch/AArch64/AArch64InstPrinter.o\n    CC      arch/AArch64/AArch64Mapping.o\n    CC      arch/AArch64/AArch64Module.o\n    CC      arch/Mips/MipsDisassembler.o\n    CC      arch/Mips/MipsInstPrinter.o\n    CC      arch/Mips/MipsMapping.o\n    CC      arch/Mips/MipsModule.o\n    CC      arch/PowerPC/PPCDisassembler.o\n    CC      arch/PowerPC/PPCInstPrinter.o\n    CC      arch/PowerPC/PPCMapping.o\n    CC      arch/PowerPC/PPCModule.o\n    CC      arch/Sparc/SparcDisassembler.o\n    CC      arch/Sparc/SparcInstPrinter.o\n    CC      arch/Sparc/SparcMapping.o\n    CC      arch/Sparc/SparcModule.o\n    CC      arch/SystemZ/SystemZDisassembler.o\n    CC      arch/SystemZ/SystemZInstPrinter.o\n    CC      arch/SystemZ/SystemZMapping.o\n    CC      arch/SystemZ/SystemZModule.o\n    CC      arch/SystemZ/SystemZMCTargetDesc.o\n    CC      arch/X86/X86DisassemblerDecoder.o\n    CC      arch/X86/X86Disassembler.o\n    CC      arch/X86/X86IntelInstPrinter.o\n    CC      arch/X86/X86ATTInstPrinter.o\n    CC      arch/X86/X86Mapping.o\n    CC      arch/X86/X86Module.o\n    CC      arch/XCore/XCoreDisassembler.o\n    CC      arch/XCore/XCoreInstPrinter.o\n    CC      arch/XCore/XCoreMapping.o\n    CC      arch/XCore/XCoreModule.o\n    CC      MCInst.o\n    LINK    libcapstone.dylib\n  ld: warning: The i386 architecture is deprecated for macOS (remove from the Xcode build setting: ARCHS)\n  ld: warning: ignoring file /Library/Developer/CommandLineTools/SDKs/MacOSX10.14.sdk/usr/lib/libSystem.tbd, missing required architecture i386 in file /Library/Developer/CommandLineTools/SDKs/MacOSX10.14.sdk/usr/lib/libSystem.tbd\n  Undefined symbols for architecture i386:\n    \"___bzero\", referenced from:\n        _Thumb_getInstruction in ARMDisassembler.o\n        _ARM_getInstruction in ARMDisassembler.o\n        _AArch64_getInstruction in AArch64Disassembler.o\n        _Mips_getInstruction in MipsDisassembler.o\n        _Mips64_getInstruction in MipsDisassembler.o\n        _PPC_getInstruction in PPCDisassembler.o\n        _Sparc_getInstruction in SparcDisassembler.o\n        ...\n    \"___memcpy_chk\", referenced from:\n        _SystemZ_getInstruction in SystemZDisassembler.o\n    \"___stack_chk_fail\", referenced from:\n        _cs_disasm in cs.o\n        _cs_disasm_iter in cs.o\n        _ARM_get_insn_id in ARMMapping.o\n        _A64SysRegMapper_toString in AArch64BaseInfo.o\n        _decodeToMCInst in AArch64Disassembler.o\n        _printInstruction in AArch64InstPrinter.o\n        _printTypedVectorList in AArch64InstPrinter.o\n        ...\n    \"___stack_chk_guard\", referenced from:\n        _cs_disasm in cs.o\n        _cs_disasm_iter in cs.o\n        _ARM_get_insn_id in ARMMapping.o\n        _A64SysRegMapper_toString in AArch64BaseInfo.o\n        _decodeToMCInst in AArch64Disassembler.o\n        _printInstruction in AArch64InstPrinter.o\n        _printTypedVectorList in AArch64InstPrinter.o\n        ...\n        _cs_disasm in cs.o\n        _cs_disasm_iter in cs.o\n        _ARM_get_insn_id in ARMMapping.o\n        _A64SysRegMapper_toString in AArch64BaseInfo.o\n        _decodeToMCInst in AArch64Disassembler.o\n        _printInstruction in AArch64InstPrinter.o\n        _printTypedVectorList in AArch64InstPrinter.o\n        ...\n        _cs_disasm in cs.o\n        _cs_disasm_iter in cs.o\n        _ARM_get_insn_id in ARMMapping.o\n        _A64SysRegMapper_toString in AArch64BaseInfo.o\n        _decodeToMCInst in AArch64Disassembler.o\n        _printInstruction in AArch64InstPrinter.o\n        _printTypedVectorList in AArch64InstPrinter.o\n        ...\n        _cs_disasm in cs.o\n        _cs_disasm_iter in cs.o\n        _ARM_get_insn_id in ARMMapping.o\n        _A64SysRegMapper_toString in AArch64BaseInfo.o\n        _decodeToMCInst in AArch64Disassembler.o\n        _printInstruction in AArch64InstPrinter.o\n        _printTypedVectorList in AArch64InstPrinter.o\n        ...\n        _cs_disasm in cs.o\n        _cs_disasm_iter in cs.o\n        _ARM_get_insn_id in ARMMapping.o\n        _A64SysRegMapper_toString in AArch64BaseInfo.o\n        _decodeToMCInst in AArch64Disassembler.o\n        _printInstruction in AArch64InstPrinter.o\n        _printTypedVectorList in AArch64InstPrinter.o\n        ...\n        _cs_disasm in cs.o\n        _cs_disasm_iter in cs.o\n        _ARM_get_insn_id in ARMMapping.o\n        _A64SysRegMapper_toString in AArch64BaseInfo.o\n        _decodeToMCInst in AArch64Disassembler.o\n        _printInstruction in AArch64InstPrinter.o\n        _printTypedVectorList in AArch64InstPrinter.o\n        ...\n        _cs_disasm in cs.o\n        _cs_disasm_iter in cs.o\n        _ARM_get_insn_id in ARMMapping.o\n        _A64SysRegMapper_toString in AArch64BaseInfo.o\n        _decodeToMCInst in AArch64Disassembler.o\n        _printInstruction in AArch64InstPrinter.o\n        _printTypedVectorList in AArch64InstPrinter.o\n        ...\n        ...\n    \"___strcpy_chk\", referenced from:\n        _XCore_insn_extract in XCoreInstPrinter.o\n    \"___strncpy_chk\", referenced from:\n        _Sparc_printInst in SparcInstPrinter.o\n    \"___tolower\", referenced from:\n        _A64NamedImmMapper_fromString in AArch64BaseInfo.o\n    \"___udivdi3\", referenced from:\n        _A64SysRegMapper_toString in AArch64BaseInfo.o\n    \"_atoi\", referenced from:\n        _XCore_insn_extract in XCoreInstPrinter.o\n    \"_calloc\", referenced from:\n        _cs_mem_calloc in cs.o\n       (maybe you meant: _cs_mem_calloc)\n    \"_free\", referenced from:\n        _cs_mem_free in cs.o\n       (maybe you meant: _cs_free, _cs_mem_free )\n    \"_malloc\", referenced from:\n        _cs_mem_malloc in cs.o\n       (maybe you meant: _cs_malloc, _cs_mem_malloc )\n    \"_memcpy\", referenced from:\n        _cs_disasm in cs.o\n        _fill_insn in cs.o\n        _cs_disasm_iter in cs.o\n        _SStream_concat0 in SStream.o\n    \"_memmove\", referenced from:\n        _cs_strdup in utils.o\n        _X86_Intel_printInst in X86IntelInstPrinter.o\n        _X86_ATT_printInst in X86ATTInstPrinter.o\n    \"_realloc\", referenced from:\n        _cs_mem_realloc in cs.o\n       (maybe you meant: _cs_mem_realloc)\n    \"_strchr\", referenced from:\n        _Sparc_printInst in SparcInstPrinter.o\n        _XCore_insn_extract in XCoreInstPrinter.o\n        _XCore_printInst in XCoreInstPrinter.o\n    \"_strcmp\", referenced from:\n        _name2id in utils.o\n        _A64NamedImmMapper_fromString in AArch64BaseInfo.o\n        _printPredicateOperand in PPCInstPrinter.o\n        _PPC_alias_insn in PPCMapping.o\n        _printMemOperand in SparcInstPrinter.o\n        _Sparc_map_hint in SparcMapping.o\n        _XCore_reg_id in XCoreMapping.o\n        ...\n    \"_strcpy\", referenced from:\n        _A64SysRegMapper_toString in AArch64BaseInfo.o\n    \"_strlen\", referenced from:\n        _cs_strdup in utils.o\n        _SStream_concat0 in SStream.o\n        _ARM_post_printer in ARMInstPrinter.o\n        _PPC_printInst in PPCInstPrinter.o\n        _Sparc_printInst in SparcInstPrinter.o\n        _Sparc_map_hint in SparcMapping.o\n    \"_strncmp\", referenced from:\n        _ARM_post_printer in ARMInstPrinter.o\n    \"_strncpy\", referenced from:\n        _cs_disasm in cs.o\n        _fill_insn in cs.o\n        _cs_disasm_iter in cs.o\n    \"_strrchr\", referenced from:\n        _ARM_post_printer in ARMInstPrinter.o\n        _AArch64_post_printer in AArch64InstPrinter.o\n        _PPC_post_printer in PPCInstPrinter.o\n    \"_strstr\", referenced from:\n        _AArch64_printInst in AArch64InstPrinter.o\n        _PPC_post_printer in PPCInstPrinter.o\n        _printAliasInstr in SparcInstPrinter.o\n    \"_vsnprintf\", referenced from:\n        _cs_vsnprintf in cs.o\n       (maybe you meant: _cs_vsnprintf)\n  ld: symbol(s) not found for architecture i386\n  clang: error: linker command failed with exit code 1 (use -v to see invocation)\n  make: *** [libcapstone.dylib] Error 1\n  error: [Errno 2] No such file or directory: 'libcapstone.dylib'\n\n  ----------------------------------------\n  Failed building wheel for capstone\n  Running setup.py clean for capstone\n  Running setup.py bdist_wheel for unicorn ... error\n  Complete output from command /usr/local/opt/python/bin/python3.7 -u -c \"import setuptools, tokenize;__file__='/private/var/folders/gg/v_3n0tps1zsbm17vmkmshp640000gn/T/pip-install-hfg12cnd/unicorn/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" bdist_wheel -d /private/var/folders/gg/v_3n0tps1zsbm17vmkmshp640000gn/T/pip-wheel-uwmfqniu --python-tag cp37:\n  running bdist_wheel\n  running build\n  Building C extensions\n  cd qemu &amp;&amp; \\\n    ./configure --cc=\"cc\" --extra-cflags=\"-DUNICORN_HAS_X86 -DUNICORN_HAS_ARM -DUNICORN_HAS_ARMEB -DUNICORN_HAS_M68K -DUNICORN_HAS_ARM64 -DUNICORN_HAS_MIPS -DUNICORN_HAS_MIPSEL -DUNICORN_HAS_MIPS64 -DUNICORN_HAS_MIPS64EL -DUNICORN_HAS_SPARC -fPIC -fvisibility=hidden -m32 -arch i386 -m64 -arch x86_64\" --target-list=\"x86_64-softmmu, arm-softmmu, armeb-softmmu, m68k-softmmu, aarch64-softmmu, mips-softmmu, mipsel-softmmu, mips64-softmmu, mips64el-softmmu, sparc-softmmu,sparc64-softmmu,\"\n\n  ERROR: pthread check failed\n         Make sure to have the pthread libs and headers installed.\n\n  make: *** [qemu/config-host.h-timestamp] Error 1\n  error: [Errno 2] No such file or directory: 'libunicorn.dylib'\n\n  ----------------------------------------\n  Failed building wheel for unicorn\n  Running setup.py clean for unicorn\nFailed to build capstone unicorn\nInstalling collected packages: filelock, toml, virtualenv, pluggy, py, tox, capstone, pyserial, unicorn, ropgadget, pwntools\n  Running setup.py install for capstone ... error\n    Complete output from command /usr/local/opt/python/bin/python3.7 -u -c \"import setuptools, tokenize;__file__='/private/var/folders/gg/v_3n0tps1zsbm17vmkmshp640000gn/T/pip-install-hfg12cnd/capstone/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" install --record /private/var/folders/gg/v_3n0tps1zsbm17vmkmshp640000gn/T/pip-record-l5k9b1g9/install-record.txt --single-version-externally-managed --compile:\n    running install\n    running build\n    Building C extensions\n      LINK    libcapstone.dylib\n    ld: warning: The i386 architecture is deprecated for macOS (remove from the Xcode build setting: ARCHS)\n    ld: warning: ignoring file /Library/Developer/CommandLineTools/SDKs/MacOSX10.14.sdk/usr/lib/libSystem.tbd, missing required architecture i386 in file /Library/Developer/CommandLineTools/SDKs/MacOSX10.14.sdk/usr/lib/libSystem.tbd\n    Undefined symbols for architecture i386:\n      \"___bzero\", referenced from:\n          _Thumb_getInstruction in ARMDisassembler.o\n          _ARM_getInstruction in ARMDisassembler.o\n          _AArch64_getInstruction in AArch64Disassembler.o\n          _Mips_getInstruction in MipsDisassembler.o\n          _Mips64_getInstruction in MipsDisassembler.o\n          _PPC_getInstruction in PPCDisassembler.o\n          _Sparc_getInstruction in SparcDisassembler.o\n          ...\n      \"___memcpy_chk\", referenced from:\n          _SystemZ_getInstruction in SystemZDisassembler.o\n      \"___stack_chk_fail\", referenced from:\n          _cs_disasm in cs.o\n          _cs_disasm_iter in cs.o\n          _ARM_get_insn_id in ARMMapping.o\n          _A64SysRegMapper_toString in AArch64BaseInfo.o\n          _decodeToMCInst in AArch64Disassembler.o\n          _printInstruction in AArch64InstPrinter.o\n          _printTypedVectorList in AArch64InstPrinter.o\n          ...\n      \"___stack_chk_guard\", referenced from:\n          _cs_disasm in cs.o\n          _cs_disasm_iter in cs.o\n          _ARM_get_insn_id in ARMMapping.o\n          _A64SysRegMapper_toString in AArch64BaseInfo.o\n          _decodeToMCInst in AArch64Disassembler.o\n          _printInstruction in AArch64InstPrinter.o\n          _printTypedVectorList in AArch64InstPrinter.o\n          ...\n          _cs_disasm in cs.o\n          _cs_disasm_iter in cs.o\n          _ARM_get_insn_id in ARMMapping.o\n          _A64SysRegMapper_toString in AArch64BaseInfo.o\n          _decodeToMCInst in AArch64Disassembler.o\n          _printInstruction in AArch64InstPrinter.o\n          _printTypedVectorList in AArch64InstPrinter.o\n          ...\n          _cs_disasm in cs.o\n          _cs_disasm_iter in cs.o\n          _ARM_get_insn_id in ARMMapping.o\n          _A64SysRegMapper_toString in AArch64BaseInfo.o\n          _decodeToMCInst in AArch64Disassembler.o\n          _printInstruction in AArch64InstPrinter.o\n          _printTypedVectorList in AArch64InstPrinter.o\n          ...\n          _cs_disasm in cs.o\n          _cs_disasm_iter in cs.o\n          _ARM_get_insn_id in ARMMapping.o\n          _A64SysRegMapper_toString in AArch64BaseInfo.o\n          _decodeToMCInst in AArch64Disassembler.o\n          _printInstruction in AArch64InstPrinter.o\n          _printTypedVectorList in AArch64InstPrinter.o\n          ...\n          _cs_disasm in cs.o\n          _cs_disasm_iter in cs.o\n          _ARM_get_insn_id in ARMMapping.o\n          _A64SysRegMapper_toString in AArch64BaseInfo.o\n          _decodeToMCInst in AArch64Disassembler.o\n          _printInstruction in AArch64InstPrinter.o\n          _printTypedVectorList in AArch64InstPrinter.o\n          ...\n          _cs_disasm in cs.o\n          _cs_disasm_iter in cs.o\n          _ARM_get_insn_id in ARMMapping.o\n          _A64SysRegMapper_toString in AArch64BaseInfo.o\n          _decodeToMCInst in AArch64Disassembler.o\n          _printInstruction in AArch64InstPrinter.o\n          _printTypedVectorList in AArch64InstPrinter.o\n          ...\n          _cs_disasm in cs.o\n          _cs_disasm_iter in cs.o\n          _ARM_get_insn_id in ARMMapping.o\n          _A64SysRegMapper_toString in AArch64BaseInfo.o\n          _decodeToMCInst in AArch64Disassembler.o\n          _printInstruction in AArch64InstPrinter.o\n          _printTypedVectorList in AArch64InstPrinter.o\n          ...\n          ...\n      \"___strcpy_chk\", referenced from:\n          _XCore_insn_extract in XCoreInstPrinter.o\n      \"___strncpy_chk\", referenced from:\n          _Sparc_printInst in SparcInstPrinter.o\n      \"___tolower\", referenced from:\n          _A64NamedImmMapper_fromString in AArch64BaseInfo.o\n      \"___udivdi3\", referenced from:\n          _A64SysRegMapper_toString in AArch64BaseInfo.o\n      \"_atoi\", referenced from:\n          _XCore_insn_extract in XCoreInstPrinter.o\n      \"_calloc\", referenced from:\n          _cs_mem_calloc in cs.o\n         (maybe you meant: _cs_mem_calloc)\n      \"_free\", referenced from:\n          _cs_mem_free in cs.o\n         (maybe you meant: _cs_free, _cs_mem_free )\n      \"_malloc\", referenced from:\n          _cs_mem_malloc in cs.o\n         (maybe you meant: _cs_malloc, _cs_mem_malloc )\n      \"_memcpy\", referenced from:\n          _cs_disasm in cs.o\n          _fill_insn in cs.o\n          _cs_disasm_iter in cs.o\n          _SStream_concat0 in SStream.o\n      \"_memmove\", referenced from:\n          _cs_strdup in utils.o\n          _X86_Intel_printInst in X86IntelInstPrinter.o\n          _X86_ATT_printInst in X86ATTInstPrinter.o\n      \"_realloc\", referenced from:\n          _cs_mem_realloc in cs.o\n         (maybe you meant: _cs_mem_realloc)\n      \"_strchr\", referenced from:\n          _Sparc_printInst in SparcInstPrinter.o\n          _XCore_insn_extract in XCoreInstPrinter.o\n          _XCore_printInst in XCoreInstPrinter.o\n      \"_strcmp\", referenced from:\n          _name2id in utils.o\n          _A64NamedImmMapper_fromString in AArch64BaseInfo.o\n          _printPredicateOperand in PPCInstPrinter.o\n          _PPC_alias_insn in PPCMapping.o\n          _printMemOperand in SparcInstPrinter.o\n          _Sparc_map_hint in SparcMapping.o\n          _XCore_reg_id in XCoreMapping.o\n          ...\n      \"_strcpy\", referenced from:\n          _A64SysRegMapper_toString in AArch64BaseInfo.o\n      \"_strlen\", referenced from:\n          _cs_strdup in utils.o\n          _SStream_concat0 in SStream.o\n          _ARM_post_printer in ARMInstPrinter.o\n          _PPC_printInst in PPCInstPrinter.o\n          _Sparc_printInst in SparcInstPrinter.o\n          _Sparc_map_hint in SparcMapping.o\n      \"_strncmp\", referenced from:\n          _ARM_post_printer in ARMInstPrinter.o\n      \"_strncpy\", referenced from:\n          _cs_disasm in cs.o\n          _fill_insn in cs.o\n          _cs_disasm_iter in cs.o\n      \"_strrchr\", referenced from:\n          _ARM_post_printer in ARMInstPrinter.o\n          _AArch64_post_printer in AArch64InstPrinter.o\n          _PPC_post_printer in PPCInstPrinter.o\n      \"_strstr\", referenced from:\n          _AArch64_printInst in AArch64InstPrinter.o\n          _PPC_post_printer in PPCInstPrinter.o\n          _printAliasInstr in SparcInstPrinter.o\n      \"_vsnprintf\", referenced from:\n          _cs_vsnprintf in cs.o\n         (maybe you meant: _cs_vsnprintf)\n    ld: symbol(s) not found for architecture i386\n    clang: error: linker command failed with exit code 1 (use -v to see invocation)\n    make: *** [libcapstone.dylib] Error 1\n    error: [Errno 2] No such file or directory: 'libcapstone.dylib'\n\n    ----------------------------------------\nCommand \"/usr/local/opt/python/bin/python3.7 -u -c \"import setuptools, tokenize;__file__='/private/var/folders/gg/v_3n0tps1zsbm17vmkmshp640000gn/T/pip-install-hfg12cnd/capstone/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" install --record /private/var/folders/gg/v_3n0tps1zsbm17vmkmshp640000gn/T/pip-record-l5k9b1g9/install-record.txt --single-version-externally-managed --compile\" failed with error code 1 in /private/var/folders/gg/v_3n0tps1zsbm17vmkmshp640000gn/T/pip-install-hfg12cnd/capstone/\n\n\nI\u2019ve also tried brew install pwntools, which says that it is already installed, but import pwn in python still fails with a \u201cno module named pwn\u201d error.\nI thought it might be a temporary bug in capstone, but this has not worked for a few weeks now so that seems unlikely.\n\nThanks if anyone can help.\n    \n\ncapstone==3.0.5 still tries to build for both i386 and x86_64, this is already fixed on master and will be released with the next version. Looking at the Makefile, there are two possibilities:\n\n\nTurn off MACOS_UNIVERSAL:\n\n$ MACOS_UNIVERSAL=no pip install capstone\n\nInstall the development version from current master branch, with LIBARCHS already adapted for Mojave:\n\n\n    $ pip install \"git+https://github.com/aquynh/capstone.git#egg=capstone&amp;subdirectory=bindings/python\"\n\n\nMake sure you use quotes in the last command or escape the ampersand (&amp;), otherwise bash will cut the command and run in background instead.\n\nOnce capstone is installed, you will have to deal with unicorn in the same manner and finally should be able to install pwntools. I didn't test it anymore, but the one-liner for the installation will be\n\n$ MACOS_UNIVERSAL=no pip install pwntools\n\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to install Python packages from the tar.gz file without using pip install", "id": 925, "answers": [{"answer_id": 920, "document_id": 576, "question_id": 925, "text": "\nYou basically download the file get-pip.py\n\nand install pip2.7 with\n\npython2.7 get-pip.py\nnow you can run\n\npython2.7 -m pip install https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.5.0-cp27-none-linux_x86_64.whl", "answer_start": 1382, "answer_category": null}], "is_impossible": false}], "context": "Platform not supported for Tensorflow on Ubuntu 14.04.2\nAsked 6 years ago\nActive 6 years ago\nViewed 16k times\n\n5\n\n\n2\nTensorflow is a numerical computation software http://tensorflow.org/get_started/os_setup.md#binary_installation\n\nThe installation instruction says to simply pip install on Ubuntu but I'm getting the following error:\n\n$ sudo -H pip install https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.5.0-cp27-none-linux_x86_64.whl\ntensorflow-0.5.0-cp27-none-linux_x86_64.whl is not a supported wheel on this platform.\nMy Ubuntu version:\n\nalvas@ubi:~$ lsb_release -a\nNo LSB modules are available.\nDistributor ID: Ubuntu\nDescription:    Ubuntu 14.04.2 LTS\nRelease:    14.04\nCodename:   trusty\nMy kernel:\n\n$ uname -a\nLinux ubi 3.13.0-57-generic #95-Ubuntu SMP Fri Jun 19 09:28:15 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux\nAny clues on how to install tensorflow in ubuntu or to resolve the platform problem?\n\nsoftware-installation\npython\nc++\ngoogle\npip\nShare\nImprove this question\nFollow\nedited Nov 10 '15 at 5:52\nasked Nov 9 '15 at 19:50\n\nalvas\n2,60788 gold badges3232 silver badges4545 bronze badges\nWhat does uname -a report? \u2013 \nMarco Ceppi\n\u2666\n Nov 10 '15 at 1:06\nAdd a comment\n1 Answer\n\n4\n\nI had the same problem, because I used the wrong pip version (python 3). I fixed it by installing pip2.7 as described here: http://pip.readthedocs.org/en/stable/installing/\n\nYou basically download the file get-pip.py\n\nand install pip2.7 with\n\npython2.7 get-pip.py\nnow you can run\n\npython2.7 -m pip install https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.5.0-cp27-none-linux_x86_64.whl\nyou need to be a superuser and tensorflow will install.\n\ncheers\n\nShare\nImprove this answer\nFollow\nanswered Nov 10 '15 at 13:26\n\nChristof\n15622 bronze badges\n1\nIndeed, using pip2 works: pip2 install https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.5.0-cp27-none-linux_x86_64.whl \u2013 \nalvas\n Nov 10 '15 at 16:53\nYou alternatively could just change the 0.5.0-cp27 to 0.5.0-cp3 and bam installed for python3 \u2013 \nmjwrazor\n Jun 15 '18 at 15:07\nAdd a comment\nHighly active question. Earn 10 reputation (not counting the association bonus) in order to answer this question. The reputation requirement helps protect this question from spam and non-answer activity.\nNot the answer you're looking for? Browse other questions tagged software-installation python c++ google pip or ask your own question.", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Do you put Babel and Webpack in devDependencies or Dependencies?", "id": 1831, "answers": [{"answer_id": 1817, "document_id": 1402, "question_id": 1831, "text": "The babel and webpack packages will go into the devDependencies section because these packages are used in when transpiling and bundle-ing your code into vanilla javascript in the bundle.js & etc file(s).", "answer_start": 468, "answer_category": null}], "is_impossible": false}], "context": "I'm new to npm and don't really understand what should go into dependencies vs. devDependencies. I know that for testing libraries they should go into dev, but how about for things like babel and webpack? Should they be in dev too, because they're only used to transcompile es6 and JSX into vanilla JS? My understanding is that when you deploy to heroku, it do the transcompiliation with the necessary libraries already, so there's no need to host them on production?\nThe babel and webpack packages will go into the devDependencies section because these packages are used in when transpiling and bundle-ing your code into vanilla javascript in the bundle.js & etc file(s).\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Deploying 2 different heroku apps with same code and git repository", "id": 1068, "answers": [{"answer_id": 1061, "document_id": 646, "question_id": 1068, "text": "You will need to setup different git remote end points for each application at Heroku so you can push to either application from the one local repo.", "answer_start": 256, "answer_category": null}], "is_impossible": false}], "context": "I'm trying to create 2 different Heroku apps using the same code with the same git repository. App1 is created in Heroku by my friend and i'm not a collaborator and app2 is the branch of the same git repository that i'm trying to deploy. Is this possible?\nYou will need to setup different git remote end points for each application at Heroku so you can push to either application from the one local repo. I don't use the 'heroku' name as my remote though (not that it really matters) I use production and staging mapped to different Heroku applications.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "\"Unable to update dependencies of the project\" after committing to Subversion", "id": 1891, "answers": [{"answer_id": 1878, "document_id": 1462, "question_id": 1891, "text": "Maybe you need to VS2010 and then re-open it, this is always worked for me", "answer_start": 196, "answer_category": null}], "is_impossible": false}], "context": "I have a setup project in .NET. When I save the project and the other projects to subversion, the setup project no longer compiles. I get the error \"Unable to update dependencies of the project.\"\nMaybe you need to VS2010 and then re-open it, this is always worked for me\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Scapy installation fails due to invalid token", "id": 1221, "answers": [{"answer_id": 1214, "document_id": 797, "question_id": 1221, "text": "pip3.5 install scapy-python3", "answer_start": 252, "answer_category": null}], "is_impossible": false}], "context": "I have recently taken up learning networks, and I want to install scapy. I have downloaded the latest version (2.2.0), and have two versions of python on my computer- 2.6.1 and 3.3.2. My OS is windows 7 64 bit.\nThe following works for me on Python 3.5\npip3.5 install scapy-python3\n\nAfter extracting scapy and navigating to the correct folder in the terminal, I was instructed to run \"python setup.py install\". I get the following error-\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "how to install package in anaconda", "id": 1395, "answers": [{"answer_id": 1384, "document_id": 967, "question_id": 1395, "text": "pip install https://pypi.python.org/packages/source/m/music21/music21-1.9.3.tar.gz#md5=d271e4a8c60cfa634796fc81d1278eaf\n\n\nor\n\nconda install https://pypi.python.org/packages/source/m/music21/music21-1.9.3.tar.gz#md5=d271e4a8c60cfa634796fc81d1278eaf", "answer_start": 949, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI want to add music package to anaconda interpreter.\nI'm using ubuntu 14.04 64bit.\nI downloaded music21-1.9.3.tar.gz from anaconda cloud. \nI unpacked it to anaconda3/pkgs\n\next                installer.py  music21           PKG-INFO   setup.cfg\ninstaller.command  MANIFEST.in   music21.egg-info  README.md  setup.py\n\n\nI found nothing on the web, or doesn't work. How can I install it? \n    \n\nAre you using Windows? If so, open up a Command Prompt window.\n\nWhat I like to do is Copy the Link Address of the package that I would like to install. In this case, a simple google search lead me to a popular python package site : https://pypi.python.org/pypi/music21/1.9.3\n\nI right-click the tar.gz hyperlink and click \"Copy Link Address\" to get this : https://pypi.python.org/packages/source/m/music21/music21-1.9.3.tar.gz#md5=d271e4a8c60cfa634796fc81d1278eaf\n\nNow to install this, in your command prompt window, type the following :\n\npip install https://pypi.python.org/packages/source/m/music21/music21-1.9.3.tar.gz#md5=d271e4a8c60cfa634796fc81d1278eaf\n\n\nor\n\nconda install https://pypi.python.org/packages/source/m/music21/music21-1.9.3.tar.gz#md5=d271e4a8c60cfa634796fc81d1278eaf\n\n\nAnd it should automatically download the package from that link address, unzip it, and then attempt to install it in your python environment. \n\nIt's good to know how to install python packages manually as well for distributions that don't lend themselves as easily to cross-platform auto-installations. \n\nWhat you would do is unzip the tar.gz file ( or any other compressed package file ) until you have a folder directory with a \"setup.py\" file name. You would go into your command prompt window and \"cd\" into that directory. \n\nThen you would call the Python Executable by typing \"python\" which lets the command prompt know that you are calling python to run your command and finish the line so in total it looks like this : \n\npython setup.py install \n\n\nThere you have it. \n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "installing application on sd card in android sdk 2 2", "id": 1477, "answers": [{"answer_id": 1466, "document_id": 1052, "question_id": 1477, "text": "adb shell pm setInstallLocation 2\n\n\nfrom the android-sdk/tools directory to force the emulator to install to your sd card, whatever its size -- preferably large enough to hold your app(s), obviously.\n\nIf you want to go back to the default installation location (phone app memory), do:\n\nadb shell pm setInstallLocation 0\n\n\nYou can also use these adb commands on your phone / tablet.", "answer_start": 2947, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI am facing this problem and finding solution for this issue since last 2 weeks.\n\nRight now i have developed an android application for the client perpose, whose size is 54 MB, from which 52 MB of only Images/Photos.\n\n[Edit: I need to keep images in \"drawable\" folder ]\n\nSo i want to install it in sd-card on Android SDK 2.2 for that i have already set android:installLocation=\"preferExternal\" in the AndroidManifest.xml file. I have created 256MB sd-card while creating an avd , heap size - 192 , ram size - 192\n\nbut it still showing me an error:\n\n\n  [2010-08-27 17:58:28 - demo_test]\n  Failed to upload demo_test.apk on\n  device 'emulator-5554'\n  [2010-08-27\n  17:58:28 - demo_test]\n  java.io.IOException: Unable to upload\n  file: No space left on device\n  [2010-08-27 17:58:28 - demo_test]\n  Launch canceled!\n\n\nEdit:\n\n\n\n\nIs this memory related issue of internal memory or external memory?\nWhat i have to do to run application and still test with emulator?\nHow do i install application in sd-card in Android sdk 2.2?\n\n    \n\nIs the sd-cards size really 256MB? I sometimes forget the suffix and end up with.. something very small.\n\nYou can also always raise the sizes to like 512, 256,266 and try again to be certain it's something else.\n\nAlso, Logcat output would be nice too.\n\nEdit:  As it seems, you cant just \"install\" the app on the SDcard even if you have 30+Gigabyte free on it. Installation depends also on the internal memory of the phone even in 2.2.\n\nExample: Nexus one has 512mb internal memory. The android os takes the needed ram for the camera, gpu, kernel etc leaving a user with only around 190MB app space (which will be even lower due to apps already installed etc).\n\nHTC Hero on the other hand has only 288mb internal memory, leaving it with a very small \"app size ram\".\n\nDepending on the phone, 90mb app will install to SDcard on nexus one, but won't on Hero due to memory limitations.\n\nThe reason you are getting that error is that, after the android os takes the needed ram out of those 192MB, the \"app size ram\" is not enough to hold that 50+mb application.\n\nI thought that installLocation would install directly to SD, but that is not the case.\n\nVidar Vestnes blog confirmed what I described above by performing a test with different app sizes on his HTC Desire..\n    \n\nA better approach would be to put your resources in their own directory on the SD card. Then your app can load them when it needs.\n    \n\nI suggest you try increasing the internal memory available (screenshot says 43MB), in case the .apk is being copied there first before it is installed to the SD card by the OS.\n    \n\nAssuming you need the images on your device instead of on the cloud, the easiest way would be to make the app connect to a server of yours and download a zip containing all the images on its first run. Until devices get a larger \"app ram size\" this might be your only solution.\n    \n\nYou can use the following:\n\nadb shell pm setInstallLocation 2\n\n\nfrom the android-sdk/tools directory to force the emulator to install to your sd card, whatever its size -- preferably large enough to hold your app(s), obviously.\n\nIf you want to go back to the default installation location (phone app memory), do:\n\nadb shell pm setInstallLocation 0\n\n\nYou can also use these adb commands on your phone / tablet.\n\nOnce you have set up the emulator to install to the sd card, you can then go into Settings --&gt; Applications --&gt; Manage Applications, choose the app you want to be on the sd card, and the \"Move to SD Card\" button will no longer be grayed out.\n    \n\nyou can check if the sd card is really mounted and enough space is available by the running\n\nadb shell df\n\n\ncommand in the android-sdk/tools directory\nit shows you which partitions are currently mounted, how big they are and how much free space is available\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to install Server JRE 8 for Linux Platforms?", "id": 227, "answers": [{"answer_id": 235, "document_id": 119, "question_id": 227, "text": "Download the file. Before the file can be downloaded, you must accept the license agreement. The archive binary can be installed by anyone (not only root users), in any location that you can write to. However, only the root user can install the Server JRE into the system location.\n\nBefore the file can be downloaded, you must accept the license agreement. The archive binary can be installed by anyone (not only root users), in any location that you can write to. However, only the root user can install the Server JRE into the system location.\n\nChange directory to the location where you would like the Server JRE to be installed, then move the .tar.gz archive binary to the current directory.\n\nUnpack the tarball and install the Server JRE.\n\n% tar zxvf server-jre-8uversion-linux-x64.tar.gz\nThe Server JRE files are installed in a directory called jdk1.8.0_version in the current directory.\n\nDelete the .tar.gz file if you want to save disk space.", "answer_start": 1792, "answer_category": null}], "is_impossible": false}, {"question": "I am trying to install server jre into /usr/jre for Linux,but I don't get  necessary permission\uff0c what shall I do?", "id": 228, "answers": [{"answer_id": 236, "document_id": 119, "question_id": 228, "text": "If you do not have root access, simply install the Java SE Server JRE into your home directory, or a subdirectory that you have permission to write to.", "answer_start": 3074, "answer_category": null}], "is_impossible": false}], "context": "Skip to Content\nHome PageJava SoftwareJava SE DownloadsJava SE 8 DocumentationSearch\nJava Platform, Standard Edition Installation Guide\nContents    Previous    Next\n8 Server JRE 8 Installation for Linux Platforms\nThis page describes server JRE for Linux system requirements and gives installation instructions.\n\nThis page has these topics:\n\n\"System Requirements\"\n\n\"Server JRE Installation Instructions\"\n\n\"General Installation Notes\"\n\nSee \"JDK 8 and JRE 8 Installation Start Here\" for general information about installing.\n\nFor information on enhancements to JDK 8 that relate to the installer, see \"Installer Enhancements in JDK 8\"\n\n\nNote:\n\nFor RPM-based Linux distributions, such as Red Hat or SuSE, refer to the RPM-based installation instructions.\nSystem Requirements\nSee http://www.oracle.com/technetwork/java/javase/certconfig-2095354.html for information about supported platforms, operating systems, and browsers.\n\nServer JRE Installation Instructions\nThis topic describes version notation and gives the instructions for installing Server JRE.\n\nInstallation Instruction Notation\nFor instructions containing the notation version, substitute the appropriate Server JRE update version number. For example, if you are installing Server JRE 8 update release 21, the following string representing the name of the bundle:\n\nserver-jre-8uversion-linux-i586.tar.gz\nbecomes:\n\nserver-jre-8u21-linux-i586.tar.gz\nNote that, as in the preceding example, the version number is sometimes preceded with the letter u, for example, 8u21, and sometimes it is preceded with an underbar, for example, jre1.8.0_21.\n\nInstallation\nThis procedure installs the Server JRE for 64-bit Linux, using an archive binary file (.tar.gz).\n\nThese instructions use the following file:\n\nserver-jre-8uversion-linux-x64.tar.gz\nDownload the file. Before the file can be downloaded, you must accept the license agreement. The archive binary can be installed by anyone (not only root users), in any location that you can write to. However, only the root user can install the Server JRE into the system location.\n\nBefore the file can be downloaded, you must accept the license agreement. The archive binary can be installed by anyone (not only root users), in any location that you can write to. However, only the root user can install the Server JRE into the system location.\n\nChange directory to the location where you would like the Server JRE to be installed, then move the .tar.gz archive binary to the current directory.\n\nUnpack the tarball and install the Server JRE.\n\n% tar zxvf server-jre-8uversion-linux-x64.tar.gz\nThe Server JRE files are installed in a directory called jdk1.8.0_version in the current directory.\n\nDelete the .tar.gz file if you want to save disk space.\n\nGeneral Installation Notes\nThis topic gives supplementary installation information.\n\nRoot Access\nInstalling the software creates a directory called jdk1.8.0_version. Note that if you choose to install the Java SE Server JRE into system-wide location such as /usr/jre, you must first become root to gain the necessary permissions. If you do not have root access, simply install the Java SE Server JRE into your home directory, or a subdirectory that you have permission to write to.\n\nOverwriting Files\nIf you install the software in a directory that contains a subdirectory named jdk1.8.0_version, the new software overwrites files of the same name in that jdk1.8.0_version directory. Rename the old directory if it contains files you would like to keep.\n\nSystem Preferences\nBy default, the installation script configures the system such that the backing store for system preferences is created inside the Server JRE's installation directory. If the Server JRE is installed on a network-mounted drive, it and the system preferences can be exported for sharing with Java runtime environments on other machines.\n\nSee http://docs.oracle.com/javase/8/docs/technotes/guides/preferences/index.html for more information about preferences in the Java platform.\n\nContents    Previous    Next\nCopyright \u00a9 1993, 2021, Oracle and/or its affiliates. All rights reserved. | Cookie \u559c\u597d\u8bbe\u7f6e | Ad Choices.Contact Us", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "how to install gcc on windows 7 machine?", "id": 881, "answers": [{"answer_id": 876, "document_id": 560, "question_id": 881, "text": "Download mingw-get and simply issue:\n\nmingw-get install gcc.", "answer_start": 5942, "answer_category": null}], "is_impossible": false}], "context": "I have MinGW on my windows 7 machine. I wish to install and use complete gcc for C compiler. I found there is no single pre-compiled ready-made installation file for this purpose. I checked the following page : http://gcc.gnu.org/install/ It is difficult and I find it above my level of understanding. Could any one please provide me step by step guidance along with links?\n\ngcc\nwindows-7\ninstallation\nShare\nImprove this question\nFollow\nasked Jun 18 '11 at 8:16\n\nKawaiKx\n8,5011616 gold badges6565 silver badges9494 bronze badges\nAdd a comment\n5 Answers\n\n96\n\nEDIT Since not so recently by now, MinGW-w64 has \"absorbed\" one of the toolchain building projects. The downloads can be found here. The installer should work, and allow you to pick a version that you need.\n\nNote the Qt SDK comes with the same toolchain. So if you are developing in Qt and using the SDK, just use the toolchain it comes with.\n\nAnother alternative that has up to date toolchains comes from... harhar... a Microsoft developer, none other than STL (Stephan T. Lavavej, isn't that a spot-on name for the maintainer of MSVC++ Standard Library!). You can find it here. It includes Boost.\n\nAnother option which is highly useful if you care for prebuilt dependencies is MSYS2, which provides a Unix shell (a Cygwin fork modified to work better with Windows pathnames and such), also provides a GCC. It usually lags a bit behind, but that is compensated for by its good package management system and stability. They also provide a functional Clang with libc++ if you care for such thing.\n\nI leave the below for reference, but I strongly suggest against using MinGW.org, due to limitations detailed below. TDM-GCC (the MinGW-w64 version) provides some hacks that you may find useful in your specific situation, although I recommend using vanilla GCC at all times for maximum compatibility.\n\nGCC for Windows is provided by two projects currently. They both provide a very own implementation of the Windows SDK (headers and libraries) which is necessary because GCC does not work with Visual Studio files.\n\nThe older mingw.org, which @Mat already pointed you to. They provide only a 32-bit compiler. See here for the downloads you need:\n\nBinutils is the linker and resource compiler etc.\nGCC is the compiler, and is split in core and language packages\nGDB is the debugger.\nruntime library is required only for mingw.org\nYou might need to download mingw32-make seperately.\nFor support, you can try (don't expect friendly replies) mingw-users@lists.sourceforge.net\nAlternatively, download mingw-get and use that.\n\nThe newer mingw-w64, which as the name predicts, also provides a 64-bit variant, and in the future hopefully some ARM support. I use it and built toolchains with their CRT. Personal and auto builds are found under \"Toolchains targetting Win32/64\" here. They also provide Linux to Windows cross-compilers. I suggest you try a personal build first, they are more complete. Try mine (rubenvb) for GCC 4.6 to 4.8, or use sezero's for GCC 4.4 and 4.5. Both of us provide 32-bit and 64-bit native toolchains. These packages include everything listed above. I currently recommend the \"MinGW-Builds\" builds, as these are currently sanctioned as \"official builds\", and come with an installer (see above).\n\nFor support, send an email to mingw-w64-public@lists.sourceforge.net or post on the forum via sourceforge.net.\n\nBoth projects have their files listed on sourceforge, and all you have to do is either run the installer (in case of mingw.org) or download a suitable zipped package and extract it (in the case of mingw-w64).\n\nThere are a lot of \"non-official\" toolchain builders, one of the most popular is TDM-GCC. They may use patches that break binary compatibility with official/unpatched toolchains, so be careful using them. It's best to use the official releases.\n\nShare\nImprove this answer\nFollow\nedited Jun 26 '18 at 7:15\nanswered Jun 18 '11 at 9:19\n\nrubenvb\n70.8k3232 gold badges175175 silver badges312312 bronze badges\n6\nyour answer looks certainly complete but I don't get all of it. lot of terms in your answer are new to me.. My intention is to get gcc c compiler, linker, header files and library files. I have downloaded your personal build for GCC 4.6. now what do I do next? please tolerate with me :-) \u2013 \nKawaiKx\n Jun 18 '11 at 12:49\n9\n@Saurabh: no problem, I tried to be correct in terms of words. You now extract the package, to for example C:` so that C:\\mingw64\\bin` (or C:\\mingw32\\bin) contains gcc.exe. Then, open a cmd.exe command prompt, and execute set PATH=C:\\mingw64\\bin;%PATH%`. Then you will be able to call mingw32-make, gcc, gdb, etc from that commandline. Programs using it (like cmake, Qt Creator, Codeblocks) also need it in PATH. In case you need a system/user-wide PATH change, go to Control Panel->System->Advanced->Environment variables, and add or modify PATH there. \u2013 \nrubenvb\n Jun 18 '11 at 13:12\n1\ndone that. its working well. thanks a lot.. why there is so much hassle in installing gcc compiler on windows machine? was it not meant for x86 platform originally? \u2013 \nKawaiKx\n Jun 18 '11 at 14:56\n4\nThere are at most 4-5 developers concerned with GCC/binutils for Windows. They do an great job, but leave it to the community to deliver their tools. Don't understand wrong here: TDM has been around a long time and provides a handy installer, but he doesn't submit patches upstream due to copyright issues. Codeblocks delivers a MinGW toolchain with the IDE, so does Qt. But none of them deliver 64-bit :). Building GCC itself requires a Unix environment, like Cygwin/MSYS, or even better: Linux. The build process is not straightforward. But it's not much hassle to install once you know what to do. \u2013 \nrubenvb\n Jun 18 '11 at 15:03\n1\n@rubenvb, I noticed that Mingw64 now has an installer that allows you to pick stuff out. Is this a worse idea than getting a personal build? \u2013 \nbatbrat\n Feb 7 '14 at 6:23\nShow 2 more comments\n\n26\n\nDownload mingw-get and simply issue:\n\nmingw-get install gcc.\nSee the Getting Started page.\n\nShare\nImprove this answer\nFollow\nanswered Jun 18 '11 at 8:20\n\nMat\n192k3939 gold badges376376 silver badges390390 bronze badges\n2\nthis installation does not install gcc standard C library.. instead it uses msvcr***.a. why so? is it because gcc standard C library contains object codes meant for non-x86 chips? \u2013 \nKawaiKx\n Jun 18 '11 at 12:52\n3\nbecause the MinGW port of GCC on windows uses the windows standard C library. If you want to use another C library, look at cygwin (uses newlib). Not sure you'll have much luck getting GLIBC on Windows (I don't really see the point either). \u2013 \nMat\n Jun 18 '11 at 12:56\n1\nwhat do you mean by \"MinGW port of GCC on windows\"exactly? sorry, it may sound stupid but I am new to these terms.. \u2013 \nKawaiKx\n Jun 18 '11 at 14:02\n2\nThat means the version of GCC produced by MinGW to run on and target windows environments. \u2013 \nMat\n Jun 18 '11 at 14:06\nAdd a comment\n\n12\n\nExtract the package to C:\\ from here and install it\n\nCopy the path C:\\MinGW\\bin which contains gcc.exe.\n\ngo to Control Panel->System->Advanced>Environment variables, and add or modify PATH. (just concatenate with ';')\n\nThen, open a cmd.exe command prompt (Windows + R and type cmd, if already opened, please close and open a new one, to get the path change)\n\nchange the folder to your file path by cd D:\\c code Path\n\ntype gcc main.c -o helloworld.o. It will compile the code. for C++ use g++\n\n7 type ./helloworld to run the program.\n\nIf zlib1.dll is missing, download from here\n\nShare\nImprove this answer\nFollow\nanswered Jul 13 '14 at 10:23\n\nVinoj John Hosan\n5,14022 gold badges3535 silver badges3333 bronze badges\nAdd a comment\n\n6\n\nFollowing up on Mat's answer (use Cygwin), here are some detailed instructions for : installing gcc on Windows The packages you want are gcc, gdb and make. Cygwin installer lets you install additional packages if you need them.\n\nShare\nImprove this answer\nFollow\nedited Feb 26 '19 at 20:37\nanswered Oct 11 '11 at 20:04\n\nmichaelok\n1,09411 gold badge1313 silver badges2020 bronze badges\nI installed Cygwin and required packages. I also verified from Windows command line that gcc, gdb and make are accessible. But still installation of rails gem fails with same error! Has anyone tried the Cygwin and Rails on Windows successfully? \u2013 \nVishal Biyani\n Jun 28 '13 at 8:10\nAlso, when I run gcc on command prompt, I get access denied, not sure why \u2013 \nVishal Biyani\n Jun 28 '13 at 8:14\n3\nNowadays, I'd probably go with something like Oracle's VirtualBox (virtualbox.org) or VMWare instead of Cygwin. Then you can run both OSes on the same machine. As for Rails, have you tried Rails Installer? railsinstaller.org \u2013 \nmichaelok\n Jul 11 '13 at 20:08 \nThanks for the tip @michaelok, will try that for sure \u2013 \nVishal Biyani\n Jul 22 '13 at 12:55\nI couldn't find it either, so I updated it to the main Cygwin page. What is tricky about the installer is making sure you are selecting the right packages. From the instructions, which has this to say about gcc: \"By default, setup.exe will install only the packages in the Base category and their dependencies, resulting in a minimal Cygwin installation. However, this will not include many commonly used tools such as gcc (which you will find in the Devel category)\" \u2013 \nmichaelok\n Dec 4 '14 at 21:24\nShow 1 more comment\n\n1\n\nI use msysgit to install gcc on Windows, it has a nice installer which installs most everything that you might need. Most devs will need more than just the compiler, e.g. the shell, shell tools, make, git, svn, etc. msysgit comes with all of that. https://msysgit.github.io/\n\nedit: I am now using msys2. Msys2 uses pacman from Arch Linux to install packages, and includes three environments, for building msys2 apps, 32-bit native apps, and 64-bit native apps. (You probably want to build 32-bit native apps.)\n\nhttps://msys2.github.io/\n\nYou could also go full-monty and install code::blocks or some other gui editor that comes with a compiler. I prefer to use vim and make.", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to best implement software updates on windows?", "id": 1751, "answers": [{"answer_id": 1738, "document_id": 1323, "question_id": 1751, "text": "There's a Windows port of Sparkle may help you, see http://winsparkle.org.", "answer_start": 415, "answer_category": null}], "is_impossible": false}], "context": "I want to implement an \"automatic update\" system for a windows application. Right now I'm semi-manually creating an \"appcast\" which my program checks, and notifies the user that a new version is available. (I'm using NSIS for my installers).\nIs there software that I can use that will handle the \"automatic\" part of the updates, perhaps similar to Sparkle on the mac? Any issues/pitfalls that I should be aware of?\nThere's a Windows port of Sparkle may help you, see http://winsparkle.org.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Maven deploy jar with dependencies to repo", "id": 1098, "answers": [{"answer_id": 1090, "document_id": 675, "question_id": 1098, "text": "You should do the following steps: Save all the dependencies into a lib folder,Build a jar with a classpath slurping up that lib folder,Use the assembly plugin to make another deployable jar.", "answer_start": 216, "answer_category": null}], "is_impossible": false}], "context": "I don't know how to stitch these together to deploy the executable jar to my Maven repo. I don't really know if this is accomplished by a new plugin or by adding a goal or other step to the existing assembly plugin.\nYou should do the following steps: Save all the dependencies into a lib folder,Build a jar with a classpath slurping up that lib folder,Use the assembly plugin to make another deployable jar.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "PackagesNotFoundError: The following packages are not available from current channels:", "id": 1603, "answers": [{"answer_id": 1590, "document_id": 1177, "question_id": 1603, "text": "Have you tried:\npip install <package>\nor\nconda install -c conda-forge <package>", "answer_start": 488, "answer_category": null}], "is_impossible": false}], "context": "I'm somewhat new to Python. I've used it in a bunch of projects, but haven't really needed to stray from its standard setup. I'm trying to install some new packages to get access to functions necessary for a university assignment.\nAnd a bunch of other channels similar to that above.\nI've been searching for a solution, but haven't found anything substantial. I've seen that it may be a problem with Windows, which is what I'm using it on. Past that I haven't a clue of what is going on.\nHave you tried:\npip install <package>\nor\nconda install -c conda-forge <package>\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to list all installed packages and their versions in Python?", "id": 1576, "answers": [{"answer_id": 1565, "document_id": 1153, "question_id": 1576, "text": "If you have pip install and you want to see what packages have been installed with your installer tools you can simply call this:\npip freeze\nIt will also include version numbers for the installed packages.", "answer_start": 268, "answer_category": null}], "is_impossible": false}], "context": "Is there a way in Python to list all installed packages and their versions?\nI know I can go inside python/Lib/site-packages and see what files and directories exist, but I find this very awkward. What I'm looking for something that is similar to npm list i.e. npm-ls.\nIf you have pip install and you want to see what packages have been installed with your installer tools you can simply call this:\npip freeze\nIt will also include version numbers for the installed packages.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "\"Could not run curl-config: [Errno 2] No such file or directory\" when installing pycurl", "id": 1542, "answers": [{"answer_id": 1531, "document_id": 1119, "question_id": 1542, "text": "On Debian I needed the following packages to fix this\nsudo apt install libcurl4-openssl-dev libssl-dev", "answer_start": 196, "answer_category": null}], "is_impossible": false}], "context": "I'm trying to install pycurl via:\nsudo pip install pycurl\nIt downloaded fine, but when when it runs setup.py I get the following traceback: Any idea why this is happening and how to get around it\nOn Debian I needed the following packages to fix this\nsudo apt install libcurl4-openssl-dev libssl-dev\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "unable to create user postgres role postgres does not exists", "id": 1384, "answers": [{"answer_id": 1373, "document_id": 955, "question_id": 1384, "text": "There seems to be some kind of a packaging bug. (details on launchpad).", "answer_start": 1117, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\ni am on ubuntu 12.04 server and i am trying to install postgresql. As of now, i have successfully installed it but unable to configure it. I need to create a role to move ahead and i ran this command in terminal : \n\n\nroot@hostname: createuser -s -r postgres\n\n\nand it said :\n\ncreateuser: could not connect to database postgres: FATAL:  role \"root\" does not exist\n\nFine, so i did :\n\nsu - postgres\n\nand then tried again\n\n\n postgres@hostname: createuser -s -r postgres\n\n\nand i got the error\n\ncreateuser: could not connect to database postgres: FATAL:  role \"postgres\" does not exist\n\nand i get the same error when i do \n\n\n  psql -d dbname\n\n\nIts like a loop, i am unable to create a role postgres because a role postgres does not already exist.\n\nHow do i fix this ?\n\nThe postgres version seems to be 9.1.x and the ubuntu version is 12.10\n    \n\nTurns out i had installed postgres-xc and postgresql on my machine. I had to knock off postgres-xc completely. And it was a little difficult to do that because, there was always an error --purge remove postgres-xc and the uninstallation could not continue.\n\nThere seems to be some kind of a packaging bug. (details on launchpad).\n\nEventually, i ended up doing this to make it work.\n\nAfter that i uninstalled postgresql and installed it back to make it work.\n    \n\nRead postgresql tutorial it doesn't matter if it's Ubuntu or other Linux.\n\nEDIT\nbefore creating role or anything else on fresh install you need to create database cluster: have you created it?\n\ninitdb -D /usr/local/pgsql/data\n\n\nYou need to be logged as user postgres on linux machine. Here is more info.\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Capistrano SSH::AuthenticationFailed, not prompting for password", "id": 464, "answers": [{"answer_id": 473, "document_id": 197, "question_id": 464, "text": "Figured it out! Apparently this issue was with net-ssh gem. I had version 2.8.0 installed recently with some updates to my development environment and was the cause.I'm not sure why it was failing, but gem uninstall net-ssh -v 2.8.0< fixed it for me.If anyone actually knows why this was an issue or how I can correct this issue with the newer version of net-ssh I'd be interested to hear it.", "answer_start": 339, "answer_category": null}], "is_impossible": false}], "context": "I've been using capistrano successfully for a while now and all of a sudden in every project I've lost the ability to deploy. I'm not using rsa_keys or anything I want capistrano to prompt for user and password. Suddenly it has decided not to ask for a password, but does ask for user. Then it rolls back and gives me the following error. Figured it out! Apparently this issue was with net-ssh gem. I had version 2.8.0 installed recently with some updates to my development environment and was the cause.I'm not sure why it was failing, but gem uninstall net-ssh -v 2.8.0< fixed it for me.If anyone actually knows why this was an issue or how I can correct this issue with the newer version of net-ssh I'd be interested to hear it.", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to install Laravel 4 to a web host subfolder without publicly exposing /app/ folder?", "id": 696, "answers": [{"answer_id": 700, "document_id": 388, "question_id": 696, "text": "1.\tPut all your Laravel stuff in the top-level folder (so that the app folder is next to the public_html folder)\n2.\tMove all files from public to public_html (be sure to also move hidden files, such as .htaccess!)\n3.\tDelete the now empty public folder\n4.\tIn bootstrap/path.php change\n'public' => __DIR__.'/../public', to\n'public' => __DIR__.'/../public_html',\n5.\tDone.", "answer_start": 661, "answer_category": null}], "is_impossible": false}], "context": "I was wondering if any of you know of a way to install Laravel 4 in a web host SUBDIRECTORY / subfolder while not exposing the /app/ folder and other sensible files to the publicly accesible part of the host.\nThe idea is, I'd be able to access http://mydomain.com/mylaravel/ to be able to use Laravel, but at the same time I want to avoid anyone doing something like going to http://mydomain.com/app/ or http://mydomain.com/mylaravel/app/ and basically being able to see my config files and other code. People, people! No need to over-complicate things!\n(For other public folders, such as htdocs or www just replace public_html in the following with your one.)\n1.\tPut all your Laravel stuff in the top-level folder (so that the app folder is next to the public_html folder)\n2.\tMove all files from public to public_html (be sure to also move hidden files, such as .htaccess!)\n3.\tDelete the now empty public folder\n4.\tIn bootstrap/path.php change\n'public' => __DIR__.'/../public', to\n'public' => __DIR__.'/../public_html',\n5.\tDone.\nThat's what I do.. bullet proof, right? If you don't want app/ to be visible inside your public directory, don't put it inside your public directory!\n\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How do I tell CPAN to install all dependencies?", "id": 1853, "answers": [{"answer_id": 1839, "document_id": 1424, "question_id": 1853, "text": "Try setting PERL_MM_USE_DEFAULT like so:\nPERL_MM_USE_DEFAULT=1 perl -MCPAN -e 'install My::Module'", "answer_start": 345, "answer_category": null}], "is_impossible": false}], "context": "I still had to answer \"y\" a couple of times (but fewer than before it feels like).\nIs there a way to get it to always go ahead and install? I want to make it unattended.\nIt would seem that I want a flag to always trust CPAN to do the right thing, and if it suggests an answer I would like to follow it (always hit Enter when it asks something).\nTry setting PERL_MM_USE_DEFAULT like so:\nPERL_MM_USE_DEFAULT=1 perl -MCPAN -e 'install My::Module'\nIt should make CPAN answer the default to all prompts.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "how to use numpy with openblas instead of atlas in ubuntu", "id": 1435, "answers": [{"answer_id": 1424, "document_id": 1008, "question_id": 1435, "text": "Run sudo update-alternatives --all and set liblapack.so.3gf to /usr/lib/lapack/liblapack.so.3gf", "answer_start": 731, "answer_category": null}], "is_impossible": false}, {"question": "how to use change Blas version ubuntu", "id": 1436, "answers": [{"answer_id": 1425, "document_id": 1008, "question_id": 1436, "text": ", do to following:\n\n\nsudo update-alternatives --config libblas.so\nsudo update-alternatives -", "answer_start": 1585, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI have looked for an easy way to install/compile Numpy with OpenBLAS but didn't find an easy answer. All the documentation I have seen takes too much knowledge as granted for someone like me who is not used to compile software.\nThere are two packages in Ubuntu related to OpenBLAS : libopenblas-base and libopenblas-dev.\nOnce they are installed, what should I do to install Numpy again with them?\nThanks!\n\nNote that when these OpenBLAS packages are installed, Numpy doesn't work anymore: it can't be imported: ImportError: /usr/lib/liblapack.so.3gf: undefined symbol: ATL_chemv.\nThe problem occurs as well when installing Theano with their website instructions for Ubuntu.\n\nThis was noticed here already.\n    \n\nRun sudo update-alternatives --all and set liblapack.so.3gf to /usr/lib/lapack/liblapack.so.3gf\n    \n\nTo add to the accepted answer (of using update-alternatives), the reason for this is because OpenBlas is not compatible with the Atlas version of Lapack. For each of the Blas and Lapack versions:\n\n\nDefault Blas + Default Lapack =&gt; OK\nOpenBlas + Default Lapack =&gt; OK\nAtlas-Blas + Default Lapack =&gt; OK\nAtlas-Blas + Atlas-Lapack =&gt; OK\nOpenBlas + Atlas-Lapack =&gt; ERROR! (The following case here.)\n\n\nThis is from both personal experience (with the exact same issue) and realizing why such a combination wasn't mentioned in this comparison blog.\n\nBy the way, you can just find the necessary files in /etc/alternatives/, usually with a filename starting with lib*. For each one do sudo update-alternatives --config &lt;filename&gt;. For example, do to following:\n\n\nsudo update-alternatives --config libblas.so\nsudo update-alternatives --config libblas.so.3 \n\n\nto change the Blas version.\n    \n\nConsider using EasyBuild (http://hpcugent.github.io/easybuild/), an open-source framework for building and installing software.\n\nIt allows you to (very easily) build and install (scientific) software with various compiler, and using different BLAS libraries (ATLAS, OpenBLAS, ACML, Intel MKL, ...).\n\nOnce you install EasyBuild (pro tip: use the bootstrapping procedure described at https://github.com/hpcugent/easybuild/wiki/Bootstrapping-EasyBuild), it boils down to running a single command, something like:\n\neb numpy-1.6.2-goolf-1.4.10-Python-2.7.3.eb -ldr\n\nThis will first build and install of full compiler toolchain (goolf: GCC+OpenBLAS+OpenMPI+LAPACK+FFTW), and subsequently build Python and numpy with that toolchain. And all that while you're getting lunch. ;-)\n\nDisclaimer: I'm one of the EasyBuild developers.\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "phpunit require_once() error", "id": 749, "answers": [{"answer_id": 750, "document_id": 437, "question_id": 749, "text": "sudo pear channel-discover pear.symfony-project.com\nsudo pear channel-discover components.ez.no\nsudo pear install --alldeps phpunit/PHPUnit", "answer_start": 422, "answer_category": null}], "is_impossible": false}], "context": " recently installed phpunit on my server via the pear installer. After doing some searching, I tried making some modifications to the include_path in my php.ini file on the server. But that hasn't done a thing. I had this problem on OS X. Fixed it with the following commands which force all the php dependencies to be reinstalled, which included a couple of packages from other channels that were not already configured:\nsudo pear channel-discover pear.symfony-project.com\nsudo pear channel-discover components.ez.no\nsudo pear install --alldeps phpunit/PHPUnit\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Multi-project test dependencies with gradle", "id": 1881, "answers": [{"answer_id": 1867, "document_id": 1452, "question_id": 1881, "text": "In Project B, you just need to add a testCompile dependency:\ndependencies {\n  ...\n  testCompile project(':A').sourceSets.test.output\n}", "answer_start": 166, "answer_category": null}], "is_impossible": false}], "context": "I have a multi-project configuration and I want to use gradle.\nThe task compileJava work great but the compileTestJava does not compile the test file from Project A.\nIn Project B, you just need to add a testCompile dependency:\ndependencies {\n  ...\n  testCompile project(':A').sourceSets.test.output\n}\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "adobe creative suit 5 cs5 5 installation on mac os sierra", "id": 1905, "answers": [{"answer_id": 1892, "document_id": 1476, "question_id": 1905, "text": "Get the correct path (***) to the installation disk image and run this line in the Terminal:\n\nxattr -c /Users/***/Downloads/DesignStandard_CS5_5_LS1.dmg", "answer_start": 1710, "answer_category": null}], "is_impossible": false}], "context": "\n\n                    \n\n            \n\n        \n\n            \n\n                \n\n                    \n\n                        Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.\n\n                        \n\n                    \n\n                \n\n            \n\n        \n\n            \n\n        \n\n                \n\n                    \n\n                \n\n            \n\n                \n\n                    Want to improve this question? Update the question so it's on-topic for Stack Overflow.\n\n                \n\n                    Closed 4 years ago.\n\n\n\n            \n\n        \n\n            \n\n                    \n\n                        Improve this question\n\n                    \n\n            \n\n\n\n\n\n    \n\n\n\nI got a new MacBook Pro running Mac OS Sierra and tried to install Adobe Creative Suite CS5.5 on it, which I already had running on my old MacBook Pro. I downloaded the package directly from the Adobe web page, but when running the Install.app from the disk image I get the error message: \n\n\u201cInstall.app\u201d can\u2019t be opened. You should eject the disk image.\n\n\n\nWhen hitting the help button from that error message I got this explanation:\n\n\n\nI searched the web and ran into suggestions to disable the Gate Keeper functionality from the Terminal, to be allowed to install software from unidentified developers, but this did not work for me.\n    \n\nI had to search for it for quite a while, but finally I found a solution to this problem, which I would like to share here as well.\n\nI found it in this thread: https://forums.adobe.com/thread/1098181 from user \"ralfellis\".\n\nBefore running the installation, all attributes of the disk image need to be removed. Get the correct path (***) to the installation disk image and run this line in the Terminal:\n\nxattr -c /Users/***/Downloads/DesignStandard_CS5_5_LS1.dmg\n\n\nThis did the trick for me and thereafter I could run Install.app without getting any further error messages from the system.\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to deploy to a single specific server using Capistrano", "id": 1706, "answers": [{"answer_id": 1694, "document_id": 1279, "question_id": 1706, "text": "cap HOSTS=app2.example.com ROLE=app deploy\nIf you want to deploy to >1 server with the same role:\ncap HOSTS=app2.example.com,app3.example.com,app4.example.com ROLE=app deploy", "answer_start": 759, "answer_category": null}], "is_impossible": false}], "context": "I have a system in production that has several servers in several roles. I would like to test a new app server by deploying to that specific server, without having to redeploy to every server in production. Is there a way to ask Capistrano to deploy to a specific server? Ideally I'd like to be able to run something like\ncap SERVER=app2.example.com ROLE=app production deploy\nif I just wanted to deploy to app2.example.com.\nThanks!\n[update] I tried the solution suggested by wulong by executing:\ncap HOSTS=app2.server.hostname ROLE=app qa deploy \nbut capistrano seemed be trying to execute tasks for other roles on that server in addition to app tasks. Maybe I need to update my version of cap (I'm running v2.2.0)?\nThe following should work out of the box:\ncap HOSTS=app2.example.com ROLE=app deploy\nIf you want to deploy to >1 server with the same role:\ncap HOSTS=app2.example.com,app3.example.com,app4.example.com ROLE=app deploy\n\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to check k8s deploy history?", "id": 1307, "answers": [{"answer_id": 1298, "document_id": 877, "question_id": 1307, "text": "You should use --record while creating the deployment so that it will start recroding the deployment into the ReplicaSet.\n$ kubectl create -f deploy.yaml --record=true", "answer_start": 181, "answer_category": null}], "is_impossible": false}], "context": "I tried kubectl rollout history deployment/my-app, it returns only No rollout history found.\nI think there exists a method to get all the deploy histories. It will be very helpful.\nYou should use --record while creating the deployment so that it will start recroding the deployment into the ReplicaSet.\n$ kubectl create -f deploy.yaml --record=true\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to install and start a Windows Service using WiX", "id": 784, "answers": [{"answer_id": 780, "document_id": 467, "question_id": 784, "text": "I removed service for both install and uninstall\n<ServiceControl Remove=\"both\" />", "answer_start": 477, "answer_category": null}], "is_impossible": false}], "context": "I tried to use the codes below in Wix.\nBut when installing, the installer was freezing for like 3 minutes on status: Starting services, then I got this message \"Service Jobservice failed to start. Verify that you have sufficient privileges to start system services\". Is there any wrong in my codes? And can I ask the user to input the windows system user name and password during the installation to get the \"privileges\"?\nThanks a lot! For me, it helped for at least one time, I removed service for both install and uninstall\n<ServiceControl Remove=\"both\" />\nI assume this removed something from Regedit\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to run .bat file on server after web deploy publish", "id": 1334, "answers": [{"answer_id": 1324, "document_id": 903, "question_id": 1334, "text": "You could create a Windows Service that runs on your IIS box and uses the FileSystemWatcher to monitor changes to your web root path and when it detects a file change run a batch file.", "answer_start": 162, "answer_category": null}], "is_impossible": false}], "context": "I am using Web Deploy for publishing an ASP.NET MVCwebsite in Visual Studio 2010.\nIs there any way to run a .bat file on server after this publish automatically?\nYou could create a Windows Service that runs on your IIS box and uses the FileSystemWatcher to monitor changes to your web root path and when it detects a file change run a batch file.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "OpenShift rhc setup using multiple accounts", "id": 668, "answers": [{"answer_id": 673, "document_id": 361, "question_id": 668, "text": "The command line also supports --conf - where you pass a file. You can always alias the command via a shell script.", "answer_start": 180, "answer_category": null}], "is_impossible": false}], "context": "I have two accounts on Openshift platform. How can I setup my computer so that I can manage both of them with rhc? I cannot find any relevant option in the command line arguments. The command line also supports --conf - where you pass a file. You can always alias the command via a shell script. Quite old question, but I use yet another solution which seems to be more comfortable (at least for me) - the environment variable OPENSHIFT_CONFIG.", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "error when installing apache 2 4 on windows not logged in as administrator", "id": 1369, "answers": [{"answer_id": 1358, "document_id": 938, "question_id": 1369, "text": "sc.exe create apache2.4 start= auto obj= \"&lt;username&gt;\" password= \"&lt;password&gt;\" DisplayName= \"Apache 2.4\" depend= \"Tcpip/Afd\" binpath= \"\\\"c:\\wamp\\bin\\apache\\apache2.4.33\\bin\\httpd.exe\\\" -k", "answer_start": 1101, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI am trying to setup Apache 2.4 on my windows vista following this guide. But I am getting an eror when I run the comman httpd -k install\n\n// Error\n(OS 5)Access is denied.  : AH00369: Failed to open the WinNT service manager, pe\nrhaps you forgot to log in as Administrator?\n\nI don't login as administrator set when I swith on my computer. And when I check  Control Panel, Administrative Tools, then Services Apache is not on the list.\n\nCan anybody help with this?\n    \n\nFind cmd.exe and right click to select run as administrator \n\n\n\n    \n\nYou have to start the command line in Administrator mode.\n\nOpening Command Line window in Administrator mode\n\nNote: Snap is from Windows 8.\n    \n\nOn Windows server 2008, for some reason the command httpd -k install did not work (AH00369) even though I was in an Administrator command prompt.\n\nHowever I was able to install the service using the sc command (references https://support.microsoft.com/en-us/help/251192/how-to-create-a-windows-service-by-using-sc-exe and When creating a service with sc.exe how to pass in context parameters?)\n\nsc.exe create apache2.4 start= auto obj= \"&lt;username&gt;\" password= \"&lt;password&gt;\" DisplayName= \"Apache 2.4\" depend= \"Tcpip/Afd\" binpath= \"\\\"c:\\wamp\\bin\\apache\\apache2.4.33\\bin\\httpd.exe\\\" -k runservice\"\n\n\nusername and password are necessary if you access network drives or UNC pathnames from apache\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Best way to deploy Visual Studio application that can run without installing", "id": 332, "answers": [{"answer_id": 340, "document_id": 144, "question_id": 332, "text": "It is possible and is deceptively easy:\"Publish\" the application (to, say, some folder on drive C), either from menu Build or from the project's properties \u2192 Publish. This will create an installer for a ClickOnce application.But instead of using the produced installer, find the produced files (the EXE file and the .config, .manifest, and .application files, along with any DLL files, etc.) - they are all in the same folder and typically in the bin\\Debug folder below the project file (.csproj).\nZip that folder (leave out any *.vhost.* files and the app.publish folder (they are not needed), and the .pdb files unless you foresee debugging directly on your user's system (for example, by remote control)), and provide it to the users.", "answer_start": 594, "answer_category": null}], "is_impossible": false}], "context": "I wrote a fairly simple application with C#/.NET and can't figure out a good way to publish it. It's a sort of a \"tool\" that users would only run once, or run every few months. Because of this, I'm hoping that there is a way I could deploy it where it wouldn't need installing to run (it could just be run by double-clicking an EXE file straight after downloading).\nHowever, it still needs (somehow) to include the correct version of .NET, libraries, etc. so it will run correctly. I know this is included when using ClickOnce, but that still installs the application onto the user's computer.\nIt is possible and is deceptively easy:\"Publish\" the application (to, say, some folder on drive C), either from menu Build or from the project's properties \u2192 Publish. This will create an installer for a ClickOnce application.But instead of using the produced installer, find the produced files (the EXE file and the .config, .manifest, and .application files, along with any DLL files, etc.) - they are all in the same folder and typically in the bin\\Debug folder below the project file (.csproj).\nZip that folder (leave out any *.vhost.* files and the app.publish folder (they are not needed), and the .pdb files unless you foresee debugging directly on your user's system (for example, by remote control)), and provide it to the users.\n\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Is it possible to answer dialog questions when installing under docker?", "id": 94, "answers": [{"answer_id": 102, "document_id": 74, "question_id": 94, "text": "_FRONTEND=noninteractive changes the behav", "answer_start": 32361, "answer_category": null}], "is_impossible": false}], "context": "Dockerfile reference\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSearch\n\n\n\n\n\n\n\nToggle navigation\n\n\n\n\n\n\n\nHomeGuidesProduct manualsReferenceSamples\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDockerfile referenceEstimated reading time: 81 minutes\nDocker can build images automatically by reading the instructions from a\nDockerfile. A Dockerfile is a text document that contains all the commands a\nuser could call on the command line to assemble an image. Using docker build\nusers can create an automated build that executes several command-line\ninstructions in succession.\nThis page describes the commands you can use in a Dockerfile. When you are\ndone reading this page, refer to the Dockerfile Best\nPractices for a tip-oriented guide.\nUsage\ud83d\udd17\nThe docker build command builds an image from\na Dockerfile and a context. The build\u2019s context is the set of files at a\nspecified location PATH or URL. The PATH is a directory on your local\nfilesystem. The URL is a Git repository location.\nA context is processed recursively. So, a PATH includes any subdirectories and\nthe URL includes the repository and its submodules. This example shows a\nbuild command that uses the current directory as context:\n$ docker build .\n\nSending build context to Docker daemon  6.51 MB\n...\n\nThe build is run by the Docker daemon, not by the CLI. The first thing a build\nprocess does is send the entire context (recursively) to the daemon.  In most\ncases, it\u2019s best to start with an empty directory as context and keep your\nDockerfile in that directory. Add only the files needed for building the\nDockerfile.\n\nWarning\nDo not use your root directory, /, as the PATH as it causes the build to\ntransfer the entire contents of your hard drive to the Docker daemon.\n\nTo use a file in the build context, the Dockerfile refers to the file specified\nin an instruction, for example,  a COPY instruction. To increase the build\u2019s\nperformance, exclude files and directories by adding a .dockerignore file to\nthe context directory.  For information about how to create a .dockerignore\nfile see the documentation on this page.\nTraditionally, the Dockerfile is called Dockerfile and located in the root\nof the context. You use the -f flag with docker build to point to a Dockerfile\nanywhere in your file system.\n$ docker build -f /path/to/a/Dockerfile .\n\nYou can specify a repository and tag at which to save the new image if\nthe build succeeds:\n$ docker build -t shykes/myapp .\n\nTo tag the image into multiple repositories after the build,\nadd multiple -t parameters when you run the build command:\n$ docker build -t shykes/myapp:1.0.2 -t shykes/myapp:latest .\n\nBefore the Docker daemon runs the instructions in the Dockerfile, it performs\na preliminary validation of the Dockerfile and returns an error if the syntax is incorrect:\n$ docker build -t test/myapp .\n\nSending build context to Docker daemon 2.048 kB\nError response from daemon: Unknown instruction: RUNCMD\n\nThe Docker daemon runs the instructions in the Dockerfile one-by-one,\ncommitting the result of each instruction\nto a new image if necessary, before finally outputting the ID of your\nnew image. The Docker daemon will automatically clean up the context you\nsent.\nNote that each instruction is run independently, and causes a new image\nto be created - so RUN cd /tmp will not have any effect on the next\ninstructions.\nWhenever possible, Docker will re-use the intermediate images (cache),\nto accelerate the docker build process significantly. This is indicated by\nthe Using cache message in the console output.\n(For more information, see the Dockerfile best practices guide:\n$ docker build -t svendowideit/ambassador .\n\nSending build context to Docker daemon 15.36 kB\nStep 1/4 : FROM alpine:3.2\n---> 31f630c65071\nStep 2/4 : MAINTAINER SvenDowideit@home.org.au\n---> Using cache\n---> 2a1c91448f5f\nStep 3/4 : RUN apk update &&      apk add socat &&        rm -r /var/cache/\n---> Using cache\n---> 21ed6e7fbb73\nStep 4/4 : CMD env | grep _TCP= | (sed 's/.*_PORT_\\([0-9]*\\)_TCP=tcp:\\/\\/\\(.*\\):\\(.*\\)/socat -t 100000000 TCP4-LISTEN:\\1,fork,reuseaddr TCP4:\\2:\\3 \\&/' && echo wait) | sh\n---> Using cache\n---> 7ea8aef582cc\nSuccessfully built 7ea8aef582cc\n\nBuild cache is only used from images that have a local parent chain. This means\nthat these images were created by previous builds or the whole chain of images\nwas loaded with docker load. If you wish to use build cache of a specific\nimage you can specify it with --cache-from option. Images specified with\n--cache-from do not need to have a parent chain and may be pulled from other\nregistries.\nWhen you\u2019re done with your build, you\u2019re ready to look into Pushing a\nrepository to its registry.\nBuildKit\ud83d\udd17\nStarting with version 18.09, Docker supports a new backend for executing your\nbuilds that is provided by the moby/buildkit\nproject. The BuildKit backend provides many benefits compared to the old\nimplementation. For example, BuildKit can:\n\nDetect and skip executing unused build stages\nParallelize building independent build stages\nIncrementally transfer only the changed files in your build context between builds\nDetect and skip transferring unused files in your build context\nUse external Dockerfile implementations with many new features\nAvoid side-effects with rest of the API (intermediate images and containers)\nPrioritize your build cache for automatic pruning\n\nTo use the BuildKit backend, you need to set an environment variable\nDOCKER_BUILDKIT=1 on the CLI before invoking docker build.\nTo learn about the experimental Dockerfile syntax available to BuildKit-based\nbuilds refer to the documentation in the BuildKit repository.\nFormat\ud83d\udd17\nHere is the format of the Dockerfile:\n# Comment\nINSTRUCTION arguments\n\nThe instruction is not case-sensitive. However, convention is for them to\nbe UPPERCASE to distinguish them from arguments more easily.\nDocker runs instructions in a Dockerfile in order. A Dockerfile must\nbegin with a FROM instruction. This may be after parser\ndirectives, comments, and globally scoped\nARGs. The FROM instruction specifies the Parent\nImage from which you are\nbuilding. FROM may only be preceded by one or more ARG instructions, which\ndeclare arguments that are used in FROM lines in the Dockerfile.\nDocker treats lines that begin with # as a comment, unless the line is\na valid parser directive. A # marker anywhere\nelse in a line is treated as an argument. This allows statements like:\n# Comment\nRUN echo 'we are running some # of cool things'\n\nComment lines are removed before the Dockerfile instructions are executed, which\nmeans that the comment in the following example is not handled by the shell\nexecuting the echo command, and both examples below are equivalent:\nRUN echo hello \\\n# comment\nworld\n\nRUN echo hello \\\nworld\n\nLine continuation characters are not supported in comments.\n\nNote on whitespace\nFor backward compatibility, leading whitespace before comments (#) and\ninstructions (such as RUN) are ignored, but discouraged. Leading whitespace\nis not preserved in these cases, and the following examples are therefore\nequivalent:\n# this is a comment-line\nRUN echo hello\nRUN echo world\n\n# this is a comment-line\nRUN echo hello\nRUN echo world\n\nNote however, that whitespace in instruction arguments, such as the commands\nfollowing RUN, are preserved, so the following example prints `    hello    world`\nwith leading whitespace as specified:\nRUN echo \"\\\nhello\\\nworld\"\n\n\nParser directives\ud83d\udd17\nParser directives are optional, and affect the way in which subsequent lines\nin a Dockerfile are handled. Parser directives do not add layers to the build,\nand will not be shown as a build step. Parser directives are written as a\nspecial type of comment in the form # directive=value. A single directive\nmay only be used once.\nOnce a comment, empty line or builder instruction has been processed, Docker\nno longer looks for parser directives. Instead it treats anything formatted\nas a parser directive as a comment and does not attempt to validate if it might\nbe a parser directive. Therefore, all parser directives must be at the very\ntop of a Dockerfile.\nParser directives are not case-sensitive. However, convention is for them to\nbe lowercase. Convention is also to include a blank line following any\nparser directives. Line continuation characters are not supported in parser\ndirectives.\nDue to these rules, the following examples are all invalid:\nInvalid due to line continuation:\n# direc \\\ntive=value\n\nInvalid due to appearing twice:\n# directive=value1\n# directive=value2\n\nFROM ImageName\n\nTreated as a comment due to appearing after a builder instruction:\nFROM ImageName\n# directive=value\n\nTreated as a comment due to appearing after a comment which is not a parser\ndirective:\n# About my dockerfile\n# directive=value\nFROM ImageName\n\nThe unknown directive is treated as a comment due to not being recognized. In\naddition, the known directive is treated as a comment due to appearing after\na comment which is not a parser directive.\n# unknowndirective=value\n# knowndirective=value\n\nNon line-breaking whitespace is permitted in a parser directive. Hence, the\nfollowing lines are all treated identically:\n#directive=value\n# directive =value\n#\tdirective= value\n# directive = value\n#\t  dIrEcTiVe=value\n\nThe following parser directives are supported:\n\nsyntax\nescape\n\nsyntax\ud83d\udd17\n# syntax=[remote image reference]\n\nFor example:\n# syntax=docker/dockerfile\n# syntax=docker/dockerfile:1.0\n# syntax=docker.io/docker/dockerfile:1\n# syntax=docker/dockerfile:1.0.0-experimental\n# syntax=example.com/user/repo:tag@sha256:abcdef...\n\nThis feature is only enabled if the BuildKit backend is used.\nThe syntax directive defines the location of the Dockerfile builder that is used for\nbuilding the current Dockerfile. The BuildKit backend allows to seamlessly use\nexternal implementations of builders that are distributed as Docker images and\nexecute inside a container sandbox environment.\nCustom Dockerfile implementation allows you to:\n\nAutomatically get bugfixes without updating the daemon\nMake sure all users are using the same implementation to build your Dockerfile\nUse the latest features without updating the daemon\nTry out new experimental or third-party features\n\nOfficial releases\ud83d\udd17\nDocker distributes official versions of the images that can be used for building\nDockerfiles under docker/dockerfile repository on Docker Hub. There are two\nchannels where new images are released: stable and experimental.\nStable channel follows semantic versioning. For example:\n\ndocker/dockerfile:1.0.0 - only allow immutable version 1.0.0\ndocker/dockerfile:1.0 - allow versions 1.0.*\ndocker/dockerfile:1 - allow versions 1.*.*\ndocker/dockerfile:latest - latest release on stable channel\n\nThe experimental channel uses incremental versioning with the major and minor\ncomponent from the stable channel on the time of the release. For example:\n\ndocker/dockerfile:1.0.1-experimental - only allow immutable version 1.0.1-experimental\ndocker/dockerfile:1.0-experimental - latest experimental releases after 1.0\ndocker/dockerfile:experimental - latest release on experimental channel\n\nYou should choose a channel that best fits your needs. If you only want\nbugfixes, you should use docker/dockerfile:1.0. If you want to benefit from\nexperimental features, you should use the experimental channel. If you are using\nthe experimental channel, newer releases may not be backwards compatible, so it\nis recommended to use an immutable full version variant.\nFor master builds and nightly feature releases refer to the description in\nthe source repository.\nescape\ud83d\udd17\n# escape=\\ (backslash)\n\nOr\n# escape=` (backtick)\n\nThe escape directive sets the character used to escape characters in a\nDockerfile. If not specified, the default escape character is \\.\nThe escape character is used both to escape characters in a line, and to\nescape a newline. This allows a Dockerfile instruction to\nspan multiple lines. Note that regardless of whether the escape parser\ndirective is included in a Dockerfile, escaping is not performed in\na RUN command, except at the end of a line.\nSetting the escape character to ` is especially useful on\nWindows, where \\ is the directory path separator. ` is consistent\nwith Windows PowerShell.\nConsider the following example which would fail in a non-obvious way on\nWindows. The second \\ at the end of the second line would be interpreted as an\nescape for the newline, instead of a target of the escape from the first \\.\nSimilarly, the \\ at the end of the third line would, assuming it was actually\nhandled as an instruction, cause it be treated as a line continuation. The result\nof this dockerfile is that second and third lines are considered a single\ninstruction:\nFROM microsoft/nanoserver\nCOPY testfile.txt c:\\\\\nRUN dir c:\\\n\nResults in:\nPS C:\\John> docker build -t cmd .\nSending build context to Docker daemon 3.072 kB\nStep 1/2 : FROM microsoft/nanoserver\n---> 22738ff49c6d\nStep 2/2 : COPY testfile.txt c:\\RUN dir c:\nGetFileAttributesEx c:RUN: The system cannot find the file specified.\nPS C:\\John>\n\nOne solution to the above would be to use / as the target of both the COPY\ninstruction, and dir. However, this syntax is, at best, confusing as it is not\nnatural for paths on Windows, and at worst, error prone as not all commands on\nWindows support / as the path separator.\nBy adding the escape parser directive, the following Dockerfile succeeds as\nexpected with the use of natural platform semantics for file paths on Windows:\n# escape=`\n\nFROM microsoft/nanoserver\nCOPY testfile.txt c:\\\nRUN dir c:\\\n\nResults in:\nPS C:\\John> docker build -t succeeds --no-cache=true .\nSending build context to Docker daemon 3.072 kB\nStep 1/3 : FROM microsoft/nanoserver\n---> 22738ff49c6d\nStep 2/3 : COPY testfile.txt c:\\\n---> 96655de338de\nRemoving intermediate container 4db9acbb1682\nStep 3/3 : RUN dir c:\\\n---> Running in a2c157f842f5\nVolume in drive C has no label.\nVolume Serial Number is 7E6D-E0F7\n\nDirectory of c:\\\n\n10/05/2016  05:04 PM             1,894 License.txt\n10/05/2016  02:22 PM    <DIR>          Program Files\n10/05/2016  02:14 PM    <DIR>          Program Files (x86)\n10/28/2016  11:18 AM                62 testfile.txt\n10/28/2016  11:20 AM    <DIR>          Users\n10/28/2016  11:20 AM    <DIR>          Windows\n2 File(s)          1,956 bytes\n4 Dir(s)  21,259,096,064 bytes free\n---> 01c7f3bef04f\nRemoving intermediate container a2c157f842f5\nSuccessfully built 01c7f3bef04f\nPS C:\\John>\n\nEnvironment replacement\ud83d\udd17\nEnvironment variables (declared with the ENV statement) can also be\nused in certain instructions as variables to be interpreted by the\nDockerfile. Escapes are also handled for including variable-like syntax\ninto a statement literally.\nEnvironment variables are notated in the Dockerfile either with\n$variable_name or ${variable_name}. They are treated equivalently and the\nbrace syntax is typically used to address issues with variable names with no\nwhitespace, like ${foo}_bar.\nThe ${variable_name} syntax also supports a few of the standard bash\nmodifiers as specified below:\n\n${variable:-word} indicates that if variable is set then the result\nwill be that value. If variable is not set then word will be the result.\n${variable:+word} indicates that if variable is set then word will be\nthe result, otherwise the result is the empty string.\n\nIn all cases, word can be any string, including additional environment\nvariables.\nEscaping is possible by adding a \\ before the variable: \\$foo or \\${foo},\nfor example, will translate to $foo and ${foo} literals respectively.\nExample (parsed representation is displayed after the #):\nFROM busybox\nENV FOO=/bar\nWORKDIR ${FOO}   # WORKDIR /bar\nADD . $FOO       # ADD . /bar\nCOPY \\$FOO /quux # COPY $FOO /quux\n\nEnvironment variables are supported by the following list of instructions in\nthe Dockerfile:\n\nADD\nCOPY\nENV\nEXPOSE\nFROM\nLABEL\nSTOPSIGNAL\nUSER\nVOLUME\nWORKDIR\nONBUILD (when combined with one of the supported instructions above)\n\nEnvironment variable substitution will use the same value for each variable\nthroughout the entire instruction. In other words, in this example:\nENV abc=hello\nENV abc=bye def=$abc\nENV ghi=$abc\n\nwill result in def having a value of hello, not bye. However,\nghi will have a value of bye because it is not part of the same instruction\nthat set abc to bye.\n.dockerignore file\ud83d\udd17\nBefore the docker CLI sends the context to the docker daemon, it looks\nfor a file named .dockerignore in the root directory of the context.\nIf this file exists, the CLI modifies the context to exclude files and\ndirectories that match patterns in it.  This helps to avoid\nunnecessarily sending large or sensitive files and directories to the\ndaemon and potentially adding them to images using ADD or COPY.\nThe CLI interprets the .dockerignore file as a newline-separated\nlist of patterns similar to the file globs of Unix shells.  For the\npurposes of matching, the root of the context is considered to be both\nthe working and the root directory.  For example, the patterns\n/foo/bar and foo/bar both exclude a file or directory named bar\nin the foo subdirectory of PATH or in the root of the git\nrepository located at URL.  Neither excludes anything else.\nIf a line in .dockerignore file starts with # in column 1, then this line is\nconsidered as a comment and is ignored before interpreted by the CLI.\nHere is an example .dockerignore file:\n# comment\n*/temp*\n*/*/temp*\ntemp?\n\nThis file causes the following build behavior:\n\n\n\nRule\nBehavior\n\n\n\n\n# comment\nIgnored.\n\n\n*/temp*\nExclude files and directories whose names start with temp in any immediate subdirectory of the root.  For example, the plain file /somedir/temporary.txt is excluded, as is the directory /somedir/temp.\n\n\n*/*/temp*\nExclude files and directories starting with temp from any subdirectory that is two levels below the root. For example, /somedir/subdir/temporary.txt is excluded.\n\n\ntemp?\nExclude files and directories in the root directory whose names are a one-character extension of temp.  For example, /tempa and /tempb are excluded.\n\n\n\nMatching is done using Go\u2019s\nfilepath.Match rules.  A\npreprocessing step removes leading and trailing whitespace and\neliminates . and .. elements using Go\u2019s\nfilepath.Clean.  Lines\nthat are blank after preprocessing are ignored.\nBeyond Go\u2019s filepath.Match rules, Docker also supports a special\nwildcard string ** that matches any number of directories (including\nzero). For example, **/*.go will exclude all files that end with .go\nthat are found in all directories, including the root of the build context.\nLines starting with ! (exclamation mark) can be used to make exceptions\nto exclusions.  The following is an example .dockerignore file that\nuses this mechanism:\n*.md\n!README.md\n\nAll markdown files except README.md are excluded from the context.\nThe placement of ! exception rules influences the behavior: the last\nline of the .dockerignore that matches a particular file determines\nwhether it is included or excluded.  Consider the following example:\n*.md\n!README*.md\nREADME-secret.md\n\nNo markdown files are included in the context except README files other than\nREADME-secret.md.\nNow consider this example:\n*.md\nREADME-secret.md\n!README*.md\n\nAll of the README files are included.  The middle line has no effect because\n!README*.md matches README-secret.md and comes last.\nYou can even use the .dockerignore file to exclude the Dockerfile\nand .dockerignore files.  These files are still sent to the daemon\nbecause it needs them to do its job.  But the ADD and COPY instructions\ndo not copy them to the image.\nFinally, you may want to specify which files to include in the\ncontext, rather than which to exclude. To achieve this, specify * as\nthe first pattern, followed by one or more ! exception patterns.\n\nNote\nFor historical reasons, the pattern . is ignored.\n\nFROM\ud83d\udd17\nFROM [--platform=<platform>] <image> [AS <name>]\n\nOr\nFROM [--platform=<platform>] <image>[:<tag>] [AS <name>]\n\nOr\nFROM [--platform=<platform>] <image>[@<digest>] [AS <name>]\n\nThe FROM instruction initializes a new build stage and sets the\nBase Image for subsequent instructions. As such, a\nvalid Dockerfile must start with a FROM instruction. The image can be\nany valid image \u2013 it is especially easy to start by pulling an image from\nthe Public Repositories.\n\nARG is the only instruction that may precede FROM in the Dockerfile.\nSee Understand how ARG and FROM interact.\nFROM can appear multiple times within a single Dockerfile to\ncreate multiple images or use one build stage as a dependency for another.\nSimply make a note of the last image ID output by the commit before each new\nFROM instruction. Each FROM instruction clears any state created by previous\ninstructions.\nOptionally a name can be given to a new build stage by adding AS name to the\nFROM instruction. The name can be used in subsequent FROM and\nCOPY --from=<name> instructions to refer to the image built in this stage.\nThe tag or digest values are optional. If you omit either of them, the\nbuilder assumes a latest tag by default. The builder returns an error if it\ncannot find the tag value.\n\nThe optional --platform flag can be used to specify the platform of the image\nin case FROM references a multi-platform image. For example, linux/amd64,\nlinux/arm64, or windows/amd64. By default, the target platform of the build\nrequest is used. Global build arguments can be used in the value of this flag,\nfor example automatic platform ARGs\nallow you to force a stage to native build platform (--platform=$BUILDPLATFORM),\nand use it to cross-compile to the target platform inside the stage.\nUnderstand how ARG and FROM interact\ud83d\udd17\nFROM instructions support variables that are declared by any ARG\ninstructions that occur before the first FROM.\nARG  CODE_VERSION=latest\nFROM base:${CODE_VERSION}\nCMD  /code/run-app\n\nFROM extras:${CODE_VERSION}\nCMD  /code/run-extras\n\nAn ARG declared before a FROM is outside of a build stage, so it\ncan\u2019t be used in any instruction after a FROM. To use the default value of\nan ARG declared before the first FROM use an ARG instruction without\na value inside of a build stage:\nARG VERSION=latest\nFROM busybox:$VERSION\nARG VERSION\nRUN echo $VERSION > image_version\n\nRUN\ud83d\udd17\nRUN has 2 forms:\n\nRUN <command> (shell form, the command is run in a shell, which by\ndefault is /bin/sh -c on Linux or cmd /S /C on Windows)\nRUN [\"executable\", \"param1\", \"param2\"] (exec form)\n\nThe RUN instruction will execute any commands in a new layer on top of the\ncurrent image and commit the results. The resulting committed image will be\nused for the next step in the Dockerfile.\nLayering RUN instructions and generating commits conforms to the core\nconcepts of Docker where commits are cheap and containers can be created from\nany point in an image\u2019s history, much like source control.\nThe exec form makes it possible to avoid shell string munging, and to RUN\ncommands using a base image that does not contain the specified shell executable.\nThe default shell for the shell form can be changed using the SHELL\ncommand.\nIn the shell form you can use a \\ (backslash) to continue a single\nRUN instruction onto the next line. For example, consider these two lines:\nRUN /bin/bash -c 'source $HOME/.bashrc; \\\necho $HOME'\n\nTogether they are equivalent to this single line:\nRUN /bin/bash -c 'source $HOME/.bashrc; echo $HOME'\n\nTo use a different shell, other than \u2018/bin/sh\u2019, use the exec form passing in\nthe desired shell. For example:\nRUN [\"/bin/bash\", \"-c\", \"echo hello\"]\n\n\nNote\nThe exec form is parsed as a JSON array, which means that\nyou must use double-quotes (\u201c) around words not single-quotes (\u2018).\n\nUnlike the shell form, the exec form does not invoke a command shell.\nThis means that normal shell processing does not happen. For example,\nRUN [ \"echo\", \"$HOME\" ] will not do variable substitution on $HOME.\nIf you want shell processing then either use the shell form or execute\na shell directly, for example: RUN [ \"sh\", \"-c\", \"echo $HOME\" ].\nWhen using the exec form and executing a shell directly, as in the case for\nthe shell form, it is the shell that is doing the environment variable\nexpansion, not docker.\n\nNote\nIn the JSON form, it is necessary to escape backslashes. This is\nparticularly relevant on Windows where the backslash is the path separator.\nThe following line would otherwise be treated as shell form due to not\nbeing valid JSON, and fail in an unexpected way:\nRUN [\"c:\\windows\\system32\\tasklist.exe\"]\n\nThe correct syntax for this example is:\nRUN [\"c:\\\\windows\\\\system32\\\\tasklist.exe\"]\n\n\nThe cache for RUN instructions isn\u2019t invalidated automatically during\nthe next build. The cache for an instruction like\nRUN apt-get dist-upgrade -y will be reused during the next build. The\ncache for RUN instructions can be invalidated by using the --no-cache\nflag, for example docker build --no-cache.\nSee the Dockerfile Best Practices\nguide for more information.\nThe cache for RUN instructions can be invalidated by ADD and COPY instructions.\nKnown issues (RUN)\ud83d\udd17\n\n\nIssue 783 is about file\npermissions problems that can occur when using the AUFS file system. You\nmight notice it during an attempt to rm a file, for example.\nFor systems that have recent aufs version (i.e., dirperm1 mount option can\nbe set), docker will attempt to fix the issue automatically by mounting\nthe layers with dirperm1 option. More details on dirperm1 option can be\nfound at aufs man page\nIf your system doesn\u2019t have support for dirperm1, the issue describes a workaround.\n\n\nCMD\ud83d\udd17\nThe CMD instruction has three forms:\n\nCMD [\"executable\",\"param1\",\"param2\"] (exec form, this is the preferred form)\nCMD [\"param1\",\"param2\"] (as default parameters to ENTRYPOINT)\nCMD command param1 param2 (shell form)\n\nThere can only be one CMD instruction in a Dockerfile. If you list more than one CMD\nthen only the last CMD will take effect.\nThe main purpose of a CMD is to provide defaults for an executing\ncontainer. These defaults can include an executable, or they can omit\nthe executable, in which case you must specify an ENTRYPOINT\ninstruction as well.\nIf CMD is used to provide default arguments for the ENTRYPOINT instruction,\nboth the CMD and ENTRYPOINT instructions should be specified with the JSON\narray format.\n\nNote\nThe exec form is parsed as a JSON array, which means that you must use\ndouble-quotes (\u201c) around words not single-quotes (\u2018).\n\nUnlike the shell form, the exec form does not invoke a command shell.\nThis means that normal shell processing does not happen. For example,\nCMD [ \"echo\", \"$HOME\" ] will not do variable substitution on $HOME.\nIf you want shell processing then either use the shell form or execute\na shell directly, for example: CMD [ \"sh\", \"-c\", \"echo $HOME\" ].\nWhen using the exec form and executing a shell directly, as in the case for\nthe shell form, it is the shell that is doing the environment variable\nexpansion, not docker.\nWhen used in the shell or exec formats, the CMD instruction sets the command\nto be executed when running the image.\nIf you use the shell form of the CMD, then the <command> will execute in\n/bin/sh -c:\nFROM ubuntu\nCMD echo \"This is a test.\" | wc -\n\nIf you want to run your <command> without a shell then you must\nexpress the command as a JSON array and give the full path to the executable.\nThis array form is the preferred format of CMD. Any additional parameters\nmust be individually expressed as strings in the array:\nFROM ubuntu\nCMD [\"/usr/bin/wc\",\"--help\"]\n\nIf you would like your container to run the same executable every time, then\nyou should consider using ENTRYPOINT in combination with CMD. See\nENTRYPOINT.\nIf the user specifies arguments to docker run then they will override the\ndefault specified in CMD.\n\nNote\nDo not confuse RUN with CMD. RUN actually runs a command and commits\nthe result; CMD does not execute anything at build time, but specifies\nthe intended command for the image.\n\nLABEL\ud83d\udd17\nLABEL <key>=<value> <key>=<value> <key>=<value> ...\n\nThe LABEL instruction adds metadata to an image. A LABEL is a\nkey-value pair. To include spaces within a LABEL value, use quotes and\nbackslashes as you would in command-line parsing. A few usage examples:\nLABEL \"com.example.vendor\"=\"ACME Incorporated\"\nLABEL com.example.label-with-value=\"foo\"\nLABEL version=\"1.0\"\nLABEL description=\"This text illustrates \\\nthat label-values can span multiple lines.\"\n\nAn image can have more than one label. You can specify multiple labels on a\nsingle line. Prior to Docker 1.10, this decreased the size of the final image,\nbut this is no longer the case. You may still choose to specify multiple labels\nin a single instruction, in one of the following two ways:\nLABEL multi.label1=\"value1\" multi.label2=\"value2\" other=\"value3\"\n\nLABEL multi.label1=\"value1\" \\\nmulti.label2=\"value2\" \\\nother=\"value3\"\n\nLabels included in base or parent images (images in the FROM line) are\ninherited by your image. If a label already exists but with a different value,\nthe most-recently-applied value overrides any previously-set value.\nTo view an image\u2019s labels, use the docker image inspect command. You can use\nthe --format option to show just the labels;\ndocker image inspect --format='' myimage\n\n{\n\"com.example.vendor\": \"ACME Incorporated\",\n\"com.example.label-with-value\": \"foo\",\n\"version\": \"1.0\",\n\"description\": \"This text illustrates that label-values can span multiple lines.\",\n\"multi.label1\": \"value1\",\n\"multi.label2\": \"value2\",\n\"other\": \"value3\"\n}\n\nMAINTAINER (deprecated)\ud83d\udd17\nMAINTAINER <name>\n\nThe MAINTAINER instruction sets the Author field of the generated images.\nThe LABEL instruction is a much more flexible version of this and you should use\nit instead, as it enables setting any metadata you require, and can be viewed\neasily, for example with docker inspect. To set a label corresponding to the\nMAINTAINER field you could use:\nLABEL maintainer=\"SvenDowideit@home.org.au\"\n\nThis will then be visible from docker inspect with the other labels.\nEXPOSE\ud83d\udd17\nEXPOSE <port> [<port>/<protocol>...]\n\nThe EXPOSE instruction informs Docker that the container listens on the\nspecified network ports at runtime. You can specify whether the port listens on\nTCP or UDP, and the default is TCP if the protocol is not specified.\nThe EXPOSE instruction does not actually publish the port. It functions as a\ntype of documentation between the person who builds the image and the person who\nruns the container, about which ports are intended to be published. To actually\npublish the port when running the container, use the -p flag on docker run\nto publish and map one or more ports, or the -P flag to publish all exposed\nports and map them to high-order ports.\nBy default, EXPOSE assumes TCP. You can also specify UDP:\nEXPOSE 80/udp\n\nTo expose on both TCP and UDP, include two lines:\nEXPOSE 80/tcp\nEXPOSE 80/udp\n\nIn this case, if you use -P with docker run, the port will be exposed once\nfor TCP and once for UDP. Remember that -P uses an ephemeral high-ordered host\nport on the host, so the port will not be the same for TCP and UDP.\nRegardless of the EXPOSE settings, you can override them at runtime by using\nthe -p flag. For example\ndocker run -p 80:80/tcp -p 80:80/udp ...\n\nTo set up port redirection on the host system, see using the -P flag.\nThe docker network command supports creating networks for communication among\ncontainers without the need to expose or publish specific ports, because the\ncontainers connected to the network can communicate with each other over any\nport. For detailed information, see the\noverview of this feature.\nENV\ud83d\udd17\nENV <key>=<value> ...\n\nThe ENV instruction sets the environment variable <key> to the value\n<value>. This value will be in the environment for all subsequent instructions\nin the build stage and can be replaced inline in\nmany as well. The value will be interpreted for other environment variables, so\nquote characters will be removed if they are not escaped. Like command line parsing,\nquotes and backslashes can be used to include spaces within values.\nExample:\nENV MY_NAME=\"John Doe\"\nENV MY_DOG=Rex\\ The\\ Dog\nENV MY_CAT=fluffy\n\nThe ENV instruction allows for multiple <key>=<value> ... variables to be set\nat one time, and the example below will yield the same net results in the final\nimage:\nENV MY_NAME=\"John Doe\" MY_DOG=Rex\\ The\\ Dog \\\nMY_CAT=fluffy\n\nThe environment variables set using ENV will persist when a container is run\nfrom the resulting image. You can view the values using docker inspect, and\nchange them using docker run --env <key>=<value>.\nEnvironment variable persistence can cause unexpected side effects. For example,\nsetting ENV DEBIAN_FRONTEND=noninteractive changes the behavior of apt-get,\nand may confuse users of your image.\nIf an environment variable is only needed during build, and not in the final\nimage, consider setting a value for a single command instead:\nRUN DEBIAN_FRONTEND=noninteractive apt-get update && apt-get install -y ...\n\nOr using ARG, which is not persisted in the final image:\nARG DEBIAN_FRONTEND=noninteractive\nRUN apt-get update && apt-get install -y ...\n\n\nAlternative syntax\nThe ENV instruction also allows an alternative syntax ENV <key> <value>,\nomitting the =. For example:\nENV MY_VAR my-value\n\nThis syntax does not allow for multiple environment-variables to be set in a\nsingle ENV instruction, and can be confusing. For example, the following\nsets a single environment variable (ONE) with value \"TWO= THREE=world\":\nENV ONE TWO= THREE=world\n\nThe alternative syntax is supported for backward compatibility, but discouraged\nfor the reasons outlined above, and may be removed in a future release.\n\nADD\ud83d\udd17\nADD has two forms:\nADD [--chown=<user>:<group>] <src>... <dest>\nADD [--chown=<user>:<group>] [\"<src>\",... \"<dest>\"]\n\nThe latter form is required for paths containing whitespace.\n\nNote\nThe --chown feature is only supported on Dockerfiles used to build Linux containers,\nand will not work on Windows containers. Since user and group ownership concepts do\nnot translate between Linux and Windows, the use of /etc/passwd and /etc/group for\ntranslating user and group names to IDs restricts this feature to only be viable\nfor Linux OS-based containers.\n\nThe ADD instruction copies new files, directories or remote file URLs from <src>\nand adds them to the filesystem of the image at the path <dest>.\nMultiple <src> resources may be specified but if they are files or\ndirectories, their paths are interpreted as relative to the source of\nthe context of the build.\nEach <src> may contain wildcards and matching will be done using Go\u2019s\nfilepath.Match rules. For example:\nTo add all files starting with \u201chom\u201d:\nADD hom* /mydir/\n\nIn the example below, ? is replaced with any single character, e.g., \u201chome.txt\u201d.\nADD hom?.txt /mydir/\n\nThe <dest> is an absolute path, or a path relative to WORKDIR, into which\nthe source will be copied inside the destination container.\nThe example below uses a relative path, and adds \u201ctest.txt\u201d to <WORKDIR>/relativeDir/:\nADD test.txt relativeDir/\n\nWhereas this example uses an absolute path, and adds \u201ctest.txt\u201d to /absoluteDir/\nADD test.txt /absoluteDir/\n\nWhen adding files or directories that contain special characters (such as [\nand ]), you need to escape those paths following the Golang rules to prevent\nthem from being treated as a matching pattern. For example, to add a file\nnamed arr[0].txt, use the following;\nADD arr[[]0].txt /mydir/\n\nAll new files and directories are created with a UID and GID of 0, unless the\noptional --chown flag specifies a given username, groupname, or UID/GID\ncombination to request specific ownership of the content added. The\nformat of the --chown flag allows for either username and groupname strings\nor direct integer UID and GID in any combination. Providing a username without\ngroupname or a UID without GID will use the same numeric UID as the GID. If a\nusername or groupname is provided, the container\u2019s root filesystem\n/etc/passwd and /etc/group files will be used to perform the translation\nfrom name to integer UID or GID respectively. The following examples show\nvalid definitions for the --chown flag:\nADD --chown=55:mygroup files* /somedir/\nADD --chown=bin files* /somedir/\nADD --chown=1 files* /somedir/\nADD --chown=10:11 files* /somedir/\n\nIf the container root filesystem does not contain either /etc/passwd or\n/etc/group files and either user or group names are used in the --chown\nflag, the build will fail on the ADD operation. Using numeric IDs requires\nno lookup and will not depend on container root filesystem content.\nIn the case where <src> is a remote file URL, the destination will\nhave permissions of 600. If the remote file being retrieved has an HTTP\nLast-Modified header, the timestamp from that header will be used\nto set the mtime on the destination file. However, like any other file\nprocessed during an ADD, mtime will not be included in the determination\nof whether or not the file has changed and the cache should be updated.\n\nNote\nIf you build by passing a Dockerfile through STDIN (docker\nbuild - < somefile), there is no build context, so the Dockerfile\ncan only contain a URL based ADD instruction. You can also pass a\ncompressed archive through STDIN: (docker build - < archive.tar.gz),\nthe Dockerfile at the root of the archive and the rest of the\narchive will be used as the context of the build.\n\nIf your URL files are protected using authentication, you need to use RUN wget,\nRUN curl or use another tool from within the container as the ADD instruction\ndoes not support authentication.\n\nNote\nThe first encountered ADD instruction will invalidate the cache for all\nfollowing instructions from the Dockerfile if the contents of <src> have\nchanged. This includes invalidating the cache for RUN instructions.\nSee the Dockerfile Best Practices\nguide \u2013\u00a0Leverage build cache\nfor more information.\n\nADD obeys the following rules:\n\n\nThe <src> path must be inside the context of the build;\nyou cannot ADD ../something /something, because the first step of a\ndocker build is to send the context directory (and subdirectories) to the\ndocker daemon.\n\n\nIf <src> is a URL and <dest> does not end with a trailing slash, then a\nfile is downloaded from the URL and copied to <dest>.\n\n\nIf <src> is a URL and <dest> does end with a trailing slash, then the\nfilename is inferred from the URL and the file is downloaded to\n<dest>/<filename>. For instance, ADD http://example.com/foobar / would\ncreate the file /foobar. The URL must have a nontrivial path so that an\nappropriate filename can be discovered in this case (http://example.com\nwill not work).\n\n\nIf <src> is a directory, the entire contents of the directory are copied,\nincluding filesystem metadata.\n\n\n\nNote\nThe directory itself is not copied, just its contents.\n\n\n\nIf <src> is a local tar archive in a recognized compression format\n(identity, gzip, bzip2 or xz) then it is unpacked as a directory. Resources\nfrom remote URLs are not decompressed. When a directory is copied or\nunpacked, it has the same behavior as tar -x, the result is the union of:\n\nWhatever existed at the destination path and\nThe contents of the source tree, with conflicts resolved in favor\nof \u201c2.\u201d on a file-by-file basis.\n\n\nNote\nWhether a file is identified as a recognized compression format or not\nis done solely based on the contents of the file, not the name of the file.\nFor example, if an empty file happens to end with .tar.gz this will not\nbe recognized as a compressed file and will not generate any kind of\ndecompression error message, rather the file will simply be copied to the\ndestination.\n\n\n\nIf <src> is any other kind of file, it is copied individually along with\nits metadata. In this case, if <dest> ends with a trailing slash /, it\nwill be considered a directory and the contents of <src> will be written\nat <dest>/base(<src>).\n\n\nIf multiple <src> resources are specified, either directly or due to the\nuse of a wildcard, then <dest> must be a directory, and it must end with\na slash /.\n\n\nIf <dest> does not end with a trailing slash, it will be considered a\nregular file and the contents of <src> will be written at <dest>.\n\n\nIf <dest> doesn\u2019t exist, it is created along with all missing directories\nin its path.\n\n\nCOPY\ud83d\udd17\nCOPY has two forms:\nCOPY [--chown=<user>:<group>] <src>... <dest>\nCOPY [--chown=<user>:<group>] [\"<src>\",... \"<dest>\"]\n\nThis latter form is required for paths containing whitespace\n\nNote\nThe --chown feature is only supported on Dockerfiles used to build Linux containers,\nand will not work on Windows containers. Since user and group ownership concepts do\nnot translate between Linux and Windows, the use of /etc/passwd and /etc/group for\ntranslating user and group names to IDs restricts this feature to only be viable for\nLinux OS-based containers.\n\nThe COPY instruction copies new files or directories from <src>\nand adds them to the filesystem of the container at the path <dest>.\nMultiple <src> resources may be specified but the paths of files and\ndirectories will be interpreted as relative to the source of the context\nof the build.\nEach <src> may contain wildcards and matching will be done using Go\u2019s\nfilepath.Match rules. For example:\nTo add all files starting with \u201chom\u201d:\nCOPY hom* /mydir/\n\nIn the example below, ? is replaced with any single character, e.g., \u201chome.txt\u201d.\nCOPY hom?.txt /mydir/\n\nThe <dest> is an absolute path, or a path relative to WORKDIR, into which\nthe source will be copied inside the destination container.\nThe example below uses a relative path, and adds \u201ctest.txt\u201d to <WORKDIR>/relativeDir/:\nCOPY test.txt relativeDir/\n\nWhereas this example uses an absolute path, and adds \u201ctest.txt\u201d to /absoluteDir/\nCOPY test.txt /absoluteDir/\n\nWhen copying files or directories that contain special characters (such as [\nand ]), you need to escape those paths following the Golang rules to prevent\nthem from being treated as a matching pattern. For example, to copy a file\nnamed arr[0].txt, use the following;\nCOPY arr[[]0].txt /mydir/\n\nAll new files and directories are created with a UID and GID of 0, unless the\noptional --chown flag specifies a given username, groupname, or UID/GID\ncombination to request specific ownership of the copied content. The\nformat of the --chown flag allows for either username and groupname strings\nor direct integer UID and GID in any combination. Providing a username without\ngroupname or a UID without GID will use the same numeric UID as the GID. If a\nusername or groupname is provided, the container\u2019s root filesystem\n/etc/passwd and /etc/group files will be used to perform the translation\nfrom name to integer UID or GID respectively. The following examples show\nvalid definitions for the --chown flag:\nCOPY --chown=55:mygroup files* /somedir/\nCOPY --chown=bin files* /somedir/\nCOPY --chown=1 files* /somedir/\nCOPY --chown=10:11 files* /somedir/\n\nIf the container root filesystem does not contain either /etc/passwd or\n/etc/group files and either user or group names are used in the --chown\nflag, the build will fail on the COPY operation. Using numeric IDs requires\nno lookup and does not depend on container root filesystem content.\n\nNote\nIf you build using STDIN (docker build - < somefile), there is no\nbuild context, so COPY can\u2019t be used.\n\nOptionally COPY accepts a flag --from=<name> that can be used to set\nthe source location to a previous build stage (created with FROM .. AS <name>)\nthat will be used instead of a build context sent by the user. In case a build\nstage with a specified name can\u2019t be found an image with the same name is\nattempted to be used instead.\nCOPY obeys the following rules:\n\n\nThe <src> path must be inside the context of the build;\nyou cannot COPY ../something /something, because the first step of a\ndocker build is to send the context directory (and subdirectories) to the\ndocker daemon.\n\n\nIf <src> is a directory, the entire contents of the directory are copied,\nincluding filesystem metadata.\n\n\n\nNote\nThe directory itself is not copied, just its contents.\n\n\n\nIf <src> is any other kind of file, it is copied individually along with\nits metadata. In this case, if <dest> ends with a trailing slash /, it\nwill be considered a directory and the contents of <src> will be written\nat <dest>/base(<src>).\n\n\nIf multiple <src> resources are specified, either directly or due to the\nuse of a wildcard, then <dest> must be a directory, and it must end with\na slash /.\n\n\nIf <dest> does not end with a trailing slash, it will be considered a\nregular file and the contents of <src> will be written at <dest>.\n\n\nIf <dest> doesn\u2019t exist, it is created along with all missing directories\nin its path.\n\n\n\nNote\nThe first encountered COPY instruction will invalidate the cache for all\nfollowing instructions from the Dockerfile if the contents of <src> have\nchanged. This includes invalidating the cache for RUN instructions.\nSee the Dockerfile Best Practices\nguide \u2013\u00a0Leverage build cache\nfor more information.\n\nENTRYPOINT\ud83d\udd17\nENTRYPOINT has two forms:\nThe exec form, which is the preferred form:\nENTRYPOINT [\"executable\", \"param1\", \"param2\"]\n\nThe shell form:\nENTRYPOINT command param1 param2\n\nAn ENTRYPOINT allows you to configure a container that will run as an executable.\nFor example, the following starts nginx with its default content, listening\non port 80:\n$ docker run -i -t --rm -p 80:80 nginx\n\nCommand line arguments to docker run <image> will be appended after all\nelements in an exec form ENTRYPOINT, and will override all elements specified\nusing CMD.\nThis allows arguments to be passed to the entry point, i.e., docker run <image> -d\nwill pass the -d argument to the entry point.\nYou can override the ENTRYPOINT instruction using the docker run --entrypoint\nflag.\nThe shell form prevents any CMD or run command line arguments from being\nused, but has the disadvantage that your ENTRYPOINT will be started as a\nsubcommand of /bin/sh -c, which does not pass signals.\nThis means that the executable will not be the container\u2019s PID 1 - and\nwill not receive Unix signals - so your executable will not receive a\nSIGTERM from docker stop <container>.\nOnly the last ENTRYPOINT instruction in the Dockerfile will have an effect.\nExec form ENTRYPOINT example\ud83d\udd17\nYou can use the exec form of ENTRYPOINT to set fairly stable default commands\nand arguments and then use either form of CMD to set additional defaults that\nare more likely to be changed.\nFROM ubuntu\nENTRYPOINT [\"top\", \"-b\"]\nCMD [\"-c\"]\n\nWhen you run the container, you can see that top is the only process:\n$ docker run -it --rm --name test  top -H\n\ntop - 08:25:00 up  7:27,  0 users,  load average: 0.00, 0.01, 0.05\nThreads:   1 total,   1 running,   0 sleeping,   0 stopped,   0 zombie\n%Cpu(s):  0.1 us,  0.1 sy,  0.0 ni, 99.7 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st\nKiB Mem:   2056668 total,  1616832 used,   439836 free,    99352 buffers\nKiB Swap:  1441840 total,        0 used,  1441840 free.  1324440 cached Mem\n\nPID USER      PR  NI    VIRT    RES    SHR S %CPU %MEM     TIME+ COMMAND\n1 root      20   0   19744   2336   2080 R  0.0  0.1   0:00.04 top\n\nTo examine the result further, you can use docker exec:\n$ docker exec -it test ps aux\n\nUSER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\nroot         1  2.6  0.1  19752  2352 ?        Ss+  08:24   0:00 top -b -H\nroot         7  0.0  0.1  15572  2164 ?        R+   08:25   0:00 ps aux\n\nAnd you can gracefully request top to shut down using docker stop test.\nThe following Dockerfile shows using the ENTRYPOINT to run Apache in the\nforeground (i.e., as PID 1):\nFROM debian:stable\nRUN apt-get update && apt-get install -y --force-yes apache2\nEXPOSE 80 443\nVOLUME [\"/var/www\", \"/var/log/apache2\", \"/etc/apache2\"]\nENTRYPOINT [\"/usr/sbin/apache2ctl\", \"-D\", \"FOREGROUND\"]\n\nIf you need to write a starter script for a single executable, you can ensure that\nthe final executable receives the Unix signals by using exec and gosu\ncommands:\n#!/usr/bin/env bash\nset -e\n\nif [ \"$1\" = 'postgres' ]; then\nchown -R postgres \"$PGDATA\"\n\nif [ -z \"$(ls -A \"$PGDATA\")\" ]; then\ngosu postgres initdb\nfi\n\nexec gosu postgres \"$@\"\nfi\n\nexec \"$@\"\n\nLastly, if you need to do some extra cleanup (or communicate with other containers)\non shutdown, or are co-ordinating more than one executable, you may need to ensure\nthat the ENTRYPOINT script receives the Unix signals, passes them on, and then\ndoes some more work:\n#!/bin/sh\n# Note: I've written this using sh so it works in the busybox container too\n\n# USE the trap if you need to also do manual cleanup after the service is stopped,\n#     or need to start multiple services in the one container\ntrap \"echo TRAPed signal\" HUP INT QUIT TERM\n\n# start service in background here\n/usr/sbin/apachectl start\n\necho \"[hit enter key to exit] or run 'docker stop <container>'\"\nread\n\n# stop service and clean up here\necho \"stopping apache\"\n/usr/sbin/apachectl stop\n\necho \"exited $0\"\n\nIf you run this image with docker run -it --rm -p 80:80 --name test apache,\nyou can then examine the container\u2019s processes with docker exec, or docker top,\nand then ask the script to stop Apache:\n$ docker exec -it test ps aux\n\nUSER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\nroot         1  0.1  0.0   4448   692 ?        Ss+  00:42   0:00 /bin/sh /run.sh 123 cmd cmd2\nroot        19  0.0  0.2  71304  4440 ?        Ss   00:42   0:00 /usr/sbin/apache2 -k start\nwww-data    20  0.2  0.2 360468  6004 ?        Sl   00:42   0:00 /usr/sbin/apache2 -k start\nwww-data    21  0.2  0.2 360468  6000 ?        Sl   00:42   0:00 /usr/sbin/apache2 -k start\nroot        81  0.0  0.1  15572  2140 ?        R+   00:44   0:00 ps aux\n\n$ docker top test\n\nPID                 USER                COMMAND\n10035               root                {run.sh} /bin/sh /run.sh 123 cmd cmd2\n10054               root                /usr/sbin/apache2 -k start\n10055               33                  /usr/sbin/apache2 -k start\n10056               33                  /usr/sbin/apache2 -k start\n\n$ /usr/bin/time docker stop test\n\ntest\nreal\t0m 0.27s\nuser\t0m 0.03s\nsys\t0m 0.03s\n\n\nNote\nYou can override the ENTRYPOINT setting using --entrypoint,\nbut this can only set the binary to exec (no sh -c will be used).\n\n\nNote\nThe exec form is parsed as a JSON array, which means that\nyou must use double-quotes (\u201c) around words not single-quotes (\u2018).\n\nUnlike the shell form, the exec form does not invoke a command shell.\nThis means that normal shell processing does not happen. For example,\nENTRYPOINT [ \"echo\", \"$HOME\" ] will not do variable substitution on $HOME.\nIf you want shell processing then either use the shell form or execute\na shell directly, for example: ENTRYPOINT [ \"sh\", \"-c\", \"echo $HOME\" ].\nWhen using the exec form and executing a shell directly, as in the case for\nthe shell form, it is the shell that is doing the environment variable\nexpansion, not docker.\nShell form ENTRYPOINT example\ud83d\udd17\nYou can specify a plain string for the ENTRYPOINT and it will execute in /bin/sh -c.\nThis form will use shell processing to substitute shell environment variables,\nand will ignore any CMD or docker run command line arguments.\nTo ensure that docker stop will signal any long running ENTRYPOINT executable\ncorrectly, you need to remember to start it with exec:\nFROM ubuntu\nENTRYPOINT exec top -b\n\nWhen you run this image, you\u2019ll see the single PID 1 process:\n$ docker run -it --rm --name test top\n\nMem: 1704520K used, 352148K free, 0K shrd, 0K buff, 140368121167873K cached\nCPU:   5% usr   0% sys   0% nic  94% idle   0% io   0% irq   0% sirq\nLoad average: 0.08 0.03 0.05 2/98 6\nPID  PPID USER     STAT   VSZ %VSZ %CPU COMMAND\n1     0 root     R     3164   0%   0% top -b\n\nWhich exits cleanly on docker stop:\n$ /usr/bin/time docker stop test\n\ntest\nreal\t0m 0.20s\nuser\t0m 0.02s\nsys\t0m 0.04s\n\nIf you forget to add exec to the beginning of your ENTRYPOINT:\nFROM ubuntu\nENTRYPOINT top -b\nCMD --ignored-param1\n\nYou can then run it (giving it a name for the next step):\n$ docker run -it --name test top --ignored-param2\n\nMem: 1704184K used, 352484K free, 0K shrd, 0K buff, 140621524238337K cached\nCPU:   9% usr   2% sys   0% nic  88% idle   0% io   0% irq   0% sirq\nLoad average: 0.01 0.02 0.05 2/101 7\nPID  PPID USER     STAT   VSZ %VSZ %CPU COMMAND\n1     0 root     S     3168   0%   0% /bin/sh -c top -b cmd cmd2\n7     1 root     R     3164   0%   0% top -b\n\nYou can see from the output of top that the specified ENTRYPOINT is not PID 1.\nIf you then run docker stop test, the container will not exit cleanly - the\nstop command will be forced to send a SIGKILL after the timeout:\n$ docker exec -it test ps aux\n\nPID   USER     COMMAND\n1 root     /bin/sh -c top -b cmd cmd2\n7 root     top -b\n8 root     ps aux\n\n$ /usr/bin/time docker stop test\n\ntest\nreal\t0m 10.19s\nuser\t0m 0.04s\nsys\t0m 0.03s\n\nUnderstand how CMD and ENTRYPOINT interact\ud83d\udd17\nBoth CMD and ENTRYPOINT instructions define what command gets executed when running a container.\nThere are few rules that describe their co-operation.\n\n\nDockerfile should specify at least one of CMD or ENTRYPOINT commands.\n\n\nENTRYPOINT should be defined when using the container as an executable.\n\n\nCMD should be used as a way of defining default arguments for an ENTRYPOINT command\nor for executing an ad-hoc command in a container.\n\n\nCMD will be overridden when running the container with alternative arguments.\n\n\nThe table below shows what command is executed for different ENTRYPOINT / CMD combinations:\n\n\n\n\nNo ENTRYPOINT\nENTRYPOINT exec_entry p1_entry\nENTRYPOINT [\u201cexec_entry\u201d, \u201cp1_entry\u201d]\n\n\n\n\nNo CMD\nerror, not allowed\n/bin/sh -c exec_entry p1_entry\nexec_entry p1_entry\n\n\nCMD [\u201cexec_cmd\u201d, \u201cp1_cmd\u201d]\nexec_cmd p1_cmd\n/bin/sh -c exec_entry p1_entry\nexec_entry p1_entry exec_cmd p1_cmd\n\n\nCMD [\u201cp1_cmd\u201d, \u201cp2_cmd\u201d]\np1_cmd p2_cmd\n/bin/sh -c exec_entry p1_entry\nexec_entry p1_entry p1_cmd p2_cmd\n\n\nCMD exec_cmd p1_cmd\n/bin/sh -c exec_cmd p1_cmd\n/bin/sh -c exec_entry p1_entry\nexec_entry p1_entry /bin/sh -c exec_cmd p1_cmd\n\n\n\n\nNote\nIf CMD is defined from the base image, setting ENTRYPOINT will\nreset CMD to an empty value. In this scenario, CMD must be defined in the\ncurrent image to have a value.\n\nVOLUME\ud83d\udd17\nVOLUME [\"/data\"]\n\nThe VOLUME instruction creates a mount point with the specified name\nand marks it as holding externally mounted volumes from native host or other\ncontainers. The value can be a JSON array, VOLUME [\"/var/log/\"], or a plain\nstring with multiple arguments, such as VOLUME /var/log or VOLUME /var/log\n/var/db. For more information/examples and mounting instructions via the\nDocker client, refer to\nShare Directories via Volumes\ndocumentation.\nThe docker run command initializes the newly created volume with any data\nthat exists at the specified location within the base image. For example,\nconsider the following Dockerfile snippet:\nFROM ubuntu\nRUN mkdir /myvol\nRUN echo \"hello world\" > /myvol/greeting\nVOLUME /myvol\n\nThis Dockerfile results in an image that causes docker run to\ncreate a new mount point at /myvol and copy the  greeting file\ninto the newly created volume.\nNotes about specifying volumes\ud83d\udd17\nKeep the following things in mind about volumes in the Dockerfile.\n\n\nVolumes on Windows-based containers: When using Windows-based containers,\nthe destination of a volume inside the container must be one of:\n\na non-existing or empty directory\na drive other than C:\n\n\n\nChanging the volume from within the Dockerfile: If any build steps change the\ndata within the volume after it has been declared, those changes will be discarded.\n\n\nJSON formatting: The list is parsed as a JSON array.\nYou must enclose words with double quotes (\") rather than single quotes (').\n\n\nThe host directory is declared at container run-time: The host directory\n(the mountpoint) is, by its nature, host-dependent. This is to preserve image\nportability, since a given host directory can\u2019t be guaranteed to be available\non all hosts. For this reason, you can\u2019t mount a host directory from\nwithin the Dockerfile. The VOLUME instruction does not support specifying a host-dir\nparameter.  You must specify the mountpoint when you create or run the container.\n\n\nUSER\ud83d\udd17\nUSER <user>[:<group>]\n\nor\nUSER <UID>[:<GID>]\n\nThe USER instruction sets the user name (or UID) and optionally the user\ngroup (or GID) to use when running the image and for any RUN, CMD and\nENTRYPOINT instructions that follow it in the Dockerfile.\n\nNote that when specifying a group for the user, the user will have only the\nspecified group membership. Any other configured group memberships will be ignored.\n\n\nWarning\nWhen the user doesn\u2019t have a primary group then the image (or the next\ninstructions) will be run with the root group.\nOn Windows, the user must be created first if it\u2019s not a built-in account.\nThis can be done with the net user command called as part of a Dockerfile.\n\nFROM microsoft/windowsservercore\n# Create Windows user in the container\nRUN net user /add patrick\n# Set it for subsequent commands\nUSER patrick\n\nWORKDIR\ud83d\udd17\nWORKDIR /path/to/workdir\n\nThe WORKDIR instruction sets the working directory for any RUN, CMD,\nENTRYPOINT, COPY and ADD instructions that follow it in the Dockerfile.\nIf the WORKDIR doesn\u2019t exist, it will be created even if it\u2019s not used in any\nsubsequent Dockerfile instruction.\nThe WORKDIR instruction can be used multiple times in a Dockerfile. If a\nrelative path is provided, it will be relative to the path of the previous\nWORKDIR instruction. For example:\nWORKDIR /a\nWORKDIR b\nWORKDIR c\nRUN pwd\n\nThe output of the final pwd command in this Dockerfile would be /a/b/c.\nThe WORKDIR instruction can resolve environment variables previously set using\nENV. You can only use environment variables explicitly set in the Dockerfile.\nFor example:\nENV DIRPATH=/path\nWORKDIR $DIRPATH/$DIRNAME\nRUN pwd\n\nThe output of the final pwd command in this Dockerfile would be\n/path/$DIRNAME\nARG\ud83d\udd17\nARG <name>[=<default value>]\n\nThe ARG instruction defines a variable that users can pass at build-time to\nthe builder with the docker build command using the --build-arg <varname>=<value>\nflag. If a user specifies a build argument that was not\ndefined in the Dockerfile, the build outputs a warning.\n[Warning] One or more build-args [foo] were not consumed.\n\nA Dockerfile may include one or more ARG instructions. For example,\nthe following is a valid Dockerfile:\nFROM busybox\nARG user1\nARG buildno\n# ...\n\n\nWarning:\nIt is not recommended to use build-time variables for passing secrets like\ngithub keys, user credentials etc. Build-time variable values are visible to\nany user of the image with the docker history command.\nRefer to the \u201cbuild images with BuildKit\u201d\nsection to learn about secure ways to use secrets when building images.\n\nDefault values\ud83d\udd17\nAn ARG instruction can optionally include a default value:\nFROM busybox\nARG user1=someuser\nARG buildno=1\n# ...\n\nIf an ARG instruction has a default value and if there is no value passed\nat build-time, the builder uses the default.\nScope\ud83d\udd17\nAn ARG variable definition comes into effect from the line on which it is\ndefined in the Dockerfile not from the argument\u2019s use on the command-line or\nelsewhere.  For example, consider this Dockerfile:\nFROM busybox\nUSER ${user:-some_user}\nARG user\nUSER $user\n# ...\n\nA user builds this file by calling:\n$ docker build --build-arg user=what_user .\n\nThe USER at line 2 evaluates to some_user as the user variable is defined on the\nsubsequent line 3. The USER at line 4 evaluates to what_user as user is\ndefined and the what_user value was passed on the command line. Prior to its definition by an\nARG instruction, any use of a variable results in an empty string.\nAn ARG instruction goes out of scope at the end of the build\nstage where it was defined. To use an arg in multiple stages, each stage must\ninclude the ARG instruction.\nFROM busybox\nARG SETTINGS\nRUN ./run/setup $SETTINGS\n\nFROM busybox\nARG SETTINGS\nRUN ./run/other $SETTINGS\n\nUsing ARG variables\ud83d\udd17\nYou can use an ARG or an ENV instruction to specify variables that are\navailable to the RUN instruction. Environment variables defined using the\nENV instruction always override an ARG instruction of the same name. Consider\nthis Dockerfile with an ENV and ARG instruction.\nFROM ubuntu\nARG CONT_IMG_VER\nENV CONT_IMG_VER=v1.0.0\nRUN echo $CONT_IMG_VER\n\nThen, assume this image is built with this command:\n$ docker build --build-arg CONT_IMG_VER=v2.0.1 .\n\nIn this case, the RUN instruction uses v1.0.0 instead of the ARG setting\npassed by the user:v2.0.1 This behavior is similar to a shell\nscript where a locally scoped variable overrides the variables passed as\narguments or inherited from environment, from its point of definition.\nUsing the example above but a different ENV specification you can create more\nuseful interactions between ARG and ENV instructions:\nFROM ubuntu\nARG CONT_IMG_VER\nENV CONT_IMG_VER=${CONT_IMG_VER:-v1.0.0}\nRUN echo $CONT_IMG_VER\n\nUnlike an ARG instruction, ENV values are always persisted in the built\nimage. Consider a docker build without the --build-arg flag:\n$ docker build .\n\nUsing this Dockerfile example, CONT_IMG_VER is still persisted in the image but\nits value would be v1.0.0 as it is the default set in line 3 by the ENV instruction.\nThe variable expansion technique in this example allows you to pass arguments\nfrom the command line and persist them in the final image by leveraging the\nENV instruction. Variable expansion is only supported for a limited set of\nDockerfile instructions.\nPredefined ARGs\ud83d\udd17\nDocker has a set of predefined ARG variables that you can use without a\ncorresponding ARG instruction in the Dockerfile.\n\nHTTP_PROXY\nhttp_proxy\nHTTPS_PROXY\nhttps_proxy\nFTP_PROXY\nftp_proxy\nNO_PROXY\nno_proxy\n\nTo use these, simply pass them on the command line using the flag:\n--build-arg <varname>=<value>\n\nBy default, these pre-defined variables are excluded from the output of\ndocker history. Excluding them reduces the risk of accidentally leaking\nsensitive authentication information in an HTTP_PROXY variable.\nFor example, consider building the following Dockerfile using\n--build-arg HTTP_PROXY=http://user:pass@proxy.lon.example.com\nFROM ubuntu\nRUN echo \"Hello World\"\n\nIn this case, the value of the HTTP_PROXY variable is not available in the\ndocker history and is not cached. If you were to change location, and your\nproxy server changed to http://user:pass@proxy.sfo.example.com, a subsequent\nbuild does not result in a cache miss.\nIf you need to override this behaviour then you may do so by adding an ARG\nstatement in the Dockerfile as follows:\nFROM ubuntu\nARG HTTP_PROXY\nRUN echo \"Hello World\"\n\nWhen building this Dockerfile, the HTTP_PROXY is preserved in the\ndocker history, and changing its value invalidates the build cache.\nAutomatic platform ARGs in the global scope\ud83d\udd17\nThis feature is only available when using the BuildKit backend.\nDocker predefines a set of ARG variables with information on the platform of\nthe node performing the build (build platform) and on the platform of the\nresulting image (target platform). The target platform can be specified with\nthe --platform flag on docker build.\nThe following ARG variables are set automatically:\n\nTARGETPLATFORM - platform of the build result. Eg linux/amd64, linux/arm/v7, windows/amd64.\nTARGETOS - OS component of TARGETPLATFORM\nTARGETARCH - architecture component of TARGETPLATFORM\nTARGETVARIANT - variant component of TARGETPLATFORM\nBUILDPLATFORM - platform of the node performing the build.\nBUILDOS - OS component of BUILDPLATFORM\nBUILDARCH - architecture component of BUILDPLATFORM\nBUILDVARIANT - variant component of BUILDPLATFORM\n\nThese arguments are defined in the global scope so are not automatically\navailable inside build stages or for your RUN commands. To expose one of\nthese arguments inside the build stage redefine it without value.\nFor example:\nFROM alpine\nARG TARGETPLATFORM\nRUN echo \"I'm building for $TARGETPLATFORM\"\n\nImpact on build caching\ud83d\udd17\nARG variables are not persisted into the built image as ENV variables are.\nHowever, ARG variables do impact the build cache in similar ways. If a\nDockerfile defines an ARG variable whose value is different from a previous\nbuild, then a \u201ccache miss\u201d occurs upon its first usage, not its definition. In\nparticular, all RUN instructions following an ARG instruction use the ARG\nvariable implicitly (as an environment variable), thus can cause a cache miss.\nAll predefined ARG variables are exempt from caching unless there is a\nmatching ARG statement in the Dockerfile.\nFor example, consider these two Dockerfile:\nFROM ubuntu\nARG CONT_IMG_VER\nRUN echo $CONT_IMG_VER\n\nFROM ubuntu\nARG CONT_IMG_VER\nRUN echo hello\n\nIf you specify --build-arg CONT_IMG_VER=<value> on the command line, in both\ncases, the specification on line 2 does not cause a cache miss; line 3 does\ncause a cache miss.ARG CONT_IMG_VER causes the RUN line to be identified\nas the same as running CONT_IMG_VER=<value> echo hello, so if the <value>\nchanges, we get a cache miss.\nConsider another example under the same command line:\nFROM ubuntu\nARG CONT_IMG_VER\nENV CONT_IMG_VER=$CONT_IMG_VER\nRUN echo $CONT_IMG_VER\n\nIn this example, the cache miss occurs on line 3. The miss happens because\nthe variable\u2019s value in the ENV references the ARG variable and that\nvariable is changed through the command line. In this example, the ENV\ncommand causes the image to include the value.\nIf an ENV instruction overrides an ARG instruction of the same name, like\nthis Dockerfile:\nFROM ubuntu\nARG CONT_IMG_VER\nENV CONT_IMG_VER=hello\nRUN echo $CONT_IMG_VER\n\nLine 3 does not cause a cache miss because the value of CONT_IMG_VER is a\nconstant (hello). As a result, the environment variables and values used on\nthe RUN (line 4) doesn\u2019t change between builds.\nONBUILD\ud83d\udd17\nONBUILD <INSTRUCTION>\n\nThe ONBUILD instruction adds to the image a trigger instruction to\nbe executed at a later time, when the image is used as the base for\nanother build. The trigger will be executed in the context of the\ndownstream build, as if it had been inserted immediately after the\nFROM instruction in the downstream Dockerfile.\nAny build instruction can be registered as a trigger.\nThis is useful if you are building an image which will be used as a base\nto build other images, for example an application build environment or a\ndaemon which may be customized with user-specific configuration.\nFor example, if your image is a reusable Python application builder, it\nwill require application source code to be added in a particular\ndirectory, and it might require a build script to be called after\nthat. You can\u2019t just call ADD and RUN now, because you don\u2019t yet\nhave access to the application source code, and it will be different for\neach application build. You could simply provide application developers\nwith a boilerplate Dockerfile to copy-paste into their application, but\nthat is inefficient, error-prone and difficult to update because it\nmixes with application-specific code.\nThe solution is to use ONBUILD to register advance instructions to\nrun later, during the next build stage.\nHere\u2019s how it works:\n\nWhen it encounters an ONBUILD instruction, the builder adds a\ntrigger to the metadata of the image being built. The instruction\ndoes not otherwise affect the current build.\nAt the end of the build, a list of all triggers is stored in the\nimage manifest, under the key OnBuild. They can be inspected with\nthe docker inspect command.\nLater the image may be used as a base for a new build, using the\nFROM instruction. As part of processing the FROM instruction,\nthe downstream builder looks for ONBUILD triggers, and executes\nthem in the same order they were registered. If any of the triggers\nfail, the FROM instruction is aborted which in turn causes the\nbuild to fail. If all triggers succeed, the FROM instruction\ncompletes and the build continues as usual.\nTriggers are cleared from the final image after being executed. In\nother words they are not inherited by \u201cgrand-children\u201d builds.\n\nFor example you might add something like this:\nONBUILD ADD . /app/src\nONBUILD RUN /usr/local/bin/python-build --dir /app/src\n\n\nWarning\nChaining ONBUILD instructions using ONBUILD ONBUILD isn\u2019t allowed.\n\n\nWarning\nThe ONBUILD instruction may not trigger FROM or MAINTAINER instructions.\n\nSTOPSIGNAL\ud83d\udd17\nSTOPSIGNAL signal\n\nThe STOPSIGNAL instruction sets the system call signal that will be sent to the container to exit.\nThis signal can be a valid unsigned number that matches a position in the kernel\u2019s syscall table, for instance 9,\nor a signal name in the format SIGNAME, for instance SIGKILL.\nHEALTHCHECK\ud83d\udd17\nThe HEALTHCHECK instruction has two forms:\n\nHEALTHCHECK [OPTIONS] CMD command (check container health by running a command inside the container)\nHEALTHCHECK NONE (disable any healthcheck inherited from the base image)\n\nThe HEALTHCHECK instruction tells Docker how to test a container to check that\nit is still working. This can detect cases such as a web server that is stuck in\nan infinite loop and unable to handle new connections, even though the server\nprocess is still running.\nWhen a container has a healthcheck specified, it has a health status in\naddition to its normal status. This status is initially starting. Whenever a\nhealth check passes, it becomes healthy (whatever state it was previously in).\nAfter a certain number of consecutive failures, it becomes unhealthy.\nThe options that can appear before CMD are:\n\n--interval=DURATION (default: 30s)\n--timeout=DURATION (default: 30s)\n--start-period=DURATION (default: 0s)\n--retries=N (default: 3)\n\nThe health check will first run interval seconds after the container is\nstarted, and then again interval seconds after each previous check completes.\nIf a single run of the check takes longer than timeout seconds then the check\nis considered to have failed.\nIt takes retries consecutive failures of the health check for the container\nto be considered unhealthy.\nstart period provides initialization time for containers that need time to bootstrap.\nProbe failure during that period will not be counted towards the maximum number of retries.\nHowever, if a health check succeeds during the start period, the container is considered\nstarted and all consecutive failures will be counted towards the maximum number of retries.\nThere can only be one HEALTHCHECK instruction in a Dockerfile. If you list\nmore than one then only the last HEALTHCHECK will take effect.\nThe command after the CMD keyword can be either a shell command (e.g. HEALTHCHECK\nCMD /bin/check-running) or an exec array (as with other Dockerfile commands;\nsee e.g. ENTRYPOINT for details).\nThe command\u2019s exit status indicates the health status of the container.\nThe possible values are:\n\n0: success - the container is healthy and ready for use\n1: unhealthy - the container is not working correctly\n2: reserved - do not use this exit code\n\nFor example, to check every five minutes or so that a web-server is able to\nserve the site\u2019s main page within three seconds:\nHEALTHCHECK --interval=5m --timeout=3s \\\nCMD curl -f http://localhost/ || exit 1\n\nTo help debug failing probes, any output text (UTF-8 encoded) that the command writes\non stdout or stderr will be stored in the health status and can be queried with\ndocker inspect. Such output should be kept short (only the first 4096 bytes\nare stored currently).\nWhen the health status of a container changes, a health_status event is\ngenerated with the new status.\nThe HEALTHCHECK feature was added in Docker 1.12.\nSHELL\ud83d\udd17\nSHELL [\"executable\", \"parameters\"]\n\nThe SHELL instruction allows the default shell used for the shell form of\ncommands to be overridden. The default shell on Linux is [\"/bin/sh\", \"-c\"], and on\nWindows is [\"cmd\", \"/S\", \"/C\"]. The SHELL instruction must be written in JSON\nform in a Dockerfile.\nThe SHELL instruction is particularly useful on Windows where there are\ntwo commonly used and quite different native shells: cmd and powershell, as\nwell as alternate shells available including sh.\nThe SHELL instruction can appear multiple times. Each SHELL instruction overrides\nall previous SHELL instructions, and affects all subsequent instructions. For example:\nFROM microsoft/windowsservercore\n\n# Executed as cmd /S /C echo default\nRUN echo default\n\n# Executed as cmd /S /C powershell -command Write-Host default\nRUN powershell -command Write-Host default\n\n# Executed as powershell -command Write-Host hello\nSHELL [\"powershell\", \"-command\"]\nRUN Write-Host hello\n\n# Executed as cmd /S /C echo hello\nSHELL [\"cmd\", \"/S\", \"/C\"]\nRUN echo hello\n\nThe following instructions can be affected by the SHELL instruction when the\nshell form of them is used in a Dockerfile: RUN, CMD and ENTRYPOINT.\nThe following example is a common pattern found on Windows which can be\nstreamlined by using the SHELL instruction:\nRUN powershell -command Execute-MyCmdlet -param1 \"c:\\foo.txt\"\n\nThe command invoked by docker will be:\ncmd /S /C powershell -command Execute-MyCmdlet -param1 \"c:\\foo.txt\"\n\nThis is inefficient for two reasons. First, there is an un-necessary cmd.exe command\nprocessor (aka shell) being invoked. Second, each RUN instruction in the shell\nform requires an extra powershell -command prefixing the command.\nTo make this more efficient, one of two mechanisms can be employed. One is to\nuse the JSON form of the RUN command such as:\nRUN [\"powershell\", \"-command\", \"Execute-MyCmdlet\", \"-param1 \\\"c:\\\\foo.txt\\\"\"]\n\nWhile the JSON form is unambiguous and does not use the un-necessary cmd.exe,\nit does require more verbosity through double-quoting and escaping. The alternate\nmechanism is to use the SHELL instruction and the shell form,\nmaking a more natural syntax for Windows users, especially when combined with\nthe escape parser directive:\n# escape=`\n\nFROM microsoft/nanoserver\nSHELL [\"powershell\",\"-command\"]\nRUN New-Item -ItemType Directory C:\\Example\nADD Execute-MyCmdlet.ps1 c:\\example\\\nRUN c:\\example\\Execute-MyCmdlet -sample 'hello world'\n\nResulting in:\nPS E:\\docker\\build\\shell> docker build -t shell .\nSending build context to Docker daemon 4.096 kB\nStep 1/5 : FROM microsoft/nanoserver\n---> 22738ff49c6d\nStep 2/5 : SHELL powershell -command\n---> Running in 6fcdb6855ae2\n---> 6331462d4300\nRemoving intermediate container 6fcdb6855ae2\nStep 3/5 : RUN New-Item -ItemType Directory C:\\Example\n---> Running in d0eef8386e97\n\n\nDirectory: C:\\\n\n\nMode                LastWriteTime         Length Name\n----                -------------         ------ ----\nd-----       10/28/2016  11:26 AM                Example\n\n\n---> 3f2fbf1395d9\nRemoving intermediate container d0eef8386e97\nStep 4/5 : ADD Execute-MyCmdlet.ps1 c:\\example\\\n---> a955b2621c31\nRemoving intermediate container b825593d39fc\nStep 5/5 : RUN c:\\example\\Execute-MyCmdlet 'hello world'\n---> Running in be6d8e63fe75\nhello world\n---> 8e559e9bf424\nRemoving intermediate container be6d8e63fe75\nSuccessfully built 8e559e9bf424\nPS E:\\docker\\build\\shell>\n\nThe SHELL instruction could also be used to modify the way in which\na shell operates. For example, using SHELL cmd /S /C /V:ON|OFF on Windows, delayed\nenvironment variable expansion semantics could be modified.\nThe SHELL instruction can also be used on Linux should an alternate shell be\nrequired such as zsh, csh, tcsh and others.\nThe SHELL feature was added in Docker 1.12.\nExternal implementation features\ud83d\udd17\nThis feature is only available when using the  BuildKit backend.\nDocker build supports experimental features like cache mounts, build secrets and\nssh forwarding that are enabled by using an external implementation of the\nbuilder with a syntax directive. To learn about these features,\nrefer to the documentation in BuildKit repository.\nDockerfile examples\ud83d\udd17\nBelow you can see some examples of Dockerfile syntax.\n# Nginx\n#\n# VERSION               0.0.1\n\nFROM      ubuntu\nLABEL Description=\"This image is used to start the foobar executable\" Vendor=\"ACME Products\" Version=\"1.0\"\nRUN apt-get update && apt-get install -y inotify-tools nginx apache2 openssh-server\n\n# Firefox over VNC\n#\n# VERSION               0.3\n\nFROM ubuntu\n\n# Install vnc, xvfb in order to create a 'fake' display and firefox\nRUN apt-get update && apt-get install -y x11vnc xvfb firefox\nRUN mkdir ~/.vnc\n# Setup a password\nRUN x11vnc -storepasswd 1234 ~/.vnc/passwd\n# Autostart firefox (might not be the best way, but it does the trick)\nRUN bash -c 'echo \"firefox\" >> /.bashrc'\n\nEXPOSE 5900\nCMD    [\"x11vnc\", \"-forever\", \"-usepw\", \"-create\"]\n\n# Multiple images example\n#\n# VERSION               0.1\n\nFROM ubuntu\nRUN echo foo > bar\n# Will output something like ===> 907ad6c2736f\n\nFROM ubuntu\nRUN echo moo > oink\n# Will output something like ===> 695d7793cbe4\n\n# You'll now have two images, 907ad6c2736f with /bar, and 695d7793cbe4 with\n# /oink.\n\nbuilder, docker, Dockerfile, automation, image creationRate this page:\u00a01234\u00a0329\u00a0i\n\n\n\n\n\nCommand-line referenceDocker CLI (docker)Docker run referenceUse the Docker command linedocker (base command)docker appdocker appdocker app bundledocker app completiondocker app initdocker app inspectdocker app installdocker app listdocker app mergedocker app pulldocker app pushdocker app renderdocker app splitdocker app statusdocker app uninstalldocker app upgradedocker app validatedocker app versiondocker attachdocker builddocker builderdocker builderdocker builder builddocker builder prunedocker buildxdocker buildxdocker buildx bakedocker buildx builddocker buildx createdocker buildx dudocker buildx imagetoolsdocker buildx imagetools createdocker buildx imagetools inspectdocker buildx inspectdocker buildx lsdocker buildx prunedocker buildx rmdocker buildx stopdocker buildx usedocker buildx versiondocker checkpointdocker checkpointdocker checkpoint createdocker checkpoint lsdocker checkpoint rmdocker clusterdocker clusterdocker cluster backupdocker cluster createdocker cluster inspectdocker cluster lsdocker cluster restoredocker cluster rmdocker cluster updatedocker cluster versiondocker commitdocker configdocker configdocker config createdocker config inspectdocker config lsdocker config rmdocker containerdocker containerdocker container attachdocker container commitdocker container cpdocker container createdocker container diffdocker container execdocker container exportdocker container inspectdocker container killdocker container logsdocker container lsdocker container pausedocker container portdocker container prunedocker container renamedocker container restartdocker container rmdocker container rundocker container startdocker container statsdocker container stopdocker container topdocker container unpausedocker container updatedocker container waitdocker contextdocker contextdocker context createdocker context exportdocker context importdocker context inspectdocker context lsdocker context rmdocker context updatedocker context usedocker cpdocker createdocker diffdocker eventsdocker execdocker exportdocker historydocker imagedocker imagedocker image builddocker image historydocker image importdocker image inspectdocker image loaddocker image lsdocker image prunedocker image pulldocker image pushdocker image rmdocker image savedocker image tagdocker imagesdocker importdocker infodocker inspectdocker killdocker loaddocker logindocker logoutdocker logsdocker manifestdocker manifestdocker manifest annotatedocker manifest createdocker manifest inspectdocker manifest pushdocker networkdocker networkdocker network connectdocker network createdocker network disconnectdocker network inspectdocker network lsdocker network prunedocker network rmdocker nodedocker nodedocker node demotedocker node inspectdocker node lsdocker node promotedocker node psdocker node rmdocker node updatedocker pausedocker plugindocker plugindocker plugin createdocker plugin disabledocker plugin enabledocker plugin inspectdocker plugin installdocker plugin lsdocker plugin rmdocker plugin setdocker plugin upgradedocker portdocker psdocker pulldocker pushdocker registrydocker registrydocker registry eventsdocker registry historydocker registry infodocker registry inspectdocker registry joblogsdocker registry lsdocker registry rmidocker renamedocker restartdocker rmdocker rmidocker rundocker savedocker searchdocker secretdocker secretdocker secret createdocker secret inspectdocker secret lsdocker secret rmdocker servicedocker servicedocker service createdocker service inspectdocker service logsdocker service lsdocker service psdocker service rollbackdocker service rmdocker service scaledocker service updatedocker stackdocker stackdocker stack deploydocker stack psdocker stack rmdocker stack servicesdocker startdocker statsdocker stopdocker swarmdocker swarmdocker swarm cadocker swarm initdocker swarm join-tokendocker swarm joindocker swarm leavedocker swarm unlock-keydocker swarm unlockdocker swarm updatedocker systemdocker systemdocker system dfdocker system eventsdocker system infodocker system prunedocker tagdocker topdocker trustdocker trustdocker trust inspectdocker trust keydocker trust key generatedocker trust key loaddocker trust revokedocker trust signdocker trust signerdocker trust signer adddocker trust signer removedocker unpausedocker updatedocker versiondocker volumedocker volume createdocker volume inspectdocker volume lsdocker volume prunedocker volume rmdocker waitDocker Compose CLI referenceOverview of docker-compose CLICLI environment variablesCommand-line completiondocker-compose builddocker-compose configdocker-compose createdocker-compose downdocker-compose eventsdocker-compose execdocker-compose helpdocker-compose killdocker-compose logsdocker-compose pausedocker-compose portdocker-compose psdocker-compose pulldocker-compose pushdocker-compose restartdocker-compose rmdocker-compose rundocker-compose scaledocker-compose startdocker-compose stopdocker-compose topdocker-compose unpausedocker-compose upDaemon CLI (dockerd)API referenceDocker Engine APIOverviewSDKsSDK examplesv1.40 reference (latest)API reference by versionVersion history overviewv1.40 referencev1.39 referencev1.38 referencev1.37 referencev1.36 referencev1.35 referencev1.34 referencev1.33 referencev1.32 Referencev1.31 Referencev1.30 Referencev1.29 Referencev1.28 referencev1.27 referencev1.26 referencev1.25 referencev1.24 referencev1.23 referencev1.22 referencev1.21 referencev1.20 referencev1.19 referencev1.18 referenceRegistry APIDockerfile referenceCompose file referenceVersion 3Version 2Version 1About versions and upgradingFrequently asked questionsDrivers and specificationsRegistry image manifestsImage manifest v 2, schema 1Image manifest v 2, schema 2Update deprecated schema v1 imagesRegistry token authorizationDocker Registry token authenticationToken authentication implementationOauth2 token authenticationToken scope documentationToken authentication specificationRegistry storage driversStorage driver overviewAliyun OSS storage driverFilesystem storage driverGCS storage driverIn-memory storage driverMicrosoft Azure storage driverS3 storage driverSwift storage driverGlossary\n\n\n\n\n\n\n\nEdit this page Request docs changes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn this page:\n\nUsage\nBuildKit\nFormat\nParser directives\nsyntax\n\nOfficial releases\n\n\nescape\nEnvironment replacement\n.dockerignore file\nFROM\n\nUnderstand how ARG and FROM interact\n\n\nRUN\n\nKnown issues (RUN)\n\n\nCMD\nLABEL\nMAINTAINER (deprecated)\nEXPOSE\nENV\nADD\nCOPY\nENTRYPOINT\n\nExec form ENTRYPOINT example\nShell form ENTRYPOINT example\nUnderstand how CMD and ENTRYPOINT interact\n\n\nVOLUME\n\nNotes about specifying volumes\n\n\nUSER\nWORKDIR\nARG\n\nDefault values\nScope\nUsing ARG variables\nPredefined ARGs\nAutomatic platform ARGs in the global scope\nImpact on build caching\n\n\nONBUILD\nSTOPSIGNAL\nHEALTHCHECK\nSHELL\nExternal implementation features\nDockerfile examples\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhy Docker?\nWhat is a Container?\n\n\n\n\nProducts\nDocker Desktop\nDocker Hub\nFeatures\nContainer Runtime\nDeveloper Tools\nKubernetes\n\n\n\n\nDevelopers\nUse Cases\nPlay with Docker\nCommunity\nOpen Source\nDocker Captains\n\n\n\n\nCompany\nAbout Us\nBlog\nCustomers\nPartners\nNewsroom\nCareers\nContact Us\n\n\n\n\n\n\nStatus\nSecurity\nLegal\nContact\n\n\n\n\n\n\n\nCopyright \u00a9 2013-2020 Docker Inc. All rights reserved.\n\n\n\nTwitter\nYoutube\nGitHub\nLinkedin\nFacebook\nSlideshare\nReddit\n\n\n\n\n\n\n\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How do I copy directories recursively with gulp?", "id": 1655, "answers": [{"answer_id": 1643, "document_id": 1229, "question_id": 1655, "text": "The following works without flattening the folder structure:\ngulp.src(['input/folder/**/*']).pipe(gulp.dest('output/folder'));", "answer_start": 245, "answer_category": null}], "is_impossible": false}], "context": "I would expect to see all the files copied. However it flattens the dir structure - all directories are copied but every file is placed in the root /var/www\nGulp seems like a great build tool but copying items should be a simple process surely?\nThe following works without flattening the folder structure:\ngulp.src(['input/folder/**/*']).pipe(gulp.dest('output/folder'));\nThe '**/*' is the important part. That expression is a glob which is a powerful file selection tool. For example, for copying only .js files use: 'input/folder/**/*.js'\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to install android ndk in linux?", "id": 1223, "answers": [{"answer_id": 1216, "document_id": 799, "question_id": 1223, "text": "You can install NDK using the SDK Manager from within Android Studio\nFrom an open project, select Tools > Android > SDK Manager from the menu bar. Click the SDK Tools tab. Check the boxes next to LLDB, CMake, and NDK. Apply", "answer_start": 331, "answer_category": null}], "is_impossible": false}], "context": "I have downloaded android NDK from here: http://developer.android.com/tools/sdk/ndk/index.html\nfor Linux 64-bit (x86) android-ndk-r10c-linux-x86_64.bin. How I do install it? The instructions don't work. My OS is\n57-Ubuntu SMP Tue Jul 15 03:51:08 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux\nIf you already have AndroidStudio installed:\nYou can install NDK using the SDK Manager from within Android Studio\nFrom an open project, select Tools > Android > SDK Manager from the menu bar. Click the SDK Tools tab. Check the boxes next to LLDB, CMake, and NDK. Apply\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to pull elasticsearch docker image?", "id": 200685, "answers": [{"answer_id": 239830, "document_id": 357790, "question_id": 200685, "text": "docker pull docker.elastic.co/elasticsearch/elasticsearch:7.15.0", "answer_start": 834, "answer_category": null}], "is_impossible": false}, {"question": "Where is the Elasticsearch configuration files?", "id": 200691, "answers": [{"answer_id": 239855, "document_id": 357790, "question_id": 200691, "text": "/usr/share/elasticsearch/config/.", "answer_start": 10910, "answer_category": null}], "is_impossible": false}, {"question": "How to starting elasticsearch container with multiple nodes cluster?\n", "id": 200686, "answers": [{"answer_id": 239831, "document_id": 357790, "question_id": 200686, "text": "docker run -p 9200:9200 -p 9300:9300 -e \"discovery.type=single-node\" docker.elastic.co/elasticsearch/elasticsearch:7.15.0\n\n", "answer_start": 1083, "answer_category": null}], "is_impossible": false}, {"question": "How many memory is need for ES node?", "id": 200687, "answers": [{"answer_id": 239838, "document_id": 357790, "question_id": 200687, "text": "at least 4GiB", "answer_start": 3836, "answer_category": null}], "is_impossible": false}, {"question": "How to increase ulimits for nofile and nproc?", "id": 200688, "answers": [{"answer_id": 239841, "document_id": 357790, "question_id": 200688, "text": "docker run --rm centos:8 /bin/bash -c 'ulimit -Hn && ulimit -Sn && ulimit -Hu && ulimit -Su'\n", "answer_start": 8108, "answer_category": null}], "is_impossible": false}, {"question": "How to disable ElasticSearch node swapping?", "id": 200689, "answers": [{"answer_id": 239848, "document_id": 357790, "question_id": 200689, "text": "When using docker run, you can specify:\n-e \"bootstrap.memory_lock=true\" --ulimit memlock=-1:-1\n", "answer_start": 8692, "answer_category": null}], "is_impossible": false}, {"question": "Why ElasticSearch needs to bind data volumes? ", "id": 200690, "answers": [{"answer_id": 239849, "document_id": 357790, "question_id": 200690, "text": "The data of your Elasticsearch node won\u2019t be lost if the container is killed\n\n\nElasticsearch is I/O sensitive and the Docker storage driver is not ideal for fast I/O\n\n\nIt allows the use of advanced\nDocker volume plugins\n", "answer_start": 10217, "answer_category": null}], "is_impossible": false}, {"question": "How to avoid using loop-lvm mode?", "id": 200692, "answers": [{"answer_id": 239860, "document_id": 357790, "question_id": 200692, "text": "Configure docker-engine to use\ndirect-lvm.", "answer_start": 10558, "answer_category": null}], "is_impossible": false}, {"question": "How to randomly choose published ports?", "id": 200693, "answers": [{"answer_id": 239862, "document_id": 357790, "question_id": 200693, "text": "with --publish-all", "answer_start": 8917, "answer_category": null}], "is_impossible": false}], "context": "\n\nInstall Elasticsearch with Docker\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDocs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nElasticsearch Guide [7.15]\n\u00bb\nSet up Elasticsearch\n\u00bb\nInstalling Elasticsearch\n\u00bb\nInstall Elasticsearch with Docker\n\n\n\n\u00ab Install Elasticsearch with Windows MSI Installer\n\n\nInstall Elasticsearch on macOS with Homebrew \u00bb\n\n\n\n\nInstall Elasticsearch with Docker\n\nElasticsearch is also available as Docker images.\nThe images use centos:8 as the base image.\nA list of all published Docker images and tags is available at\nwww.docker.elastic.co. The source files\nare in\nGithub.\nThis package contains both free and subscription features.\nStart a 30-day trial to try out all of the features.\n\n\nPulling the image\n\nObtaining Elasticsearch for Docker is as simple as issuing a docker pull command\nagainst the Elastic Docker registry.\n\ndocker pull docker.elastic.co/elasticsearch/elasticsearch:7.15.0\n\n\n\n\nStarting a single node cluster with Docker\n\nTo start a single-node Elasticsearch cluster for development or testing, specify\nsingle-node discovery to bypass the bootstrap checks:\n\ndocker run -p 9200:9200 -p 9300:9300 -e \"discovery.type=single-node\" docker.elastic.co/elasticsearch/elasticsearch:7.15.0\n\n\n\n\nStarting a multi-node cluster with Docker Compose\n\nTo get a three-node Elasticsearch cluster up and running in Docker,\nyou can use Docker Compose:\n\n\n\nCreate a docker-compose.yml file:\n\n\n\n\nversion: '2.2'\nservices:\nes01:\nimage: docker.elastic.co/elasticsearch/elasticsearch:7.15.0\ncontainer_name: es01\nenvironment:\n- node.name=es01\n- cluster.name=es-docker-cluster\n- discovery.seed_hosts=es02,es03\n- cluster.initial_master_nodes=es01,es02,es03\n- bootstrap.memory_lock=true\n- \"ES_JAVA_OPTS=-Xms512m -Xmx512m\"\nulimits:\nmemlock:\nsoft: -1\nhard: -1\nvolumes:\n- data01:/usr/share/elasticsearch/data\nports:\n- 9200:9200\nnetworks:\n- elastic\nes02:\nimage: docker.elastic.co/elasticsearch/elasticsearch:7.15.0\ncontainer_name: es02\nenvironment:\n- node.name=es02\n- cluster.name=es-docker-cluster\n- discovery.seed_hosts=es01,es03\n- cluster.initial_master_nodes=es01,es02,es03\n- bootstrap.memory_lock=true\n- \"ES_JAVA_OPTS=-Xms512m -Xmx512m\"\nulimits:\nmemlock:\nsoft: -1\nhard: -1\nvolumes:\n- data02:/usr/share/elasticsearch/data\nnetworks:\n- elastic\nes03:\nimage: docker.elastic.co/elasticsearch/elasticsearch:7.15.0\ncontainer_name: es03\nenvironment:\n- node.name=es03\n- cluster.name=es-docker-cluster\n- discovery.seed_hosts=es01,es02\n- cluster.initial_master_nodes=es01,es02,es03\n- bootstrap.memory_lock=true\n- \"ES_JAVA_OPTS=-Xms512m -Xmx512m\"\nulimits:\nmemlock:\nsoft: -1\nhard: -1\nvolumes:\n- data03:/usr/share/elasticsearch/data\nnetworks:\n- elastic\n\nvolumes:\ndata01:\ndriver: local\ndata02:\ndriver: local\ndata03:\ndriver: local\n\nnetworks:\nelastic:\ndriver: bridge\n\n\n\n\nThis sample docker-compose.yml file uses the ES_JAVA_OPTS\nenvironment variable to manually set the heap size to 512MB. We do not recommend\nusing ES_JAVA_OPTS in production. See Manually set the heap size.\n\n\nThis sample Docker Compose file brings up a three-node Elasticsearch cluster.\nNode es01 listens on localhost:9200 and es02 and es03 talk to es01 over a Docker network.\nPlease note that this configuration exposes port 9200 on all network interfaces, and given how\nDocker manipulates iptables on Linux, this means that your Elasticsearch cluster is publically accessible,\npotentially ignoring any firewall settings. If you don\u2019t want to expose port 9200 and instead use\na reverse proxy, replace 9200:9200 with 127.0.0.1:9200:9200 in the docker-compose.yml file.\nElasticsearch will then only be accessible from the host machine itself.\nThe Docker named volumes\ndata01, data02, and data03 store the node data directories so the data persists across restarts.\nIf they don\u2019t already exist, docker-compose creates them when you bring up the cluster.\n\n\n\nMake sure Docker Engine is allotted at least 4GiB of memory.\nIn Docker Desktop, you configure resource usage on the Advanced tab in Preference (macOS)\nor Settings (Windows).\n\n\n\nDocker Compose is not pre-installed with Docker on Linux.\nSee docs.docker.com for installation instructions:\nInstall Compose on Linux\n\n\n\n\nRun docker-compose to bring up the cluster:\n\ndocker-compose up\n\n\n\nSubmit a _cat/nodes request to see that the nodes are up and running:\n\ncurl -X GET \"localhost:9200/_cat/nodes?v=true&pretty\"\n\n\n\n\nLog messages go to the console and are handled by the configured Docker logging driver.\nBy default you can access logs with docker logs. If you would prefer the Elasticsearch\ncontainer to write logs to disk, set the ES_LOG_STYLE environment variable to file.\nThis causes Elasticsearch to use the same logging configuration as other Elasticsearch distribution formats.\nTo stop the cluster, run docker-compose down.\nThe data in the Docker volumes is preserved and loaded\nwhen you restart the cluster with docker-compose up.\nTo delete the data volumes when you bring down the cluster,\nspecify the -v option: docker-compose down -v.\n\n\nStart a multi-node cluster with TLS enabled\n\nSee Encrypting communications in an Elasticsearch Docker Container and\nRun the Elastic Stack in Docker with TLS enabled.\n\n\n\n\nUsing the Docker images in production\n\nThe following requirements and recommendations apply when running Elasticsearch in Docker in production.\n\n\nSet vm.max_map_count to at least 262144\n\nThe vm.max_map_count kernel setting must be set to at least 262144 for production use.\nHow you set vm.max_map_count depends on your platform:\n\n\n\nLinux\nThe vm.max_map_count setting should be set permanently in /etc/sysctl.conf:\n\ngrep vm.max_map_count /etc/sysctl.conf\nvm.max_map_count=262144\n\nTo apply the setting on a live system, run:\n\nsysctl -w vm.max_map_count=262144\n\n\n\nmacOS with Docker for Mac\nThe vm.max_map_count setting must be set within the xhyve virtual machine:\n\n\n\nFrom the command line, run:\n\nscreen ~/Library/Containers/com.docker.docker/Data/vms/0/tty\n\n\n\nPress enter and use`sysctl` to configure vm.max_map_count:\n\nsysctl -w vm.max_map_count=262144\n\n\n\nTo exit the screen session, type Ctrl a d.\n\n\n\n\n\nWindows and macOS with Docker Desktop\nThe vm.max_map_count setting must be set via docker-machine:\n\ndocker-machine ssh\nsudo sysctl -w vm.max_map_count=262144\n\n\n\nWindows with Docker Desktop WSL 2 backend\nThe vm.max_map_count setting must be set in the docker-desktop container:\n\nwsl -d docker-desktop\nsysctl -w vm.max_map_count=262144\n\n\n\n\n\n\n\nConfiguration files must be readable by the elasticsearch user\n\nBy default, Elasticsearch runs inside the container as user elasticsearch using\nuid:gid 1000:0.\n\n\n\nOne exception is Openshift,\nwhich runs containers using an arbitrarily assigned user ID.\nOpenshift presents persistent volumes with the gid set to 0, which works without any adjustments.\n\n\nIf you are bind-mounting a local directory or file, it must be readable by the elasticsearch user.\nIn addition, this user must have write access to the config, data and log dirs\n(Elasticsearch needs write access to the config directory so that it can generate a keystore).\nA good strategy is to grant group access to gid 0 for the local directory.\nFor example, to prepare a local directory for storing data through a bind-mount:\n\nmkdir esdatadir\nchmod g+rwx esdatadir\nchgrp 0 esdatadir\n\nYou can also run an Elasticsearch container using both a custom UID and GID. Unless you\nbind-mount each of the config, data` and logs directories, you must pass\nthe command line option --group-add 0 to docker run. This ensures that the user\nunder which Elasticsearch is running is also a member of the root (GID 0) group inside the\ncontainer.\nAs a last resort, you can force the container to mutate the ownership of\nany bind-mounts used for the data and log dirs through the\nenvironment variable TAKE_FILE_OWNERSHIP. When you do this, they will be owned by\nuid:gid 1000:0, which provides the required read/write access to the Elasticsearch process.\n\n\n\nIncrease ulimits for nofile and nproc\n\nIncreased ulimits for nofile and nproc\nmust be available for the Elasticsearch containers.\nVerify the init system\nfor the Docker daemon sets them to acceptable values.\nTo check the Docker daemon defaults for ulimits, run:\n\ndocker run --rm centos:8 /bin/bash -c 'ulimit -Hn && ulimit -Sn && ulimit -Hu && ulimit -Su'\n\nIf needed, adjust them in the Daemon or override them per container.\nFor example, when using docker run, set:\n\n--ulimit nofile=65535:65535\n\n\n\n\nDisable swapping\n\nSwapping needs to be disabled for performance and node stability.\nFor information about ways to do this, see Disable swapping.\nIf you opt for the bootstrap.memory_lock: true approach,\nyou also need to define the memlock: true ulimit in the\nDocker Daemon,\nor explicitly set for the container as shown in the  sample compose file.\nWhen using docker run, you can specify:\n-e \"bootstrap.memory_lock=true\" --ulimit memlock=-1:-1\n\n\n\nRandomize published ports\n\nThe image exposes\nTCP ports 9200 and 9300. For production clusters, randomizing the\npublished ports with --publish-all is recommended,\nunless you are pinning one container per host.\n\n\n\nManually set the heap size\n\nBy default, Elasticsearch automatically sizes JVM heap based on a nodes\u2019s\nroles and the total memory available to the node\u2019s container. We\nrecommend this default sizing for most production environments. If needed, you\ncan override default sizing by manually setting JVM heap size.\nTo manually set the heap size in production, bind mount a JVM\noptions file under /usr/share/elasticsearch/config/jvm.options.d that\nincludes your desired heap size settings.\nFor testing, you can also manually set the heap size using the ES_JAVA_OPTS\nenvironment variable. For example, to use 16GB, specify -e\nES_JAVA_OPTS=\"-Xms16g -Xmx16g\" with docker run. The ES_JAVA_OPTS variable\noverrides all other JVM options. The ES_JAVA_OPTS variable overrides all other\nJVM options. We do not recommend using ES_JAVA_OPTS in production. The\ndocker-compose.yml file above sets the heap size to 512MB.\n\n\n\nPin deployments to a specific image version\n\nPin your deployments to a specific version of the Elasticsearch Docker image. For\nexample docker.elastic.co/elasticsearch/elasticsearch:7.15.0.\n\n\n\nAlways bind data volumes\n\nYou should use a volume bound on /usr/share/elasticsearch/data for the following reasons:\n\n\n\nThe data of your Elasticsearch node won\u2019t be lost if the container is killed\n\n\nElasticsearch is I/O sensitive and the Docker storage driver is not ideal for fast I/O\n\n\nIt allows the use of advanced\nDocker volume plugins\n\n\n\n\n\n\nAvoid using loop-lvm mode\n\nIf you are using the devicemapper storage driver, do not use the default loop-lvm mode.\nConfigure docker-engine to use\ndirect-lvm.\n\n\n\nCentralize your logs\n\nConsider centralizing your logs by using a different\nlogging driver. Also\nnote that the default json-file logging driver is not ideally suited for\nproduction use.\n\n\n\n\nConfiguring Elasticsearch with Docker\n\nWhen you run in Docker, the Elasticsearch configuration files are loaded from\n/usr/share/elasticsearch/config/.\nTo use custom configuration files, you bind-mount the files\nover the configuration files in the image.\nYou can set individual Elasticsearch configuration parameters using Docker environment variables.\nThe sample compose file and the\nsingle-node example use this method. You can\nuse the setting name directly as the environment variable name. If\nyou cannot do this, for example because your orchestration platform forbids\nperiods in environment variable names, then you can use an alternative\nstyle by converting the setting name as follows.\n\n\n\nChange the setting name to uppercase\n\n\nPrefix it with ES_SETTING_\n\n\nEscape any underscores (_) by duplicating them\n\n\nConvert all periods (.) to underscores (_)\n\n\n\nFor example, -e bootstrap.memory_lock=true becomes\n-e ES_SETTING_BOOTSTRAP_MEMORY__LOCK=true.\nYou can use the contents of a file to set the value of the\nELASTIC_PASSWORD or KEYSTORE_PASSWORD environment variables, by\nsuffixing the environment variable name with _FILE. This is useful for\npassing secrets such as passwords to Elasticsearch without specifying them directly.\nFor example, to set the Elasticsearch bootstrap password from a file, you can bind mount the\nfile and set the ELASTIC_PASSWORD_FILE environment variable to the mount location.\nIf you mount the password file to /run/secrets/bootstrapPassword.txt, specify:\n\n-e ELASTIC_PASSWORD_FILE=/run/secrets/bootstrapPassword.txt\n\nYou can override the default command for the image to pass Elasticsearch configuration\nparameters as command line options. For example:\n\ndocker run <various parameters> bin/elasticsearch -Ecluster.name=mynewclustername\n\nWhile bind-mounting your configuration files is usually the preferred method in production,\nyou can also create a custom Docker image\nthat contains your configuration.\n\n\nMounting Elasticsearch configuration files\n\nCreate custom config files and bind-mount them over the corresponding files in the Docker image.\nFor example, to bind-mount custom_elasticsearch.yml with docker run, specify:\n\n-v full_path_to/custom_elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml\n\n\n\n\nThe container runs Elasticsearch as user elasticsearch using\nuid:gid 1000:0. Bind mounted host directories and files must be accessible by this user,\nand the data and log directories must be writable by this user.\n\n\n\n\n\nCreate an encrypted Elasticsearch keystore\n\nBy default, Elasticsearch will auto-generate a keystore file for secure\nsettings. This file is obfuscated but not encrypted.\nTo encrypt your secure settings with a password and have them persist outside\nthe container, use a docker run command to manually create the keystore\ninstead. The command must:\n\n\n\nBind-mount the config directory. The command will create an\nelasticsearch.keystore file in this directory. To avoid errors, do\nnot directly bind-mount the elasticsearch.keystore file.\n\n\nUse the elasticsearch-keystore tool with the create -p option. You\u2019ll be\nprompted to enter a password for the keystore.\n\n\n\nFor example:\n\ndocker run -it --rm \\\n-v full_path_to/config:/usr/share/elasticsearch/config \\\ndocker.elastic.co/elasticsearch/elasticsearch:7.15.0 \\\nbin/elasticsearch-keystore create -p\n\nYou can also use a docker run command to add or update secure settings in the\nkeystore. You\u2019ll be prompted to enter the setting values. If the keystore is\nencrypted, you\u2019ll also be prompted to enter the keystore password.\n\ndocker run -it --rm \\\n-v full_path_to/config:/usr/share/elasticsearch/config \\\ndocker.elastic.co/elasticsearch/elasticsearch:7.15.0 \\\nbin/elasticsearch-keystore \\\nadd my.secure.setting \\\nmy.other.secure.setting\n\nIf you\u2019ve already created the keystore and don\u2019t need to update it, you can\nbind-mount the elasticsearch.keystore file directly. You can use the\nKEYSTORE_PASSWORD environment variable to provide the keystore password to the\ncontainer at startup. For example, a docker run command might have the\nfollowing options:\n\n-v full_path_to/config/elasticsearch.keystore:/usr/share/elasticsearch/config/elasticsearch.keystore\n-e KEYSTORE_PASSWORD=mypassword\n\n\n\n\nUsing custom Docker images\n\nIn some environments, it might make more sense to prepare a custom image that contains\nyour configuration. A Dockerfile to achieve this might be as simple as:\n\nFROM docker.elastic.co/elasticsearch/elasticsearch:7.15.0\nCOPY --chown=elasticsearch:elasticsearch elasticsearch.yml /usr/share/elasticsearch/config/\n\nYou could then build and run the image with:\n\ndocker build --tag=elasticsearch-custom .\ndocker run -ti -v /usr/share/elasticsearch/data elasticsearch-custom\n\nSome plugins require additional security permissions.\nYou must explicitly accept them either by:\n\n\n\nAttaching a tty when you run the Docker image and allowing the permissions when prompted.\n\n\nInspecting the security permissions and accepting them (if appropriate) by adding the --batch flag to the plugin install command.\n\n\n\nSee Plugin management\nfor more information.\n\n\n\n\nTroubleshoot Docker errors for Elasticsearch\n\nHere\u2019s how to resolve common errors when running Elasticsearch with Docker.\n\n\nelasticsearch.keystore is a directory\n\n\nException in thread \"main\" org.elasticsearch.bootstrap.BootstrapException: java.io.IOException: Is a directory: SimpleFSIndexInput(path=\"/usr/share/elasticsearch/config/elasticsearch.keystore\") Likely root cause: java.io.IOException: Is a directory\n\nA keystore-related docker run command attempted\nto directly bind-mount an elasticsearch.keystore file that doesn\u2019t exist. If\nyou use the -v or --volume flag to mount a file that doesn\u2019t exist, Docker\ninstead creates a directory with the same name.\nTo resolve this error:\n\n\n\nDelete the elasticsearch.keystore directory in the config directory.\n\n\nUpdate the -v or --volume flag to point to the config directory path\nrather than the keystore file\u2019s path. For an example, see\nCreate an encrypted Elasticsearch keystore.\n\n\nRetry the command.\n\n\n\n\n\n\nelasticsearch.keystore: Device or resource busy\n\n\nException in thread \"main\" java.nio.file.FileSystemException: /usr/share/elasticsearch/config/elasticsearch.keystore.tmp -> /usr/share/elasticsearch/config/elasticsearch.keystore: Device or resource busy\n\nA docker run command attempted to update the\nkeystore while directly bind-mounting the elasticsearch.keystore file. To\nupdate the keystore, the container requires access to other files in the\nconfig directory, such as keystore.tmp.\nTo resolve this error:\n\n\n\nUpdate the -v or --volume flag to point to the config directory\npath rather than the keystore file\u2019s path. For an example, see\nCreate an encrypted Elasticsearch keystore.\n\n\nRetry the command.\n\n\n\n\n\n\n\nNext steps\n\nYou now have a test Elasticsearch environment set up. Before you start\nserious development or go into production with Elasticsearch, you must do some additional\nsetup:\n\n\n\nLearn how to configure Elasticsearch.\n\n\nConfigure important Elasticsearch settings.\n\n\nConfigure important system settings.\n\n\n\n\n\n\n\n\u00ab Install Elasticsearch with Windows MSI Installer\n\n\nInstall Elasticsearch on macOS with Homebrew \u00bb\n\n\n\n\n\n\n\n\nMost Popular\n\nGet Started with Elasticsearch: Video\nIntro to Kibana: Video\nELK for Logs & Metrics: Video\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "installing gems using rvm", "id": 1781, "answers": [{"answer_id": 1767, "document_id": 1352, "question_id": 1781, "text": "There's a permission issue with your .gem folder. Make sure the owner is your current user.\nsudo chown -R tee /home/tee/.gem\nIf it doesn't work, remove the .gem folder. It is automatically created when you update the gem cache.", "answer_start": 381, "answer_category": null}], "is_impossible": false}], "context": "When trying to install gems using rvm i get this error\n$ rvm gem install sproutcore\nERROR:  While executing gem ... (Errno::EACCES)\nPermission denied - /home/tee/.gem/specs\nbut it says on the rvm site that you should not use sudo so I'm not sure whats wrong with my setup\nwhen i type $GEM_HOME it shows that the directory is pointing to the rvm dir\nAny ideas what I'm doing wrong?\nThere's a permission issue with your .gem folder. Make sure the owner is your current user.\nsudo chown -R tee /home/tee/.gem\nIf it doesn't work, remove the .gem folder. It is automatically created when you update the gem cache.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to include data files with the app's APK?", "id": 1136, "answers": [{"answer_id": 1129, "document_id": 713, "question_id": 1136, "text": "you should use /assets. These are not given any R-constants and can be scanned by your application\nTo open an asset-fil:\nInputStream is = getAssets().open(\"path/file.ext\");\nTo list a directory:\nString[] files = getAssets().list(\"\");", "answer_start": 305, "answer_category": null}], "is_impossible": false}], "context": "I want to create some pre-created files when my Android application is installed.\nI would like to create the file in both the internal memory (data/data//files/) and in a newly created sdcard directories (/sdcard//data1/).\nHow can I do this?\nIf you have a larger number of files and a directory structure you should use /assets. These are not given any R-constants and can be scanned by your application\nTo open an asset-fil:\nInputStream is = getAssets().open(\"path/file.ext\");\nTo list a directory:\nString[] files = getAssets().list(\"\");\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to install a leiningen plugin?", "id": 537, "answers": [{"answer_id": 539, "document_id": 262, "question_id": 537, "text": "With Leiningen 2.0 and greater you specify which plugins you want as values to :plugins in your project map. See the sample project.clj file. Note that \"sample\" is a bit of a misnomer, it's a reference for all possible (built-in) keys and documentation of their defaults.", "answer_start": 114, "answer_category": null}], "is_impossible": false}], "context": "I see this thing called \"clojars.org\", and how to \"push\" to it, but I don't see anything about \"pulling\" from it. With Leiningen 2.0 and greater you specify which plugins you want as values to :plugins in your project map. See the sample project.clj file. Note that \"sample\" is a bit of a misnomer, it's a reference for all possible (built-in) keys and documentation of their defaults.", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to resolve Error listenerStart when deploying web-app in Tomcat 5.5?", "id": 1724, "answers": [{"answer_id": 1712, "document_id": 1297, "question_id": 1724, "text": "After upgrading Java and changing JAVA_HOME in our startup script (/etc/init.d/tomcat) the error went away.", "answer_start": 203, "answer_category": null}], "is_impossible": false}], "context": "I encountered this error when the JDK that I compiled the app under was different from the tomcat JVM. I verified that the Tomcat manager was running jvm 1.6.0 but the app was compiled under java 1.7.0.\nAfter upgrading Java and changing JAVA_HOME in our startup script (/etc/init.d/tomcat) the error went away.\nI've deployed an Apache Wicket web-application that uses Spring and Hibernate to my Tomcat 5.5 instance. When I navigate to the Tomcat Manager interface I see that the web-application I deployed is not running. When I press 'Start' I get the following error message; \"FAIL - Application at context path /spaghetti could not be started\".\nMy catalina.log contains the following:\nApr 15, 2010 1:51:22 AM org.apache.catalina.loader.WebappClassLoader validateJarFile  \nINFO: validateJarFile(/var/lib/tomcat5.5/webapps/spaghetti/WEB-INF/lib/jsp-api-6.0.16.jar)   - jar not loaded. See Servlet Spec 2.3, section 9.7.2. Offending class: javax/servlet/jsp/JspPage.class  \nApr 15, 2010 1:51:22 AM org.apache.catalina.loader.WebappClassLoader validateJarFile  \nINFO: validateJarFile(/var/lib/tomcat5.5/webapps/spaghetti/WEB-INF/lib/servlet-api-6.0.16.jar) - jar not loaded. See Servlet Spec 2.3, section 9.7.2. Offending class: javax/servlet/Servlet.class  \nApr 15, 2010 1:51:24 AM org.apache.catalina.core.StandardContext start  \nSEVERE: Error listenerStart  \nApr 15, 2010 1:51:24 AM org.apache.catalina.core.StandardContext start  \nSEVERE: Context [/spaghetti] startup failed due to previous errors\nExcerpt from web.xml:\n    <listener>\n        <listener-class>\n            org.springframework.web.context.ContextLoaderListener\n        </listener-class>\n    </listener>\nAny help is greatly appreciated.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Best Practice for loading 3rd party JARs in JBoss AS7 standalone deployment?", "id": 472, "answers": [{"answer_id": 481, "document_id": 205, "question_id": 472, "text": "You can use Maven and the maven-dependency-plugin to produce modules with the transitive dependencies already included if you're willing to do a bit of work. You can see one example of that in a module I wrote for EclipseLink integration in AS 7. I automate the creation of AS7 modules whenever possible.", "answer_start": 318, "answer_category": null}], "is_impossible": false}], "context": "What is the best practice for loading 3rd party JARs in JBoss-as-7.0.x standalone deployment?\nI have tried:\ndeploying each JAR as an independent module with it's own module.xml desriptor;\ndeploying the JARs in the WEB-INF/lib directory of a WAR;\nand the foo.ear/lib directory for any JARs shared across multiple WARs.\nYou can use Maven and the maven-dependency-plugin to produce modules with the transitive dependencies already included if you're willing to do a bit of work. You can see one example of that in a module I wrote for EclipseLink integration in AS 7. I automate the creation of AS7 modules whenever possible.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Using WebDeploy with an Azure node website doesn't trigger npm install", "id": 1349, "answers": [{"answer_id": 1339, "document_id": 918, "question_id": 1349, "text": "if you want CI function, please setup continues deployment, here is tutorial for setting up local git https://azure.microsoft.com/en-us/documentation/articles/web-sites-publish-source-control/", "answer_start": 284, "answer_category": null}], "is_impossible": false}], "context": "I am using WebDeploy to deploy a node website to azure. I've seen in samples and demos that it should trigger a npm install on deploy. But it is not. I've also seen almost every demo uses git deployment. Is automatic npm install not supported for WebDeploy or am I missing something?\nif you want CI function, please setup continues deployment, here is tutorial for setting up local git https://azure.microsoft.com/en-us/documentation/articles/web-sites-publish-source-control/\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to install golang 3rd-party projects from download sources?", "id": 823, "answers": [{"answer_id": 818, "document_id": 505, "question_id": 823, "text": " Set GOPATH. Move code under $GOPATH. Then\ncd $GOPATH/src/github.com/user/package\ngo get . ", "answer_start": 183, "answer_category": null}], "is_impossible": false}], "context": "But it failed because of some cert issues.\nSo I manually download the sources of mgo to local E:\\mgo, but I don't know to how install it. So, what's the correct command to install it? Set GOPATH. Move code under $GOPATH. Then\ncd $GOPATH/src/github.com/user/package\ngo get . The fly in the ointment of all this, is that GOPATH is a path - similar to Java's classpath, or Unix's PATH. It is not a single directory location: it is a sequence of directory locations. \n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Error Installing Jekyll - Native Extension Build", "id": 850, "answers": [{"answer_id": 845, "document_id": 530, "question_id": 850, "text": "Sounds like you are missing the ruby-dev package, try installing it with sudo apt-get install ruby-dev and see if it works then with the newest version of Ruby.\n", "answer_start": 1696, "answer_category": null}], "is_impossible": false}], "context": "Installing Jekyll: ERROR: Failed to build gem native extension. #209\n Closed\nalexwoolley opened this issue on 10 Dec 2014 \u00b7 20 comments\n Closed\nInstalling Jekyll: ERROR: Failed to build gem native extension.\n#209\nalexwoolley opened this issue on 10 Dec 2014 \u00b7 20 comments\nComments\nAssignees\nNo one assigned\nLabels\nNone yet\nProjects\nNone yet\nMilestone\nNo milestone\nLinked pull requests\nSuccessfully merging a pull request may close this issue.\n\nNone yet\nNotifications\nCustomize\nYou\u2019re not receiving notifications from this thread.\n17 participants\n@redtrumpet\n@femontanha\n@schmanat\n@cesards\n@joaobentes\n@sondr3\n@marceloavan\n@tshriver\n@ragmha\n@dhanvi\n@atru\n@sara-02\n@redknitin\n@HieronyM\n@alexwoolley\n@dylan-cooper\n@msudgh\n@alexwoolley\nalexwoolley commented on 10 Dec 2014\nHello,\n\nCompletely new to Jekyll, so apologies if this is a stupid question. After running\n\ngem install jekyll\n\nI get the following error message:\n\nBuilding native extensions. This could take a while...\nERROR: Error installing jekyll:\nERROR: Failed to build gem native extension.\n\n``/usr/bin/ruby2.1 -r ./siteconf20141210-5038-1u40f3s.rb extconf.rb``\nmkmf.rb can't find header files for ruby at /usr/lib/ruby/include/ruby.h\n\nextconf failed, exit code 1\n\nGem files will remain installed in /var/lib/gems/2.1.0/gems/fast-stemmer-1.0.2 for inspection.\nResults logged to /var/lib/gems/2.1.0/extensions/x86_64-linux/2.1.0/fast-stemmer-1.0.2/gem_make.out\n\nSeems like the commonest solution to this is to run\n\nsudo apt-get install ruby1.9.1-dev\n\nBut this ain't solving the problem for me -- anyone got any tips..?\n\n@alexwoolley\nAuthor\nalexwoolley commented on 10 Dec 2014\nPS this is on Ubuntu\n\n@sondr3\nsondr3 commented on 11 Dec 2014\nSounds like you are missing the ruby-dev package, try installing it with sudo apt-get install ruby-dev and see if it works then with the newest version of Ruby.\n\n@redknitin\nredknitin commented on 12 Dec 2014\n@alexwoolley , as @sondr3 mentioned, the issue is most definitely because you are missing the ruby-dev package. You may also have to install other dependencies (Javascript and Python).\n\nThere is a jekyll package available for Ubuntu (look it up on Synaptic Package Manager) though I'm not sure of what version of jekyll it is. The package version is 0.11.2-1, which could possibly refer to Jekyll version 2.1 - that's just a guess from the package version and I could be wrong.\n\n@sondr3\nsondr3 commented on 13 Dec 2014\nDon't use the Jekyll package on Ubuntu, it is incredibly old and not supported at all, people have previously been burnt by it, stay with the version you get from the gem.\n\n@alexwoolley\nAuthor\nalexwoolley commented on 14 Dec 2014\nThanks so much guys -- so sorry I've taken a while to reply!\n\nAll's working now\n\n@alexwoolley alexwoolley closed this on 14 Dec 2014", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "ClickOnce deployment is leaving multiple versions (yes, more than two)", "id": 1052, "answers": [{"answer_id": 1047, "document_id": 633, "question_id": 1052, "text": "The cleanup attempts to run as the old version exits, but because you restarted the app, the new version is already starting, blocking the cleanup.", "answer_start": 745, "answer_category": null}], "is_impossible": false}], "context": "I've got a ClickOnce application that is leaving all old versions on my disk. It's an internal corporate application that gets frequent updates, so this is a disaster for rapidly inflating our backup size.\nAccording to the documentation and other Stack Overflow questions, it is supposed to only leave the current and previous versions on disk. However, each time I deploy the project and upgrade a client, I get another copy of all EXE, DLL and data files. I'm not making any changes whatsoever to the application, just pushing deploy again in Visual Studio.\nHow do I fix this problem?\nFor some unknown (and unacceptable, frankly) reason, the cleanup of older versions will not happen if the newer version is running -- it just fails silently.\nThe cleanup attempts to run as the old version exits, but because you restarted the app, the new version is already starting, blocking the cleanup.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "vagrant command not found after install on mac osx 10 10 4", "id": 1943, "answers": [{"answer_id": 1930, "document_id": 1522, "question_id": 1943, "text": "sudo mkdir /usr/local\n\nThen bin.\n\ncd /usr/local\nsudo mkdir bin", "answer_start": 1691, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nDownloaded Vagrant, and went through th installation process.\nWhen I ran vagrant -v it says vagrant: command not found\nIt has put the files in /opt/vagrant/...\n\nIt should install in the Applications folder with a link to the /usr/bin so it is added to the shell path.\nThere is no directory that has been created during this process.\nI am afraid I can't even get started with it. ITs obviously not creating all the shortcuts it needs to be able to run the commands.\n\nI have looked for support on this issue and reported a bug.\n\nI also accepted the xcode licence agreement.\n\nHas anybody else had this issue.\nMy next step is to manually create the shortcuts.\n    \n\nIf you look at the uninstall script including in the Vagrant DMG, it is referring to /usr/local/bin which does not exist. It should be error handling (the Vagrant pkg installer) and create it (permissions/ownership etc) but it doesn't.\n\nYou need to mkdir the /usr/local/bin. I tried a symlink between /usr/bin/vagrant (as /usr/bin exists, and in-path) to /opt/vagrant/bin/vagrant BUT this does not work, as later on in life, Vagrant refers in a hard fashion to /usr/local/bin/vagrant because its stupid.\n\n$ sudo mkdir /usr/local/bin\n\n\nIf you sudo it, it should be made with correct ownership etc. Now just re-run the Vagrant installer pkg.\n    \n\nMake sure the Virtual Box setup is installed on Mac before installing Vagrant. \n    \n\nI raised the bug with https://github.com/mitchellh/vagrant/issues/6034\nThe issue is that I had no /usr/local directory at all.\n\nIt is an apple issue.\nAfter I created the directories as you described I was able to install vagrant vitrual boax and laravel.\n\nHere is what I did.\n\nsudo mkdir /usr/local\n\nThen bin.\n\ncd /usr/local\nsudo mkdir bin\n\nI uninstalled Vagrant, re-installed and when I ran vagrant -v I then got the version.\n\ncd /usr/local/bin\nls\nsudo chmod 755 vagrant\n\nThanks for your Answer.\n    \n\nWeird, on 1.8.5 I couldn't run Vagrant either, noticed that /usr/local/bin was owned by root. Ran command \n\n# chown -R User:Group /usr/local/bin \n\n\nas root and works from my username now. \n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "JSF managed bean causing java.io.NotSerializableException during Tomcat deployment", "id": 1306, "answers": [{"answer_id": 1297, "document_id": 876, "question_id": 1306, "text": "Here's the relevant bit of the trace:\nSCHWERWIEGEND: Exception loading sessions from persistent storage\njava.io.WriteAbortedException: writing aborted; java.io.NotSerializableException: org.dhbw.stg.wwi2008c.mopro.ui.viewscoped.MachineReservationListBean\n     ...\n     at org.apache.catalina.session.StandardSession.readObject(StandardSession.java:1576)\n     at org.apache.catalina.session.StandardSession.readObjectData(StandardSession.java:1059)\n     at org.apache.catalina.session.StandardManager.doLoad(StandardManager.java:284)\n     at org.apache.catalina.session.StandardManager.load(StandardManager.java:204)\n     at org.apache.catalina.session.StandardManager.startInternal(StandardManager.java:465)\n     at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:140)\n     ...\nThis one concerns a different problem than the question which you linked. By default, when Tomcat shutdowns, it will serialize the HttpSession to disk which will then be reloaded from disk on startup so that the endusers can just continue with the browser session without losing any session data.", "answer_start": 262, "answer_category": null}], "is_impossible": false}], "context": "At deployment of my webApplication on Tomcat 7 I am getting the console output below. After restarting the server twice or three times it works without exceptions.\nI am using JSF, Tomcat and an RMI connection to businesslogic part (which shouldn't matter here?)\nHere's the relevant bit of the trace:\nSCHWERWIEGEND: Exception loading sessions from persistent storage\njava.io.WriteAbortedException: writing aborted; java.io.NotSerializableException: org.dhbw.stg.wwi2008c.mopro.ui.viewscoped.MachineReservationListBean\n     ...\n     at org.apache.catalina.session.StandardSession.readObject(StandardSession.java:1576)\n     at org.apache.catalina.session.StandardSession.readObjectData(StandardSession.java:1059)\n     at org.apache.catalina.session.StandardManager.doLoad(StandardManager.java:284)\n     at org.apache.catalina.session.StandardManager.load(StandardManager.java:204)\n     at org.apache.catalina.session.StandardManager.startInternal(StandardManager.java:465)\n     at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:140)\n     ...\nThis one concerns a different problem than the question which you linked. By default, when Tomcat shutdowns, it will serialize the HttpSession to disk which will then be reloaded from disk on startup so that the endusers can just continue with the browser session without losing any session data.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "What's the time cost of DSO at start time?", "id": 429, "answers": [{"answer_id": 438, "document_id": 177, "question_id": 429, "text": "20%", "answer_start": 8969, "answer_category": null}], "is_impossible": false}, {"question": "What's the time cost of DSO at execution time?", "id": 430, "answers": [{"answer_id": 439, "document_id": 177, "question_id": 430, "text": "5%", "answer_start": 9096, "answer_category": null}], "is_impossible": false}], "context": "\n\nDynamic Shared Object (DSO) Support\n\n\n\n\n\n\nModules | Directives | FAQ | Glossary | Sitemap\nApache HTTP Server Version 2.4\n\n\n\nApache > HTTP Server > Documentation > Version 2.4Dynamic Shared Object (DSO) Support\nThe Apache HTTP Server is a modular program where the\nadministrator can choose the functionality to include in the\nserver by selecting a set of modules.\nModules will be compiled as Dynamic Shared Objects (DSOs)\nthat exist separately from the main httpd\nbinary file. DSO modules may be compiled at the time the server\nis built, or they may be compiled and added at a later time\nusing the Apache Extension Tool (apxs).\nAlternatively, the modules can be statically compiled into\nthe httpd binary when the server is built.\nThis document describes how to use DSO modules as well as\nthe theory behind their use.\n\nImplementation\nUsage Summary\nBackground\nAdvantages and Disadvantages\n\n\n\nImplementation\nRelated ModulesRelated Directivesmod_soLoadModule\nThe DSO support for loading individual Apache httpd modules is based\non a module named mod_so which must be statically\ncompiled into the Apache httpd core. It is the only module besides\ncore which cannot be put into a DSO\nitself. Practically all other distributed Apache httpd modules will then\nbe placed into a DSO. After a module is compiled into a DSO named\nmod_foo.so you can use mod_so's LoadModule directive in your\nhttpd.conf file to load this module at server startup\nor restart.\nThe DSO builds for individual modules can be disabled via\nconfigure's --enable-mods-static\noption as discussed in the install\ndocumentation.\nTo simplify this creation of DSO files for Apache httpd modules\n(especially for third-party modules) a support program\nnamed apxs (APache\neXtenSion) is available. It can be used to build DSO based\nmodules outside of the Apache httpd source tree. The idea is\nsimple: When installing Apache HTTP Server the configure's\nmake install procedure installs the Apache httpd C\nheader files and puts the platform-dependent compiler and\nlinker flags for building DSO files into the apxs\nprogram. This way the user can use apxs to compile\nhis Apache httpd module sources without the Apache httpd distribution\nsource tree and without having to fiddle with the\nplatform-dependent compiler and linker flags for DSO\nsupport.\n\n\nUsage Summary\nTo give you an overview of the DSO features of Apache HTTP Server 2.x,\nhere is a short and concise summary:\n\n\nBuild and install a distributed Apache httpd module, say\nmod_foo.c, into its own DSO\nmod_foo.so:\n\n$ ./configure --prefix=/path/to/install --enable-foo\n$ make install\n\n\n\nConfigure Apache HTTP Server with all modules enabled. Only a basic\nset will be loaded during server startup. You can change the set of loaded\nmodules by activating or deactivating the LoadModule directives in\nhttpd.conf.\n\n$ ./configure --enable-mods-shared=all\n$ make install\n\n\n\nSome modules are only useful for developers and will not be build.\nwhen using the module set all. To build all available modules\nincluding developer modules use reallyall. In addition the\nLoadModule directives for all\nbuilt modules can be activated via the configure option\n--enable-load-all-modules.\n\n$ ./configure --enable-mods-shared=reallyall --enable-load-all-modules\n$ make install\n\n\n\nBuild and install a third-party Apache httpd module, say\nmod_foo.c, into its own DSO\nmod_foo.so outside of the Apache httpd\nsource tree using apxs:\n\n\n$ cd /path/to/3rdparty\n$ apxs -cia mod_foo.c\n\n\n\nIn all cases, once the shared module is compiled, you must\nuse a LoadModule\ndirective in httpd.conf to tell Apache httpd to activate\nthe module.\nSee the apxs documentation for more details.\n\n\nBackground\nOn modern Unix derivatives there exists a mechanism\ncalled dynamic linking/loading of Dynamic Shared\nObjects (DSO) which provides a way to build a piece of\nprogram code in a special format for loading it at run-time\ninto the address space of an executable program.\nThis loading can usually be done in two ways: automatically\nby a system program called ld.so when an\nexecutable program is started or manually from within the\nexecuting program via a programmatic system interface to the\nUnix loader through the system calls\ndlopen()/dlsym().\nIn the first way the DSO's are usually called shared\nlibraries or DSO libraries and named\nlibfoo.so or libfoo.so.1.2. They\nreside in a system directory (usually /usr/lib)\nand the link to the executable program is established at\nbuild-time by specifying -lfoo to the linker\ncommand. This hard-codes library references into the executable\nprogram file so that at start-time the Unix loader is able to\nlocate libfoo.so in /usr/lib, in\npaths hard-coded via linker-options like -R or in\npaths configured via the environment variable\nLD_LIBRARY_PATH. It then resolves any (yet\nunresolved) symbols in the executable program which are\navailable in the DSO.\nSymbols in the executable program are usually not referenced\nby the DSO (because it's a reusable library of general code)\nand hence no further resolving has to be done. The executable\nprogram has no need to do anything on its own to use the\nsymbols from the DSO because the complete resolving is done by\nthe Unix loader. (In fact, the code to invoke\nld.so is part of the run-time startup code which\nis linked into every executable program which has been bound\nnon-static). The advantage of dynamic loading of common library\ncode is obvious: the library code needs to be stored only once,\nin a system library like libc.so, saving disk\nspace for every program.\nIn the second way the DSO's are usually called shared\nobjects or DSO files and can be named with an\narbitrary extension (although the canonical name is\nfoo.so). These files usually stay inside a\nprogram-specific directory and there is no automatically\nestablished link to the executable program where they are used.\nInstead the executable program manually loads the DSO at\nrun-time into its address space via dlopen(). At\nthis time no resolving of symbols from the DSO for the\nexecutable program is done. But instead the Unix loader\nautomatically resolves any (yet unresolved) symbols in the DSO\nfrom the set of symbols exported by the executable program and\nits already loaded DSO libraries (especially all symbols from\nthe ubiquitous libc.so). This way the DSO gets\nknowledge of the executable program's symbol set as if it had\nbeen statically linked with it in the first place.\nFinally, to take advantage of the DSO's API the executable\nprogram has to resolve particular symbols from the DSO via\ndlsym() for later use inside dispatch tables\netc. In other words: The executable program has to\nmanually resolve every symbol it needs to be able to use it.\nThe advantage of such a mechanism is that optional program\nparts need not be loaded (and thus do not spend memory) until\nthey are needed by the program in question. When required,\nthese program parts can be loaded dynamically to extend the\nbase program's functionality.\nAlthough this DSO mechanism sounds straightforward there is\nat least one difficult step here: The resolving of symbols from\nthe executable program for the DSO when using a DSO to extend a\nprogram (the second way). Why? Because \"reverse resolving\" DSO\nsymbols from the executable program's symbol set is against the\nlibrary design (where the library has no knowledge about the\nprograms it is used by) and is neither available under all\nplatforms nor standardized. In practice the executable\nprogram's global symbols are often not re-exported and thus not\navailable for use in a DSO. Finding a way to force the linker\nto export all global symbols is the main problem one has to\nsolve when using DSO for extending a program at run-time.\nThe shared library approach is the typical one, because it\nis what the DSO mechanism was designed for, hence it is used\nfor nearly all types of libraries the operating system\nprovides.\n\n\nAdvantages and Disadvantages\nThe above DSO based features have the following\nadvantages:\n\nThe server package is more flexible at run-time because\nthe server process can be assembled at run-time via\nLoadModule\nhttpd.conf configuration directives instead of\nconfigure options at build-time. For instance,\nthis way one is able to run different server instances\n(standard & SSL version, minimalistic & dynamic\nversion [mod_perl, mod_php], etc.) with only one Apache httpd\ninstallation.\nThe server package can be easily extended with\nthird-party modules even after installation. This is\na great benefit for vendor package maintainers, who can create\nan Apache httpd core package and additional packages containing\nextensions like PHP, mod_perl, mod_security, etc.\nEasier Apache httpd module prototyping, because with the\nDSO/apxs pair you can both work outside the\nApache httpd source tree and only need an apxs -i\ncommand followed by an apachectl restart to\nbring a new version of your currently developed module into\nthe running Apache HTTP Server.\n\nDSO has the following disadvantages:\n\nThe server is approximately 20% slower at startup time\nbecause of the symbol resolving overhead the Unix loader now\nhas to do.\nThe server is approximately 5% slower at execution time\nunder some platforms, because position independent code (PIC)\nsometimes needs complicated assembler tricks for relative\naddressing, which are not necessarily as fast as absolute\naddressing.\nBecause DSO modules cannot be linked against other\nDSO-based libraries (ld -lfoo) on all platforms\n(for instance a.out-based platforms usually don't provide\nthis functionality while ELF-based platforms do) you cannot\nuse the DSO mechanism for all types of modules. Or in other\nwords, modules compiled as DSO files are restricted to only\nuse symbols from the Apache httpd core, from the C library\n(libc) and all other dynamic or static libraries\nused by the Apache httpd core, or from static library archives\n(libfoo.a) containing position independent code.\nThe only chances to use other code is to either make sure the\nhttpd core itself already contains a reference to it or\nloading the code yourself via dlopen().\n\n\n\nCopyright 2021 The Apache Software Foundation.Licensed under the Apache License, Version 2.0.\nModules | Directives | FAQ | Glossary | Sitemap\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "how to maintain long lived python projects w r t dependencies and python version on MAC OS?", "id": 1513, "answers": [{"answer_id": 1502, "document_id": 1089, "question_id": 1513, "text": "on Mac OS X, use only the python installation in /Library/Frameworks/Python.framework", "answer_start": 2512, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nshort version: how can I get rid of the multiple-versions-of-python nightmare ?\n\nlong version: over the years, I've used several versions of python, and what is worse, several extensions to python (e.g. pygame, pylab, wxPython...). Each time it was on a different setup, with different OSes, sometimes different architectures (like my old PowerPC mac).\n\nNowadays I'm using a mac (OSX 10.6 on x86-64) and it's a dependency nightmare each time I want to revive script older than a few months. Python itself already comes in three different flavours in /usr/bin (2.5, 2.6, 3.1), but I had to install 2.4 from macports for pygame, something else (cannot remember what) forced me to install all three others from macports as well, so at the end of the day I'm the happy owner of seven (!) instances of python on my system. \n\nBut that's not the problem, the problem is, none of them has the right (i.e. same set of) libraries installed, some of them are 32bits, some 64bits, and now I'm pretty much lost.\n\nFor example right now I'm trying to run a three-year-old script (not written by me) which used to use matplotlib/numpy to draw a real-time plot within a rectangle of a wxwidgets window. But I'm failing miserably: py26-wxpython from macports won't install, stock python has wxwidgets included but also has some conflict between 32 bits and 64 bits, and it doesn't have numpy... what a mess !\n\nObviously, I'm doing things the wrong way. How do you usally cope with all that chaos ?\n    \n\nI solve this using virtualenv. I sympathise with wanting to avoid further layers of nightmare abstraction, but virtualenv is actually amazingly clean and simple to use. You literally do this (command line, Linux):\n\nvirtualenv my_env\n\n\nThis creates a new python binary and library location, and symlinks to your existing system libraries by default. Then, to switch paths to use the new environment, you do this:\n\nsource my_env/bin/activate\n\n\nThat's it. Now if you install modules (e.g. with easy_install), they get installed to the lib directory of the my_env directory. They don't interfere with existing libraries, you don't get weird conflicts, stuff doesn't stop working in your old environment. They're completely isolated.\n\nTo exit the environment, just do\n\ndeactivate\n\n\nIf you decide you made a mistake with an installation, or you don't want that environment anymore, just delete the directory:\n\nrm -rf my_env\n\n\nAnd you're done. It's really that simple.\n\nvirtualenv is great. ;)\n    \n\nSome tips:\n\n\non Mac OS X, use only the python installation in /Library/Frameworks/Python.framework.\nwhenever you use numpy/scipy/matplotlib, install the enthought python distribution\nuse virtualenv and virtualenvwrapper to keep those \"system\" installations pristine; ideally use one virtual environment per project, so each project's dependencies are fulfilled. And, yes, that means potentially a lot of code will be replicated in the various virtual envs.\n\n\nThat seems like a bigger mess indeed, but at least things work that way. Basically, if one of the projects works in a virtualenv, it will keep working no matter what upgrades you perform, since you never change the \"system\" installs.\n    \n\nTake a look at virtualenv.\n    \n\nWhat I usually do is trying to (progressively) keep up with the Python versions as they come along (and once all of the external dependencies have correct versions available).\n\nMost of the time the Python code itself can be transferred as-is with only minor needed modifications.\n\nMy biggest Python project @ work (15.000+ LOC) is now on Python 2.6 a few months (upgrading everything from Python 2.5 did take most of a day due to installing / checking 10+ dependencies...)\n\nIn general I think this is the best strategy with most of the interdependent components in the free software stack (think the dependencies in the linux software repositories): keep your versions (semi)-current (or at least: progressing at the same pace).\n    \n\n\ninstall the python versions you need, better if from sources\nwhen you write a script, include the full python version into it (such as #!/usr/local/bin/python2.6)\n\n\nI can't see what could go wrong.\n\nIf something does, it's probably macports fault anyway, not yours (one of the reasons I don't use macports anymore).\n\nI know I'm probably missing something and this will get downvoted, but please leave at least a little comment in that case, thanks :)\n    \n\nI use the MacPorts version for everything, but as you note a lot of the default versions are bizarrely old.  For example vim omnicomplete in Snow Leopard has python25 as a dependency.  A lot of python related ports have old dependencies but you can usually flag the newer version at build time, for example port install vim +python26 instead of port install vim +python. Do a dry run before installing anything to see if you are pulling, for example, the whole of python24 when it isn't necessary.  Check portfiles often because the naming convention as Darwin ports was getting off the ground left something to be desired.  In practice I just leave everything in the default /opt... folders of MacPorts, including a copy of the entire framework with duplicates of PyObjC, etc., and just stick with one version at a time, retaining the option to return to the system default if things break unexpectedly.  Which is all perhaps a bit too much work to avoid using virtualenv, which I've been meaning to get around to using.\n    \n\nI've had good luck using Buildout.  You set up a list of which eggs and which versions you want. Buildout then downloads and installs private versions of each for you.  It makes a private \"python\" binary with all the eggs already installed.  A local \"nosetests\" makes things easy to debug.  You can extend the build with your own functions.\n\nOn the down side, Buildout can be quite mysterious. Do \"buildout -vvvv\" for a while to see exactly what it's doing and why.\n\nhttp://www.buildout.org/docs/tutorial.html\n    \n\nAt least under Linux, multiple pythons can co-exist fairly happily. I use Python 2.6 on a CentOS system that needs Python 2.4 to be the default for various system things. I simply compiled and installed python 2.6 into a separate directory tree (and added the appropriate bin directory to my path) which was fairly painless. It's then invoked by typing \"python2.6\".\n\nOnce you have separate pythons up and running, installing libraries for a specific version is straightforward. If you invoke the setup.py script with the python you want, it will be installed in directories appropriate to that python, and scripts will be installed in the same directory as the python executable itself and will automagically use the correct python when invoked.\n\nI also try to avoid using too many libraries. When I only need one or two functions from a library (eg scipy), I'll often see if I can just copy them to my own project.\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to build & install GLFW 3 and use it in a Linux project?", "id": 870, "answers": [{"answer_id": 865, "document_id": 550, "question_id": 870, "text": "\nsudo apt-get update\nsudo apt-get install libglfw3\nsudo apt-get install libglfw3-dev", "answer_start": 1769, "answer_category": null}], "is_impossible": false}], "context": "Better ubuntu (apt-get) install instructions #808\n Open\ntaiya opened this issue on 15 Jul 2016 \u00b7 13 comments\nComments\nAssignees\nNo one assigned\nLabels\ndocumentation\nenhancement\nX11\nProjects\nNone yet\nMilestone\n3.4\nLinked pull requests\nSuccessfully merging a pull request may close this issue.\n\nNone yet\nNotifications\nCustomize\nYou\u2019re not receiving notifications from this thread.\n9 participants\n@elmindreda\n@jcowgill\n@tombsar\n@taiya\n@skippa\n@kevinkit\n@qyalexwen\n@zheyuanWang\n@twinblasterrex\n@taiya\n \ntaiya commented on 15 Jul 2016\nI know how to compile from sources, but it would be great if you could provide instructions on how to install glfw3 (system/root level) by leveraging the debian packages (ubuntu).\n\nIt seems there are some repositories containing glfw3, but from this page it is not clear which repositories should be added: https://launchpad.net/ubuntu/+source/glfw3\n\nThis stackoverflow post also discusses similar issues, but there is not straightforward solution that was identified: http://stackoverflow.com/questions/17768008/how-to-build-install-glfw-3-and-use-it-in-a-linux-project\n\nThis is another github issue that seems to address a similar problem:\nfilipwasil/fillwave#58\n\nAs you can see, it seems to be a pretty widespread question! :)\n\n@elmindreda elmindreda added the documentation label on 15 Jul 2016\n@elmindreda elmindreda self-assigned this on 15 Jul 2016\n@elmindreda elmindreda added the X11 label on 15 Jul 2016\n@taiya\n \nAuthor\ntaiya commented on 15 Jul 2016\nBy the way, this \"works\" but I don't love the fact I rely on an independent \"keithw\" repository!\n\n(Tested on a virgin copy of Ubuntu 16.04)\nAdd the following line to /etc/apt/sources.list\n\ndeb http://ppa.launchpad.net/keithw/glfw3/ubuntu trusty main\nThen execute these commands\n\nsudo apt-get update\nsudo apt-get install libglfw3\nsudo apt-get install libglfw3-dev\nThe library should now be available here:\n\n/usr/lib/x86_64-linux-gnu/libglfw.so\n/usr/lib/x86_64-linux-gnu/libglfw.so.3\n/usr/lib/x86_64-linux-gnu/libglfw.so.3.1\nAnd the headers here:\n\n/usr/include/GLFW/glfw3.h\n@jcowgill\n \njcowgill commented on 15 Jul 2016\nThe first link you posted is to the main Ubuntu repositories. If you have 14.10 or later and you only want a shared library, you can just run this to get glfw3:\n\nsudo apt-get install libglfw3-dev\nYou don't need any extra repositories.\n\n@taiya\n \nAuthor\ntaiya commented on 16 Jul 2016 \u2022 \nRight, I guess I assumed I needed this one, which was not on the repo. Nonetheless, a google search didn't pop up anything on how to get glfw3 installed on Ubuntu beside what I show above. Perhaps you should include what you say above on your official page?\n\nsudo apt-get install libglfw3-dev\n\n@elmindreda\n \nMember\nelmindreda commented on 21 Jul 2016\nThe main reason there aren't any package installation instructions any more is that many distros have often lagged far behind the current stable version and some (not mentioning any names of Fedora) used to ship with random, buggy pre-release snapshots.\n\nUsing the distro package is fine if one is making FOSS that will also be packaged or is making personal projects where stability and transferability aren't important. Instead I try to make GLFW easy to build as part of a larger project, like fillwave is doing.\n\n@taiya\n \nAuthor\ntaiya commented on 21 Jul 2016\n@elmindreda I am not saying that having a great build-from-sources system is not good, but having clear instructions on how to fetch it in standard distros simplifies people's life substantially.\nWhen you have 20+ libraries to install, compiling each from sources is cumbersome.\n\nFurther, I use this library for student assignment/projects, and as they are not very experienced C++ developers, a non packaged library can create a substantial problem...\n\n@elmindreda elmindreda added the enhancement label on 21 Jul 2016\n@elmindreda elmindreda added this to the 3.3 milestone on 21 Jul 2016\n@elmindreda\n \nMember\nelmindreda commented on 2 Aug 2016\nI will look into whether the current set of packages are worth mentioning for 3.3.\n\n@elmindreda elmindreda removed their assignment on 2 May 2017\n@kevinkit\n \nkevinkit commented on 11 May 2017\n+1\n\n@elmindreda elmindreda removed this from the 3.3 milestone on 11 Sep 2017\n@elmindreda elmindreda added this to the 3.4 milestone on 11 Sep 2017\n@elmindreda elmindreda added this to the 3.4 milestone on 11 Sep 2017\n@elmindreda elmindreda removed this from the 3.3 milestone on 11 Sep 2017\n@qianyizh qianyizh mentioned this issue on 25 Oct 2017\nInitialization fails on Xdummy #1004\n Open\n@skippa\n \nskippa commented on 7 Sep 2018\nI'm using Elementary Loki, and I need to install glfw 3.2.1 or greater, however I can't as it says the newest one is installed. How do I force the installation of the latest one, not the latest one as far as the Elementary OS repo's say?\n\n@dorodnic dorodnic mentioned this issue on 14 Jan 2019\nUpdating to GLFW 3.3 IntelRealSense/librealsense#3051\n Merged\n@michaelachrisco michaelachrisco mentioned this issue on 28 Oct 2019\nUnable to run the example tetris vlang/v#2518\n Closed\n@zheyuanWang\n \nzheyuanWang commented on 14 Dec 2019 \u2022 \nif I download the zip from https://www.glfw.org/\n\nhow can I let it be properly imported then? I found the code in the binding tool of glfw, the library is supposed to be imported with\n\nfrom .library import glfw as _glfw\nDoes it mean that if I unzip the downloaded zip file in the right place, the glfw could find its library?\nEdit1:\nI followed the guide from https://www.glfw.org/docs/latest/compile.html#compile_generate to compile the downloaded zip file\n\n@qyalexwen\n \nqyalexwen commented on 24 May 2020\nAfter installation, I found these files:\n$ ls libglfw*\nlibglfw.so libglfw.so.3 libglfw.so.3.2\n\nBut when I tried to link files, it still can not find glfw3:\n\n$ g++ main.cpp -lglfw3\n/usr/bin/ld: cannot find -lglfw3\ncollect2: error: ld returned 1 exit status\n\nThe main.cpp is from this link:\nhttps://www.glfw.org/documentation.html\n\n@taiya\n \nAuthor\ntaiya commented on 24 May 2020 \u2022 \nsudo updatedb #< wait a few minutes\nlocate libglfw #< find out where\nexport LD_LIBRARY_PATH=\"...\" #< folder where locate found it\nCheers\n\n@tombsar\n \nContributor\ntombsar commented on 24 May 2020\n@qyalexwen For linking to the shared library (*.so) you should use -lglfw, not -lglfw3.\n\n@twinblasterrex\n \ntwinblasterrex commented on 15 Jun\nhow do I do this for fedora linux", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "What's the difference between require and require-dev?", "id": 1627, "answers": [{"answer_id": 1615, "document_id": 1201, "question_id": 1627, "text": "The require-dev packages are packages that aren't necessary for your project to work and shouldn't be included in the production version of your project.\nTypically, these are packages such as phpunit/phpunit that you would only use during development.", "answer_start": 319, "answer_category": null}], "is_impossible": false}], "context": "I'm new to the composer and I would like to know the difference between require and require-dev. The composer website doesn't offer a good explanation the difference between these two.\nThe part that I don't get is Lists packages required for developing this package, or running tests, etc. from Composer Official Docs.\nThe require-dev packages are packages that aren't necessary for your project to work and shouldn't be included in the production version of your project.\nTypically, these are packages such as phpunit/phpunit that you would only use during development.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to configure teamcity to build deployment package of asp.net mvc web project and put it into specified directory", "id": 1345, "answers": [{"answer_id": 1335, "document_id": 914, "question_id": 1345, "text": "You can do the following step and add next build parameters\n/p:Configuration=QA\n/p:DeployOnBuild=true \n/p:PublishDir=M:\\MyPackage", "answer_start": 366, "answer_category": null}], "is_impossible": false}], "context": "I created build step with type \"MSBuild\", set Target to \"Clean;Build;Publish\", added command line parameters to /p:Configuration=Release;PublishDir=M:\\MyPackage\nafter running configuration I got \"success\" status but M:\\MyPackage folder is empty.\nI need just revive deployment package files in directory on same computer but do not deploy to server or somewhere else\nYou can do the following step and add next build parameters\n/p:Configuration=QA\n/p:DeployOnBuild=true \n/p:PublishDir=M:\\MyPackage\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "\"pip install unroll\": \"python setup.py egg_info\" failed with error code 1", "id": 1606, "answers": [{"answer_id": 1593, "document_id": 1180, "question_id": 1606, "text": "You can try this after upgrading pip:\npython -m pip install --upgrade pip\npip install \"package-name\"", "answer_start": 255, "answer_category": null}], "is_impossible": false}], "context": "I'm new to Python and have been trying to install some packages with pip.\nBut pip install unroll gives me\nCommand \"python setup.py egg_info\" failed with error code 1 in C:\\Users\\MARKAN~1\\AppData\\Local\\Temp\\pip-build-wa7uco0k\\unroll\\\nHow can I solve this?\nYou can try this after upgrading pip:\npython -m pip install --upgrade pip\npip install \"package-name\"\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "The executable was signed with invalid entitlements", "id": 767, "answers": [{"answer_id": 767, "document_id": 454, "question_id": 767, "text": "Make sure your device is included in the dev provisioning profile you want to use. Somehow the error message is misleading", "answer_start": 670, "answer_category": null}], "is_impossible": false}], "context": "I am having a problem with ad-hoc distribution on my iPhone. I have developed an application with SDK 3.0. I have a developer's license. I have added certificates and provisioning profiles in my project. So, no problem with that.\nBut, when I try to install the app on my iPhone, it compiles the project and then displays the error: \"The executable was signed with invalid entitlements\" in the Organizer window. Am I missing something? I have upgraded my iPhone from 2.2.1 and have downloaded latest SDK from Apple.\nPlease help me with this issue. This error also may occur if you're trying to profile an app where the device is not included in the provisioning profile.\nMake sure your device is included in the dev provisioning profile you want to use. Somehow the error message is misleading. My entitlements were actually ok. I just had this happen to a developer on the team I administer.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "mysql data directory location?", "id": 861, "answers": [{"answer_id": 856, "document_id": 541, "question_id": 861, "text": "/usr/local/var/mysql", "answer_start": 501, "answer_category": null}], "is_impossible": false}], "context": "\n                \nI installed mysql in Mac after downloding its dmg file version 64 bit.\nWhile trying to create a database it gave me error 1006 -- can't create database. After browsing a number of website, it seems due to user ownership setting of mysql \"data directory\" location that needs to be changed.\n\nWhere is mysql default \"data directory\"? I could not find /var/lib/mysql in localhost.\n\nTHANKS.\n    \n\nIf the software is Sequel pro the default install mysql on Mac OSX has data located here:\n\n/usr/local/var/mysql\n\n    \nSee if you have a file located under /etc/my.cnf.  If so, it should tell you where the data directory is. \n\nFor example:\n\n[mysqld]\nset-variable=local-infile=0\ndatadir=/var/lib/mysql\nsocket=/var/lib/mysql/mysql.sock\nuser=mysql\n...\n\n\nMy guess is that your mysql might be installed to /usr/local/mysql-XXX.\n\nYou may find these MySQL reference manual links useful:\n\n\nInstalling MySQL 8.0 on MacOS\nInstalling MySQL 5.7 on MacOS\nInstalling MySQL 5.6 on MacOS\nInstalling MySQL 5.5 on MacOS\n\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "get installed applications in a system?", "id": 862, "answers": [{"answer_id": 857, "document_id": 542, "question_id": 862, "text": "The Win32_Product WMI class represents products as they are installed by Windows Installer. A product generally correlates to one installation package.", "answer_start": 0, "answer_category": null}], "is_impossible": false}], "context": "The Win32_Product WMI class represents products as they are installed by Windows Installer. A product generally correlates to one installation package.\n\nNote  For more information about support or requirements for installation of a specific operating system, see Operating System Availability of WMI Components.\n\n \n\nThe following syntax is simplified from Managed Object Format (MOF) code and includes all inherited properties. Properties and methods are in alphabetic order, not MOF order.\n\nSyntax\n\nCopy\n[Provider(\"MSIProv\"), Dynamic]\nclass Win32_Product : CIM_Product\n{\n  uint16   AssignmentType;\n  string   Caption;\n  string   Description;\n  string   IdentifyingNumber;\n  string   InstallDate;\n  datetime InstallDate2;\n  string   InstallLocation;\n  sint16   InstallState;\n  string   HelpLink;\n  string   HelpTelephone;\n  string   InstallSource;\n  string   Language;\n  string   LocalPackage;\n  string   Name;\n  string   PackageCache;\n  string   PackageCode;\n  string   PackageName;\n  string   ProductID;\n  string   RegOwner;\n  string   RegCompany;\n  string   SKUNumber;\n  string   Transforms;\n  string   URLInfoAbout;\n  string   URLUpdateInfo;\n  string   Vendor;\n  uint32   WordCount;\n  string   Version;\n};\nMembers\nThe Win32_Product class has these types of members:\n\nMethods\nProperties\nMethods\nThe Win32_Product class has these methods.\n\nMETHODS\nMethod\tDescription\nAdmin\t\nPerforms an administrative install of an associated Win32_Product instance using the installation package provided through PackageLocation, and any command line options that are supplied.\n\nAdvertise\t\nAdvertises an associated Win32_Product instance using the installation package provided through PackageLocation and any command line options that are supplied.\n\nConfigure\t\nConfigures the associated instance of Win32_Product to the specified install state and level.\n\nInstall\t\nInstalls an associated Win32_Product instance using the installation package provided through PackageLocation and any command line options that are supplied.\n\nReinstall\t\nReinstalls the associated instance of Win32_Product using the specified reinstallation mode.\n\nUninstall\t\nUninstalls the associated instance of Win32_Product.\n\nUpgrade\t\nUpgrades the associated Win32_Product instance using the upgrade package provided through PackageLocation and any command line options that are supplied.\n\n \n\nProperties\nThe Win32_Product class has these properties.\n\nAssignmentType\n\nData type: uint16\n\nAccess type: Read-only\n\nAssignment type of the product.\n\nWindows Server 2003: This property is not available.\n\nPossible values are\n\nTABLE 2\nValue\tMeaning\n0\t\nThe product is assigned by user.\n\n1\t\nThe product is assigned by computer.\n\n \n\nCaption\n\nData type: string\n\nAccess type: Read-only\n\nShort textual description for the product\u2014a one-line string.\n\nDescription\n\nData type: string\n\nAccess type: Read-only\n\nDescription of the product.\n\nHelpLink\n\nData type: string\n\nAccess type: Read-only\n\nThe support link for the product.\n\nWindows Server 2003: This property is not available.\n\nHelpTelephone\n\nData type: string\n\nAccess type: Read-only\n\nThe support telephone for the product.\n\nWindows Server 2003: This property is not available.\n\nIdentifyingNumber\n\nData type: string\n\nAccess type: Read-only\n\nProduct identification such as a serial number on software, or a die number on a hardware chip.\n\nInstallDate\n\nData type: string\n\nAccess type: Read-only\n\nThis property is no longer supported for Win32_Product. Instead, use the InstallDate2 property, which is in the WMI CIM_DATETIME format.\n\nInstallDate2\n\nData type: datetime\n\nAccess type: Read-only\n\nDate that this product was installed on the system. This property does not require a value to indicate that the object is installed. For more information about WMI dates and times, see Date and Time Format.\n\nInstallLocation\n\nData type: string\n\nAccess type: Read-only\n\nLocation of the installed product.\n\nInstallSource\n\nData type: string\n\nAccess type: Read-only\n\nThe installation source directory of the product.\n\nWindows Server 2003: This property is not available.\n\nInstallState\n\nData type: sint16\n\nAccess type: Read-only\n\nInstalled state of the product.\n\nTABLE 3\nValue\tMeaning\n-6\t\nBad Configuration\n\n-2\t\nInvalid Argument\n\n-1\t\nUnknown Package\n\n1\t\nAdvertised\n\n2\t\nAbsent\n\n5\t\nInstalled\n\n \n\nLanguage\n\nData type: string\n\nAccess type: Read-only\n\nThe language of the product.\n\nWindows Server 2003: This property is not available.\n\nLocalPackage\n\nData type: string\n\nAccess type: Read-only\n\nThe location of the locally cached package for this product.\n\nWindows Server 2003: This property is not available.\n\nName\n\nData type: string\n\nAccess type: Read-only\n\nQualifiers: Key, Dynamic\n\nCommonly used product name.\n\nPackageCache\n\nData type: string\n\nAccess type: Read-only\n\nLocation of the locally cached package for this product.\n\nPackageCode\n\nData type: string\n\nAccess type: Read-only\n\nThe identifier for the package from which this product was installed.\n\nWindows Server 2003: This property is not available.\n\nPackageName\n\nData type: string\n\nAccess type: Read-only\n\nThe original package name for the product.\n\nWindows Server 2003: This property is not available.\n\nProductID\n\nData type: string\n\nAccess type: Read-only\n\nThe product ID.\n\nWindows Server 2003: This property is not available.\n\nRegCompany\n\nData type: string\n\nAccess type: Read-only\n\nThe company registered to use the product.\n\nWindows Server 2003: This property is not available.\n\nRegOwner\n\nData type: string\n\nAccess type: Read-only\n\nThe owner registered to use the product.\n\nWindows Server 2003: This property is not available.\n\nSKUNumber\n\nData type: string\n\nAccess type: Read-only\n\nProduct SKU (stock-keeping unit) information.\n\nTransforms\n\nData type: string\n\nAccess type: Read-only\n\nThe transforms of the product.\n\nWindows Server 2003: This property is not available.\n\nURLInfoAbout\n\nData type: string\n\nAccess type: Read-only\n\nThe URL information for the product.\n\nWindows Server 2003: This property is not available.\n\nURLUpdateInfo\n\nData type: string\n\nAccess type: Read-only\n\nThe URL update information the product.\n\nWindows Server 2003: This property is not available.\n\nVendor\n\nData type: string\n\nAccess type: Read-only\n\nName of the product supplier. Corresponds to the Vendor property in the product object in the Desktop Management Task Force (DMTF) Solution Exchange Standard.\n\nVersion\n\nData type: string\n\nAccess type: Read-only\n\nProduct version information. Corresponds to the Version property in the product object in the DMTF Solution Exchange Standard.\n\nWordCount\n\nData type: uint32\n\nAccess type: Read-only\n\nNumber of words in the summary information for the product.\n\nWindows Server 2003: This property is not available.", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "how to deploy django under a suburl behind nginx", "id": 529, "answers": [{"answer_id": 531, "document_id": 254, "question_id": 529, "text": "You need to do something in the server that hosts your Django app at :12345. You could set the prefix there, and pass it to Django using the WSGIScriptAlias or its equivalent outside mod_wsgi. I cannot give more information as I don't know how your Django application is run. Also, maybe you should consider running your Django app directly from Django, using uWSGI or gunicorn.", "answer_start": 345, "answer_category": null}], "is_impossible": false}], "context": "The question is, when configured like the above, how to make the urls in my response pages to have a prefix of \"/myapp\" so that the nginx can direct them correctly to myapp. E.g., the urls in a page like \"/foo/far\" ought to be changed to \"/myapp/foo/bar\" to allow nginx proxy to myapp. what is the right nginx configure to use to achieve this ?\nYou need to do something in the server that hosts your Django app at :12345. You could set the prefix there, and pass it to Django using the WSGIScriptAlias or its equivalent outside mod_wsgi. I cannot give more information as I don't know how your Django application is run. Also, maybe you should consider running your Django app directly from Django, using uWSGI or gunicorn.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to disable System.Security.Permissions.ReflectionPermission usage in new VS2013 Asp.net template so it works on shared hosting", "id": 1343, "answers": [{"answer_id": 1333, "document_id": 912, "question_id": 1343, "text": "You can see the solution 4 in this: http://www.codeproject.com/Questions/586223/SecurityplusExceptionpluscomingplusinplusaplusrunn", "answer_start": 229, "answer_category": null}], "is_impossible": false}], "context": "I created a brand new Visual Studio 2013 ASp.net MVC application off of the default MVC template. However I am getting an error message Request for the permission of type 'System.Security.Permissions.ReflectionPermission failed.\nYou can see the solution 4 in this: http://www.codeproject.com/Questions/586223/SecurityplusExceptionpluscomingplusinplusaplusrunn\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "which should i install first visual studio 2008 or sql server 2008", "id": 1473, "answers": [{"answer_id": 1462, "document_id": 1046, "question_id": 1473, "text": "always first add IIS to your os\nthen intsall Office (if you need)\nthen use windows update as long as you're sure you have no more updates \nthen install Sql Server\nthen use windows update to be sure you have any ServicePack needed by Sql Server\nthen install Visual Studio\nthen use windows update as long as you're sure you have no more updates ", "answer_start": 2609, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nHalf a year ago, I installed Visual Studio 2008 and SQL Server 2008. I don't quite remember which I installed first, but since then I am unable to connect to any file-based SQL server databases from the Visual Studio IDE. I think it was related to a bug in Visual Studio, the fact that I had previously installed Visual Studio 2005 on the same system and the order in which I installed both programs. \n\nNow I am soon going to upgrade to Visual Studio Team System and I would rather avoid having the same database connection problems. Should I \n\nA) Uninstall both Visual Studio and SQL Server, then reinstall SQL Server and afterwards Visual Studio?\nB) Just uninstall Visual Studio and update to VS Team System?\n\nThanks,\n\nAdrian\n    \n\nI install the Developer Edition of SQL Server instead of the SQL Server Express edition that comes with VS2008.  I find that the install works better if you install SQL Server first, that way there is no need to uninstall the Express edition before installing SQL Server Developer edition.  I don't recommend having both editions installed.  I had no end of problems until I figured out that having two versions installed was the issue.  Uninstalling VS and SQL Server Express may be necessary, but I believe it was enough just to uninstall SQL Server Express and (re)install SQL Server Developer edition.\n    \n\nI don't think it matters what order they are installed.\n    \n\nThere is no technical reasons why you should install one or the other first. The VS IDE is the same for both and the Visual Studio add-on's for development (C#, VB are add-ons) or SQL should install no matter what else is installed.\n\nPersonally I would install DB first - the reason is that if you install DB second the default feed for the start page is set to the SQL teams RSS feed and not the MSDN one.\n    \n\nMy experience after changing from SQL 2005 Express to SQL 2008 Express was that VS would not recognize SQL 2008 and was still looking for SQL 2005.\n\nI took these steps:\n\n\nuninstall Visual Studio\ndelete all the registry keys I could find that seemed to relate to Visual Studio\ninstall Visual Studio\n\n\nVS was now able to connect to SQL Server and user instances (file mdf)\nHowever, in the least priviledged user access account I got this error message:\n\"Failed to generate a user instance of SQL Server due to a failure in starting the process for the user instance.\"\n\nTo fix this error:\n\n\nunder the C:\\Users[user]\\AppData directory if found the folders that SQL Express was using and deleted them.\n\n\nI was now able to make a database connection in Visual Studio.\n    \n\nalways first add IIS to your os\nthen intsall Office (if you need)\nthen use windows update as long as you're sure you have no more updates \nthen install Sql Server\nthen use windows update to be sure you have any ServicePack needed by Sql Server\nthen install Visual Studio\nthen use windows update as long as you're sure you have no more updates \n\nYou can have developer and express edition of sql (even of different versions) on same machine. \nAnd you can have all the Visual Studio versions from 2003 (did not tried having also previous) to 2010. \n\nThe only caveat is: all in same language, or you will experience MSDN crash every too often, and the merge action of msdn will make an unreadable msdn on your pc.\n\nCimpy\n    \n\nInstall visual studio by disabling sqlsever express editin option in custom installation,then install sqlserver express edition 2005 seperately and then after install mangaement studio ..u can connect to D.B...it's working...\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to install RVM system requirements without giving sudo access for RVM user", "id": 322, "answers": [{"answer_id": 331, "document_id": 135, "question_id": 322, "text": "you can install RVM without autolibs with this command:$ \\curl -L https://get.rvm.io | bash -s -- --autolibs=read-fail.", "answer_start": 201, "answer_category": null}], "is_impossible": false}], "context": "On my Debian server I have a user called \"deployer\" that does not have sudo access, and has RVM installed.When installing Ruby using \"deployer\", like 1.9.3, it triggers a task to install dependencies. you can install RVM without autolibs with this command:$ \\curl -L https://get.rvm.io | bash -s -- --autolibs=read-fail.", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "installing pyspark", "id": 1400, "answers": [{"answer_id": 1389, "document_id": 972, "question_id": 1400, "text": "The easiest way to \"install\" Spark is to simply download Spark (I recommend Spark 1.6.1 -- personal preference). Then unzip the file in the directory you want to have Spark \"installed\" in, say C:/spark-folder (Windows) or /home/usr/local/spark-folder (Ubuntu).\n\nAfter you install it in your desired directory, you need to set your environment variables. Depending on your OS, this will depend; this step is, however, not necessary to run Spark (i.e. pyspark).", "answer_start": 1295, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI am trying to install PySpark and following the instructions and running this from the command line on the cluster node where I have Spark installed:\n\n$ sbt/sbt assembly\n\n\nThis produces the following error:\n\n-bash: sbt/sbt: No such file or directory\n\n\nI try the next command:\n\n$ ./bin/pyspark\n\n\nI get this error:\n\n-bash: ./bin/pyspark: No such file or directory\n\n\nI feel like I'm missing something basic.\nWhat is missing?\nI have spark installed and am able to access it using the command:\n\n$ spark-shell\n\n\nI have python on the node and am able to open python using the command:\n\n$ python\n\n    \n\nWhat's your current working directory?  The sbt/sbt and ./bin/pyspark commands are relative to the directory containing Spark's code ($SPARK_HOME), so you should be in that directory when running those commands.\n\nNote that Spark offers pre-built binary distributions that are compatible with many common Hadoop distributions; this may be an easier option if you're using one of those distros.\n\nAlso, it looks like you linked to the Spark 0.9.0 documentation; if you're building Spark from scratch, I recommend following the latest version of the documentation.\n    \n\nSBT is used to build a Scala project. If you're new to Scala/SBT/Spark, you're doing things the difficult way.\n\nThe easiest way to \"install\" Spark is to simply download Spark (I recommend Spark 1.6.1 -- personal preference). Then unzip the file in the directory you want to have Spark \"installed\" in, say C:/spark-folder (Windows) or /home/usr/local/spark-folder (Ubuntu).\n\nAfter you install it in your desired directory, you need to set your environment variables. Depending on your OS, this will depend; this step is, however, not necessary to run Spark (i.e. pyspark).\n\nIf you do not set your environment variables, or don't know how to, an alternative is to simply to go your directory on a terminal window, cd C:/spark-folder (Windows) or cd /home/usr/local/spark-folder (Ubuntu) then type \n\n./bin/pyspark\n\n\nand spark should run.\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "pip install pygraphviz: No package 'libcgraph' found", "id": 771, "answers": [{"answer_id": 771, "document_id": 458, "question_id": 771, "text": "sudo apt-get install python3-dev graphviz libgraphviz-dev pkg-config\nthen\npip install pygraphviz", "answer_start": 178, "answer_category": null}], "is_impossible": false}], "context": "I encounter the issue No package 'libcgraph' found while running sudo pip install pygraphviz. Below is the full stacktrace. the following might be needed if you're using Python \nsudo apt-get install python3-dev graphviz libgraphviz-dev pkg-config\nthen\npip install pygraphviz\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Restarting Unicorn with USR2 doesn't seem to reload production.rb settings", "id": 571, "answers": [{"answer_id": 575, "document_id": 296, "question_id": 571, "text": "My guess is that your unicorns are being restarted in the old production directory rather than the new production directory -- in other words, if your working directory in unicorn.rb is <capistrano_directory>/current, you need to make sure the symlink happens before you attempt to restart the unicorns.", "answer_start": 199, "answer_category": null}], "is_impossible": false}], "context": "I'm running unicorn and am trying to get zero downtime restarts working.\nSo far it is all awesome sauce, the master process forks and starts 4 new workers, then kills the old one, everyone is happy. My guess is that your unicorns are being restarted in the old production directory rather than the new production directory -- in other words, if your working directory in unicorn.rb is <capistrano_directory>/current, you need to make sure the symlink happens before you attempt to restart the unicorns.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Git command to checkout any branch and overwrite local changes", "id": 320, "answers": [{"answer_id": 329, "document_id": 133, "question_id": 320, "text": "You could follow a solution similar to \"How do I force \u201cgit pull\u201d to overwrite local files?\":git fetch \u2013all,git reset --hard origin/abranch,git checkout $branch", "answer_start": 207, "answer_category": null}], "is_impossible": false}], "context": "Is there a Git command (or a short sequence of commands) that will safely and surely do the following? Get rid of any local changes.Fetch the given branch from origin if necessary.Checkout the given branch? You could follow a solution similar to \"How do I force \u201cgit pull\u201d to overwrite local files?\":git fetch \u2013all,git reset --hard origin/abranch,git checkout $branch", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Error:Unable to locate adb within SDK in Android Studio", "id": 1553, "answers": [{"answer_id": 1542, "document_id": 1130, "question_id": 1553, "text": "You may simply went to Project Structure dialog (alt+ctrl+shift+s or button 1 on the screen) and then to project-> Project SDK's was selected <no SDK>. Just changed it to the latest", "answer_start": 184, "answer_category": null}], "is_impossible": false}], "context": "Does anyone know what this means? When I click the \"run\" button on my simulator I get this message.\nThrowable: Unable to locate adb within SDK\nI am running the latest version, 0.8.14.\nYou may simply went to Project Structure dialog (alt+ctrl+shift+s or button 1 on the screen) and then to project-> Project SDK's was selected <no SDK>. Just changed it to the latest\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "if I don't want to to uninstall Java Plug-in but to create a new symbolic link for linux, what shall I do?", "id": 178, "answers": [{"answer_id": 185, "document_id": 104, "question_id": 178, "text": "If you are going to reinstall Java, you don't need to uninstall Java Plug-in. Just use the -f option for ln when creating a new symbolic link.\nExample: ln -f /usr/lib/mozilla/plugins/libnpjp2.so", "answer_start": 344, "answer_category": null}], "is_impossible": false}, {"question": "if I want remove Java from my Linux system, how can I uninstall plug-in?", "id": 179, "answers": [{"answer_id": 186, "document_id": 104, "question_id": 179, "text": "Open Terminal Window\nLog in as the super user\nType: rm /usr/lib/mozilla/plugins/libnpjp2.so\nYou will be prompted to remove symbolic link:\nrm: /usr/lib/mozilla/plugins/libnpjp2.so ?\nType: Y\nJava Uninstall\nThere are two ways to uninstall Java. Please use the method that you used when you installed Java. For example, if you used RPM to install Java, then use the RPM uninstall method.", "answer_start": 663, "answer_category": null}], "is_impossible": false}, {"question": "If I install java with RPM, how to uninstall it?", "id": 180, "answers": [{"answer_id": 187, "document_id": 104, "question_id": 180, "text": "Open Terminal Window\nLogin as the super user\nTry to find jre package by typing: rpm -qa\nIf RPM reports a package similar to jre--fcs then Java is installed with RPM.\n\nNote: Normally, you do not need to uninstall Java with RPM, because RPM is able to uninstall the old version of Java when installing a new version! You may skip reading, unless you want to remove Java permanently.\nTo uninstall Java, type: rpm -e jre--fcs", "answer_start": 1230, "answer_category": null}], "is_impossible": false}, {"question": "how to uninstall java with elf-extracting file?", "id": 181, "answers": [{"answer_id": 188, "document_id": 104, "question_id": 181, "text": "Find out if Java is installed in some folder. Common locations are /usr/java/jre_ or /opt/jre_nb/jre_/bin/java/\nWhen you have located the folder, you may delete folder.\nWarning: You should be certain that Java is not already installed using RPM before removing the folder.\nType: rm -r jre\nFor example: rm -r jre1.7.0", "answer_start": 1683, "answer_category": null}], "is_impossible": false}], "context": "\nHow do I uninstall Java for Linux ?\nThis article applies to:\nPlatform(s): Oracle Enterprise Linux, Oracle Linux, Red Hat Linux, SLES, SUSE Linux\nBrowser(s) Firefox\nJava version(s): 7.0, 8.0\nThe uninstall process consists of:\nJava Plug-in Uninstall\nJava Uninstall\nRPM Uninstall\nSelf extracting Uninstall\nJava Plug-in Uninstall: Mozilla browser\nIf you are going to reinstall Java, you don't need to uninstall Java Plug-in. Just use the -f option for ln when creating a new symbolic link.\nExample: ln -f /usr/lib/mozilla/plugins/libnpjp2.so\n\nIf you want to completely remove Java from your Linux box, the procedure of removing the symbolic link is described below.\nOpen Terminal Window\nLog in as the super user\nType: rm /usr/lib/mozilla/plugins/libnpjp2.so\nYou will be prompted to remove symbolic link:\nrm: /usr/lib/mozilla/plugins/libnpjp2.so ?\nType: Y\nJava Uninstall\nThere are two ways to uninstall Java. Please use the method that you used when you installed Java. For example, if you used RPM to install Java, then use the RPM uninstall method.\n\nRPM uninstall\nNote: If you have RPM on your Linux box, you should first find out if Java is already installed using RPM. If Java is not installed using RPM, you should skip reading.\nOpen Terminal Window\nLogin as the super user\nTry to find jre package by typing: rpm -qa\nIf RPM reports a package similar to jre--fcs then Java is installed with RPM.\n\nNote: Normally, you do not need to uninstall Java with RPM, because RPM is able to uninstall the old version of Java when installing a new version! You may skip reading, unless you want to remove Java permanently.\nTo uninstall Java, type: rpm -e jre--fcs\nSelf-extracting file uninstall\nFind out if Java is installed in some folder. Common locations are /usr/java/jre_ or /opt/jre_nb/jre_/bin/java/\nWhen you have located the folder, you may delete folder.\nWarning: You should be certain that Java is not already installed using RPM before removing the folder.\nType: rm -r jre\nFor example: rm -r jre1.7.0", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "gradlew command not found?", "id": 922, "answers": [{"answer_id": 917, "document_id": 574, "question_id": 922, "text": "gradle wrapper --gradle-version 6.0.1", "answer_start": 10550, "answer_category": null}], "is_impossible": false}], "context": "Building Java Projects with Gradle\nThis guide walks you through using Gradle to build a simple Java project.\n\nWhat you\u2019ll build\nYou\u2019ll create a simple app and then build it using Gradle.\n\nWhat you\u2019ll need\nAbout 15 minutes\n\nA favorite text editor or IDE\n\nJDK 6 or later\n\nHow to complete this guide\nLike most Spring Getting Started guides, you can start from scratch and complete each step or you can bypass basic setup steps that are already familiar to you. Either way, you end up with working code.\n\nTo start from scratch, move on to Set up the project.\n\nTo skip the basics, do the following:\n\nDownload and unzip the source repository for this guide, or clone it using Git: git clone https://github.com/spring-guides/gs-gradle.git\n\ncd into gs-gradle/initial\n\nJump ahead to Install Gradle.\n\nWhen you finish, you can check your results against the code in gs-gradle/complete.\n\nSet up the project\nFirst you set up a Java project for Gradle to build. To keep the focus on Gradle, make the project as simple as possible for now.\n\nCreate the directory structure\nIn a project directory of your choosing, create the following subdirectory structure; for example, with mkdir -p src/main/java/hello on *nix systems:\n\n\u2514\u2500\u2500 src\n    \u2514\u2500\u2500 main\n        \u2514\u2500\u2500 java\n            \u2514\u2500\u2500 hello\nWithin the src/main/java/hello directory, you can create any Java classes you want. For simplicity\u2019s sake and for consistency with the rest of this guide, Spring recommends that you create two classes: HelloWorld.java and Greeter.java.\n\nsrc/main/java/hello/HelloWorld.java\n\npackage hello;\n\npublic class HelloWorld {\n  public static void main(String[] args) {\n  Greeter greeter = new Greeter();\n  System.out.println(greeter.sayHello());\n  }\n}COPY\nsrc/main/java/hello/Greeter.java\n\npackage hello;\n\npublic class Greeter {\n  public String sayHello() {\n  return \"Hello world!\";\n  }\n}COPY\nInstall Gradle\nNow that you have a project that you can build with Gradle, you can install Gradle.\n\nIt\u2019s highly recommended to use an installer:\n\nSDKMAN\n\nHomebrew (brew install gradle)\n\nAs a last resort, if neither of these tools suit your needs, you can download the binaries from https://www.gradle.org/downloads. Only the binaries are required, so look for the link to gradle-version-bin.zip. (You can also choose gradle-version-all.zip to get the sources and documentation as well as the binaries.)\n\nUnzip the file to your computer, and add the bin folder to your path.\n\nTo test the Gradle installation, run Gradle from the command-line:\n\ngradle\nIf all goes well, you see a welcome message:\n\n:help\n\nWelcome to Gradle 6.0.1.\n\nTo run a build, run gradle <task> ...\n\nTo see a list of available tasks, run gradle tasks\n\nTo see a list of command-line options, run gradle --help\n\nTo see more detail about a task, run gradle help --task <task>\n\nFor troubleshooting, visit https://help.gradle.org\n\nDeprecated Gradle features were used in this build, making it incompatible with Gradle 7.0.\nUse '--warning-mode all' to show the individual deprecation warnings.\nSee https://docs.gradle.org/6.0.1/userguide/command_line_interface.html#sec:command_line_warnings\n\nBUILD SUCCESSFUL in 455ms\n1 actionable task: 1 executed\nYou now have Gradle installed.\n\nFind out what Gradle can do\nNow that Gradle is installed, see what it can do. Before you even create a build.gradle file for the project, you can ask it what tasks are available:\n\ngradle tasks\nYou should see a list of available tasks. Assuming you run Gradle in a folder that doesn\u2019t already have a build.gradle file, you\u2019ll see some very elementary tasks such as this:\n\n:tasks\n\n------------------------------------------------------------\nTasks runnable from root project\n------------------------------------------------------------\n\nBuild Setup tasks\n-----------------\ninit - Initializes a new Gradle build.\nwrapper - Generates Gradle wrapper files.\n\nHelp tasks\n----------\nbuildEnvironment - Displays all buildscript dependencies declared in root project 'gs-gradle'.\ncomponents - Displays the components produced by root project 'gs-gradle'. [incubating]\ndependencies - Displays all dependencies declared in root project 'gs-gradle'.\ndependencyInsight - Displays the insight into a specific dependency in root project 'gs-gradle'.\ndependentComponents - Displays the dependent components of components in root project 'gs-gradle'. [incubating]\nhelp - Displays a help message.\nmodel - Displays the configuration model of root project 'gs-gradle'. [incubating]\noutgoingVariants - Displays the outgoing variants of root project 'gs-gradle'.\nprojects - Displays the sub-projects of root project 'gs-gradle'.\nproperties - Displays the properties of root project 'gs-gradle'.\ntasks - Displays the tasks runnable from root project 'gs-gradle'.\n\nTo see all tasks and more detail, run gradle tasks --all\n\nTo see more detail about a task, run gradle help --task <task>\n\nDeprecated Gradle features were used in this build, making it incompatible with Gradle 7.0.\nUse '--warning-mode all' to show the individual deprecation warnings.\nSee https://docs.gradle.org/6.0.1/userguide/command_line_interface.html#sec:command_line_warnings\n\nBUILD SUCCESSFUL in 477ms\n1 actionable task: 1 executed\nEven though these tasks are available, they don\u2019t offer much value without a project build configuration. As you flesh out the build.gradle file, some tasks will be more useful. The list of tasks will grow as you add plugins to build.gradle, so you\u2019ll occasionally want to run tasks again to see what tasks are available.\n\nSpeaking of adding plugins, next you add a plugin that enables basic Java build functionality.\n\nBuild Java code\nStarting simple, create a very basic build.gradle file in the <project folder> you created at the beginning of this guide. Give it just just one line:\n\napply plugin: 'java'COPY\nThis single line in the build configuration brings a significant amount of power. Run gradle tasks again, and you see new tasks added to the list, including tasks for building the project, creating JavaDoc, and running tests.\n\nYou\u2019ll use the gradle build task frequently. This task compiles, tests, and assembles the code into a JAR file. You can run it like this:\n\ngradle build\nAfter a few seconds, \"BUILD SUCCESSFUL\" indicates that the build has completed.\n\nTo see the results of the build effort, take a look in the build folder. Therein you\u2019ll find several directories, including these three notable folders:\n\nclasses. The project\u2019s compiled .class files.\n\nreports. Reports produced by the build (such as test reports).\n\nlibs. Assembled project libraries (usually JAR and/or WAR files).\n\nThe classes folder has .class files that are generated from compiling the Java code. Specifically, you should find HelloWorld.class and Greeter.class.\n\nAt this point, the project doesn\u2019t have any library dependencies, so there\u2019s nothing in the dependency_cache folder.\n\nThe reports folder should contain a report of running unit tests on the project. Because the project doesn\u2019t yet have any unit tests, that report will be uninteresting.\n\nThe libs folder should contain a JAR file that is named after the project\u2019s folder. Further down, you\u2019ll see how you can specify the name of the JAR and its version.\n\nDeclare dependencies\nThe simple Hello World sample is completely self-contained and does not depend on any additional libraries. Most applications, however, depend on external libraries to handle common and/or complex functionality.\n\nFor example, suppose that in addition to saying \"Hello World!\", you want the application to print the current date and time. You could use the date and time facilities in the native Java libraries, but you can make things more interesting by using the Joda Time libraries.\n\nFirst, change HelloWorld.java to look like this:\n\npackage hello;\n\nimport org.joda.time.LocalTime;\n\npublic class HelloWorld {\n  public static void main(String[] args) {\n    LocalTime currentTime = new LocalTime();\n    System.out.println(\"The current local time is: \" + currentTime);\n\n    Greeter greeter = new Greeter();\n    System.out.println(greeter.sayHello());\n  }\n}COPY\nHere HelloWorld uses Joda Time\u2019s LocalTime class to get and print the current time.\n\nIf you ran gradle build to build the project now, the build would fail because you have not declared Joda Time as a compile dependency in the build.\n\nFor starters, you need to add a source for 3rd party libraries.\n\nrepositories { \n    mavenCentral() \n}COPY\nThe repositories block indicates that the build should resolve its dependencies from the Maven Central repository. Gradle leans heavily on many conventions and facilities established by the Maven build tool, including the option of using Maven Central as a source of library dependencies.\n\nNow that we\u2019re ready for 3rd party libraries, let\u2019s declare some.\n\nsourceCompatibility = 1.8\ntargetCompatibility = 1.8\n\ndependencies {\n    implementation \"joda-time:joda-time:2.2\"\n    testImplementation \"junit:junit:4.12\"\n}COPY\nWith the dependencies block, you declare a single dependency for Joda Time. Specifically, you\u2019re asking for (reading right to left) version 2.2 of the joda-time library, in the joda-time group.\n\nAnother thing to note about this dependency is that it is a compile dependency, indicating that it should be available during compile-time (and if you were building a WAR file, included in the /WEB-INF/libs folder of the WAR). Other notable types of dependencies include:\n\nimplementation. Required dependencies for compiling the project code, but that will be provided at runtime by a container running the code (for example, the Java Servlet API).\n\ntestImplementation. Dependencies used for compiling and running tests, but not required for building or running the project\u2019s runtime code.\n\nFinally, let\u2019s specify the name for our JAR artifact.\n\njar {\n    archiveBaseName = 'gs-gradle'\n    archiveVersion =  '0.1.0'\n}COPY\nThe jar block specifies how the JAR file will be named. In this case, it will render gs-gradle-0.1.0.jar.\n\nNow if you run gradle build, Gradle should resolve the Joda Time dependency from the Maven Central repository and the build will succeed.\n\nBuild your project with Gradle Wrapper\nThe Gradle Wrapper is the preferred way of starting a Gradle build. It consists of a batch script for Windows and a shell script for OS X and Linux. These scripts allow you to run a Gradle build without requiring that Gradle be installed on your system. This used to be something added to your build file, but it\u2019s been folded into Gradle, so there is no longer any need. Instead, you simply use the following command.\n\n$ gradle wrapper --gradle-version 6.0.1\nAfter this task completes, you will notice a few new files. The two scripts are in the root of the folder, while the wrapper jar and properties files have been added to a new gradle/wrapper folder.\n\n\u2514\u2500\u2500 <project folder>\n    \u2514\u2500\u2500 gradlew\n    \u2514\u2500\u2500 gradlew.bat\n    \u2514\u2500\u2500 gradle\n        \u2514\u2500\u2500 wrapper\n            \u2514\u2500\u2500 gradle-wrapper.jar\n            \u2514\u2500\u2500 gradle-wrapper.properties\nThe Gradle Wrapper is now available for building your project. Add it to your version control system, and everyone that clones your project can build it just the same. It can be used in the exact same way as an installed version of Gradle. Run the wrapper script to perform the build task, just like you did previously:\n\n./gradlew build\nThe first time you run the wrapper for a specified version of Gradle, it downloads and caches the Gradle binaries for that version. The Gradle Wrapper files are designed to be committed to source control so that anyone can build the project without having to first install and configure a specific version of Gradle.\n\nAt this stage, you will have built your code. You can see the results here:\n\nbuild\n\u251c\u2500\u2500 classes\n\u2502   \u2514\u2500\u2500 main\n\u2502       \u2514\u2500\u2500 hello\n\u2502           \u251c\u2500\u2500 Greeter.class\n\u2502           \u2514\u2500\u2500 HelloWorld.class\n\u251c\u2500\u2500 dependency-cache\n\u251c\u2500\u2500 libs\n\u2502   \u2514\u2500\u2500 gs-gradle-0.1.0.jar\n\u2514\u2500\u2500 tmp\n    \u2514\u2500\u2500 jar\n        \u2514\u2500\u2500 MANIFEST.MF\nIncluded are the two expected class files for Greeter and HelloWorld, as well as a JAR file. Take a quick peek:\n\n$ jar tvf build/libs/gs-gradle-0.1.0.jar\n  0 Fri May 30 16:02:32 CDT 2014 META-INF/\n 25 Fri May 30 16:02:32 CDT 2014 META-INF/MANIFEST.MF\n  0 Fri May 30 16:02:32 CDT 2014 hello/\n369 Fri May 30 16:02:32 CDT 2014 hello/Greeter.class\n988 Fri May 30 16:02:32 CDT 2014 hello/HelloWorld.class\nThe class files are bundled up. It\u2019s important to note, that even though you declared joda-time as a dependency, the library isn\u2019t included here. And the JAR file isn\u2019t runnable either.\n\nTo make this code runnable, we can use gradle\u2019s application plugin. Add this to your build.gradle file.\n\napply plugin: 'application'\n\nmainClassName = 'hello.HelloWorld'\nThen you can run the app!\n\n$ ./gradlew run\n:compileJava UP-TO-DATE\n:processResources UP-TO-DATE\n:classes UP-TO-DATE\n:run\nThe current local time is: 16:16:20.544\nHello world!\n\nBUILD SUCCESSFUL\n\nTotal time: 3.798 secs\nTo bundle up dependencies requires more thought. For example, if we were building a WAR file, a format commonly associated with packing in 3rd party dependencies, we could use gradle\u2019s WAR plugin. If you are using Spring Boot and want a runnable JAR file, the spring-boot-gradle-plugin is quite handy. At this stage, gradle doesn\u2019t know enough about your system to make a choice. But for now, this should be enough to get started using gradle.\n\nTo wrap things up for this guide, here is the completed build.gradle file:\n\nbuild.gradle\n\napply plugin: 'java'\napply plugin: 'eclipse'\napply plugin: 'application'\n\nmainClassName = 'hello.HelloWorld'\n\n// tag::repositories[]\nrepositories { \n    mavenCentral() \n}\n// end::repositories[]\n\n// tag::jar[]\njar {\n    archiveBaseName = 'gs-gradle'\n    archiveVersion =  '0.1.0'\n}\n// end::jar[]\n\n// tag::dependencies[]\nsourceCompatibility = 1.8\ntargetCompatibility = 1.8\n\ndependencies {\n    implementation \"joda-time:joda-time:2.2\"\n    testImplementation \"junit:junit:4.12\"\n}\n// end::dependencies[]\n\n// tag::wrapper[]\n// end::wrapper[]COPY\nThere are many start/end comments embedded here. This makes it possible to extract bits of the build file into this guide for the detailed explanations above. You don\u2019t need them in your production build file.\nSummary\nCongratulations! You have now created a simple yet effective Gradle build file for building Java projects.\n\nSee Also\nThe following guide may also be helpful:\n\nBuilding Java Projects with Maven\n\nWant to write a new guide or contribute to an existing one? Check out our contribution guidelines.\n\nAll guides are released with an ASLv2 license for the code, and an Attribution, NoDerivatives creative commons license for the writing.", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "What's the accepted method for deploying a linux application that relies on shared libraries?", "id": 1069, "answers": [{"answer_id": 1062, "document_id": 647, "question_id": 1069, "text": "In general, you're best off depending on the 'normal' versions of the libraries for whatever distribution you're targetting (and saying you don't support dists that don't support recent enough versions of the lib), but if you REALLY need to depend on a bleeding edge version of some shared lib, you can link your app with -Wl,-rpath,'$ORIGIN' and then install a copy of the exact version you want in the same directory as your executable.", "answer_start": 451, "answer_category": null}], "is_impossible": false}], "context": "I have an application that relies on Qt, GDCM, and VTK, with the main build environment being Qt. All of these libraries are cross-platform and compile on Windows, Mac, and Linux. I need to deploy the application to Linux after deploying on Windows. The versions of vtk and gdcm I'm using are trunk versions from git (about a month old), more recent than what I can get apt-get on Ubuntu 11.04, which is my current (and only) Linux deployment target.\nIn general, you're best off depending on the 'normal' versions of the libraries for whatever distribution you're targetting (and saying you don't support dists that don't support recent enough versions of the lib), but if you REALLY need to depend on a bleeding edge version of some shared lib, you can link your app with -Wl,-rpath,'$ORIGIN' and then install a copy of the exact version you want in the same directory as your executable.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Missing folder errors during capistrano deploy", "id": 1315, "answers": [{"answer_id": 1305, "document_id": 884, "question_id": 1315, "text": "Add set :normalize_asset_timestamps, false to your deploy file. By default it's set to true and it runs a touch command on all your images/javascripts/stylesheets but it's no longer needed if you're using the asset pipeline.", "answer_start": 599, "answer_category": null}], "is_impossible": false}], "context": "I'm not sure I understand the error messages I'm getting while attempting to deploy my rails app with capistrano. The deploy is failing because of some missing directory (more specifically: images, stylesheets and javascript) but I'm not sure why these directories are being searched for in the public directory of the release directory. These folders should be in the assets directory of my app directory, but capistrano searching the public folder of the release folder for /images, /stylesheets and /javascript? Any idea why? Any idea what I'm doing wrong? Any help would be greatly appreciated!\nAdd set :normalize_asset_timestamps, false to your deploy file. By default it's set to true and it runs a touch command on all your images/javascripts/stylesheets but it's no longer needed if you're using the asset pipeline.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Why does \"pip install\" inside Python raise a SyntaxError?", "id": 1605, "answers": [{"answer_id": 1592, "document_id": 1179, "question_id": 1605, "text": "pip is run from the command line, not the Python interpreter. It is a program that installs modules, so you can use them from Python. Once you have installed the module, then you can open the Python shell and do import selenium.", "answer_start": 236, "answer_category": null}], "is_impossible": false}], "context": "I'm trying to use pip to install a package. I try to run pip install from the Python shell, but I get a SyntaxError. Why do I get this error? How do I use pip to install the package?\n>>> pip install selenium\nSyntaxError: invalid syntax\npip is run from the command line, not the Python interpreter. It is a program that installs modules, so you can use them from Python. Once you have installed the module, then you can open the Python shell and do import selenium.\nThe Python shell is not a command line, it is an interactive interpreter. You type Python code into it, not commands.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Downloading Eclipse plug-in update sites for offline installation", "id": 754, "answers": [{"answer_id": 754, "document_id": 441, "question_id": 754, "text": "During installation I selected the C++ tools. It downloaded almost 5GB of data. I restarted the machine after installation and compiling the code worked fine.", "answer_start": 219, "answer_category": null}], "is_impossible": false}], "context": "I have installed Rust on windows from Rust installation page. After installation I tried running the \"hello world\" program but got the following error. I downloaded and installed the Build Tools for Visual Studio 2019. During installation I selected the C++ tools. It downloaded almost 5GB of data. I restarted the machine after installation and compiling the code worked fine. Do we need to leave all of the default check-boxes checked? Otherwise, what would be the minimum set of dependencies we could install to get Rust to worse.", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "What does the logical DNS hostname do in MongoDB clusters?", "id": 210, "answers": [{"answer_id": 218, "document_id": 115, "question_id": 210, "text": " The use of logical DNS hostnames avoids configuration\nchanges due to ip address changes", "answer_start": 2680, "answer_category": null}], "is_impossible": false}, {"question": " How to prevent MongoDB from overloading machines?", "id": 211, "answers": [{"answer_id": 219, "document_id": 115, "question_id": 211, "text": "The --oplogSize setting reduces the\ndisk space that each mongod instance uses.", "answer_start": 6499, "answer_category": null}], "is_impossible": false}], "context": "\n\nDeploy a Replica Set for Testing and DevelopmentClose \u00d7MongoDB ManualVersion 5.0IntroductionInstallationMongoDB Shell (mongosh)MongoDB CRUD OperationsAggregationData ModelsTransactionsIndexesSecurityChange StreamsReplicationReplica Set MembersReplica Set OplogReplica Set Data SynchronizationReplica Set Deployment ArchitecturesReplica Set High AvailabilityReplica Set Read and Write SemanticsReplica Set Deployment TutorialsDeploy a Replica SetDeploy a Replica Set for Testing and DevelopmentDeploy a Geographically Redundant Replica SetAdd an Arbiter to Replica SetConvert a Standalone to a Replica SetAdd Members to a Replica SetRemove Members from Replica SetReplace a Replica Set MemberMember Configuration TutorialsReplica Set Maintenance TutorialsReplication ReferenceShardingAdministrationStorageFrequently Asked QuestionsReferenceRelease NotesTechnical SupportNavigationReplication > Replica Set Deployment TutorialsDeploy a Replica Set for Testing and Development\u00b6On this pageOverviewRequirementsConsiderationsProcedureThis procedure describes deploying a replica set in a development or\ntest environment. For a production deployment, refer to the\nDeploy a Replica Set tutorial.This tutorial describes how to create a three-member replica set from three existing mongod instances running with\naccess control disabled.To deploy a replica set with enabled access control, see\nDeploy Replica Set With Keyfile Authentication. If you wish to deploy a\nreplica set from a single MongoDB instance, see\nConvert a Standalone to a Replica Set. For more\ninformation on replica set deployments, see the Replication and\nReplica Set Deployment Architectures documentation.Overview\u00b6Three member replica sets provide enough\nredundancy to survive most network partitions and other system\nfailures. These sets also have sufficient capacity for many distributed\nread operations. Replica sets should always have an odd number of\nmembers. This ensures that elections will proceed smoothly. For more about\ndesigning replica sets, see the Replication overview.Requirements\u00b6For test and development systems, you can run your mongod\ninstances on a local system, or within a virtual instance.Before you can deploy a replica set, you must install MongoDB on\neach system that will be part of your replica set.\nIf you have not already installed MongoDB, see the installation tutorials.Each member must be able to connect to every other member. For\ninstructions on how to check your connection, see\nTest Connections Between all Members.Considerations\u00b6TipWhen possible, use a logical DNS hostname instead of an ip address,\nparticularly when configuring replica set members or sharded cluster\nmembers. The use of logical DNS hostnames avoids configuration\nchanges due to ip address changes.IP Binding\u00b6Starting in MongoDB 3.6, MongoDB binaries, mongod and\nmongos, bind to localhost by default. If the\nnet.ipv6 configuration file setting or the --ipv6\ncommand line option is set for the binary, the binary additionally binds\nto the localhost IPv6 address.Previously, starting from MongoDB 2.6, only the binaries from the\nofficial MongoDB RPM (Red Hat, CentOS, Fedora Linux, and derivatives)\nand DEB (Debian, Ubuntu, and derivatives) packages bind to localhost by\ndefault.When bound only to the localhost, these MongoDB 3.6 binaries can only\naccept connections from clients (including mongosh and\nother members of your deployment in replica sets and sharded clusters)\nthat are running on the same machine. Remote clients cannot connect to\nthe binaries bound only to localhost.To override and bind to other ip addresses, you can use the\nnet.bindIp configuration file setting or the\n--bind_ip command-line option to specify a list of hostnames or ip\naddresses.WarningBefore binding to a non-localhost (e.g. publicly accessible)\nIP address, ensure you have secured your cluster from unauthorized\naccess. For a complete list of security recommendations, see\nSecurity Checklist. At minimum, consider\nenabling authentication and\nhardening network infrastructure.For example, the following mongod instance binds to both\nthe localhost and the hostname My-Example-Associated-Hostname, which is\nassociated with the ip address 198.51.100.1:mongod --bind_ip localhost,My-Example-Associated-HostnameIn order to connect to this instance, remote clients must specify\nthe hostname  or its associated ip address 198.51.100.1:mongosh --host My-Example-Associated-Hostnamemongosh --host 198.51.100.1In this test deployment, the three members run on the same machine.Replica Set Naming\u00b6ImportantThese instructions should only be used for test or\ndevelopment deployments.The examples in this procedure create a new replica set named rs0.If your application connects to more than one replica set, each set must\nhave a distinct name. Some drivers group replica set connections by\nreplica set name.Procedure\u00b6TipWhen possible, use a logical DNS hostname instead of an ip address,\nparticularly when configuring replica set members or sharded cluster\nmembers. The use of logical DNS hostnames avoids configuration\nchanges due to ip address changes.Create the necessary data directories for each member by issuing a\ncommand similar to the following:mkdir -p /srv/mongodb/rs0-0  /srv/mongodb/rs0-1 /srv/mongodb/rs0-2This will create directories called \"rs0-0\", \"rs0-1\", and \"rs0-2\", which\nwill contain the instances' database files.Start your mongod instances in their own shell windows by issuing the following\ncommands:WarningBefore binding to a non-localhost (e.g. publicly accessible)\nIP address, ensure you have secured your cluster from unauthorized\naccess. For a complete list of security recommendations, see\nSecurity Checklist. At minimum, consider\nenabling authentication and\nhardening network infrastructure.First member:mongod --replSet rs0 --port 27017 --bind_ip localhost,<hostname(s)|ip address(es)> --dbpath /srv/mongodb/rs0-0  --oplogSize 128Second member:mongod --replSet rs0 --port 27018 --bind_ip localhost,<hostname(s)|ip address(es)> --dbpath /srv/mongodb/rs0-1  --oplogSize 128Third member:mongod --replSet rs0 --port 27019 --bind_ip localhost,<hostname(s)|ip address(es)> --dbpath /srv/mongodb/rs0-2 --oplogSize 128This starts each instance as a member of a replica set named\nrs0, each running on a distinct port, and specifies the path to\nyour data directory with the --dbpath setting.\nIf you are already using the suggested ports, select different ports.The instances bind to both the localhost and the ip address\nof the host.The --oplogSize setting reduces the\ndisk space that each mongod instance uses. [1]\nThis is ideal for testing and development deployments as it prevents\noverloading your machine. For more information on this and other\nconfiguration options, see Configuration File Options.Connect to one of your mongod instances through\nmongosh. You will need to indicate which instance by\nspecifying its port number. For the sake of simplicity and clarity,\nyou may want to choose the first one, as in the following command;mongosh --port 27017In mongosh, use rs.initiate() to\ninitiate the replica set. You can create a replica set\nconfiguration object in mongosh environment, as\nin the following example:rsconf = {  _id: \"rs0\",  members: [    {     _id: 0,     host: \"<hostname>:27017\"    },    {     _id: 1,     host: \"<hostname>:27018\"    },    {     _id: 2,     host: \"<hostname>:27019\"    }   ]}replacing <hostname> with your system's hostname,\nand then pass the rsconf file to rs.initiate() as\nfollows:rs.initiate( rsconf )Display the current replica configuration\nby issuing the following command:rs.conf()The replica set configuration object resembles the following:{   \"_id\" : \"rs0\",   \"version\" : 1,   \"protocolVersion\" : NumberLong(1),   \"members\" : [      {         \"_id\" : 0,         \"host\" : \"<hostname>:27017\",         \"arbiterOnly\" : false,         \"buildIndexes\" : true,         \"hidden\" : false,         \"priority\" : 1,         \"tags\" : {         },         \"secondaryDelaySecs\" : NumberLong(0),         \"votes\" : 1      },      {         \"_id\" : 1,         \"host\" : \"<hostname>:27018\",         \"arbiterOnly\" : false,         \"buildIndexes\" : true,         \"hidden\" : false,         \"priority\" : 1,         \"tags\" : {         },         \"secondaryDelaySecs\" : NumberLong(0),         \"votes\" : 1      },      {         \"_id\" : 2,         \"host\" : \"<hostname>:27019\",         \"arbiterOnly\" : false,         \"buildIndexes\" : true,         \"hidden\" : false,         \"priority\" : 1,         \"tags\" : {         },         \"secondaryDelaySecs\" : NumberLong(0),         \"votes\" : 1      }   ],   \"settings\" : {      \"chainingAllowed\" : true,      \"heartbeatIntervalMillis\" : 2000,      \"heartbeatTimeoutSecs\" : 10,      \"electionTimeoutMillis\" : 10000,      \"catchUpTimeoutMillis\" : -1,      \"getLastErrorModes\" : {      },      \"getLastErrorDefaults\" : {         \"w\" : 1,         \"wtimeout\" : 0      },      \"replicaSetId\" : ObjectId(\"598f630adc9053c6ee6d5f38\")   }}Check the status of your replica set at any time with the\nrs.status() operation.TipSee also: The documentation of the following shell functions for\nmore information:rs.initiate()rs.conf()rs.reconfig()rs.add()You may also consider the simple setup script\nas an example of a basic automatically-configured replica set.Refer to Replica Set Read and Write Semantics\nfor a detailed explanation of read and write semantics in MongoDB.[1] Starting in MongoDB 4.0, the oplog can grow past its configured size\nlimit to avoid deleting the majority commit point.\u00a9 MongoDB, Inc 2008-present. MongoDB, Mongo, and the leaf logo are registered trademarks of MongoDB, Inc.Give Feedback\u2190 \u00a0Deploy a Replica SetDeploy a Geographically Redundant Replica Set\u00a0\u2192On this pageOverviewRequirementsConsiderationsProcedure\n\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "windows sdk 7 1 setup failure?", "id": 872, "answers": [{"answer_id": 867, "document_id": 553, "question_id": 872, "text": "I had to uninstall the following:\n\n\"Microsoft Visual C++ 2010 x64 Redistributable\"\n\"Microsoft Visual C++ 2010 x86 Redistributable\"\nBefore installing the Windows 7.1 SDK, and the install package reinstalls those two during installation.", "answer_start": 3162, "answer_category": null}], "is_impossible": false}], "context": "106\n\n\n25\nI am trying to install Windows SDK for Windows 7 with .NET Framework 4 but when I open the setup I receive an error:\n\nSome Windows SDK components require the RTM .NET Framework 4. Setup detected a pre-release version of .NET Framework 4. If you continue with Setup, these components will not be installed. If you want to install these components, click Cancel, then install the .NET Framework 4 from https://go.microsoft.com/fwlink/?LinkID=187668 and then rerun Setup.\n\nClick OK to continue.\n\nWhen I went to install the .NET Framework 4 it appears a message saying that there is already the .NET Framework 4 on my PC:\n\nThe Microsoft .NET Framework 4 is already part of the operating system. No need to install the .NET Framework 4 redistributable. More information.\n\nAn equal or higher version of the .NET Framework 4 has already been installed on the computer.\n\nI don't know what to do anymore. I am using Windows 10 Enterprise (x64).\n\n.net\nwindows\nsdk\n.net-4.0\ninstallation\nShare\nImprove this question\nFollow\nedited Jan 20 '18 at 19:19\n\nivan_pozdeev\n29.9k1414 gold badges8787 silver badges134134 bronze badges\nasked Jul 16 '15 at 13:41\n\ndngadelha\n1,26222 gold badges1111 silver badges1414 bronze badges\n8\nPossible duplicate of Cannot install windows SDK 7.1 on windows 10 \u2013 \nPalec\n May 16 '16 at 14:24\n3\nNowadays, the link they provide is broken and just redirects to the generic .NET page >:( \u2013 \nNick T\n Apr 5 '17 at 4:53\nAdd a comment\n5 Answers\n\n107\n\nWith Windows 10 x64, the setup is blocked by:\n\nHKEY_LOCAL_MACHINE\\SOFTWARE\\Wow6432Node\\Microsoft\\NET Framework Setup\\NDP\\v4\\Full\\Version\nHKEY_LOCAL_MACHINE\\SOFTWARE\\Wow6432Node\\Microsoft\\NET Framework Setup\\NDP\\v4\\Client\\Version\nChange both values temporarily to 4.0.30319 and the setup will let you continue. Make sure you edit the registry with elevated privileges, otherwise you will not be allowed to change the values.\n\nShare\nImprove this answer\nFollow\nedited Oct 3 '17 at 6:08\n\nrogerdpack\n53.6k3131 gold badges227227 silver badges346346 bronze badges\nanswered Oct 21 '15 at 13:02\n\nTorsten Hoffmann\n1,09411 gold badge88 silver badges22 bronze badges\n12\nOn Windows 7 (32 bit) these keys can be found at HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\NET Framework Setup\\NDP\\v4\\. \u2013 \nJarno\n Apr 26 '16 at 11:30 \nI tried all the above, yours did the trick! Is it required to reset the verion values to the before? \u2013 \nlinusg\n Apr 30 '16 at 14:16\n35\nIf you get an error like 'Error writing the value\u2019s new contents.', you don't have permissions to change the value. See this guide to change that: groovypost.com/howto/\u2026 \u2013 \nMaarten Kieft\n Jun 6 '16 at 13:27\n1\n@ArvoBowen : The fix is for installing SDK 7.1. not the .NET Framework 4. The fix is for fooling the SDK to think the already installed framework is an older version to allow the installation to continue. \u2013 \nTapio\n Oct 6 '16 at 12:08\n5\nThis did unblock the installer at an early point in the install process, but didn't allow full install. MandM's solution, in combination with this one, and uninstalling all .NET Frameworks >= 4.0, and reinstalling just 4.0, seemed to work for me. \u2013 \nAdrian Keister\n Apr 18 '17 at 14:44\nShow 5 more comments\n\n\n26\n\nI had to uninstall the following:\n\n\"Microsoft Visual C++ 2010 x64 Redistributable\"\n\"Microsoft Visual C++ 2010 x86 Redistributable\"\nBefore installing the Windows 7.1 SDK, and the install package reinstalls those two during installation.\n\nShare\nImprove this answer\nFollow\nanswered Aug 21 '15 at 16:38\n\nMandM\n3,10344 gold badges3030 silver badges5252 bronze badges\n4\nIf error message persists despite this workaround, just click OK and proceed with installation. The installation will succeed this time.. \u2013 \nRobert Va\u017ean\n Sep 21 '15 at 13:16 \nI have the same problem, and I did install \"compatible\" redistribution of the MVC++2010, as per Microsoft article here: support.microsoft.com/en-us/kb/2717426 but there are still part of the SDK that refuses to install properly. In particular, I cannot managed to install resgen.exe I may have to uninstall 4.6 completely as proposed in the other comment, and revert to version 4 of .NET Note, this is link to this other question: stackoverflow.com/questions/16516139/\u2026 \u2013 \nThomas Corriol\n Oct 7 '15 at 17:04 \nAdd a comment\n\n17\n\nIn order to deal with this problem, I uninstalled my .NET framework version 4.6 and installed 4. Then I installed the SDK, and the problem was gone.\n\nShare\nImprove this answer\nFollow\nedited Oct 1 '15 at 23:10\n\nNathaniel Ford\n17.6k1919 gold badges7979 silver badges9191 bronze badges\nanswered Oct 1 '15 at 22:46\n\nEricS\n72511 gold badge88 silver badges1919 bronze badges\n1\nReverting to version 4 of the .NET Framework, then installing the SDK solved my problem as well. Thanks. \u2013 \nThomas Corriol\n Oct 7 '15 at 17:36\n2\nHow exactly do you uninstall 4.6 and install 4.0? I see no option to install framework 4.0 in the Software List and 4.0 refuses to install saying a newer version is already part of the operating system. \u2013 \nPhilipp\n Mar 2 '16 at 14:00\n6\nSo if I have all Visual Studio components installed I cant install this thing? This (Windows SDK for Windows 7 with .NET Framework 4) is requirement to compile C-code in MATLAB. For this kind of situation I hate windows system. \u2013 \nHelder\n Mar 26 '16 at 13:24 \n1\nI tried this on Windows Server 2012 and managed to somehow uninstall the Windows GUI, resorting to a command line on start up! Luckily I had a backup of that server so just restored from that, but thought that this was worth mentioning. \u2013 \nA. Murray\n Jun 2 '16 at 15:43\n1\nHow to uninstall 4.6.1: Search for or scroll down the list of Microsoft Windows updates to find Update for Microsoft Windows (KB3102467) and click Uninstall. Click Yes to uninstall .NET Framework 4.6.1. After a minute or so you will be prompted to restart the computer. Click Restart Now source: expta.com \u2013 \nbkwdesign\n Sep 19 '16 at 16:45 \nShow 2 more comments\n\n1\n\nI was also facing exactly the same problem mentioned above. After so many attempts the suggestion mentioned in the below link helped\n\nhttps://social.msdn.microsoft.com/Forums/windowsdesktop/en-US/6e6c8a17-1666-42fa-9b5b-dfc21845d2f9/error-installing-windows-7-sdk-71-with-vs2008-vs2010-premium-on-win-7-32bit?forum=windowssdk\n\nThe steps executed by me are as follows\n\nUninstall Visual C++ and .NET 4.x version\nRemove register entries corresponding to these installations. I was having Windows 7, but deleted entries as suggested here\nRestarted Windows\nTried installing SDK using web installer\nThis did not work. Later, invoked the SDK installation when the installation screen is still on [with the Installation Error msg] executed debug related msi [you can also install any extracted msi\nOnce after the installation completes, close the Installation error UI [step 5] and reinvoke the installation of SDK.\nThis time, UI screen option would change. Options that you can see is Change, Repair etc. Select Change and complete other component installation.\nShare\nImprove this answer\nFollow\nedited Sep 18 '16 at 12:01\n\nKrish Munot\n1,03711 gold badge1818 silver badges2828 bronze badges\nanswered Jul 15 '16 at 8:12\n\nYashwanth\n12911 silver badge77 bronze badges\nAdd a comment\n\n0\n\nI had an older version of .NET Framework and the C++ 2010 Redistributable x64 and x86 both. Uninstalling the 2010 allowed me to continue with no issues. I didn't uninstall the older version of .NET, but I did download and install the latest version FIRST.\n\nThe only component in the SDK I wanted to install was the Windows Performance Toolkit. I still got the warning at the start of the install. But it went through with anyway.\n\nMaybe that adds a little thought in here...", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "iOS App deployment without AppStore", "id": 1095, "answers": [{"answer_id": 1087, "document_id": 672, "question_id": 1095, "text": "You only have two real options\nAd Hoc - Limited to 100 Devices. Devices must be explicitly added to a provision.\nEnterprise - No device limit, devices do not need to explicitly added to provisions. In effect, these builds will run on any device; the caveat, you are not legally allowed to distribute these builds to anyone outside your company.", "answer_start": 515, "answer_category": null}], "is_impossible": false}], "context": "I'm developping an App in my company. We want to distribute this App to our customers but without using the AppStore from Apple, is it possible\nI heard about MDM (mobile device manager) but I'm not really sure if it will cover this need\nI heard also about Enterprise developer license for in house deployment but if I'm understanding correctly it means the App can be deployed only inside my company and not to our customers, is it correct\nIf you definitely do not want to participate in the App Store environment.\nYou only have two real options\nAd Hoc - Limited to 100 Devices. Devices must be explicitly added to a provision.\nEnterprise - No device limit, devices do not need to explicitly added to provisions. In effect, these builds will run on any device; the caveat, you are not legally allowed to distribute these builds to anyone outside your company.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to deploy a existing SSIS Package in sql server 2012?", "id": 1086, "answers": [{"answer_id": 1078, "document_id": 663, "question_id": 1086, "text": "The 2012 SSIS Project Deployment model in Visual Studio contains a file for project parameters, project level connection managers, packages and anything else you've added to the project.", "answer_start": 229, "answer_category": null}], "is_impossible": false}], "context": "I am working on SSIS Package .I added one more data flow task to existing ssis package.After complition of adding new task i rebuilded the Package it was suceed with out any errors . Do i need to deploy it to Development server?\nThe 2012 SSIS Project Deployment model in Visual Studio contains a file for project parameters, project level connection managers, packages and anything else you've added to the project.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How do I include a dependency's test jar into a Maven project's deployment?", "id": 1717, "answers": [{"answer_id": 1705, "document_id": 1290, "question_id": 1717, "text": "try the following?\n<dependency>\n    <groupId>com.example</groupId>\n    <artifactId>foo</artifactId>\n    <version>1.0.0-SNAPSHOT</version>\n    <classifier>tests</classifier>\n    <scope>test</scope>\n</dependency>\n", "answer_start": 857, "answer_category": null}], "is_impossible": false}], "context": "This causes the source jar to be deployed with a different name: com.example-foo.jar and does not deploy the test jar. I also tried using <classifier> instead of <type> in the dependency, but it still does the same. I tried using the above dependency outside of the profile (alongside the others), but it still behaves the same.\nIf I add the <type> to the main dependency (without adding the other dependency) I get the test jar deployed (with the same name as above), but the source, naturally, does not get deployed.\nThe only difference from what's written in the documentation is the fact that the scope is not specified for the test dependency. Does it only work for the test scope? Can I somehow deploy the test classes differently.\nI see you use test-jar as type and you used classifier instead of type but probably also with test-jar ... but did you try the following?\n<dependency>\n    <groupId>com.example</groupId>\n    <artifactId>foo</artifactId>\n    <version>1.0.0-SNAPSHOT</version>\n    <classifier>tests</classifier>\n    <scope>test</scope>\n</dependency>\n\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Makefile, header dependencies", "id": 1874, "answers": [{"answer_id": 1860, "document_id": 1445, "question_id": 1874, "text": "If you are using a GNU compiler, the compiler can assemble a list of dependencies for you. Makefile fragment:\ndepend: .depend\n.depend: $(SRCS)\n        rm -f \"$@\"\n        $(CC) $(CFLAGS) -MM $^ -MF \"$@\"\ninclude .depend\t", "answer_start": 427, "answer_category": null}], "is_impossible": false}], "context": "Let's say I have a makefile with the rule\n%.o: %.c\n gcc -Wall -Iinclude ...\nI want *.o to be rebuilt whenever a header file changes. Rather than work out a list of dependencies, whenever any header file in /include changes, then all objects in the dir must be rebuilt.\nI can't think of a nice way to change the rule to accomodate this, I'm open to suggestions. Bonus points if the list of headers doesn't have to be hard-coded\nIf you are using a GNU compiler, the compiler can assemble a list of dependencies for you. Makefile fragment:\ndepend: .depend\n.depend: $(SRCS)\n        rm -f \"$@\"\n        $(CC) $(CFLAGS) -MM $^ -MF \"$@\"\ninclude .depend\t\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "ssh_init: Network error: Cannot assign requested address", "id": 1704, "answers": [{"answer_id": 1692, "document_id": 1277, "question_id": 1704, "text": " try:\npscp -P 22 c:\\documents\\foo.txt user@example.com:/tmp/foo.", "answer_start": 298, "answer_category": null}], "is_impossible": false}], "context": "I am trying to set up a connection and transfer files using putty on a windows 10 platform. I have verified that the default port in putty is 22. When I run the command in the command line to connect and transfer files though I get the above error. Any idea why this is or what I should do? Did you try:\npscp -P 22 c:\\documents\\foo.txt user@example.com:/tmp/foo. this was it for me too. Not sure what it takes to get into that state, but for me it was on a pretty new install. The \"Default Settings\" saved session in putty showed port 22, and reloading kept port 22. But sure enough, saving it again made pscp start using port 22 for every IP I gave it, instead of the port 0 it had been using for every IP.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Best way to deploy on Glassfish V3", "id": 1044, "answers": [{"answer_id": 1039, "document_id": 625, "question_id": 1044, "text": "You could use the asadmin command. A remote deployment in its simplest from would look like:asadmin deploy --user=<adminuser> --host=<hostname> <path to jar/war/ear>", "answer_start": 179, "answer_category": null}], "is_impossible": false}], "context": "What is the best way to deploy my web project (or ear project) to remote server and glassfish?\nHow to use ant-deploy.xml and build-impl.xml that netbeans create for this purpose?\nYou could use the asadmin command. A remote deployment in its simplest from would look like:asadmin deploy --user=<adminuser> --host=<hostname> <path to jar/war/ear>\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Creating an installer for Java desktop application", "id": 490, "answers": [{"answer_id": 495, "document_id": 219, "question_id": 490, "text": "You should use InnoSetup. It has always worked very well. It can do everything you need (unpack files, put shortcuts on desktop, start menu etc) and generates installers that we are used to.", "answer_start": 175, "answer_category": null}], "is_impossible": false}], "context": "I know this question has been asked many a times and all the time there is an answer which says about using an executable jar or making an .exe using launch4j or similar app.\nYou should use InnoSetup. It has always worked very well. It can do everything you need (unpack files, put shortcuts on desktop, start menu etc) and generates installers that we are used to.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Tomcat Intellij Idea: Remote deploy", "id": 1346, "answers": [{"answer_id": 1336, "document_id": 915, "question_id": 1346, "text": "The correct way to deploy remotely is editing JAVA_OPTS environment variable on the remote server. Just enter the command below:\nexport JAVA_OPTS=\"-Dcom.sun.management.jmxremote=\n-Dcom.sun.management.jmxremote.port=1099\n-Dcom.sun.management.jmxremote.ssl=false\n-Dcom.sun.management.jmxremote.authenticate=false\"", "answer_start": 720, "answer_category": null}], "is_impossible": false}], "context": "RackSpace Cloud Server Ubuntu-12.04, Intellij Idea-11.1.2, Windows-8, Tomcat-7.0.26, JDK-6.\nOn Intellij Idea when i try to run jsf project on my remote Tomcat 7 server it says:\nError running servername: Unable to connect to the ip-address:1099\nWhat I've tried?\nSetting CATALINA_OPTS or JAVA_OPTS on the server side with:\n CATALINA_OPTS=-Dcom.sun.management.jmxremote \n-Dcom.sun.management.jmxremote.port=1099 \n-Dcom.sun.management.jmxremote.ssl=false \n-Dcom.sun.management.jmxremote.authenticate=false\nand\nJAVA_OPTS=-Dcom.sun.management.jmxremote \n-Dcom.sun.management.jmxremote.port=1099 \n-Dcom.sun.management.jmxremote.ssl=false \n-Dcom.sun.management.jmxremote.authenticate=false\nBut this one did not work, any ideas?\nThe correct way to deploy remotely is editing JAVA_OPTS environment variable on the remote server. Just enter the command below:\nexport JAVA_OPTS=\"-Dcom.sun.management.jmxremote=\n-Dcom.sun.management.jmxremote.port=1099\n-Dcom.sun.management.jmxremote.ssl=false\n-Dcom.sun.management.jmxremote.authenticate=false\"\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to build qt out of source", "id": 1752, "answers": [{"answer_id": 1739, "document_id": 1324, "question_id": 1752, "text": "Basically, you just have to run configure.exe from your build directory. For example:\nmkdir \\qt\\4.5.2-build\ncd \\qt\\4.5.2-build\nset PATH=%cd%\\bin;%PATH%\n\\qt\\4.5.2-sources\\configure.exe -platform win32-msvc2005", "answer_start": 1006, "answer_category": null}], "is_impossible": false}], "context": "I was searching a lot through Qt forums and Google for the last few days, but I could not find any obvious answer to this question.\nI found the -prefix option (not even documented on Windows) that can be supplied to configure to specify different install directory, but this is not clear separation of the sources and binaries at all, since the build is still done in the source directory and then the files needed for installation are copied to the install directoy. I tried this -prefix option, and came to some problems. (i.e It doesn't copy the .pdb files to the install directory.)\nThe effect of the above two lines was that qt was compiled in the source dir and not directly to my builddir specified with prefix. Then the compiled files were copied in my builddir. I was hoping for something that will build my Qt files directly to the build dir, cause this way I stil need 4 Gb space for my source dir during the compilation. Also the pdb files were not copied to my buildir which is another issue.\nBasically, you just have to run configure.exe from your build directory. For example:\nmkdir \\qt\\4.5.2-build\ncd \\qt\\4.5.2-build\nset PATH=%cd%\\bin;%PATH%\n\\qt\\4.5.2-sources\\configure.exe -platform win32-msvc2005\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "lxml installation error ubuntu 14.04 (internal compiler error)", "id": 769, "answers": [{"answer_id": 769, "document_id": 456, "question_id": 769, "text": "sudo dd if=/dev/zero of=/swapfile bs=1024 count=524288\nsudo chmod 600 /swapfile\nsudo mkswap /swapfile\nsudo swapon /swapfile", "answer_start": 99, "answer_category": null}], "is_impossible": false}], "context": "Possible solution (if you have no ability to increase memory on that machine) is to add swap file.\nsudo dd if=/dev/zero of=/swapfile bs=1024 count=524288\nsudo chmod 600 /swapfile\nsudo mkswap /swapfile\nsudo swapon /swapfile\nfrom https://github.com/pydata/pandas/issues/1880#issuecomment-9920484\nThis worked for me on smallest digital ocean machine\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "visual studio 2015 download getting stuck at applying microsoft asp net", "id": 1486, "answers": [{"answer_id": 1475, "document_id": 1062, "question_id": 1486, "text": "Create user/group accounts\nCreate directory trees with specific ownership and permissions\nInstall open-source applications, potentially compiling them from source during install\nInsert pre-compiled binaries, scripts, config files, and docs into specific directories\nRegister init-type startup and shutdown scripts\nGenerate encryption keys\nVerify connectivity to a central server", "answer_start": 794, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nWe have a Linux server application that is comprised of a number of open-source tools as well as programs we've written ourselves. Ideally we would like to be able to install this application on any common Linux distribution.\n\nIn the past, we've written perl scripts to automate installs of this application. Unfortunately, due to idiosyncrasies of different Linux distros, the logic inside these install scripts gets horribly complex, and can change as new versions of each supported distro are released. Maintaining the installer thus becomes one of the most time-intensive parts of the project!\n\nI'm looking for assistance, be it a framework, documentation, code samples, that can make this process less painful. Here are the types of things our installer needs to do:\n\n\nCreate user/group accounts\nCreate directory trees with specific ownership and permissions\nInstall open-source applications, potentially compiling them from source during install\nInsert pre-compiled binaries, scripts, config files, and docs into specific directories\nRegister init-type startup and shutdown scripts\nGenerate encryption keys\nVerify connectivity to a central server\n\n    \n\nInstead of the installer approach, I think a better way than having a single script that does it at install time is to have a build system which generates .deb or .rpm files suitable for installation on each system you have to support.\n\nA poor man's way of going at that might be to use checkinstall, which creates packages from the files installed via 'make install'. So you'd build your app on each system and have the package magically created in the distro's native format.\n    \n\nI believe that most of the tasks which you describe are fairly standardized between Linux distros.  In my experience, the following should work the Debian family (including Ubuntu) and the Red Hat family (including Fedora and CentOS):\n\n\nCreate user / group accounts - adduser command\nCreate directory trees - mkdir or install, or just expand a tarball\nInstall open source applications - Unless you have particularly esoteric needs, this should probably be left to the distro's package manager.\nInstall files - install, or just expand a tarball\nStartup and shutdown scripts - install to /etc/init.d then symlink to /etc/rc*.d\n\n\nVMware Server is freely available for Linux and does most of the tasks which you describe.  It uses Perl and maybe shell for its installation and configuration, so you might see the approach that it takes.\n\nHowever, speaking as a Linux admin, I strongly prefer applications that integrate with my package management system.  In other words, create .deb and .rpm files, as Vinko Vrsalovic suggested.  Building packages is extremely well documented:\n\n\nBuilding RPMs for Fedora (or Red Hat or CentOS): draft documentation, RPM Guide\nBuilding .debs for Debian (or Ubuntu): Debian Maintainer's Guide\n\n    \n\nI tried Autopackage a few years ago, don't know how universal it is but worked quite well (was the only truly universal way back then). Surely you have to provide some LSB-compatible ways of setting up proper directories on your own, but this piece of software should help you.\n\nThough there's probably still too much diversity among linux distributions to do everything in a completely platform-agnostic way but I may be wrong.\n    \n\nYou may want to try BitRock InstallBuilder. It is a cross platform installation tool that allows you to do exactly what you are looking for (adding users, installing services, install pre-compiled binaries, etc).  Although some of the other posts mention a number of tools that you could use in your scripts, the problem is that every Linux distribution is a bit different and simple tasks like adding an user or installer a service are suddenly non-trivial when you need to do them across Debian, Ubuntu, Mandriva, RedHat, Gentoo, etc.  A good cross platform installer should isolate you from all that. Many commercial open source companies like MySQL, SugarCRM, Zenoss, Jaspersoft, Groundwork etc. have built installers based on our technology exactly because of that (in addition to their regular source code tarballs, etc.)  We also provide free licenses for open source projects.\n    \n\nAutopackage now merged with Listaller project. Documentation's not really thorough yet but seems to be working.\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Mac PackageMaker \"Destination Select\" step A) skipping B)Option permanently disabled short answer?", "id": 969, "answers": [{"answer_id": 964, "document_id": 596, "question_id": 969, "text": "DO NOT TRY IT!", "answer_start": 8111, "answer_category": null}], "is_impossible": false}, {"question": "Mac PackageMaker \"Destination Select\" step A) skipping B)Option permanently disabled long answers?", "id": 970, "answers": [{"answer_id": 965, "document_id": 596, "question_id": 970, "text": "REALLY; DO NOT TRY IT! Read Installer Problems and Solutions", "answer_start": 8140, "answer_category": null}], "is_impossible": false}], "context": "Making macOS Installer Packages which are Developer ID ready\nAsked 9 years, 4 months ago\nActive 1 year ago\nViewed 167k times\n\nReport this ad\n\n199\n\n\n204\nNote: This is for OS X Installer packages only, packages for submission to the Mac App Store follow different rules.\n\nBecause of Mountain Lion's Gatekeeper I finally had to take my PackageMaker build script behind the barn and shoot it. PackageMaker was already removed from Xcode and moved into \"Auxiliary Tools for Xcode\", so hopefully it will be soon forgotten.\n\nThe question is how do I use pkgbuild, productbuild, and pkgutil to replace it?\n\nxcode\npackagemaker\npkgbuild\nproductbuild\nShare\nImprove this question\nFollow\nedited Jan 31 '20 at 8:53\nasked Jul 14 '12 at 21:43\n\ncatlan\n23.9k88 gold badges6565 silver badges7474 bronze badges\nso I assume the issue with packagemaker is inability to properly sign pkg files for use with gatekeeper on Mountain Lion? \u2013 \nJasonZ\n Aug 12 '12 at 21:36\n1\nIt is possible, but PackageMaker always was buggy has hell, and got deprecated with Mac OS X 10.6 Snow Leopard. It will save you time in the long run to just get familiar with the new tools. \u2013 \ncatlan\n Aug 12 '12 at 23:08 \n@catlan: Do you have an official link that says packagemaker has been deprecated on 10.6? \u2013 \nCarl\n Aug 22 '12 at 2:09\n2\n@carleeto: It was never announced as being deprecated, just removed from Xcode and eventually \"disappeared\" like a Burmese protester. \u2013 \nbug\n Oct 23 '12 at 19:53\n5\nXcode 4.6 release notes: Deprecation of Package Maker adcdownload.apple.com/Developer_Tools/xcode_4.6/\u2026 \u2013 \ncatlan\n Jan 29 '13 at 9:52\nShow 2 more comments\n5 Answers\n\n364\n\n+200\nOur example project has two build targets: HelloWorld.app and Helper.app. We make a component package for each and combine them into a product archive.\n\nA component package contains payload to be installed by the OS X Installer. Although a component package can be installed on its own, it is typically incorporated into a product archive.\n\nOur tools: pkgbuild, productbuild, and pkgutil\nAfter a successful \"Build and Archive\" open $BUILT_PRODUCTS_DIR in the Terminal.\n\n$ cd ~/Library/Developer/Xcode/DerivedData/.../InstallationBuildProductsLocation\n$ pkgbuild --analyze --root ./HelloWorld.app HelloWorldAppComponents.plist\n$ pkgbuild --analyze --root ./Helper.app HelperAppComponents.plist\nThis give us the component-plist, you find the value description in the \"Component Property List\" section. pkgbuild -root generates the component packages, if you don't need to change any of the default properties you can omit the --component-plist parameter in the following command.\n\nproductbuild --synthesize results in a Distribution Definition.\n\n$ pkgbuild --root ./HelloWorld.app \\\n    --component-plist HelloWorldAppComponents.plist \\\n    HelloWorld.pkg\n$ pkgbuild --root ./Helper.app \\\n    --component-plist HelperAppComponents.plist \\\n    Helper.pkg\n$ productbuild --synthesize \\\n    --package HelloWorld.pkg --package Helper.pkg \\\n    Distribution.xml \nIn the Distribution.xml you can change things like title, background, welcome, readme, license, and so on. You turn your component packages and distribution definition with this command into a product archive:\n\n$ productbuild --distribution ./Distribution.xml \\\n    --package-path . \\\n    ./Installer.pkg\nI recommend to take a look at iTunes Installers Distribution.xml to see what is possible. You can extract \"Install iTunes.pkg\" with:\n\n$ pkgutil --expand \"Install iTunes.pkg\" \"Install iTunes\"\nLets put it together\nI usually have a folder named Package in my project which includes things like Distribution.xml, component-plists, resources and scripts.\n\nAdd a Run Script Build Phase named \"Generate Package\", which is set to Run script only when installing:\n\nVERSION=$(defaults read \"${BUILT_PRODUCTS_DIR}/${FULL_PRODUCT_NAME}/Contents/Info\" CFBundleVersion)\n\nPACKAGE_NAME=`echo \"$PRODUCT_NAME\" | sed \"s/ /_/g\"`\nTMP1_ARCHIVE=\"${BUILT_PRODUCTS_DIR}/$PACKAGE_NAME-tmp1.pkg\"\nTMP2_ARCHIVE=\"${BUILT_PRODUCTS_DIR}/$PACKAGE_NAME-tmp2\"\nTMP3_ARCHIVE=\"${BUILT_PRODUCTS_DIR}/$PACKAGE_NAME-tmp3.pkg\"\nARCHIVE_FILENAME=\"${BUILT_PRODUCTS_DIR}/${PACKAGE_NAME}.pkg\"\n\npkgbuild --root \"${INSTALL_ROOT}\" \\\n    --component-plist \"./Package/HelloWorldAppComponents.plist\" \\\n    --scripts \"./Package/Scripts\" \\\n    --identifier \"com.test.pkg.HelloWorld\" \\\n    --version \"$VERSION\" \\\n    --install-location \"/\" \\\n    \"${BUILT_PRODUCTS_DIR}/HelloWorld.pkg\"\npkgbuild --root \"${BUILT_PRODUCTS_DIR}/Helper.app\" \\\n    --component-plist \"./Package/HelperAppComponents.plist\" \\\n    --identifier \"com.test.pkg.Helper\" \\\n    --version \"$VERSION\" \\\n    --install-location \"/\" \\\n    \"${BUILT_PRODUCTS_DIR}/Helper.pkg\"\nproductbuild --distribution \"./Package/Distribution.xml\"  \\\n    --package-path \"${BUILT_PRODUCTS_DIR}\" \\\n    --resources \"./Package/Resources\" \\\n    \"${TMP1_ARCHIVE}\"\n\npkgutil --expand \"${TMP1_ARCHIVE}\" \"${TMP2_ARCHIVE}\"\n    \n# Patches and Workarounds\n\npkgutil --flatten \"${TMP2_ARCHIVE}\" \"${TMP3_ARCHIVE}\"\n\nproductsign --sign \"Developer ID Installer: John Doe\" \\\n    \"${TMP3_ARCHIVE}\" \"${ARCHIVE_FILENAME}\"\nIf you don't have to change the package after it's generated with productbuild you could get rid of the pkgutil --expand and pkgutil --flatten steps. Also you could use the --sign paramenter on productbuild instead of running productsign.\n\nSign an OS X Installer\nPackages are signed with the Developer ID Installer certificate which you can download from Developer Certificate Utility.\n\nThey signing is done with the --sign \"Developer ID Installer: John Doe\" parameter of pkgbuild, productbuild or productsign.\n\nNote that if you are going to create a signed product archive using productbuild, there is no reason to sign the component packages.\n\nDeveloper Certificate Utility\n\nAll the way: Copy Package into Xcode Archive\nTo copy something into the Xcode Archive we can't use the Run Script Build Phase. For this we need to use a Scheme Action.\n\nEdit Scheme and expand Archive. Then click post-actions and add a New Run Script Action:\n\nIn Xcode 6:\n\n#!/bin/bash\n\nPACKAGES=\"${ARCHIVE_PATH}/Packages\"\n  \nPACKAGE_NAME=`echo \"$PRODUCT_NAME\" | sed \"s/ /_/g\"`\nARCHIVE_FILENAME=\"$PACKAGE_NAME.pkg\"\nPKG=\"${OBJROOT}/../BuildProductsPath/${CONFIGURATION}/${ARCHIVE_FILENAME}\"\n\nif [ -f \"${PKG}\" ]; then\n    mkdir \"${PACKAGES}\"\n    cp -r \"${PKG}\" \"${PACKAGES}\"\nfi\nIn Xcode 5, use this value for PKG instead:\n\nPKG=\"${OBJROOT}/ArchiveIntermediates/${TARGET_NAME}/BuildProductsPath/${CONFIGURATION}/${ARCHIVE_FILENAME}\"\nIn case your version control doesn't store Xcode Scheme information I suggest to add this as shell script to your project so you can simple restore the action by dragging the script from the workspace into the post-action.\n\nScripting\nThere are two different kinds of scripting: JavaScript in Distribution Definition Files and Shell Scripts.\n\nThe best documentation about Shell Scripts I found in WhiteBox - PackageMaker How-to, but read this with caution because it refers to the old package format.\n\nApple Silicon\nIn order for the package to run as arm64, the Distribution file has to specify in its hostArchitectures section that it supports arm64 in addition to x86_64:\n\n<options hostArchitectures=\"arm64,x86_64\" />\nAdditional Reading\nFlat Package Format - The missing documentation\nInstaller Problems and Solutions\nStupid tricks with pkgbuild\npersisting obsolescence\nKnown Issues and Workarounds\nDestination Select Pane\n\nThe user is presented with the destination select option with only a single choice - \"Install for all users of this computer\". The option appears visually selected, but the user needs to click on it in order to proceed with the installation, causing some confusion.\n\nExample showing the installer bug\n\nApples Documentation recommends to use <domains enable_anywhere ... /> but this triggers the new more buggy Destination Select Pane which Apple doesn't use in any of their Packages.\n\nUsing the deprecate <options rootVolumeOnly=\"true\" /> give you the old Destination Select Pane. Example showing old Destination Select Pane\n\nYou want to install items into the current user\u2019s home folder.\n\nShort answer: DO NOT TRY IT!\n\nLong answer: REALLY; DO NOT TRY IT! Read Installer Problems and Solutions. You know what I did even after reading this? I was stupid enough to try it. Telling myself I'm sure that they fixed the issues in 10.7 or 10.8.\n\nFirst of all I saw from time to time the above mentioned Destination Select Pane Bug. That should have stopped me, but I ignored it. If you don't want to spend the week after you released your software answering support e-mails that they have to click once the nice blue selection DO NOT use this.\n\nYou are now thinking that your users are smart enough to figure the panel out, aren't you? Well here is another thing about home folder installation, THEY DON'T WORK!\n\nI tested it for two weeks on around 10 different machines with different OS versions and what not, and it never failed. So I shipped it. Within an hour of the release I heart back from users who just couldn't install it. The logs hinted to permission issues you are not gonna be able to fix.\n\nSo let's repeat it one more time: We do not use the Installer for home folder installations!\n\nRTFD for Welcome, Read-me, License and Conclusion is not accepted by productbuild.\n\nInstaller supported since the beginning RTFD files to make pretty Welcome screens with images, but productbuild doesn't accept them.\n\nWorkarounds: Use a dummy rtf file and replace it in the package by after productbuild is done.\n\nNote: You can also have Retina images inside the RTFD file. Use multi-image tiff files for this: tiffutil -cat Welcome.tif Welcome_2x.tif -out FinalWelcome.tif. More details.\n\nStarting an application when the installation is done with a BundlePostInstallScriptPath script:\n\n#!/bin/bash\n\nLOGGED_IN_USER_ID=`id -u \"${USER}\"`\n\nif [ \"${COMMAND_LINE_INSTALL}\" = \"\" ]\nthen\n    /bin/launchctl asuser \"${LOGGED_IN_USER_ID}\" /usr/bin/open -g PATH_OR_BUNDLE_ID\nfi\n\nexit 0\nIt is important to run the app as logged in user, not as the installer user. This is done with launchctl asuser uid path. Also we only run it when it is not a command line installation, done with installer tool or Apple Remote Desktop.", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Error in installation a R package", "id": 711, "answers": [{"answer_id": 714, "document_id": 401, "question_id": 711, "text": "Just close any other R sessions running parallelly and you will be good to go", "answer_start": 211, "answer_category": null}], "is_impossible": false}], "context": "Please help me, I cannot install \"MASS\" package.\n> library(MASS)\nError in library(MASS) : there is no package called \u2018MASS\u2019\nI tried to install MASS package from local:\nI had the same problem with e1071 package. Just close any other R sessions running parallelly and you will be good to go.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How do I update Java for my Mac?", "id": 173, "answers": [{"answer_id": 180, "document_id": 102, "question_id": 173, "text": "Launch the Java Control Panel by clicking the Java icon under System Preferences.\nGo to the Update tab in Java Control Panel and click on Update Now button that brings up Installer window.\n\nUpdate Java\nClick on Install Update.\nClick on Install and Relaunch.\nSave all your work before clicking Install and Relaunch.\nOnce the installation is completed the Java application will be relaunched.\nIf the Java application does not relaunch, manually relaunch so you can take advantage of the latest Java.", "answer_start": 529, "answer_category": null}], "is_impossible": false}], "context": "\nThis article applies to:\nPlatform(s): Mac OS X\nJava version(s): 7.0, 8.0\nThe information on this page pertains to Oracle Java starting with Java 7, supported with Mac versions 10.7.3 and above.\n\nEvery time you launch a Java applet, a Java Web Start application or the Java Control Panel, the system first launches your program and then, in the background (so that performance of your Java application is not impacted), it determines if it has checked in the last 7 days for a Java update.\n\nUpdate Java in the Java Control Panel\nLaunch the Java Control Panel by clicking the Java icon under System Preferences.\nGo to the Update tab in Java Control Panel and click on Update Now button that brings up Installer window.\n\nUpdate Java\nClick on Install Update.\nClick on Install and Relaunch.\nSave all your work before clicking Install and Relaunch.\nOnce the installation is completed the Java application will be relaunched.\nIf the Java application does not relaunch, manually relaunch so you can take advantage of the latest Java.\nIf you choose Skip This Version but later decide to check for an update, you can launch the Java Control Panel by clicking the Java icon in System Preferences. Go to the Update tab to initiate an update check.\n\nIf you choose Remind Me Later you will be reminded of the update the next time you run Java.", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Uninstall boost and install another version", "id": 1799, "answers": [{"answer_id": 1785, "document_id": 1371, "question_id": 1799, "text": "You can uninstall with\napt-get --purge remove libboost-dev libboost-doc", "answer_start": 421, "answer_category": null}], "is_impossible": false}], "context": "I've installed the boost libraries on Linux Mint 12 using the command sudo apt-get install libboost-dev libboost-doc, which installs the default version available in the repositories. However, the project I have to do needs the 1.44 version of boost. How do I uninstall the default (current) version 1.46 and install 1.44?\nI couldn't find the documentation on the boost website to install boost from the .tar.gz package.\nYou can uninstall with\napt-get --purge remove libboost-dev libboost-doc\nDownload the package you need from boost website, extract and follow \"getting started\" instructions found inside index.html in the extracted directory.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to generate .env file for laravel?", "id": 1153, "answers": [{"answer_id": 1146, "document_id": 730, "question_id": 1153, "text": "You can download env.example, rename it to .env and edit it. Just set up correct DB credentials etc.\nDon't forget to When you use the php artisan key:generate it will generate the new key to your .env file", "answer_start": 202, "answer_category": null}], "is_impossible": false}], "context": "If I try the first way .env file is not created. How can I ask laravel,artisan or composer to create a .env file for me? If the .env a file is missing, then there is another way to generate a .env file\nYou can download env.example, rename it to .env and edit it. Just set up correct DB credentials etc.\nDon't forget to When you use the php artisan key:generate it will generate the new key to your .env file\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How do I set the default Java installation/runtime (Windows)?", "id": 794, "answers": [{"answer_id": 790, "document_id": 477, "question_id": 794, "text": "I have patched the behaviour of my eclipse startup shortcut in the properties dialogue\nfrom\n\"E:\\Program Files\\eclipse\\eclipse.exe\"\nto\n\"E:\\Program Files\\eclipse\\eclipse.exe\" -vm \"E:\\Program Files\\Java\\jdk1.6.0_30\\bin\"", "answer_start": 542, "answer_category": null}], "is_impossible": false}], "context": "I'm in the situation where I've installed the JDK, but I can't run applets in browsers (I may not have installed the JRE).\nHowever, when I install the JRE, it clobbers my JDK as the default runtime. This breaks pretty much everything (Eclipse, Ant) - as they require a server JVM.\nThere's no JAVA_HOME environment variable these days - it just seems to use some registry magic (setting the system path is of no use either). Previously, I've just uninstalled the JRE after I've used it to restore the JDK. This time I want to fix it properly.\nI have patched the behaviour of my eclipse startup shortcut in the properties dialogue\nfrom\n\"E:\\Program Files\\eclipse\\eclipse.exe\"\nto\n\"E:\\Program Files\\eclipse\\eclipse.exe\" -vm \"E:\\Program Files\\Java\\jdk1.6.0_30\\bin\"\n\nThis also manifests itself with the jre autoupdater - once upon a time, I had a working setup with the JDK and JRE, but it updated and bust everything.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "When to restart and not reload Nginx?", "id": 596, "answers": [{"answer_id": 602, "document_id": 321, "question_id": 596, "text": "Reloading nginx is safer than restarting because before old process will be terminated, new configuration file is parsed and whole process is aborted if there are any problems with it. On the other hand when you restart nginx you might encounter situation in which nginx will stop, and won't start back again, because of syntax error. Reloading terminates the old process, so any memory leaks should be cleared anyway.", "answer_start": 326, "answer_category": null}], "is_impossible": false}], "context": "When is it necessary to restart nginx and reload will not suffice?\nDoes it make a difference if an extension like passenger is used?\nShould the service be restarted if it consumes too much memory. Any other reasons for restarting Nginx, particularly after a configuration change either in an extension or a Nginx core config?\nReloading nginx is safer than restarting because before old process will be terminated, new configuration file is parsed and whole process is aborted if there are any problems with it. On the other hand when you restart nginx you might encounter situation in which nginx will stop, and won't start back again, because of syntax error. Reloading terminates the old process, so any memory leaks should be cleared anyway.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "HintPath vs ReferencePath in Visual Studio", "id": 1841, "answers": [{"answer_id": 1827, "document_id": 1412, "question_id": 1841, "text": "You can see this MSDN blog: https://blogs.msdn.microsoft.com/manishagarwal/2005/09/28/resolving-file-references-in-team-build-part-2/", "answer_start": 1360, "answer_category": null}], "is_impossible": false}], "context": "What exactly is the difference between the HintPath in a .csproj file and the ReferencePath in a .csproj.user file? We're trying to commit to a convention where dependency DLLs are in a \"releases\" svn repo and all projects point to a particular release. Since different developers have different folder structures, relative references won't work, so we came up with a scheme to use an environment variable pointing to the particular developer's releases folder to create an absolute reference. So after a reference is added, we manually edit the project file to change the reference to an absolute path using the environment variable.\nI've noticed that this can be done with both the HintPath and the ReferencePath, but the only difference I could find between them is that HintPath is resolved at build-time and ReferencePath when the project is loaded into the IDE. I'm not really sure what the ramifications of that are though. I have noticed that VS sometimes rewrites the .csproj.user and I have to rewrite the ReferencePath, but I'm not sure what triggers that.\nI've heard that it's best not to check in the .csproj.user file since it's user-specific, so I'd like to aim for that, but I've also heard that the HintPath-specified DLL isn't \"guaranteed\" to be loaded if the same DLL is e.g. located in the project's output directory. Any thoughts on this?\nYou can see this MSDN blog: https://blogs.msdn.microsoft.com/manishagarwal/2005/09/28/resolving-file-references-in-team-build-part-2/\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "What is Microsoft Store packages?", "id": 123, "answers": [{"answer_id": 131, "document_id": 80, "question_id": 123, "text": "The Microsoft Store package is an easily installable Python interpreter that is intended mainly for interactive use, for example, by students.", "answer_start": 56, "answer_category": null}], "is_impossible": false}, {"question": "How to launch python after install python?", "id": 124, "answers": [{"answer_id": 132, "document_id": 80, "question_id": 124, "text": "Python may be launched by finding it in Start. Alternatively, it will be available from any Command Prompt or PowerShell session by typing python. ", "answer_start": 588, "answer_category": null}], "is_impossible": false}, {"question": "How to create python virtual environment?", "id": 125, "answers": [{"answer_id": 133, "document_id": 80, "question_id": 125, "text": "Virtual environments can be created with python -m venv and activated and used as normal.", "answer_start": 1260, "answer_category": null}], "is_impossible": false}, {"question": "How to remove python?", "id": 126, "answers": [{"answer_id": 134, "document_id": 80, "question_id": 126, "text": "To remove Python, open Settings and use Apps and Features, or else find Python in Start and right-click to select Uninstall. Uninstalling will remove all packages you installed directly into this Python installation, but will not remove any virtual environments", "answer_start": 1704, "answer_category": null}], "is_impossible": false}, {"question": "What is nuget.org ?", "id": 127, "answers": [{"answer_id": 135, "document_id": 80, "question_id": 127, "text": "The nuget.org package is a reduced size Python environment intended for use on continuous integration and build systems that do not have a system-wide install of Python.", "answer_start": 2557, "answer_category": null}], "is_impossible": false}, {"question": "What is embedded package?", "id": 128, "answers": [{"answer_id": 136, "document_id": 80, "question_id": 128, "text": "The embedded distribution is a ZIP file containing a minimal Python environment. It is intended for acting as part of another application, rather than being directly accessed by end-users.", "answer_start": 4656, "answer_category": null}], "is_impossible": false}, {"question": "How to enable UTF-8 mode in python?", "id": 129, "answers": [{"answer_id": 137, "document_id": 80, "question_id": 129, "text": "You can enable the Python UTF-8 Mode via the -X utf8 command line option, or the PYTHONUTF8=1 environment variable. See PYTHONUTF8 for enabling UTF-8 mode, and Excursus: Setting environment variables for how to modify environment variables.", "answer_start": 9544, "answer_category": null}], "is_impossible": false}, {"question": "How to launch python 2.7?\n", "id": 130, "answers": [{"answer_id": 138, "document_id": 80, "question_id": 130, "text": "aunch Python 2.7, try the command:\n\npy -2.7", "answer_start": 11720, "answer_category": null}], "is_impossible": false}, {"question": "How to find python site_packages?", "id": 131, "answers": [{"answer_id": 139, "document_id": 80, "question_id": 131, "text": "Python usually stores its library (and thereby your site-packages folder) in the installation directory. So, if you had installed Python to C:\\Python\\, the default library would reside in C:\\Python\\Lib\\ and third-party modules should be stored in C:\\Python\\Lib\\site-packages\\.", "answer_start": 20897, "answer_category": null}], "is_impossible": false}, {"question": "What is pywin32?", "id": 132, "answers": [{"answer_id": 140, "document_id": 80, "question_id": 132, "text": "The PyWin32 module by Mark Hammond is a collection of modules for advanced Windows-specific support. ", "answer_start": 26610, "answer_category": null}], "is_impossible": false}, {"question": "What is cx_Freeze\uff1f", "id": 133, "answers": [{"answer_id": 141, "document_id": 80, "question_id": 133, "text": "cx_Freeze is a distutils extension (see Extending Distutils) which wraps Python scripts into executable Windows programs (*.exe files). When you have done this, you can distribute your application without requiring your users to install Python.", "answer_start": 27072, "answer_category": null}], "is_impossible": false}], "context": "4.2. The Microsoft Store package\nNew in version 3.7.2.\n\nThe Microsoft Store package is an easily installable Python interpreter that is intended mainly for interactive use, for example, by students.\n\nTo install the package, ensure you have the latest Windows 10 updates and search the Microsoft Store app for \u201cPython 3.10\u201d. Ensure that the app you select is published by the Python Software Foundation, and install it.\n\nWarning Python will always be available for free on the Microsoft Store. If you are asked to pay for it, you have not selected the correct package.\nAfter installation, Python may be launched by finding it in Start. Alternatively, it will be available from any Command Prompt or PowerShell session by typing python. Further, pip and IDLE may be used by typing pip or idle. IDLE can also be found in Start.\n\nAll three commands are also available with version number suffixes, for example, as python3.exe and python3.x.exe as well as python.exe (where 3.x is the specific version you want to launch, such as 3.10). Open \u201cManage App Execution Aliases\u201d through Start to select which version of Python is associated with each command. It is recommended to make sure that pip and idle are consistent with whichever version of python is selected.\n\nVirtual environments can be created with python -m venv and activated and used as normal.\n\nIf you have installed another version of Python and added it to your PATH variable, it will be available as python.exe rather than the one from the Microsoft Store. To access the new installation, use python3.exe or python3.x.exe.\n\nThe py.exe launcher will detect this Python installation, but will prefer installations from the traditional installer.\n\nTo remove Python, open Settings and use Apps and Features, or else find Python in Start and right-click to select Uninstall. Uninstalling will remove all packages you installed directly into this Python installation, but will not remove any virtual environments\n\n4.2.1. Known Issues\nBecause of restrictions on Microsoft Store apps, Python scripts may not have full write access to shared locations such as TEMP and the registry. Instead, it will write to a private copy. If your scripts must modify the shared locations, you will need to install the full installer.\n\nFor more detail on the technical basis for these limitations, please consult Microsoft\u2019s documentation on packaged full-trust apps, currently available at docs.microsoft.com/en-us/windows/msix/desktop/desktop-to-uwp-behind-the-scenes\n\n4.3. The nuget.org packages\nNew in version 3.5.2.\n\nThe nuget.org package is a reduced size Python environment intended for use on continuous integration and build systems that do not have a system-wide install of Python. While nuget is \u201cthe package manager for .NET\u201d, it also works perfectly fine for packages containing build-time tools.\n\nVisit nuget.org for the most up-to-date information on using nuget. What follows is a summary that is sufficient for Python developers.\n\nThe nuget.exe command line tool may be downloaded directly from https://aka.ms/nugetclidl, for example, using curl or PowerShell. With the tool, the latest version of Python for 64-bit or 32-bit machines is installed using:\n\nnuget.exe install python -ExcludeVersion -OutputDirectory .\nnuget.exe install pythonx86 -ExcludeVersion -OutputDirectory .\nTo select a particular version, add a -Version 3.x.y. The output directory may be changed from ., and the package will be installed into a subdirectory. By default, the subdirectory is named the same as the package, and without the -ExcludeVersion option this name will include the specific version installed. Inside the subdirectory is a tools directory that contains the Python installation:\n\n# Without -ExcludeVersion\n> .\\python.3.5.2\\tools\\python.exe -V\nPython 3.5.2\n\n# With -ExcludeVersion\n> .\\python\\tools\\python.exe -V\nPython 3.5.2\nIn general, nuget packages are not upgradeable, and newer versions should be installed side-by-side and referenced using the full path. Alternatively, delete the package directory manually and install it again. Many CI systems will do this automatically if they do not preserve files between builds.\n\nAlongside the tools directory is a build\\native directory. This contains a MSBuild properties file python.props that can be used in a C++ project to reference the Python install. Including the settings will automatically use the headers and import libraries in your build.\n\nThe package information pages on nuget.org are www.nuget.org/packages/python for the 64-bit version and www.nuget.org/packages/pythonx86 for the 32-bit version.\n\n4.4. The embeddable package\nNew in version 3.5.\n\nThe embedded distribution is a ZIP file containing a minimal Python environment. It is intended for acting as part of another application, rather than being directly accessed by end-users.\n\nWhen extracted, the embedded distribution is (almost) fully isolated from the user\u2019s system, including environment variables, system registry settings, and installed packages. The standard library is included as pre-compiled and optimized .pyc files in a ZIP, and python3.dll, python37.dll, python.exe and pythonw.exe are all provided. Tcl/tk (including all dependants, such as Idle), pip and the Python documentation are not included.\n\nNote The embedded distribution does not include the Microsoft C Runtime and it is the responsibility of the application installer to provide this. The runtime may have already been installed on a user\u2019s system previously or automatically via Windows Update, and can be detected by finding ucrtbase.dll in the system directory.\nThird-party packages should be installed by the application installer alongside the embedded distribution. Using pip to manage dependencies as for a regular Python installation is not supported with this distribution, though with some care it may be possible to include and use pip for automatic updates. In general, third-party packages should be treated as part of the application (\u201cvendoring\u201d) so that the developer can ensure compatibility with newer versions before providing updates to users.\n\nThe two recommended use cases for this distribution are described below.\n\n4.4.1. Python Application\nAn application written in Python does not necessarily require users to be aware of that fact. The embedded distribution may be used in this case to include a private version of Python in an install package. Depending on how transparent it should be (or conversely, how professional it should appear), there are two options.\n\nUsing a specialized executable as a launcher requires some coding, but provides the most transparent experience for users. With a customized launcher, there are no obvious indications that the program is running on Python: icons can be customized, company and version information can be specified, and file associations behave properly. In most cases, a custom launcher should simply be able to call Py_Main with a hard-coded command line.\n\nThe simpler approach is to provide a batch file or generated shortcut that directly calls the python.exe or pythonw.exe with the required command-line arguments. In this case, the application will appear to be Python and not its actual name, and users may have trouble distinguishing it from other running Python processes or file associations.\n\nWith the latter approach, packages should be installed as directories alongside the Python executable to ensure they are available on the path. With the specialized launcher, packages can be located in other locations as there is an opportunity to specify the search path before launching the application.\n\n4.4.2. Embedding Python\nApplications written in native code often require some form of scripting language, and the embedded Python distribution can be used for this purpose. In general, the majority of the application is in native code, and some part will either invoke python.exe or directly use python3.dll. For either case, extracting the embedded distribution to a subdirectory of the application installation is sufficient to provide a loadable Python interpreter.\n\nAs with the application use, packages can be installed to any location as there is an opportunity to specify search paths before initializing the interpreter. Otherwise, there is no fundamental differences between using the embedded distribution and a regular installation.\n\n4.5. Alternative bundles\nBesides the standard CPython distribution, there are modified packages including additional functionality. The following is a list of popular versions and their key features:\n\nActivePython\nInstaller with multi-platform compatibility, documentation, PyWin32\n\nAnaconda\nPopular scientific modules (such as numpy, scipy and pandas) and the conda package manager.\n\nCanopy\nA \u201ccomprehensive Python analysis environment\u201d with editors and other development tools.\n\nWinPython\nWindows-specific distribution with prebuilt scientific packages and tools for building packages.\n\nNote that these packages may not include the latest versions of Python or other libraries, and are not maintained or supported by the core Python team.\n4.7. UTF-8 mode\nNew in version 3.7.\n\nWindows still uses legacy encodings for the system encoding (the ANSI Code Page). Python uses it for the default encoding of text files (e.g. locale.getpreferredencoding()).\n\nThis may cause issues because UTF-8 is widely used on the internet and most Unix systems, including WSL (Windows Subsystem for Linux).\n\nYou can use the Python UTF-8 Mode to change the default text encoding to UTF-8. You can enable the Python UTF-8 Mode via the -X utf8 command line option, or the PYTHONUTF8=1 environment variable. See PYTHONUTF8 for enabling UTF-8 mode, and Excursus: Setting environment variables for how to modify environment variables.\n\nWhen the Python UTF-8 Mode is enabled, you can still use the system encoding (the ANSI Code Page) via the \u201cmbcs\u201d codec.\n\nNote that adding PYTHONUTF8=1 to the default environment variables will affect all Python 3.7+ applications on your system. If you have any Python 3.7+ applications which rely on the legacy system encoding, it is recommended to set the environment variable temporarily or use the -X utf8 command line option.\n\nNote Even when UTF-8 mode is disabled, Python uses UTF-8 by default on Windows for:\nConsole I/O including standard I/O (see PEP 528 for details).\n\nThe filesystem encoding (see PEP 529 for details).\n\n4.8. Python Launcher for Windows\nNew in version 3.3.\n\nThe Python launcher for Windows is a utility which aids in locating and executing of different Python versions. It allows scripts (or the command-line) to indicate a preference for a specific Python version, and will locate and execute that version.\n\nUnlike the PATH variable, the launcher will correctly select the most appropriate version of Python. It will prefer per-user installations over system-wide ones, and orders by language version rather than using the most recently installed version.\n\nThe launcher was originally specified in PEP 397.\n\n4.8.1. Getting started\n4.8.1.1. From the command-line\nChanged in version 3.6.\n\nSystem-wide installations of Python 3.3 and later will put the launcher on your PATH. The launcher is compatible with all available versions of Python, so it does not matter which version is installed. To check that the launcher is available, execute the following command in Command Prompt:\n\npy\nYou should find that the latest version of Python you have installed is started - it can be exited as normal, and any additional command-line arguments specified will be sent directly to Python.\n\nIf you have multiple versions of Python installed (e.g., 2.7 and 3.10) you will have noticed that Python 3.10 was started - to launch Python 2.7, try the command:\n\npy -2.7\nIf you want the latest version of Python 2.x you have installed, try the command:\n\npy -2\nYou should find the latest version of Python 2.x starts.\n\nIf you see the following error, you do not have the launcher installed:\n\n'py' is not recognized as an internal or external command,\noperable program or batch file.\nPer-user installations of Python do not add the launcher to PATH unless the option was selected on installation.\n\n4.8.1.2. Virtual environments\nNew in version 3.5.\n\nIf the launcher is run with no explicit Python version specification, and a virtual environment (created with the standard library venv module or the external virtualenv tool) active, the launcher will run the virtual environment\u2019s interpreter rather than the global one. To run the global interpreter, either deactivate the virtual environment, or explicitly specify the global Python version.\n\n4.8.1.3. From a script\nLet\u2019s create a test Python script - create a file called hello.py with the following contents\n\n#! python\nimport sys\nsys.stdout.write(\"hello from Python %s\\n\" % (sys.version,))\nFrom the directory in which hello.py lives, execute the command:\n\npy hello.py\nYou should notice the version number of your latest Python 2.x installation is printed. Now try changing the first line to be:\n\n#! python3\nRe-executing the command should now print the latest Python 3.x information. As with the above command-line examples, you can specify a more explicit version qualifier. Assuming you have Python 2.6 installed, try changing the first line to #! python2.6 and you should find the 2.6 version information printed.\n\nNote that unlike interactive use, a bare \u201cpython\u201d will use the latest version of Python 2.x that you have installed. This is for backward compatibility and for compatibility with Unix, where the command python typically refers to Python 2.\n\n4.8.1.4. From file associations\nThe launcher should have been associated with Python files (i.e. .py, .pyw, .pyc files) when it was installed. This means that when you double-click on one of these files from Windows explorer the launcher will be used, and therefore you can use the same facilities described above to have the script specify the version which should be used.\n\nThe key benefit of this is that a single launcher can support multiple Python versions at the same time depending on the contents of the first line.\n\n4.8.2. Shebang Lines\nIf the first line of a script file starts with #!, it is known as a \u201cshebang\u201d line. Linux and other Unix like operating systems have native support for such lines and they are commonly used on such systems to indicate how a script should be executed. This launcher allows the same facilities to be used with Python scripts on Windows and the examples above demonstrate their use.\n\nTo allow shebang lines in Python scripts to be portable between Unix and Windows, this launcher supports a number of \u2018virtual\u2019 commands to specify which interpreter to use. The supported virtual commands are:\n\n/usr/bin/env python\n\n/usr/bin/python\n\n/usr/local/bin/python\n\npython\n\nFor example, if the first line of your script starts with\n\n#! /usr/bin/python\nThe default Python will be located and used. As many Python scripts written to work on Unix will already have this line, you should find these scripts can be used by the launcher without modification. If you are writing a new script on Windows which you hope will be useful on Unix, you should use one of the shebang lines starting with /usr.\n\nAny of the above virtual commands can be suffixed with an explicit version (either just the major version, or the major and minor version). Furthermore the 32-bit version can be requested by adding \u201c-32\u201d after the minor version. I.e. /usr/bin/python2.7-32 will request usage of the 32-bit python 2.7.\n\nNew in version 3.7: Beginning with python launcher 3.7 it is possible to request 64-bit version by the \u201c-64\u201d suffix. Furthermore it is possible to specify a major and architecture without minor (i.e. /usr/bin/python3-64).\n\nThe /usr/bin/env form of shebang line has one further special property. Before looking for installed Python interpreters, this form will search the executable PATH for a Python executable. This corresponds to the behaviour of the Unix env program, which performs a PATH search.\n\n4.8.3. Arguments in shebang lines\nThe shebang lines can also specify additional options to be passed to the Python interpreter. For example, if you have a shebang line:\n\n#! /usr/bin/python -v\nThen Python will be started with the -v option\n\n4.8.4. Customization\n4.8.4.1. Customization via INI files\nTwo .ini files will be searched by the launcher - py.ini in the current user\u2019s \u201capplication data\u201d directory (i.e. the directory returned by calling the Windows function SHGetFolderPath with CSIDL_LOCAL_APPDATA) and py.ini in the same directory as the launcher. The same .ini files are used for both the \u2018console\u2019 version of the launcher (i.e. py.exe) and for the \u2018windows\u2019 version (i.e. pyw.exe).\n\nCustomization specified in the \u201capplication directory\u201d will have precedence over the one next to the executable, so a user, who may not have write access to the .ini file next to the launcher, can override commands in that global .ini file.\n\n4.8.4.2. Customizing default Python versions\nIn some cases, a version qualifier can be included in a command to dictate which version of Python will be used by the command. A version qualifier starts with a major version number and can optionally be followed by a period (\u2018.\u2019) and a minor version specifier. Furthermore it is possible to specify if a 32 or 64 bit implementation shall be requested by adding \u201c-32\u201d or \u201c-64\u201d.\n\nFor example, a shebang line of #!python has no version qualifier, while #!python3 has a version qualifier which specifies only a major version.\n\nIf no version qualifiers are found in a command, the environment variable PY_PYTHON can be set to specify the default version qualifier. If it is not set, the default is \u201c3\u201d. The variable can specify any value that may be passed on the command line, such as \u201c3\u201d, \u201c3.7\u201d, \u201c3.7-32\u201d or \u201c3.7-64\u201d. (Note that the \u201c-64\u201d option is only available with the launcher included with Python 3.7 or newer.)\n\nIf no minor version qualifiers are found, the environment variable PY_PYTHON{major} (where {major} is the current major version qualifier as determined above) can be set to specify the full version. If no such option is found, the launcher will enumerate the installed Python versions and use the latest minor release found for the major version, which is likely, although not guaranteed, to be the most recently installed version in that family.\n\nOn 64-bit Windows with both 32-bit and 64-bit implementations of the same (major.minor) Python version installed, the 64-bit version will always be preferred. This will be true for both 32-bit and 64-bit implementations of the launcher - a 32-bit launcher will prefer to execute a 64-bit Python installation of the specified version if available. This is so the behavior of the launcher can be predicted knowing only what versions are installed on the PC and without regard to the order in which they were installed (i.e., without knowing whether a 32 or 64-bit version of Python and corresponding launcher was installed last). As noted above, an optional \u201c-32\u201d or \u201c-64\u201d suffix can be used on a version specifier to change this behaviour.\n\nExamples:\n\nIf no relevant options are set, the commands python and python2 will use the latest Python 2.x version installed and the command python3 will use the latest Python 3.x installed.\n\nThe commands python3.1 and python2.7 will not consult any options at all as the versions are fully specified.\n\nIf PY_PYTHON=3, the commands python and python3 will both use the latest installed Python 3 version.\n\nIf PY_PYTHON=3.1-32, the command python will use the 32-bit implementation of 3.1 whereas the command python3 will use the latest installed Python (PY_PYTHON was not considered at all as a major version was specified.)\n\nIf PY_PYTHON=3 and PY_PYTHON3=3.1, the commands python and python3 will both use specifically 3.1\n\nIn addition to environment variables, the same settings can be configured in the .INI file used by the launcher. The section in the INI file is called [defaults] and the key name will be the same as the environment variables without the leading PY_ prefix (and note that the key names in the INI file are case insensitive.) The contents of an environment variable will override things specified in the INI file.\n\nFor example:\n\nSetting PY_PYTHON=3.1 is equivalent to the INI file containing:\n\n[defaults]\npython=3.1\nSetting PY_PYTHON=3 and PY_PYTHON3=3.1 is equivalent to the INI file containing:\n\n[defaults]\npython=3\npython3=3.1\n4.8.5. Diagnostics\nIf an environment variable PYLAUNCH_DEBUG is set (to any value), the launcher will print diagnostic information to stderr (i.e. to the console). While this information manages to be simultaneously verbose and terse, it should allow you to see what versions of Python were located, why a particular version was chosen and the exact command-line used to execute the target Python.\n\n4.9. Finding modules\nPython usually stores its library (and thereby your site-packages folder) in the installation directory. So, if you had installed Python to C:\\Python\\, the default library would reside in C:\\Python\\Lib\\ and third-party modules should be stored in C:\\Python\\Lib\\site-packages\\.\n\nTo completely override sys.path, create a ._pth file with the same name as the DLL (python37._pth) or the executable (python._pth) and specify one line for each path to add to sys.path. The file based on the DLL name overrides the one based on the executable, which allows paths to be restricted for any program loading the runtime if desired.\n\nWhen the file exists, all registry and environment variables are ignored, isolated mode is enabled, and site is not imported unless one line in the file specifies import site. Blank paths and lines starting with # are ignored. Each path may be absolute or relative to the location of the file. Import statements other than to site are not permitted, and arbitrary code cannot be specified.\n\nNote that .pth files (without leading underscore) will be processed normally by the site module when import site has been specified.\n\nWhen no ._pth file is found, this is how sys.path is populated on Windows:\n\nAn empty entry is added at the start, which corresponds to the current directory.\n\nIf the environment variable PYTHONPATH exists, as described in Environment variables, its entries are added next. Note that on Windows, paths in this variable must be separated by semicolons, to distinguish them from the colon used in drive identifiers (C:\\ etc.).\n\nAdditional \u201capplication paths\u201d can be added in the registry as subkeys of \\SOFTWARE\\Python\\PythonCore{version}\\PythonPath under both the HKEY_CURRENT_USER and HKEY_LOCAL_MACHINE hives. Subkeys which have semicolon-delimited path strings as their default value will cause each path to be added to sys.path. (Note that all known installers only use HKLM, so HKCU is typically empty.)\n\nIf the environment variable PYTHONHOME is set, it is assumed as \u201cPython Home\u201d. Otherwise, the path of the main Python executable is used to locate a \u201clandmark file\u201d (either Lib\\os.py or pythonXY.zip) to deduce the \u201cPython Home\u201d. If a Python home is found, the relevant sub-directories added to sys.path (Lib, plat-win, etc) are based on that folder. Otherwise, the core Python path is constructed from the PythonPath stored in the registry.\n\nIf the Python Home cannot be located, no PYTHONPATH is specified in the environment, and no registry entries can be found, a default path with relative entries is used (e.g. .\\Lib;.\\plat-win, etc).\n\nIf a pyvenv.cfg file is found alongside the main executable or in the directory one level above the executable, the following variations apply:\n\nIf home is an absolute path and PYTHONHOME is not set, this path is used instead of the path to the main executable when deducing the home location.\n\nThe end result of all this is:\n\nWhen running python.exe, or any other .exe in the main Python directory (either an installed version, or directly from the PCbuild directory), the core path is deduced, and the core paths in the registry are ignored. Other \u201capplication paths\u201d in the registry are always read.\n\nWhen Python is hosted in another .exe (different directory, embedded via COM, etc), the \u201cPython Home\u201d will not be deduced, so the core path from the registry is used. Other \u201capplication paths\u201d in the registry are always read.\n\nIf Python can\u2019t find its home and there are no registry value (frozen .exe, some very strange installation setup) you get a path with some default, but relative, paths.\n\nFor those who want to bundle Python into their application or distribution, the following advice will prevent conflicts with other installations:\n\nInclude a ._pth file alongside your executable containing the directories to include. This will ignore paths listed in the registry and environment variables, and also ignore site unless import site is listed.\n\nIf you are loading python3.dll or python37.dll in your own executable, explicitly call Py_SetPath() or (at least) Py_SetProgramName() before Py_Initialize().\n\nClear and/or overwrite PYTHONPATH and set PYTHONHOME before launching python.exe from your application.\n\nIf you cannot use the previous suggestions (for example, you are a distribution that allows people to run python.exe directly), ensure that the landmark file (Lib\\os.py) exists in your install directory. (Note that it will not be detected inside a ZIP file, but a correctly named ZIP file will be detected instead.)\n\nThese will ensure that the files in a system-wide installation will not take precedence over the copy of the standard library bundled with your application. Otherwise, your users may experience problems using your application. Note that the first suggestion is the best, as the others may still be susceptible to non-standard paths in the registry and user site-packages.\n\nChanged in version 3.6:\nAdds ._pth file support and removes applocal option from pyvenv.cfg.\n\nAdds pythonXX.zip as a potential landmark when directly adjacent to the executable.\n\nDeprecated since version 3.6:\nModules specified in the registry under Modules (not PythonPath) may be imported by importlib.machinery.WindowsRegistryFinder. This finder is enabled on Windows in 3.6.0 and earlier, but may need to be explicitly added to sys.meta_path in the future.\n\n4.10. Additional modules\nEven though Python aims to be portable among all platforms, there are features that are unique to Windows. A couple of modules, both in the standard library and external, and snippets exist to use these features.\n\nThe Windows-specific standard modules are documented in MS Windows Specific Services.\n\n4.10.1. PyWin32\nThe PyWin32 module by Mark Hammond is a collection of modules for advanced Windows-specific support. This includes utilities for:\n\nComponent Object Model (COM)\n\nWin32 API calls\n\nRegistry\n\nEvent log\n\nMicrosoft Foundation Classes (MFC) user interfaces\n\nPythonWin is a sample MFC application shipped with PyWin32. It is an embeddable IDE with a built-in debugger.\n\nSee also\nWin32 How Do I\u2026?\nby Tim Golden\n\nPython and COM\nby David and Paul Boddie\n\n4.10.2. cx_Freeze\ncx_Freeze is a distutils extension (see Extending Distutils) which wraps Python scripts into executable Windows programs (*.exe files). When you have done this, you can distribute your application without requiring your users to install Python.\n\n4.10.3. WConio\nSince Python\u2019s advanced terminal handling layer, curses, is restricted to Unix-like systems, there is a library exclusive to Windows as well: Windows Console I/O for Python.\n\nWConio is a wrapper for Turbo-C\u2019s CONIO.H, used to create text user interfaces.\n\n4.11. Compiling Python on Windows\nIf you want to compile CPython yourself, first thing you should do is get the source. You can download either the latest release\u2019s source or just grab a fresh checkout.\n\nThe source tree contains a build solution and project files for Microsoft Visual Studio 2015, which is the compiler used to build the official Python releases. These files are in the PCbuild directory.\n\nCheck PCbuild/readme.txt for general information on the build process.\n\nFor extension modules, consult Building C and C++ Extensions on Windows.\n\nSee also\nPython + Windows + distutils + SWIG + gcc MinGW\nor \u201cCreating Python extensions in C/C++ with SWIG and compiling them with MinGW gcc under Windows\u201d or \u201cInstalling Python extension with distutils and without Microsoft Visual C++\u201d by S\u00e9bastien Sauvage, 2003\n\n4.12. Other Platforms\nWith ongoing development of Python, some platforms that used to be supported earlier are no longer supported (due to the lack of users or developers). Check PEP 11 for details on all unsupported platforms.\n\nWindows CE is still supported.\n\nThe Cygwin installer offers to install the Python interpreter as well (cf. Cygwin package source, Maintainer releases)\n\nSee Python for Windows for detailed information about platforms with pre-compiled installers.", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Removing python and then re-installing on Mac OSX?", "id": 1014, "answers": [{"answer_id": 1009, "document_id": 616, "question_id": 1014, "text": "A framework /Library/Frameworks/Python.framework, which includes the Python executable and libraries. The installer adds this location to your shell path. To uninstall MacPython, you can simply remove these three things", "answer_start": 939, "answer_category": null}], "is_impossible": false}, {"question": "How to install python on Mac OSX?", "id": 1015, "answers": [{"answer_id": 1010, "document_id": 616, "question_id": 1015, "text": "macOS since version 10.8 comes with Python 2.7 pre-installed by Apple. If you wish, you are invited to install the most recent version of Python 3 from the Python website (https://www.python.org). A current \u201cuniversal binary\u201d build of Python, which runs natively on the Mac\u2019s new Intel and legacy PPC CPU\u2019s, is available there.", "answer_start": 315, "answer_category": null}], "is_impossible": false}, {"question": "How to run  script python  on Mac OSX from the Finder?", "id": 1016, "answers": [{"answer_id": 1011, "document_id": 616, "question_id": 1016, "text": "Drag it to PythonLauncher\n\nSelect PythonLauncher as the default application to open your script (or any .py script) through the finder Info window and double-click it. PythonLauncher has various preferences to control how your script is launched. Option-dragging allows you to change these for one invocation, or use its Preferences menu to change things globally.", "answer_start": 2959, "answer_category": null}], "is_impossible": false}, {"question": "How to run  script python  on Mac OSX from the Terminal Window?", "id": 1017, "answers": [{"answer_id": 1012, "document_id": 616, "question_id": 1017, "text": "make sure that /usr/local/bin is in your shell search path", "answer_start": 2840, "answer_category": null}], "is_impossible": false}, {"question": "How to run  script python  on Mac OSX with GUI?", "id": 1018, "answers": [{"answer_id": 1013, "document_id": 616, "question_id": 1018, "text": "With older versions of Python, there is one macOS quirk that you need to be aware of: programs that talk to the Aqua window manager (in other words, anything that has a GUI) need to be run in a special way. Use pythonw instead of python to start such scripts", "answer_start": 3359, "answer_category": null}], "is_impossible": false}, {"question": "How to run  script python  on Mac OSX python 3.9 with GUI ?", "id": 1019, "answers": [{"answer_id": 1014, "document_id": 616, "question_id": 1019, "text": "With Python 3.9, you can use either python or pythonw", "answer_start": 3620, "answer_category": null}], "is_impossible": false}, {"question": "How to installing Additional Python Packages  on Mac OSX?", "id": 1021, "answers": [{"answer_id": 1016, "document_id": 616, "question_id": 1021, "text": "Packages can be installed via the standard Python distutils mode (python setup.py install).\n\nMany packages can also be installed via the setuptools extension or pip wrapper, see https://pip.pypa.io/", "answer_start": 4451, "answer_category": null}], "is_impossible": false}, {"question": "How to set python enviroment variables on Mac OSX?", "id": 1020, "answers": [{"answer_id": 1015, "document_id": 616, "question_id": 1020, "text": "You need to create a file ~/.MacOSX/environment.plis", "answer_start": 3925, "answer_category": null}], "is_impossible": false}, {"question": "How to distribute Python Applications on Mac OSX?", "id": 1022, "answers": [{"answer_id": 1017, "document_id": 616, "question_id": 1022, "text": "The standard tool for deploying standalone Python applications on the Mac is py2app. More information on installing and using py2app can be found at http://undefined.org/python/#py2app.\n", "answer_start": 5624, "answer_category": null}], "is_impossible": false}], "context": "5. Using Python on a Mac\nAuthor\nBob Savage <bobsavage@mac.com>\n\nPython on a Mac running macOS is in principle very similar to Python on any other Unix platform, but there are a number of additional features such as the IDE and the Package Manager that are worth pointing out.\n\n5.1. Getting and Installing MacPython\nmacOS since version 10.8 comes with Python 2.7 pre-installed by Apple. If you wish, you are invited to install the most recent version of Python 3 from the Python website (https://www.python.org). A current \u201cuniversal binary\u201d build of Python, which runs natively on the Mac\u2019s new Intel and legacy PPC CPU\u2019s, is available there.\n\nWhat you get after installing is a number of things:\n\nA Python 3.9 folder in your Applications folder. In here you find IDLE, the development environment that is a standard part of official Python distributions; and PythonLauncher, which handles double-clicking Python scripts from the Finder.\n\nA framework /Library/Frameworks/Python.framework, which includes the Python executable and libraries. The installer adds this location to your shell path. To uninstall MacPython, you can simply remove these three things. A symlink to the Python executable is placed in /usr/local/bin/.\n\nThe Apple-provided build of Python is installed in /System/Library/Frameworks/Python.framework and /usr/bin/python, respectively. You should never modify or delete these, as they are Apple-controlled and are used by Apple- or third-party software. Remember that if you choose to install a newer Python version from python.org, you will have two different but functional Python installations on your computer, so it will be important that your paths and usages are consistent with what you want to do.\n\nIDLE includes a help menu that allows you to access Python documentation. If you are completely new to Python you should start reading the tutorial introduction in that document.\n\nIf you are familiar with Python on other Unix platforms you should read the section on running Python scripts from the Unix shell.\n\n5.1.1. How to run a Python script\nYour best way to get started with Python on macOS is through the IDLE integrated development environment, see section The IDE and use the Help menu when the IDE is running.\n\nIf you want to run Python scripts from the Terminal window command line or from the Finder you first need an editor to create your script. macOS comes with a number of standard Unix command line editors, vim and emacs among them. If you want a more Mac-like editor, BBEdit or TextWrangler from Bare Bones Software (see http://www.barebones.com/products/bbedit/index.html) are good choices, as is TextMate (see https://macromates.com/). Other editors include Gvim (http://macvim-dev.github.io/macvim/) and Aquamacs (http://aquamacs.org/).\n\nTo run your script from the Terminal window you must make sure that /usr/local/bin is in your shell search path.\n\nTo run your script from the Finder you have two options:\n\nDrag it to PythonLauncher\n\nSelect PythonLauncher as the default application to open your script (or any .py script) through the finder Info window and double-click it. PythonLauncher has various preferences to control how your script is launched. Option-dragging allows you to change these for one invocation, or use its Preferences menu to change things globally.\n\n5.1.2. Running scripts with a GUI\nWith older versions of Python, there is one macOS quirk that you need to be aware of: programs that talk to the Aqua window manager (in other words, anything that has a GUI) need to be run in a special way. Use pythonw instead of python to start such scripts.\n\nWith Python 3.9, you can use either python or pythonw.\n\n5.1.3. Configuration\nPython on macOS honors all standard Unix environment variables such as PYTHONPATH, but setting these variables for programs started from the Finder is non-standard as the Finder does not read your .profile or .cshrc at startup. You need to create a file ~/.MacOSX/environment.plist. See Apple\u2019s Technical Document QA1067 for details.\n\nFor more information on installation Python packages in MacPython, see section Installing Additional Python Packages.\n\n5.2. The IDE\nMacPython ships with the standard IDLE development environment. A good introduction to using IDLE can be found at http://www.hashcollision.org/hkn/python/idle_intro/index.html.\n\n5.3. Installing Additional Python Packages\nThere are several methods to install additional Python packages:\n\nPackages can be installed via the standard Python distutils mode (python setup.py install).\n\nMany packages can also be installed via the setuptools extension or pip wrapper, see https://pip.pypa.io/.\n\n5.4. GUI Programming on the Mac\nThere are several options for building GUI applications on the Mac with Python.\n\nPyObjC is a Python binding to Apple\u2019s Objective-C/Cocoa framework, which is the foundation of most modern Mac development. Information on PyObjC is available from https://pypi.org/project/pyobjc/.\n\nThe standard Python GUI toolkit is tkinter, based on the cross-platform Tk toolkit (https://www.tcl.tk). An Aqua-native version of Tk is bundled with OS X by Apple, and the latest version can be downloaded and installed from https://www.activestate.com; it can also be built from source.\n\nwxPython is another popular cross-platform GUI toolkit that runs natively on macOS. Packages and documentation are available from https://www.wxpython.org.\n\nPyQt is another popular cross-platform GUI toolkit that runs natively on macOS. More information can be found at https://riverbankcomputing.com/software/pyqt/intro.\n\n5.5. Distributing Python Applications on the Mac\nThe standard tool for deploying standalone Python applications on the Mac is py2app. More information on installing and using py2app can be found at http://undefined.org/python/#py2app.\n\n5.6. Other Resources\nThe MacPython mailing list is an excellent support resource for Python users and developers on the Mac:\n\nhttps://www.python.org/community/sigs/current/pythonmac-sig/\n\nAnother useful resource is the MacPython wiki:\n\nhttps://wiki.python.org/moin/MacPython", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Deploy ASP.NET web site changes AND SQL Server changes without downtime", "id": 1058, "answers": [{"answer_id": 1054, "document_id": 639, "question_id": 1058, "text": "You should be looking to keep downtime to an absolute minimum rather than eliminating it altogether. Nobody likes downtime, but it's a necessary evil in a lot of cases.", "answer_start": 417, "answer_category": null}], "is_impossible": false}], "context": "I have an ASP.Net 4.0 web site with a SQL Server 2008 database. I want to deploy dependent changes to both the web site and the database at the same time while keeping the site running. My normal procedure is to first deploy the web site changes, and while the web site compiles, deploy the database changes. This works if I am fast enough to get the database changes out before the first request finishes compiling.\nYou should be looking to keep downtime to an absolute minimum rather than eliminating it altogether. Nobody likes downtime, but it's a necessary evil in a lot of cases.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to install Python module without setup.py?", "id": 1765, "answers": [{"answer_id": 1751, "document_id": 1336, "question_id": 1765, "text": "The simplest way to begin using that code on your system is:\n1.put the files into a directory on your machine,\n2.add that directory's path to your PYTHONPATH", "answer_start": 420, "answer_category": null}], "is_impossible": false}], "context": "I'm new to Python and am trying to install this module: http://www.catonmat.net/blog/python-library-for-google-search/\nThere is no setup.py in the directory, but there are these files:\n BeautifulSoup.py   browser.pyc    __init__.pyc  sponsoredlinks.py\n BeautifulSoup.pyc  googlesets.py  search.py     translate.py\n browser.py         __init__.py    search.pyc\nCan someone please tell me how to setup or use this module?\nThe simplest way to begin using that code on your system is:\n1.put the files into a directory on your machine,\n2.add that directory's path to your PYTHONPATH\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Is it possible for Vagrant to use an OS .ISO install image directly/or create a Vagrant box from an ISO on the fly?", "id": 1786, "answers": [{"answer_id": 1772, "document_id": 1358, "question_id": 1786, "text": "You should see this website:https://github.com/jedi4ever/veewee\nIt already provides a lot of out-of-the-box templates for most distributions, which you can customize if required (although this is often not needed). Alternatively you can create your own definitions.", "answer_start": 257, "answer_category": null}], "is_impossible": false}], "context": "Is it possible to automate the creation of a Vagrant .box file for an OS install, from the original ISO?\nTo me, this is a significant gap in the end-to-end automation of Operating System install and configuration on a Virtual Machine that Vagrant provides.\nYou should see this website:https://github.com/jedi4ever/veewee\nIt already provides a lot of out-of-the-box templates for most distributions, which you can customize if required (although this is often not needed). Alternatively you can create your own definitions.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Is AppData now the 'correct' place to install user-specific apps (which modify their own data)?", "id": 1145, "answers": [{"answer_id": 1138, "document_id": 722, "question_id": 1145, "text": "I just installed Visual Studio Code (a Microsoft product) in the default folder of\n%userprofile%\\AppData\\Local\\Programs\\Microsoft VS Code\nThis is probably for getting around the requirement to have an administrator or UAC prompt authorise the installation", "answer_start": 1572, "answer_category": null}], "is_impossible": false}], "context": "I'm probably just being very thick here, but it's not clear to me where I'm supposed to install 'new' user-specific programs on Windows 7 (and presumably Vista too, though I've not specifically looked at that scenario yet).\nUnder Windows XP (rightly or wrongly) we always installed our programs into folders under 'Program Files' and accepted that they'd be kind-of available to everyone. From what I can gather under Windows 7 I'm supposed to install my software under the user's AppData folder (possibly AppData\\Local\\MyApp). That makes a degree of sense, but the fact that this folder is 'hidden' by default means that we're going to have 'fun' talking our users through support stuff.\nI want to install our software so that it's user specific (the Users bit in Windows 7 makes perfect sense) but I do want the user to be able to access it if required. Our program also includes a 'data' subdirectory which it needs to write into while it's running (embedded database), but as the program is intended to be single-user/standalone, the data folder being inside a user-specific folder isn't going to be a problem.\nMy problem is just that whole 'hidden folder' aspect of AppData. As much as I've trawled the MSDN, I can't work out where else I'm supposed to install user-specific programs. Taken one way it would seem to be something like AppData\\Local\\MyApp, and another way it would seem to be just as valid under the user's My Documents\\MyApp equivalent.\nHas anyone got a clear guide for where all this stuff goes? I found the MSDN docs confusing. :-)\n9\nIt's 2019, and I just installed Visual Studio Code (a Microsoft product) in the default folder of\n%userprofile%\\AppData\\Local\\Programs\\Microsoft VS Code\nThis is probably for getting around the requirement to have an administrator or UAC prompt authorise the installation\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Where do OSX applications typically store user configuration data?", "id": 702, "answers": [{"answer_id": 706, "document_id": 393, "question_id": 702, "text": "In /Users/username/Library/Preferences. You also see some stuff being placed in /Users/username/Library/Application Support.", "answer_start": 461, "answer_category": null}], "is_impossible": false}], "context": "I've noticed on OSX, installation is frequently a drag and drop one file kinda deal. I assume that file is an archive of all the applications necessary bits and that the application runs directly from it. Where does the application store configuration data, particularly per user settings when there are multiple users? On Windows, this type of stuff might go in the registry under HKLU or HKLM, or in the Application Data folder for the user or for all users. In /Users/username/Library/Preferences. You also see some stuff being placed in /Users/username/Library/Application Support.\nBoth of these folders have corresponding global locations outside /Users, namely under /Library. These however seem to be used very little by applications as such, for obvious reasons.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "gcloud command not found - while installing Google Cloud SDK", "id": 1556, "answers": [{"answer_id": 1545, "document_id": 1133, "question_id": 1556, "text": "You can try : source ~/.bashrc.", "answer_start": 380, "answer_category": null}], "is_impossible": false}], "context": "I am on a mac and am trying to install the Google Cloud SDK (including the gcloud command line utility) using this command in terminal\ncurl https://sdk.cloud.google.com | bash\nas seen at https://cloud.google.com/sdk/\nIt got all the way to the end and finished but even after I restarted my shell, the gcloud command still says it's not found.\nWhy isn't this installation working?\nYou can try : source ~/.bashrc.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to generate  Android Studio key on MAC OS?\n", "id": 950, "answers": [{"answer_id": 945, "document_id": 587, "question_id": 950, "text": "content_copy\n  keytool -genkey -v -keystore ~/upload-keystore.jks -keyalg RSA -keysize 2048 -validity 10000 -alias upload", "answer_start": 3540, "answer_category": null}], "is_impossible": false}, {"question": "How to generate  Android Studio key on Linux?\n", "id": 951, "answers": [{"answer_id": 946, "document_id": 587, "question_id": 951, "text": "content_copy\n  keytool -genkey -v -keystore ~/upload-keystore.jks -keyalg RSA -keysize 2048 -validity 10000 -alias upload", "answer_start": 3540, "answer_category": null}], "is_impossible": false}, {"question": "How to generate  Android Studio key on Windows?\n", "id": 952, "answers": [{"answer_id": 947, "document_id": 587, "question_id": 952, "text": "content_copy\n  keytool -genkey -v -keystore c:\\Users\\USER_NAME\\upload-keystore.jks -storetype JKS -keyalg RSA -keysize 2048 -validity 10000 -alias upload", "answer_start": 3702, "answer_category": null}], "is_impossible": false}, {"question": "How to install APK on device?", "id": 953, "answers": [{"answer_id": 948, "document_id": 587, "question_id": 953, "text": "Connect your Android device to your computer with a USB cable.\nEnter cd [project].\nRun flutter install.\nPublishing to the Google Play Store", "answer_start": 13311, "answer_category": null}], "is_impossible": false}, {"question": "What is the default version number for application?", "id": 954, "answers": [{"answer_id": 949, "document_id": 587, "question_id": 954, "text": " 1.0.0", "answer_start": 13643, "answer_category": null}], "is_impossible": false}], "context": "Build and release an Android app\nDuring a typical development cycle, you test an app using flutter run at the command line, or by using the Run and Debug options in your IDE. By default, Flutter builds a debug version of your app.\n\nWhen you\u2019re ready to prepare a release version of your app, for example to publish to the Google Play Store, this page can help. Before publishing, you might want to put some finishing touches on your app. This page covers the following topics:\n\nAdding a launcher icon\nEnabling Material Components\nSigning the app\nShrinking your code with R8\nEnabling multidex support\nReviewing the app manifest\nReviewing the build configuration\nBuilding the app for release\nPublishing to the Google Play Store\nUpdating the app\u2019s version number\nAndroid release FAQ\n Note: Throughout this page, [project] refers to the directory that your application is in. While following these instructions, substitute [project] with your app\u2019s directory.\n\nAdding a launcher icon\nWhen a new Flutter app is created, it has a default launcher icon. To customize this icon, you might want to check out the flutter_launcher_icons package.\n\nAlternatively, you can do it manually using the following steps:\n\nReview the Material Design product icons guidelines for icon design.\n\nIn the [project]/android/app/src/main/res/ directory, place your icon files in folders named using configuration qualifiers. The default mipmap- folders demonstrate the correct naming convention.\n\nIn AndroidManifest.xml, update the application tag\u2019s android:icon attribute to reference icons from the previous step (for example, <application android:icon=\"@mipmap/ic_launcher\" ...).\n\nTo verify that the icon has been replaced, run your app and inspect the app icon in the Launcher.\n\nEnabling Material Components\nIf your app uses Platform Views, you may want to enable Material Components by following the steps described in the Getting Started guide for Android.\n\nFor example:\n\nAdd the dependency on Android\u2019s Material in <my-app>/android/app/build.gradle:\ncontent_copy\ndependencies {\n    // ...\n    implementation 'com.google.android.material:material:<version>'\n    // ...\n}\nTo find out the latest version, visit Google Maven.\n\nSet the light theme in <my-app>/android/app/src/main/res/values/styles.xml:\ncontent_copy\n-<style name=\"NormalTheme\" parent=\"@android:style/Theme.Light.NoTitleBar\">\n+<style name=\"NormalTheme\" parent=\"Theme.MaterialComponents.Light.NoActionBar\">\nSet the dark theme in <my-app>/android/app/src/main/res/values-night/styles.xml\ncontent_copy\n-<style name=\"NormalTheme\" parent=\"@android:style/Theme.Black.NoTitleBar\">\n+<style name=\"NormalTheme\" parent=\"Theme.MaterialComponents.DayNight.NoActionBar\">\nSigning the app\nTo publish on the Play Store, you need to give your app a digital signature. Use the following instructions to sign your app.\n\nOn Android, there are two signing keys: deployment and upload. The end-users download the .apk signed with the \u2018deployment key\u2019. An \u2018upload key\u2019 is used to authenticate the .aab / .apk uploaded by developers onto the Play Store and is re-signed with the deployment key once in the Play Store.\n\nIt\u2019s highly recommended to use the automatic cloud managed signing for the deployment key. For more information, see the official Play Store documentation.\nCreate an upload keystore\nIf you have an existing keystore, skip to the next step. If not, create one by either:\n\nFollowing the Android Studio key generation steps\nRunning the following at the command line:\n\nOn Mac/Linux, use the following command:\n\ncontent_copy\n  keytool -genkey -v -keystore ~/upload-keystore.jks -keyalg RSA -keysize 2048 -validity 10000 -alias upload\nOn Windows, use the following command:\n\ncontent_copy\n  keytool -genkey -v -keystore c:\\Users\\USER_NAME\\upload-keystore.jks -storetype JKS -keyalg RSA -keysize 2048 -validity 10000 -alias upload\nThis command stores the upload-keystore.jks file in your home directory. If you want to store it elsewhere, change the argument you pass to the -keystore parameter. However, keep the keystore file private; don\u2019t check it into public source control!\n\n Note:\n\nThe keytool command might not be in your path\u2014it\u2019s part of Java, which is installed as part of Android Studio. For the concrete path, run flutter doctor -v and locate the path printed after \u2018Java binary at:\u2019. Then use that fully qualified path replacing java (at the end) with keytool. If your path includes space-separated names, such as Program Files, use platform-appropriate notation for the names. For example, on Mac/Linux use Program\\ Files, and on Windows use \"Program Files\".\n\nThe -storetype JKS tag is only required for Java 9 or newer. As of the Java 9 release, the keystore type defaults to PKS12.\n\nReference the keystore from the app\nCreate a file named [project]/android/key.properties that contains a reference to your keystore:\n\ncontent_copy\nstorePassword=<password from previous step>\nkeyPassword=<password from previous step>\nkeyAlias=upload\nstoreFile=<location of the key store file, such as /Users/<user name>/upload-keystore.jks>\n Warning: Keep the key.properties file private; don\u2019t check it into public source control.\n\nConfigure signing in gradle\nConfigure gradle to use your upload key when building your app in release mode by editing the [project]/android/app/build.gradle file.\n\nAdd the keystore information from your properties file before the android block:\n\ncontent_copy\n   def keystoreProperties = new Properties()\n   def keystorePropertiesFile = rootProject.file('key.properties')\n   if (keystorePropertiesFile.exists()) {\n       keystoreProperties.load(new FileInputStream(keystorePropertiesFile))\n   }\n\n   android {\n         ...\n   }\nLoad the key.properties file into the keystoreProperties object.\n\nFind the buildTypes block:\n\ncontent_copy\n   buildTypes {\n       release {\n           // TODO: Add your own signing config for the release build.\n           // Signing with the debug keys for now,\n           // so `flutter run --release` works.\n           signingConfig signingConfigs.debug\n       }\n   }\nAnd replace it with the following signing configuration info:\n\ncontent_copy\n   signingConfigs {\n       release {\n           keyAlias keystoreProperties['keyAlias']\n           keyPassword keystoreProperties['keyPassword']\n           storeFile keystoreProperties['storeFile'] ? file(keystoreProperties['storeFile']) : null\n           storePassword keystoreProperties['storePassword']\n       }\n   }\n   buildTypes {\n       release {\n           signingConfig signingConfigs.release\n       }\n   }\nRelease builds of your app will now be signed automatically.\n\n Note: You may need to run flutter clean after changing the gradle file. This prevents cached builds from affecting the signing process.\n\nFor more information on signing your app, see Sign your app on developer.android.com.\n\nShrinking your code with R8\nR8 is the new code shrinker from Google, and it\u2019s enabled by default when you build a release APK or AAB. To disable R8, pass the --no-shrink flag to flutter build apk or flutter build appbundle.\n\n Note: Obfuscation and minification can considerably extend compile time of the Android application.\n\nEnabling multidex support\nWhen writing large apps or making use of large plugins, you may encounter Android\u2019s dex limit of 64k methods when targeting a minimum API of 20 or below. This may also be encountered when running debug versions of your app via flutter run that does not have shrinking enabled.\n\nFlutter tool supports easily enabling multidex. The simplest way is to opt into multidex support when prompted. The tool detects multidex build errors and will ask before making changes to your Android project. Opting in allows Flutter to automatically depend on androidx.multidex:multidex and use a generated FlutterMultiDexApplication as the project\u2019s application.\n\n Note: Multidex support is natively included when targeting min sdk 21+.\n\nYou might also choose to manually support multidex by following Android\u2019s guides and modifying your project\u2019s Android directory configuration. A multidex keep file must be specified to include:\n\ncontent_copy\nio/flutter/embedding/engine/loader/FlutterLoader.class\nio/flutter/util/PathUtils.class\nAlso, include any other classes used in app startup. See the official Android documentation for more detailed guidance on adding multidex support manually.\n\nReviewing the app manifest\nReview the default App Manifest file, AndroidManifest.xml, located in [project]/android/app/src/main and verify that the values are correct, especially the following:\n\napplication\nEdit the android:label in the application tag to reflect the final name of the app.\nuses-permission\nAdd the android.permission.INTERNET permission if your application code needs Internet access. The standard template does not include this tag but allows Internet access during development to enable communication between Flutter tools and a running app.\nReviewing the build configuration\nReview the default Gradle build file, build.gradle, located in [project]/android/app and verify the values are correct, especially the following values in the defaultConfig block:\n\napplicationId\nSpecify the final, unique (Application Id)appid\nversionCode & versionName\nSpecify the internal app version number, and the version number display string. You can do this by setting the version property in the pubspec.yaml file. Consult the version information guidance in the versions documentation.\nminSdkVersion, compilesdkVersion, & targetSdkVersion\nSpecify the minimum API level, the API level on which the app was compiled, and the maximum API level on which the app is designed to run. Consult the API level section in the versions documentation for details.\nbuildToolsVersion\nSpecify the version of Android SDK Build Tools that your app uses. Alternatively, you can use the Android Gradle Plugin in Android Studio, which will automatically import the minimum required Build Tools for your app without the need for this property.\nBuilding the app for release\nYou have two possible release formats when publishing to the Play Store.\n\nApp bundle (preferred)\nAPK\n Note: The Google Play Store prefers the app bundle format. For more information, see Android App Bundle and About Android App Bundles.\n\n Warning: Recently, the Flutter team has received several reports from developers indicating they are experiencing app crashes on certain devices on Android 6.0. If you are targeting Android 6.0, use the following steps:\n\nIf you build an App Bundle Edit android/gradle.properties and add the flag: android.bundle.enableUncompressedNativeLibs=false.\n\nIf you build an APK Make sure android/app/src/AndroidManifest.xml doesn\u2019t set android:extractNativeLibs=false in the <application> tag.\n\nFor more information, see the public issue.\n\nBuild an app bundle\nThis section describes how to build a release app bundle. If you completed the signing steps, the app bundle will be signed. At this point, you might consider obfuscating your Dart code to make it more difficult to reverse engineer. Obfuscating your code involves adding a couple flags to your build command, and maintaining additional files to de-obfuscate stack traces.\n\nFrom the command line:\n\nEnter cd [project]\nRun flutter build appbundle\n(Running flutter build defaults to a release build.)\nThe release bundle for your app is created at [project]/build/app/outputs/bundle/release/app.aab.\n\nBy default, the app bundle contains your Dart code and the Flutter runtime compiled for armeabi-v7a (ARM 32-bit), arm64-v8a (ARM 64-bit), and x86-64 (x86 64-bit).\n\nTest the app bundle\nAn app bundle can be tested in multiple ways\u2014this section describes two.\n\nOffline using the bundle tool\nIf you haven\u2019t done so already, download bundletool from the GitHub repository.\nGenerate a set of APKs from your app bundle.\nDeploy the APKs to connected devices.\nOnline using Google Play\nUpload your bundle to Google Play to test it. You can use the internal test track, or the alpha or beta channels to test the bundle before releasing it in production.\nFollow these steps to upload your bundle to the Play Store.\nBuild an APK\nAlthough app bundles are preferred over APKs, there are stores that don\u2019t yet support app bundles. In this case, build a release APK for each target ABI (Application Binary Interface).\n\nIf you completed the signing steps, the APK will be signed. At this point, you might consider obfuscating your Dart code to make it more difficult to reverse engineer. Obfuscating your code involves adding a couple flags to your build command.\n\nFrom the command line:\n\nEnter cd [project]\nRun flutter build apk --split-per-abi\n(The flutter build command defaults to --release.)\nThis command results in three APK files:\n\n[project]/build/app/outputs/apk/release/app-armeabi-v7a-release.apk\n[project]/build/app/outputs/apk/release/app-arm64-v8a-release.apk\n[project]/build/app/outputs/apk/release/app-x86_64-release.apk\nRemoving the --split-per-abi flag results in a fat APK that contains your code compiled for all the target ABIs. Such APKs are larger in size than their split counterparts, causing the user to download native binaries that are not applicable to their device\u2019s architecture.\n\nInstall an APK on a device\nFollow these steps to install the APK on a connected Android device.\n\nFrom the command line:\n\nConnect your Android device to your computer with a USB cable.\nEnter cd [project].\nRun flutter install.\nPublishing to the Google Play Store\nFor detailed instructions on publishing your app to the Google Play Store, see the Google Play launch documentation.\n\nUpdating the app\u2019s version number\nThe default version number of the app is 1.0.0. To update it, navigate to the pubspec.yaml file and update the following line:\n\nversion: 1.0.0+1\n\nThe version number is three numbers separated by dots, such as 1.0.0 in the example above, followed by an optional build number such as 1 in the example above, separated by a +.\n\nBoth the version and the build number may be overridden in Flutter\u2019s build by specifying --build-name and --build-number, respectively.\n\nIn Android, build-name is used as versionName while build-number used as versionCode. For more information, see Version your app in the Android documentation.\n\nAfter updating the version number in the pubspec file, run flutter pub get from the top of the project, or use the Pub get button in your IDE. This updates the versionName and versionCode in the local.properties file, which are later updated in the build.gradle file when you rebuild the Flutter app.\n\nAndroid release FAQ\nHere are some commonly asked questions about deployment for Android apps.\n\nWhen should I build app bundles versus APKs?\nThe Google Play Store recommends that you deploy app bundles over APKs because they allow a more efficient delivery of the application to your users. However, if you\u2019re distributing your application by means other than the Play Store, an APK may be your only option.\n\nWhat is a fat APK?\nA fat APK is a single APK that contains binaries for multiple ABIs embedded within it. This has the benefit that the single APK runs on multiple architectures and thus has wider compatibility, but it has the drawback that its file size is much larger, causing users to download and store more bytes when installing your application. When building APKs instead of app bundles, it is strongly recommended to build split APKs, as described in build an APK using the --split-per-abi flag.\n\nWhat are the supported target architectures?\nWhen building your application in release mode, Flutter apps can be compiled for armeabi-v7a (ARM 32-bit), arm64-v8a (ARM 64-bit), and x86-64 (x86 64-bit). Flutter does not currently support building for x86 Android (See Issue 9253).\n\nHow do I sign the app bundle created by flutter build appbundle?\nSee Signing the app.\n\nHow do I build a release from within Android Studio?\nIn Android Studio, open the existing android/ folder under your app\u2019s folder. Then, select build.gradle (Module: app) in the project panel:", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Install multiple version of node.js using NVM (Ubuntu)", "id": 1201, "answers": [{"answer_id": 1194, "document_id": 777, "question_id": 1201, "text": "just follow the guide on the github to install :\nhttps://github.com/creationix/nvm#installation\nFor linux machines, its as simple as :\ncurl -o- https://raw.githubusercontent.com/creationix/nvm/v*/install.sh | bash\nReplace v* with the latest version from https://github.com/creationix/nvm/releases.", "answer_start": 100, "answer_category": null}], "is_impossible": false}], "context": "How to install multiple version of node.js in Ubuntu using NVM?\nThe top answer is out of date. Now, just follow the guide on the github to install :\nhttps://github.com/creationix/nvm#installation\nFor linux machines, its as simple as :\ncurl -o- https://raw.githubusercontent.com/creationix/nvm/v*/install.sh | bash\nReplace v* with the latest version from https://github.com/creationix/nvm/releases.\nHere is a detailed, up-to-date manual: https://www.digitalocean.com/community/articles/how-to-install-node-js-with-nvm-node-version-manager-on-a-vps#installation. @racar not like this, because this will not work on Ubuntu!\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to sort depended objects by dependency", "id": 1867, "answers": [{"answer_id": 1853, "document_id": 1438, "question_id": 1867, "text": "You should use a topological sort, see this example :\nhttp://en.wikipedia.org/wiki/Topological_sorting\nIt will give you exactly what you need.", "answer_start": 349, "answer_category": null}], "is_impossible": false}], "context": "I have a collection:\nList<VPair<Item, List<Item>> dependencyHierarchy;\nThe first item in pair is some object (item) and the second one is a collection of the same type objects that the first one depends on. I want to get a List<Item> in order of dependency, so there are not items that depend on the first element and so on (no cycled dependency!).\nYou should use a topological sort, see this example :\nhttp://en.wikipedia.org/wiki/Topological_sorting\nIt will give you exactly what you need.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Installing Homebrew on OS X", "id": 1591, "answers": [{"answer_id": 1580, "document_id": 1167, "question_id": 1591, "text": "It's on the top of the Homebrew homepage.\nFrom a Terminal prompt:\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/H", "answer_start": 216, "answer_category": null}], "is_impossible": false}], "context": "According to the Homebrew site, to install it, I need to type:\nbrew install wget\nI get an error message:\n-bash: brew: command not found\nFound this answer. The problem, however, is I don't see brew in /usr/local/bin.\nIt's on the top of the Homebrew homepage.\nFrom a Terminal prompt:\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/H\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "/usr/bin/codesign failed with exit code 1", "id": 1683, "answers": [{"answer_id": 1670, "document_id": 1256, "question_id": 1683, "text": "1.\tGo to keychain\n2.\tLock it\n3.\tArchive the code, build the project again", "answer_start": 668, "answer_category": null}], "is_impossible": false}], "context": "I am attempting to deploy my first development iPhone app, and am running into some problems. I have successfully went though the online Provisioning Assistant, but now I am stuck. No matter what I do, I always get the following error.\n/usr/bin/codesign failed with exit code 1\nAnyone have any ideas why this is happening?\nIt might be strange answer for codesign issue in Xcode 9.0. I was receiving this error too and did not know what to be done, because everything was correct.\nI went to the keychain, I had the login option \"unlocked\". I locked it and compiled my build again. Xcode itself asked me to open access keychain. I gave access and it worked.\nSteps were:\n1.\tGo to keychain\n2.\tLock it\n3.\tArchive the code, build the project again\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Android device chooser - My device seems offline", "id": 1663, "answers": [{"answer_id": 1650, "document_id": 1236, "question_id": 1663, "text": "\u2022\tRestart adb by issuing 'adb kill-server' followed by 'adb start-server' at a cmd prompt\n\u2022\tDisable and re-enable USB debugging on the phone\n\u2022\tRebooting the phone if it still doesn't work.", "answer_start": 524, "answer_category": null}], "is_impossible": false}], "context": "I have developed an application and i was planning to deploy it to my HTC Desire. I have installed USB driver. I turned on USB debugging on the phone and choosed charge only when phone plugged-in. When I run application Android device chooser show my device offline. I am stuck at this point. Any help would be appreciated.\nI develop at Eclipse Helios on Windows 7 (64bit)\nI've seen this happen a few times on my HTC Desire. I've never figured out whether the problem is in adb or the device but I usually do the following:\n\u2022\tRestart adb by issuing 'adb kill-server' followed by 'adb start-server' at a cmd prompt\n\u2022\tDisable and re-enable USB debugging on the phone\n\u2022\tRebooting the phone if it still doesn't work.\n99% of my issues have been resolved with these steps.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "common update procedure for auto updating apps on osx", "id": 1911, "answers": [{"answer_id": 1898, "document_id": 1483, "question_id": 1911, "text": "Old application (after launch) checks for updates\nif update found download and unpack it into a temp location\nlaunch an external application do to the \"swapping\" replace old currently running app with the new one unpacked into a temp location\nold app terminates", "answer_start": 926, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI'm working on an Objective-C Cocoa app in Xcode for OSX which will be distributed outside the App Store.\n\nOne of the menu items in the app is \"Check for Updates\". The user can click this item and check if there's an update available.\n\nIf there is an update available, the update will be downloaded. \n\nMy question is : What is the common approach for updating an app? Since the app is open, it can't overwrite itself. So how is this typically done? \n\nDo you separate your app into a launcher and the app itself? If so, I imagine that when the user starts the app, it is in fact the launcher that's started. The launcher then checks to see if an update has been downloaded and it replaces the old app binary with the new app binary. Is that how it's done on OSX or is there a smarter way?\n    \n\nSparkle is really good, however if you prefer in-house solution then here is one possible approach.\n\nSteps:  \n\n\nOld application (after launch) checks for updates\nif update found download and unpack it into a temp location\nlaunch an external application do to the \"swapping\" replace old currently running app with the new one unpacked into a temp location\nold app terminates\n\n\nWhere the real \"magic\" happens is when old app is done with the download and unpack. \nThen old app from it's bundle copies to a temp location it's relauncher (simple terminal app) and launches it with the following command line args: \n\n\nFirst param is the process ID of the old app (which is running)\nSecond is the path to the new version (this will be moved into Applications folder)\nThird (optional) path to the old application (used to delete it explicitly)\n\n\nWhen all is done, after launching the relauncher old app terminates.\nAt this point relauncher is already running and waits for old app's termination.\n\nWhen old app terminates then relauncher does three things:\n\n\nDeletes the old app from Applications\nCopies new version from temp location into Applications\nLaunches the new version from Applications. \n\n\nUpdate is done.\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How do I fix a \"Performance counter registry hive consistency\" when installing SQL Server R2 Express?", "id": 1148, "answers": [{"answer_id": 1141, "document_id": 725, "question_id": 1148, "text": "You can skip the Performance counter check in the setup altogether:\nsetup.exe /ACTION=install /SKIPRULES=PerfMonCounterNotCorruptedCheck", "answer_start": 220, "answer_category": null}], "is_impossible": false}], "context": "I have a 64-bit, Windows 7 machine.\nI have tried both the 32-bit and 64-bit versions but each fail on \"Performance counter registry hive consistency\".\nHow can I fix this so that I can install SQL Server 2008 R2 Express?\nYou can skip the Performance counter check in the setup altogether:\nsetup.exe /ACTION=install /SKIPRULES=PerfMonCounterNotCorruptedCheck\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "wix proper creation of desktop shortcut", "id": 1454, "answers": [{"answer_id": 1443, "document_id": 1027, "question_id": 1454, "text": "The first method falls in line with WiX - Create shortcut documentation.\n\nThe second method has a MergeRedirectFolder which I can't seem to find any documentation on, and I don't understand why the second example doesn't require the registry setting since according to WiX Documentation, a registry setting:\n\n\n  is required as a Shortcut cannot serve as the KeyPath for a component when installing non-advertised shortcuts for the current users.", "answer_start": 221, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nThere are two answers on Create shortcut to desktop using WiX\n\nBoth these answers lack any real explanation of what is going on. What is the difference between these two methods of creating shortcuts? The first method falls in line with WiX - Create shortcut documentation.\n\nThe second method has a MergeRedirectFolder which I can't seem to find any documentation on, and I don't understand why the second example doesn't require the registry setting since according to WiX Documentation, a registry setting:\n\n\n  is required as a Shortcut cannot serve as the KeyPath for a component when installing non-advertised shortcuts for the current users.\n\n\nDoes this mean that the second method is an advertised shortcut? Or is it an answer that assumes the user is installing per machine? Or am I lost in the sauce? (Quite possible - second day trying to use WiX, since Microsoft forced me down this path.)\n\nThe first one:\n\n&lt;Directory Id=\"TARGETDIR\" Name=\"SourceDir\"&gt;\n  &lt;Directory Id=\"DesktopFolder\" Name=\"Desktop\"&gt;\n    &lt;Component Id=\"ApplicationShortcutDesktop\" Guid=\"*\"&gt;\n      &lt;Shortcut Id=\"ApplicationDesktopShortcut\"\n         Name=\"Text under your icon\"\n         Description=\"Comment field in your shortcut\"\n         Target=\"[MYAPPDIRPROPERTY]MyApp.exe\"\n         WorkingDirectory=\"MYAPPDIRPROPERTY\"/&gt;\n      &lt;RemoveFolder Id=\"DesktopFolder\" On=\"uninstall\"/&gt;\n      &lt;RegistryValue\n        Root=\"HKCU\"\n        Key=\"Software/MyAppName\"\n        Name=\"installed\"\n        Type=\"integer\"\n        Value=\"1\"\n        KeyPath=\"yes\"/&gt;\n    &lt;/Component&gt;\n  &lt;/Directory&gt;\n    &lt;Directory Id=\"ProgramFilesFolder\" Name=\"PFiles\"&gt;\n      &lt;Directory Id=\"MyCompany\" Name=\"MyCompany\"&gt;\n        &lt;Directory Id=\"MYAPPDIRPROPERTY\" Name=\"MyAppName\"&gt;\n      &lt;/Directory&gt;\n    &lt;/Directory&gt;\n  &lt;/Directory&gt;\n\n\nThe second one:\n\n&lt;Directory Id=\"TARGETDIR\" Name=\"SourceDir\"&gt;\n  &lt;Directory Id=\"DesktopFolder\" SourceName=\"Desktop\" /&gt;\n  &lt;Directory Id=\"MergeRedirectFolder\"&gt;\n    &lt;Component Id=\"MyExeComponent\" Guid=\"*\"&gt;\n      &lt;File Id=\"MyExeFile\" Source=\"$(var.ExeSourcePath)\" KeyPath=\"yes\"&gt;\n        &lt;Shortcut\n          Id=\"DesktopShortcut\"\n          Directory=\"DesktopFolder\"\n          Name=\"$(var.ShortcutName)\"\n          WorkingDirectory=\"MergeRedirectFolder\" /&gt;\n      &lt;/File&gt;\n    &lt;/Component&gt;\n  &lt;/Directory&gt;\n&lt;/Directory&gt;\n\n    \n\nCaveat: Per Doc's comment, since neither example specified the Advertise attribute, neither should create an advertised shortcut. I don't remember what led me to write the answer below; it seems likely to be incorrect. I'll leave the answer in tact in case there is some subtle truth behind it.\n\n\n\nThe first example creates an advertised shortcut; the second creates a non-advertised shortcut. The rules for the two types of shortcuts are described with the Shortcut Table Target column.\n\nA non-advertised shortcut is a standard Windows shortcut like you would create with Windows Explorer. An advertised shortcut enhances resiliency by verifying that all the components in the feature are installed when the shortcut is activated.\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Where do I get my actual Azure Website Deployment password?", "id": 469, "answers": [{"answer_id": 478, "document_id": 202, "question_id": 469, "text": "You can get the current credentials via the Portal or PowerShell/CLI.On the portal, there is a button at the top of the webapp blade to download the publish profile (not the deployment credentials blade, but the main web app blade).", "answer_start": 666, "answer_category": null}], "is_impossible": false}], "context": "In Visual studio 2015, when I Publish my Website/Webapp to Azure, I'm able to create a new publishing profile automatically (by entering my personal Azure account credentials), this creates the .pubxml and .pubxml.user files. The deployment username is of the form \"$websiteName\" and the password is represented by a long series of bullet points. The .pubxml.user file contains the actual password, which is encrypted such that only my Visual Studio can read it, by decrypting it with my local Windows user account - I, as a human, have no way to see it. Also, the .user files are excluded from Source Control (but the .pubxml files are included in Source Control). You can get the current credentials via the Portal or PowerShell/CLI.On the portal, there is a button at the top of the webapp blade to download the publish profile (not the deployment credentials blade, but the main web app blade).", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "gcloud command not found - while installing Google Cloud SDK", "id": 927, "answers": [{"answer_id": 922, "document_id": 578, "question_id": 927, "text": "# The next line updates PATH for the Google Cloud SDK.\nsource '/path/to/google-cloud-sdk/path.bash.inc'\n\n# The next line enables bash completion for gcloud.\nsource '/path/to/google-cloud-sdk/completion.bash.inc'", "answer_start": 908, "answer_category": null}], "is_impossible": false}], "context": "Ubuntu with Oh-My-Zsh\nS\u00e9bastien Lesaint bio photo\nBY S\u00c9BASTIEN LESAINT\n OCTOBER 17, 2014\n COMMENT\n TWEET\n LIKE\n +1\nTo install the Google Cloud SDK, you can follow the installation guidelines available online. But if you are running Ubuntu and uses Oh-My-Zsh (or to some extent, Zsh alone), automatic installation won\u2019t work and you need to do some manual steps.\n\nInstall via the bash installer\nRun the following command to download and install the SDK on your disk.\n\ncurl https://sdk.cloud.google.com | bash\nBash\nIf your are running bash, answer Y when prompted to add gcloud to the PATH and install auto-completion. Don\u2019t worry, the installer creates a backup before modifying your .bashrc.\n\nZsh\nIf you are running Zsh, specify the path to your .zshrc when prompted instead of going for the default .bashrc file.\n\nYou will then have to manually modify your .zshrc.\n\nThe installer adds the following lines:\n\n# The next line updates PATH for the Google Cloud SDK.\nsource '/path/to/google-cloud-sdk/path.bash.inc'\n\n# The next line enables bash completion for gcloud.\nsource '/path/to/google-cloud-sdk/completion.bash.inc'\nJust replace the bash part in the file names with zsh to use the Zsh specific scripts provided with the SDK.\n\nOh-my-zsh compatibility\nI use Oh-My-Zsh as a shell and unfortunately, the procedure above did not work for me.\n\nWhen loading a new shell, I got errors such as the following and command line completion did not work.\n\n/path/to/google-cloud-sdk/completion.bash.inc:8: command not found: complete\n/path/to/google-cloud-sdk/completion.bash.inc:19: parse error near `]]'\nI did the following to fix the install.\n\nload the SDK files before Oh-My-Zsh\nFirst, move the lines added by the installer before the source command loading Oh-My-Zsh (source $ZSH/oh-my-zsh.sh).\n\nload missing Zsh module\nThen two lines to tell Zsh to load and init some specific modules required for completion to work before the source command for completion. I a no expert with Zsh nor Oh-My-Zsh, but looking at oh-my-zsh.sh it seems that only compinit is loaded.\n\nYou should end up with the following, at the beginning of your .zshrc.\n\n# The next line updates PATH for the Google Cloud SDK.\nsource '/home/lesaint/GOOGLE_CLOUD/google-cloud-sdk/path.zsh.inc'\n\n# The next lines enables bash completion in Zsh for gcloud. \nautoload -U compinit compdef\ncompinit\nsource '/home/lesaint/GOOGLE_CLOUD/google-cloud-sdk/completion.zsh.inc'", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "launch app if installed or open google play with install referrer", "id": 1385, "answers": [{"answer_id": 1374, "document_id": 956, "question_id": 1385, "text": "\nOne solution, if you are developing for new Android M is to use App Link: here. But again, I'm not sure that is ", "answer_start": 1540, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nWe are trying to generate a link that, when clicked in a browser, opens our App if it's installed. \nThis is usually done with something like this: \n\nintent://some.domain/some=parameters#Intent;scheme=somescheme;package=my.package.name\n\n\nIf configured correctly, this uri can launch your App if it's installed, and redirect you to the play store otherwise. However, this is not what we want. We want to redirect to the play store with an install referrer. As far as we know, the intent:// syntax cannot do this.\n\nAnother solution we came up with was to make\n\nmarket://details?id=my.package.name&amp;referrer=somereferrer\n\n\nopenable with our App. The problem with this method is that it will not automatically launch the App, but rather ask the user whether they'd like to open the link with our App or the Play Store.\n\nAny workaround for this? Even the slightest suggestion would be appreciated.\n    \n\nI think this is your answer https://stackoverflow.com/a/28792160/5034920\nBasically you must implement the intent filter like this:\n\n&lt;data android:scheme=\"https\"\n      android:host=\"www.foo.com\"\n      android:pathPrefix=\"/bar\" /&gt;\n\n\nand at the server side, create a redirect rule to google play. For example, https://www.foo.com/bar/BlahBlah will redirect to https://play.google.com/store/apps/details?id=com.bar.foo&amp;referrer=BlahBlah.\n    \n\nI'm not sure 100% to have understood your question.\nI try to reformulate: you want your link to redirect to your app if installed without having the \"open with\" choice.\n\nOne solution, if you are developing for new Android M is to use App Link: here. But again, I'm not sure that is what you want.\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "how can i correctly install ruby on rails bundler etc on a fresh mac os x", "id": 1926, "answers": [{"answer_id": 1913, "document_id": 1500, "question_id": 1926, "text": ":\n\nsudo gem update --system\nsudo gem install bund", "answer_start": 1050, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI recently reinstalled my Mac OS X (10.6.8) and want to install things properly this time.  I had some trouble last time so I just want to set things up right.\n\nIn particular, I want to install Ruby on Rails, bundler, and so on... but when I do \"gem install bundler\" I get the following error:\n\nc-69-181-106-62:~ dave$ gem install bundler\nWARNING:  Installing to ~/.gem since /Library/Ruby/Gems/1.8 and\n    /usr/bin aren't both writable.\nWARNING:  You don't have /Users/dave/.gem/ruby/1.8/bin in your PATH,\n    gem executables will not run.\nERROR:  Error installing bundler:\n    bundler requires RubyGems version &gt;= 1.3.6\n\n\nI'm confused... can someone help me out, or point me to an article saying \"OK, first, add this to this file, then download this, then do this...\"?\n\nThanks\n    \n\nThe first warning is because you aren't root. The second warning is because you don't have something in your PATH (but I don't think that will be a problem if you're root.) The third error is because you have an out-of-date RubyGems. Try this:\n\nsudo gem update --system\nsudo gem install bundler\n\n    \n\nThe easiest way is to first install RVM, then install bundler and Rails.\n    \n\nA clean way to do it, it's also to sandbox the gems to your local user.\n\n\nFor that just create a gems directory with:\n\n\nmkdir ~/gems\n\n(you can use any directory you have access at)\n\n\nThen configure the gems environment adding these lines to your shell start script (.bash_profile, .bashrc or .zshrc for instance):\n\n\nexport GEM_HOME=~/gems\nexport GEM_PATH=~/gems\nexport PATH=$GEM_PATH/bin:$PATH\n\n\n\nRestart your shell terminal\nCheck your environment with the command\n\n\necho $PATH\n\n(it should contain the new gems path bin directory)\n\n\nUpdate your rubygems with:\n\n\ngem update --system\n\n(calling with sudo can be required)\n\n\nUpdate the current gems:\n\n\ngem update\n\n\nInstall bundler (optional):\n\n\ngem install bundle\n\nAnd presto !!\n\nNOTE: Also ensure you have Xcode Command Tools installed (if not much of the gems will not be able to build natively)\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Would have removed .. in heroku deploy log", "id": 1355, "answers": [{"answer_id": 1344, "document_id": 923, "question_id": 1355, "text": "After some research, it appears this isn't a bug, but a feature!\nyou can see here https://github.com/carlhuda/bundler/pull/2237", "answer_start": 209, "answer_category": null}], "is_impossible": false}], "context": "a few days ago I started to see\n Would have removed best_in_place (2.0.2)\n Would have removed thor (0.16.0)\nin my heroku deploy output.\nIt used to say that it removed the gem.\nanyone know what's up with that?\nAfter some research, it appears this isn't a bug, but a feature!\nyou can see here https://github.com/carlhuda/bundler/pull/2237\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Staging instance on Heroku", "id": 1682, "answers": [{"answer_id": 1669, "document_id": 1255, "question_id": 1682, "text": "Step 1: Configure both production ('myapp') and staging ('staging-myapp') versions of your app as is indicated in the answer by Luke Bayes\nStep 2: In your domain management system (e.g. GoDaddy):\nCreate a CNAME record:  dev.myapp.com \nthat points to:   proxy.heroku.com\nStep 3: Configure Heroku to route dev.myapp.com to staging-myapp:\nheroku domains:add dev.myapp.com --app staging-myapp", "answer_start": 324, "answer_category": null}], "is_impossible": false}], "context": "I'd like to be able to push code to dev.myapp.com for testing and then to www.myapp.com for production use. Is this possible with Heroku? A key part of the original question is about linking up the staging app to a subdomain (dev.myapp.com) of the main app (www.myapp.com). This hasn't been addressed in any of the answers.\nStep 1: Configure both production ('myapp') and staging ('staging-myapp') versions of your app as is indicated in the answer by Luke Bayes\nStep 2: In your domain management system (e.g. GoDaddy):\nCreate a CNAME record:  dev.myapp.com \nthat points to:   proxy.heroku.com\nStep 3: Configure Heroku to route dev.myapp.com to staging-myapp:\nheroku domains:add dev.myapp.com --app staging-myapp\nAfter the CNAME record has had time to propagate, you will be able to run your staging app at dev.myapp.com.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "3.2 sdk platform install problem. \"Done. Nothing was installed\"", "id": 709, "answers": [{"answer_id": 713, "document_id": 400, "question_id": 709, "text": "Start the SDK manager as Administrator.\n\u2022\tRight Click SDK Manager\n\u2022\tSelect Run As Administrator\n\u2022\tClick the YES button", "answer_start": 145, "answer_category": null}], "is_impossible": false}], "context": "I select everything in the new 3.2 update but then I get the error message \"done. nothing was installed\" after the sdk manager runs its process. Start the SDK manager as Administrator.\n\u2022\tRight Click SDK Manager\n\u2022\tSelect Run As Administrator\n\u2022\tClick the YES button\nThe key to your problem is simple. You need to have an administrator access. The files are not being created automatically and hence the installation fails.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Visual Studio setup problem - 'A problem has been encountered while loading the setup components. Canceling setup.'", "id": 1621, "answers": [{"answer_id": 1608, "document_id": 1195, "question_id": 1621, "text": "A colleague found this MS auto-uninstall tool which has successfully uninstalled VS2008 for me and saved me hours of work!!", "answer_start": 620, "answer_category": null}], "is_impossible": false}], "context": "I've had a serious issue with my Visual Studio 2008 setup. I receive the ever-so-useful error 'A problem has been encountered while loading the setup components. Canceling setup.' whenever I try to uninstall, reinstall or repair Visual Studio 2008 (team system version). If I can't resolve this issue I have no choice but to completely wipe my computer and start again which will take all day long! I've recently received very strange errors when trying to build projects regarding components running out of memory (despite having ~2gb physical memory free at the time) which has rendered my current VS install useless.\nA colleague found this MS auto-uninstall tool which has successfully uninstalled VS2008 for me and saved me hours of work!!\nHopefully this might be useful to others. Doesn't speak highly of MS's faith in their usual VS maintenance tools that they have to provide this as well!\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "how to install docker compose offline", "id": 1928, "answers": [{"answer_id": 1915, "document_id": 1502, "question_id": 1928, "text": "From the docker github page docker compose github releases.\n\ncurl -L https://github.com/docker/compose/releases/download/1.25.1/docker-compose-`uname -s`-`uname -m` -o docker-compose &amp;&amp; chmod +x docker-", "answer_start": 1453, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI would like to download docker-compose on one machine and install it on another (this other machine is not connected to the internet). \n\nI tried downloading the binary file, making a docker-compose directory under /usr/bin (where docker is) and ran chmod +x on the directory. That didnt help. \n\nAlso the Alternative Install Options link in the docs is broken :/\n\nAny help? \n    \n\nIn situations when installing via pip or as docker container (https://docs.docker.com/compose/install/) is also not an option, the following option can help:\nDownload the package in a system with connectivity (replace the needed version as required)\nwget https://github.com/docker/compose/releases/download/1.24.0/docker-compose-Linux-x86_64\n\nJust rename the package\nmv docker-compose-Linux-x86_64 docker-compose\n\nCopy it (ssh) to the required system\nTo make it executable\nsudo mv docker-compose /usr/local/bin/\nsudo chmod +x /usr/local/bin/docker-compose\n\nRefer the post for more details: http://muralitechblog.com/how-to-install-docker-compose-offline/\n    \n\nAccording to the Docker documentation, you can install docker-compose with pip:\n\n\n  Compose can be installed from pypi using pip. If you install using pip, we recommend that you use a virtualenv because many operating systems have python system packages that conflict with docker-compose dependencies.\n\n\nAnd pip packages can be installed offline : Python Packages Offline Installation\n    \n\nFrom the docker github page docker compose github releases.\n\ncurl -L https://github.com/docker/compose/releases/download/1.25.1/docker-compose-`uname -s`-`uname -m` -o docker-compose &amp;&amp; chmod +x docker-compose\n\n\nThis will fetch the file applicable to the OS that you run the command for.  \n\nTo find the latest version follow this link Latest Docker Compose Releases\nIf you need to retrieve a version for an offline OS, on that OS run the following commands to determine the version.  NOTE: This is for linux and macOS varieties only.\n\nTo determine the OS\n\nuname -s\n\n\nTo determine the architecture:\n\nuname -m\n\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How can I view transitive dependencies of a Maven pom.xml file?", "id": 1844, "answers": [{"answer_id": 1830, "document_id": 1415, "question_id": 1844, "text": "On the CLI, you can use mvn dependency:tree\n(Here are some additional Usage notes)", "answer_start": 99, "answer_category": null}], "is_impossible": false}], "context": "Is there a CLI tool I can use to quickly view the transitive dependencies of a Maven pom.xml file?\nOn the CLI, you can use mvn dependency:tree\n(Here are some additional Usage notes)\nOtherwise, the POM Editor in M2Eclipse (Maven integration for Eclipse) is very good, and it includes a hierarchical dependency view.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Jboss 7 war deployment failed", "id": 543, "answers": [{"answer_id": 545, "document_id": 268, "question_id": 543, "text": "It was caused by insufficient space on the disk, cleared some unwanted log files and other temp files to fix this issue.", "answer_start": 212, "answer_category": null}], "is_impossible": false}], "context": "I tried restarting the JBoss server, I have never seen this before, it was working fine before. I googled around for solution and cause of this issue but cant find anything really useful, can anyone help please?\nIt was caused by insufficient space on the disk, cleared some unwanted log files and other temp files to fix this issue.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "how can i get another applications installation path programmatically", "id": 1476, "answers": [{"answer_id": 1465, "document_id": 1051, "question_id": 1476, "text": " \n\nTake a look in the registry. \n\nHKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Uninstall\\ \n\nor\n\nHKEY_LOCAL_MACHINE\\SOFTWARE\\WOW6432Node\\Microsoft\\Windows\\CurrentVersion\\Uninstall\\\n\nEach of the above contain a list of sub-keys, one for each installed application (as it appears, for example, in the \"Programs and Features\" applet)\n\nYou can search for your application there, or if you know the product code, access it direct", "answer_start": 2346, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI'd like to know where the installation path for an application is. I know it usually is in ...\\Program Files... but I guess some people install it in different locations. I do know the name of the application.\n\nThank you.\n    \n\nThe ideal way to find a program's installation path (on Windows) is to read it from the registry. Most installers will create a registry key for that program that contains the installation path. Exactly where this key is and what it will be named varies depending on the program in question. \n\nTo find if the program has a key in the registry, open 'regedit' and use the Edit &gt; Find option to try and locate a key with the program name. If such a key exists, you can read it using the RegistryKey class in the .NET Framework library. \n\nIf the program does not have a registry key then another option is just to ask the user to locate the .exe file with the OpenFileDialog, although this is obviously not ideal. \n    \n\nMany (most?) programs create an App Paths registry key. Have a look at\n\nHKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\App Paths\n\n    \n\nIf you know the application in question (as compared to any application) registry key is the probably the best option (if one exists).  \n\nThe install might put in its own custom \"install path key\" somewhere (so do a find as Fara mentioned) or it might be in the uninstall section for installed programs, so you could check:\n\n\nHKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Uninstall\n\n\nBut be aware that any new version of an install could change the key it writes out, both for a custom key or for the uninstall entry.  So checking the registry should probably be only for a known install\\version. \n\ntep\n    \n\nBest way is to use Installer APIs to find the program location.\nYou can write a Managed wrapper over the APIs\n\nSearch for MsiGetProductInfo\n\nReference: http://msdn.microsoft.com/en-us/library/aa369558(VS.85).aspx\n    \n\nYou can use MSI (I wrote a C# wrapper for it here https://github.com/alialavia/MSINet). Here is a simple example:\n\nvar location = \"\";\nforeach (var p in InstalledProduct.Enumerate())\n{\n    try\n    {\n        if (p.InstalledProductName.Contains(\"AppName\"))                     \n        {\n            location = p.InstallLocation;\n            break;\n        }\n    } \n    catch { }\n}\n\n    \n\nTake a look in the registry. \n\nHKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Uninstall\\ \n\nor\n\nHKEY_LOCAL_MACHINE\\SOFTWARE\\WOW6432Node\\Microsoft\\Windows\\CurrentVersion\\Uninstall\\\n\nEach of the above contain a list of sub-keys, one for each installed application (as it appears, for example, in the \"Programs and Features\" applet)\n\nYou can search for your application there, or if you know the product code, access it directly.\n\n    public string GetInstallPath(string applicationName)\n    {\n        var installPath = FindApplicationPath(@\"SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Uninstall\", applicationName);\n\n        if (installPath == null)\n        {\n            installPath = FindApplicationPath(@\"SOFTWARE\\WOW6432Node\\Microsoft\\Windows\\CurrentVersion\\Uninstall\", applicationName);\n        }\n\n        return installPath;\n    }\n\n    private string FindApplicationPath(string keyPath, string applicationName)\n    {\n\n        var hklm = Registry.LocalMachine;\n        var uninstall = hklm.OpenSubKey(keyPath);\n        foreach (var productSubKey in uninstall.GetSubKeyNames())\n        {\n            var product = uninstall.OpenSubKey(productSubKey);\n\n            var displayName = product.GetValue(\"DisplayName\");\n            if (displayName != null &amp;&amp; displayName.ToString() == applicationName)\n            {\n                return product.GetValue(\"InstallLocation\").ToString();\n            }\n\n        }\n\n        return null;\n    }\n\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "\"Untrusted App Developer\" message when installing enterprise iOS Application", "id": 1679, "answers": [{"answer_id": 1666, "document_id": 1252, "question_id": 1679, "text": "1.\tSettings -> General -> Profiles [Device Management on iOS 10]\n2.\tUnder ENTERPRISE APP, choose your current developer account name.\n3.\tTap Trust \"Your developer account name\"\n4.\tTap \"Trust\" in pop up.\n5.\tDone", "answer_start": 298, "answer_category": null}], "is_impossible": false}], "context": "I'm developing an enterprise application. When I was testing it in iOS8 beta I saw the following alert view:\nUntrusted App Developer\nDo you trust the developer \"iPhone Distribution: ---\" to run apps on you iPad?\nToday, I was testing this with iOS 9 Beta and found the solution.\nTo solve it, go to:\n1.\tSettings -> General -> Profiles [Device Management on iOS 10]\n2.\tUnder ENTERPRISE APP, choose your current developer account name.\n3.\tTap Trust \"Your developer account name\"\n4.\tTap \"Trust\" in pop up.\n5.\tDone\n\nIt only appears the first time. Can I avoid it somehow? And what is it related to?\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Session 'app': Error Installing APK", "id": 781, "answers": [{"answer_id": 778, "document_id": 465, "question_id": 781, "text": "Check your gradle file for debuggable false/true\n\u2022\tInvalidate caches & restart\n\u2022\tCheck your install location\n\u2022\tRestart adb", "answer_start": 73, "answer_category": null}], "is_impossible": false}], "context": "Try using a different version of Gradle(stable version). To summarize:\n\u2022\tCheck your gradle file for debuggable false/true\n\u2022\tInvalidate caches & restart\n\u2022\tCheck your install location\n\u2022\tRestart adb\n I guess the problem probably can be in my android device(it is chinese Doogee X5). It does not have a given USB Vendor ID in developer docs so I decided to follow instructions with random vendor ID from HTC.\nI'm sure there is a way to run application on any android device, but yet I didn't find an answer on how to do that.\nI'm running through Linux Ubuntu 14.04 LTS\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "how to install mysqldb on mountain lion", "id": 1481, "answers": [{"answer_id": 1470, "document_id": 1057, "question_id": 1481, "text": "he following command\n\nsource  ~/.profile\n\nInstall MySQLdb\n\nsudo pip install MySQL-python\n\n\nTo test if everything works", "answer_start": 3300, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI'm new to Python and I'm having trouble building MySQLdb, in an attempt to get Google AppEngine SDK running. I have just upgraded from Snow Leopard to Mountain Lion and have installed the latest XCode (4.4)\n\nI've downloaded http://sourceforge.net/projects/mysql-python/\n\npython setup.py build \n\n\ni get the following output in terminal \n\nrunning build\nrunning build_py\ncreating build\ncreating build/lib.macosx-10.8-intel-2.7\ncopying _mysql_exceptions.py -&gt; build/lib.macosx-10.8-intel-2.7\ncreating build/lib.macosx-10.8-intel-2.7/MySQLdb\ncopying MySQLdb/__init__.py -&gt; build/lib.macosx-10.8-intel-2.7/MySQLdb\ncopying MySQLdb/converters.py -&gt; build/lib.macosx-10.8-intel-2.7/MySQLdb\ncopying MySQLdb/connections.py -&gt; build/lib.macosx-10.8-intel-2.7/MySQLdb\ncopying MySQLdb/cursors.py -&gt; build/lib.macosx-10.8-intel-2.7/MySQLdb\ncopying MySQLdb/release.py -&gt; build/lib.macosx-10.8-intel-2.7/MySQLdb\ncopying MySQLdb/times.py -&gt; build/lib.macosx-10.8-intel-2.7/MySQLdb\ncreating build/lib.macosx-10.8-intel-2.7/MySQLdb/constants\ncopying MySQLdb/constants/__init__.py -&gt; build/lib.macosx-10.8-intel-2.7/MySQLdb/constants\ncopying MySQLdb/constants/CR.py -&gt; build/lib.macosx-10.8-intel-2.7/MySQLdb/constants\ncopying MySQLdb/constants/FIELD_TYPE.py -&gt; build/lib.macosx-10.8-intel-    2.7/MySQLdb/constants\ncopying MySQLdb/constants/ER.py -&gt; build/lib.macosx-10.8-intel-2.7/MySQLdb/constants\ncopying MySQLdb/constants/FLAG.py -&gt; build/lib.macosx-10.8-intel-2.7/MySQLdb/constants\ncopying MySQLdb/constants/REFRESH.py -&gt; build/lib.macosx-10.8-intel-2.7/MySQLdb/constants\ncopying MySQLdb/constants/CLIENT.py -&gt; build/lib.macosx-10.8-intel-2.7/MySQLdb/constants\nrunning build_ext\nbuilding '_mysql' extension\ncreating build/temp.macosx-10.8-intel-2.7\nclang -fno-strict-aliasing -fno-common -dynamic -g -Os -pipe -fno-common -fno-strict-aliasing -fwrapv -mno-fused-madd -DENABLE_DTRACE -DMACOSX -DNDEBUG -Wall -Wstrict-prototypes -Wshorten-64-to-32 -DNDEBUG -g -Os -Wall -Wstrict-prototypes -DENABLE_DTRACE -pipe -Dversion_info=(1,2,3,'final',0) -D__version__=1.2.3 -I/usr/local/mysql/include -I/System/Library/Frameworks/Python.framework/Versions/2.7/include/python2.7 -c _mysql.c -o build/temp.macosx-10.8-intel-2.7/_mysql.o -Os -g -fno-common -fno-strict-aliasing -arch x86_64\nunable to execute clang: No such file or directory\nerror: command 'clang' failed with exit status 1\n\n\nboth of the following directories exist, i have no idea how to resolve the issue with clang not being able to execute...\n\n/System/Library/Frameworks/Python.framework/Versions/2.7/include/python2.7\n/usr/local/mysql/include\n\n    \n\nIt seems that the system is complaining about not be able to find clang, which is included in Command Line Tools of Xcode. Did you installed the tool as well?\n\nCan be installed via \n\n\nOpen Xcode\nPreference (Command + ,)\nComponents under the Download tab\n\n    \n\nIf someone is interested in a quick and easy way for Mac OS X 10.8:\n\nI assume you have XCode, it's command line tool, Python and MySQL installed.\n\n\nInstall PIP:\n\nsudo easy_install pip\n\nEdit ~/.profile:\n\nnano ~/.profile\n\n\nCopy and paste the following two line\n\nexport PATH=/usr/local/mysql/bin:$PATH\nexport DYLD_LIBRARY_PATH=/usr/local/mysql/lib/\n\n\nSave and exit. Afterwords execute the following command\n\nsource  ~/.profile\n\nInstall MySQLdb\n\nsudo pip install MySQL-python\n\n\nTo test if everything works fine just try\n\npython -c \"import MySQLdb\"\n\n\n\nIt worked like a charm for me. I hope it helps.\n    \n\nIt seems that is not the only thing that you need to do, i check about the cc compiler that is not recognizing, and is not directing to the right file, googling i found that i need to change some files to found it before building, does not stop there, now that its recognizing is not charging the DYDL Libraries... it has been really hard to find a real answer just to start working with python.\n    \n\nI finally found it says in the ReadMe to edit site.cfg and put the location of mysql_config in there.\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "failed to install cairocffi", "id": 1391, "answers": [{"answer_id": 1380, "document_id": 964, "question_id": 1391, "text": "dev:\n\nsudo apt-get install python3-dev\nsudo apt-get install libffi-dev\nsudo pip3 install cffi\nsudo pip3 install cair", "answer_start": 1654, "answer_category": null}], "is_impossible": false}, {"question": "How to fix problems: failed to install cairocffi?", "id": 1392, "answers": [{"answer_id": 1381, "document_id": 964, "question_id": 1392, "text": "    \n\ninstall the libffi-dev package can fix this problem\nsimply run sudo apt install libf", "answer_start": 1795, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI'm working with python3, and i'm trying to install \" cairocffi\" on Ubuntu.\nTo do this, i've successfully installed:\n\n\npython-dev\nlibffi-dev\ncffi\n\n\nBut when i've trying to install \"cairocffi\" with pip install cairocffi,I got:\n\n      File \"/usr/local/lib/python3.2/dist-packages/setuptools/dist.py\", line 272\n, in __init__\n        _Distribution.__init__(self,attrs)\n      File \"/usr/lib/python3.2/distutils/dist.py\", line 261, in __init__\n        self.finalize_options()\n      File \"/usr/local/lib/python3.2/dist-packages/setuptools/dist.py\", line 327\n, in finalize_options\n        ep.load()(self, ep.name, value)\n      File \"/usr/local/lib/python3.2/dist-packages/cffi/setuptools_ext.py\", line\n 161, in cffi_modules\n        add_cffi_module(dist, cffi_module)\n      File \"/usr/local/lib/python3.2/dist-packages/cffi/setuptools_ext.py\", line\n 48, in add_cffi_module\n        execfile(build_file_name, mod_vars)\n      File \"/usr/local/lib/python3.2/dist-packages/cffi/setuptools_ext.py\", line\n 24, in execfile\n        exec(code, glob, glob)\n      File \"cairocffi/ffi_build.py\", line 30, in &lt;module&gt;\n        ffi.cdef(constants._CAIRO_HEADERS)\n    AttributeError: 'module' object has no attribute '_CAIRO_HEADERS'\n\n    ----------------------------------------\n    Command \"python setup.py egg_info\" failed with error code 1 in /tmp/pip-build-y0b_ir/cairocffi\n\n    \n\nMy problem is fixed by installing a specific version of cairocffi (version = 0.6)\n\npip install cairocffi==0.6\n\n    \n\nI ran into this today as well. Are you working through the Real Python course?\n\nAnyway, to fix the problem, I installed the python 3 version of python-dev:\n\nsudo apt-get install python3-dev\nsudo apt-get install libffi-dev\nsudo pip3 install cffi\nsudo pip3 install cairocffi\n\n\nHope this helps!\n    \n\ninstall the libffi-dev package can fix this problem\nsimply run sudo apt install libffi-dev\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How do you deploy your ASP.NET applications to live servers?", "id": 341, "answers": [{"answer_id": 348, "document_id": 153, "question_id": 341, "text": "We have all of our code deployed in MSIs using Setup Factory. If something has to change we redeploy the entire solution. This sounds like overkill for a css file, but it absolutely keeps all environments in sync, and we know exactly what is in production (we deploy to all test and uat environments the same way).", "answer_start": 341, "answer_category": null}], "is_impossible": false}], "context": "I am looking for different techniques/tools you use to deploy an ASP.NET web application project (NOT ASP.NET web site) to production?\nI am particularly interested of the workflow happening between the time your Continuous Integration Build server drops the binaries at some location and the time the first user request hits these binaries.\nWe have all of our code deployed in MSIs using Setup Factory. If something has to change we redeploy the entire solution. This sounds like overkill for a css file, but it absolutely keeps all environments in sync, and we know exactly what is in production (we deploy to all test and uat environments the same way).\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Android Fastboot devices not returning device", "id": 806, "answers": [{"answer_id": 801, "document_id": 488, "question_id": 806, "text": "adb reboot bootloader", "answer_start": 578, "answer_category": null}], "is_impossible": false}], "context": "At the moment I would like to reinstall Android on my device(custom hardware device). I got the image files after building. But when I enter fastboot devices nothing returns.\nadb devices is working. It return my device. fastboot flashall -w is also not working. I returns <waiting for devices> and stays like that until I exit.\nSo the 70-android.rules.d file is right. I have also set ANDROID_PRODUCT_OUT. But someone knows why fastboot does not see my device, but adb does? Are you rebooting the device into the bootloader and entering fastboot USB on the bootloader menu?\nTry\nadb reboot bootloader\nthen look for on screen instructions to enter fastboot mode.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "how to use cmake install prefix", "id": 1974, "answers": [{"answer_id": 1960, "document_id": 1559, "question_id": 1974, "text": "passing it as a command line argument just like Job mentioned: \n\ncmake -DCMAKE_INSTALL_PREFIX=&lt; install_path &gt; ..\nassigning value to it in CMakeLists.txt:\n\nSET(CMAKE_INSTALL_PREFIX &lt; install_path &gt;)\n\nBut do remember to place it BEFORE PROJECT(&lt; project_name&gt;) command, otherwis", "answer_start": 776, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI want to generate Makefile with install target, making installation to /usr instead of default /usr/local. Assuming that build directory is done in the source subdirectory, I execute:\n\ncmake -DCMAKE_INSTALL_PREFIX:PATH=/usr ..\n\nCMakeCache.txt contains: CMAKE_INSTALL_PREFIX:PATH=/usr (OK?)\n\nNow I execute:\n\n\nmake\nmake install\n\n\nAll files are still installed to usr/local. What is wrong?\n\nEdit:\nThere is no CMAKE_INSTALL_PREFIX in any of CMakeLists.txt project files. \nBefore running cmake, I delete everything from the output directory.\ninstall directives in CMakeLists.txt look like: \n\ninstall(TARGETS mylibrary DESTINATION lib)\n    \n\nThat should be (see the docs):\n\ncmake -DCMAKE_INSTALL_PREFIX=/usr ..\n\n    \n\nThere are two ways to use this variable:\n\n\npassing it as a command line argument just like Job mentioned: \n\ncmake -DCMAKE_INSTALL_PREFIX=&lt; install_path &gt; ..\nassigning value to it in CMakeLists.txt:\n\nSET(CMAKE_INSTALL_PREFIX &lt; install_path &gt;)\n\nBut do remember to place it BEFORE PROJECT(&lt; project_name&gt;) command, otherwise it will not work!\n\n    \n\n\n  But do remember to place it BEFORE PROJECT(&lt; project_name&gt;) command,\n  otherwise it will not work!\n\n\nMy first week of using cmake - after some years of GNU autotools - so I am still learning (better then writing m4 macros), but I think modifying CMAKE_INSTALL_PREFIX after setting project is the better place.\n\nCMakeLists.txt\n\ncmake_minimum_required (VERSION 2.8)\n\nset (CMAKE_INSTALL_PREFIX /foo/bar/bubba)\nmessage(\"CIP = ${CMAKE_INSTALL_PREFIX} (should be /foo/bar/bubba\")\nproject (BarkBark)\nmessage(\"CIP = ${CMAKE_INSTALL_PREFIX} (should be /foo/bar/bubba\")\nset (CMAKE_INSTALL_PREFIX /foo/bar/bubba)\nmessage(\"CIP = ${CMAKE_INSTALL_PREFIX} (should be /foo/bar/bubba\")\n\n\nFirst run (no cache)\n\nCIP = /foo/bar/bubba (should be /foo/bar/bubba\n-- The C compiler identification is GNU 4.4.7\n-- etc, etc,...\nCIP = /usr/local (should be /foo/bar/bubba\nCIP = /foo/bar/bubba (should be /foo/bar/bubba\n-- Configuring done\n-- Generating done\n\n\nSecond run\n\nCIP = /foo/bar/bubba (should be /foo/bar/bubba\nCIP = /foo/bar/bubba (should be /foo/bar/bubba\nCIP = /foo/bar/bubba (should be /foo/bar/bubba\n-- Configuring done\n-- Generating done\n\n\nLet me know if I am mistaken, I have a lot of learning to do. It's fun.\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Deploy Azure Website via MSBuild & WebDeploy, but with which credentials?", "id": 1059, "answers": [{"answer_id": 1055, "document_id": 640, "question_id": 1059, "text": "You should find the .azurewebsites.net.PublishSettings file (which can be downloaded in the management portal) contains the password in clear-text (even if it looks like encrypted).", "answer_start": 117, "answer_category": null}], "is_impossible": false}], "context": "I have a very simple ASP.NET MVC 4 application and I can publish it easily to my local or inhouse IIS via WebDeploy.\nYou should find the .azurewebsites.net.PublishSettings file (which can be downloaded in the management portal) contains the password in clear-text (even if it looks like encrypted).\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "how to setup svn on mac os x 10 8 2", "id": 1947, "answers": [{"answer_id": 1934, "document_id": 1527, "question_id": 1947, "text": "$ which svn\n/usr/bin/svn\n$ svn --version", "answer_start": 1428, "answer_category": null}], "is_impossible": false}], "context": "\n\n                    \n\n            \n\n        \n\n            \n\n                \n\n                    \n\n                        Closed. This question is off-topic. It is not currently accepting answers.\n\n                        \n\n                    \n\n                \n\n            \n\n        \n\n            \n\n        \n\n                \n\n                    \n\n                \n\n            \n\n                \n\n                    Want to improve this question? Update the question so it's on-topic for Stack Overflow.\n\n                \n\n                    Closed 9 years ago.\n\n\n\n            \n\n        \n\n            \n\n                    \n\n                        Improve this question\n\n                    \n\n            \n\n\n\n\n\n    \n\n\n\nI have found information that SVN comes with Mac OS X. But there was no SVN on my system. I have installed Subversion-1.6.17-1_10.7.x.pkg and all was good. But after update to Mac OS X 10.8.2 all SVN files were automatically removed from the system. I have tried to install Subversion-1.6.17-1_10.7.x.pkg again - but Next button is disabled. I have tried to found an updated version - but there is no Mac OS support now.\n\nHow to simply setup SVN on Mac OS X 10.8.2?\n\nThanks a lot for the help.\n    \n\nAfter you have installed Xcode 4.5 you need to follow these instructions in order to install the command line tools. Once you've done that successfully then you should see that  svn is installed:\n\n$ which svn\n/usr/bin/svn\n$ svn --version\nsvn, version 1.6.18 (r1303927)\n   compiled Aug  4 2012, 19:46:53\n\nCopyright (C) 2000-2009 CollabNet.\nSubversion is open source software, see http://subversion.apache.org/\nThis product includes software developed by CollabNet (http://www.Collab.Net/).\n\nThe following repository access (RA) modules are available:\n\n* ra_neon : Module for accessing a repository via WebDAV protocol using Neon.\n  - handles 'http' scheme\n  - handles 'https' scheme\n* ra_svn : Module for accessing a repository using the svn network protocol.\n  - handles 'svn' scheme\n* ra_local : Module for accessing a repository on local disk.\n  - handles 'file' scheme\n\n$ \n\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "What is  an android emulator?", "id": 982, "answers": [{"answer_id": 977, "document_id": 605, "question_id": 982, "text": "An Android emulator can give you full access to the Android software, including all Google Play apps. ", "answer_start": 935, "answer_category": null}], "is_impossible": false}, {"question": "What can google App Players do?", "id": 983, "answers": [{"answer_id": 978, "document_id": 605, "question_id": 983, "text": " App players will just give you access to Android apps, while larger emulators provide a full Android environment to run Google Play apps and use other Android features.", "answer_start": 355, "answer_category": null}], "is_impossible": false}, {"question": "how to fix install google play apps errors?", "id": 984, "answers": [{"answer_id": 979, "document_id": 605, "question_id": 984, "text": " Verify your ID and update your billing information to fix many problems", "answer_start": 2303, "answer_category": null}], "is_impossible": false}, {"question": "How to fix internet connection error on an Android Emulator?", "id": 985, "answers": [{"answer_id": 980, "document_id": 605, "question_id": 985, "text": "The most common errors are caused by using an expired Google ID for the Play store, but, confusingly, error messages usually report it as an Internet connection issue", "answer_start": 2136, "answer_category": null}], "is_impossible": false}], "context": "How to Install Google Play Apps on a PC\nBy Geoff Whiting\n\nUpdated August 23, 2017\n\nAndroid emulators can add many free games to your PC.\nYou don't need to buy an Android device to try the latest apps from Google Play, thanks to software that will put these apps on your PC. There are two flavors of software to consider: app players and Android emulators. App players will just give you access to Android apps, while larger emulators provide a full Android environment to run Google Play apps and use other Android features.\n\n\n \nApp Players\nIf you\u2019re interested in just playing around with Android apps on your PC and don\u2019t need any of the other features of the Android operating system, an Android app player meets these needs. These provide PC access to a certain number of supported Google Play apps. Most app players like BlueStacks limit downloads to those apps they have been verified as safe (see Resources).\n\nAndroid Emulation\nAn Android emulator can give you full access to the Android software, including all Google Play apps. To use apps in an emulator, you\u2019ll need a Google Play account because you\u2019re buying and downloading apps through Google Play itself and not a third party. Emulators require a little more time and effort to install, but most come with guides to get you up and running. YouWave and Oracle\u2019s VirtualBox offer Android emulation for a small fee or for free, respectively, and have forums for installation support (see Resources).\n\nGoogle\u2019s Software\nGoogle also offers a software development kit, or SDK, that provides an Android emulator and supports Android apps on the Play store or Android apps you create yourself (link in Resources). Google\u2019s SDK emulator is recommended for developers or very advanced users because it features many options and changes that can significantly change how well the emulator works. After installing the SDK, you\u2019ll need to use its Tools menu to create an Android Virtual Device that will allow you to download and play apps.\n\nID Issues\nMost emulation software is still in testing and developers can\u2019t guarantee that everything will work right 100 percent of the time. The most common errors are caused by using an expired Google ID for the Play store, but, confusingly, error messages usually report it as an Internet connection issue. Verify your ID and update your billing information to fix many problems. If you use the Google SDK, removing the text \u201candroid-\u201c from the device ID listed when creating an AVD will clear up many download issues.", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "libusb and how to use its packages in ubuntu", "id": 1908, "answers": [{"answer_id": 1895, "document_id": 1479, "question_id": 1908, "text": "erent locations:\n\n$ dpkg -L libusb-dev|grep /usr/include\n/usr/include\n/usr/include/usb.h\n$ dpkg -L libusb-1.0-0-dev|grep /usr/include\n/usr/include\n/usr/include/libusb-1.0\n/usr/include/l", "answer_start": 1536, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI have installed libusb by using the following command. I am not sure if it was right or not and the command was\n\nsudo apt-get install libusb-dev\n\n\nOnce I have installed (and I am not sure if it has installed or not because I am a novice user of Ubuntu), I want to know how would I use the library, because I write some sample code which uses &lt;libusb.h&gt;, but when I compile that C++ file using\n\ng++ test_libusb.cpp\n\n\nthat throws the following error,\n\n\n  test_libusb.cpp:2:20: fatal error: libusb.h: No such file or directory compilation terminated.\n\n\nI am clueless what to do. I can't find any source on the Internet to get to the bottom of this...\n\nI want to know two things here:\n\n\nHow do I add the libusb library in C/C++ so I can use &lt;libusb.h&gt;?\nWhat would some sample code be? Only a few lines to see if libusb is working...\n\n    \n\nTry including it like so:\n\n#include &lt;libusb-1.0/libusb.h&gt;\n\n\nand then compile it like so:\n\ng++ main.cpp -o main -lusb-1.0\n\n    \n\nHave a look at http://packages.debian.org/wheezy/i386/libusb-dev/filelist: The file you want to include is usb.h. Also, you'll have to tell the compiler where it can find the compiled library functions: Add -lusb to the compiler command line to make it load libusb.so.\n    \n\nActually at least in Debian 7.4 (wheezy), and probably in Ubuntu also, there are two distinct libusb packages: libusb-dev (0.1.12-20+nmu1) and libusb-1.0-0-dev (1.0.11-1).  Confusingly, they can both be installed concurrently and provide header files in different locations:\n\n$ dpkg -L libusb-dev|grep /usr/include\n/usr/include\n/usr/include/usb.h\n$ dpkg -L libusb-1.0-0-dev|grep /usr/include\n/usr/include\n/usr/include/libusb-1.0\n/usr/include/libusb-1.0/libusb.h\n\n    \n\nTry #include &lt;usb.h&gt;. The \"lib\" is part of the Linux naming convention, i.e. library \"foo\" has header foo.h and is called libfoo-dev in the Debian package structure, and linked as -lfoo, and the compiled library files are called libfoo.a and libfoo.so.\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How do you resolve Python package dependencies with pipenv?", "id": 1856, "answers": [{"answer_id": 1842, "document_id": 1427, "question_id": 1856, "text": "You should clean the cache in the lock file works beautifully every time.\n$ pipenv lock --pre --clear", "answer_start": 338, "answer_category": null}], "is_impossible": false}], "context": "I am using pipenv to handle Python package dependencies.\nThe Python package is using two packages (named pckg1 and pckg2) that rely on the same package named pckg3, but from two different versions. Showing the dependency tree :\n$ pipenv graph\n  pckg1==3.0.0\n    - pckg3 [required: >=4.1.0]\n  pckg2==1.0.2\n    - pckg3 [required: ==4.0.11]\nYou should clean the cache in the lock file works beautifully every time.\n$ pipenv lock --pre --clear\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to do opposite of preference attribute android:dependency?", "id": 1861, "answers": [{"answer_id": 1847, "document_id": 1432, "question_id": 1861, "text": "Actually found it on my own and figured I'd just post it here to help you that might have this same issue:\nandroid:disableDependentsState=\"true\"", "answer_start": 611, "answer_category": null}], "is_impossible": false}], "context": "Is there XML attribute that does the exact opposite of android:dependency?\nWhat I would like the dependent preference to be enabled when the other is NOT checked and disabled when it IS checked.\nedit: maybe the issue isn't with android:dependency maybe there is an xml attribute that I can add to make the default for that preference disabled and then android:dependency will toggle it the opposite way like i want.\nedit again: I tried setting android:enabled=\"false\" in the preference and it disables it like i want but even with it being dependent on the other preference it didn't enable it like i had hoped\nActually found it on my own and figured I'd just post it here to help you that might have this same issue:\nandroid:disableDependentsState=\"true\"\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to integrate Capistrano with Docker for deployment?", "id": 1093, "answers": [{"answer_id": 1085, "document_id": 670, "question_id": 1093, "text": "Capistrano is not the right tool for the job. This was recently discussed in a PR for SSHKit, which underlies Capistrano.\nhttps://github.com/capistrano/sshkit/pull/368", "answer_start": 260, "answer_category": null}], "is_impossible": false}], "context": "I am not sure my question is relevant as I may try to mix tools (Capistrano and Docker) that should not be mixed.\nI have recently dockerized an application that is deployed with Capistrano. Docker compose is used both for development and staging environments.\nCapistrano is not the right tool for the job. This was recently discussed in a PR for SSHKit, which underlies Capistrano.\nhttps://github.com/capistrano/sshkit/pull/368\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to check if mongo db is running on Mac?", "id": 1133, "answers": [{"answer_id": 1126, "document_id": 710, "question_id": 1133, "text": "ps -ef | grep mongod | grep -v grep | wc -l | tr -d ' '", "answer_start": 199, "answer_category": null}], "is_impossible": false}], "context": "I had installed MongoDB few days back on my Mac and am not sure how I installed, but now how do I check if MongoDB is up and running in my system? \nQuick Solution\nRun the following in your Terminal:\nps -ef | grep mongod | grep -v grep | wc -l | tr -d ' '\nThis will get you the number of MongoDB processes running, thus if it is other than 0, then you have MongoDB running on your system.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Getting a warning when installing homebrew on MacOS Big Sur (M1 chip)", "id": 812, "answers": [{"answer_id": 807, "document_id": 494, "question_id": 812, "text": "but if you look at the \"Next steps\" and run those two lines, then you would be fine", "answer_start": 387, "answer_category": null}], "is_impossible": false}], "context": "However, before I Rosetta 2, I tried installing the plain old /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\". The went through, and I saw \"Installation successful!\"\nThe only issue is that I saw the following waring.\nhomebrew Warning: /opt/homebrew/bin is not in your PATH.\nShould I be worried? What does it mean? I have this warning too, but if you look at the \"Next steps\" and run those two lines, then you would be fine.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Why does Macports take FOREVER to build simple packages?", "id": 1138, "answers": [{"answer_id": 1131, "document_id": 715, "question_id": 1138, "text": "you can double the speed of your builds by changing the Macports config option located here:\n/opt/local/etc/macports/macports.conf\n# Number of simultaneous make jobs (commands) to use when building ports\nbuildmakejobs", "answer_start": 351, "answer_category": null}], "is_impossible": false}], "context": "Building from source outside of macports is a breeze. Building with macports takes forever and seems to freeze the os every so often. Is this typical behavior? Although it seems like a nice packaging tool for os x, if I have to go through this pain every time during every install I think I'll do without it. If you are running on an Intel Core 2 Duo you can double the speed of your builds by changing the Macports config option located here:\n/opt/local/etc/macports/macports.conf\n# Number of simultaneous make jobs (commands) to use when building ports\nbuildmakejobs       2\nI was kicking myself when I discovered this AFTER I rebuilt gcc ;)\nThis option will allow you to use both cpu's for building packages.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "in the java install directory why are there multiple java exe files", "id": 1363, "answers": [{"answer_id": 1352, "document_id": 931, "question_id": 1363, "text": "  If you want to run Java programs, but not develop them, download the JRE. If you want to develop Java applications, download the Java Development Kit, or JDK. The JDK includes the JRE, so you do not have to download both separately", "answer_start": 937, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nJust curious about the directory layout for the JDK . So there are two separate java.exe files  - one is in:\n\nC:\\Program Files (x86)\\Java\\jdk1.7.0_45\\bin\n\n\nand one is in:\n\nC:\\Program Files (x86)\\Java\\jdk1.7.0_45\\jre\\bin\n\n\nWhy does there need to be two files ?  The motivation for this question arises from some challenge I'm having installing a program(SQL Developer).\n    \n\nThere's a difference between installing the jdk vs. the jre.\n\nThe jdk package is the developer package and includes tools such as the compiler (javac).\n\nThe jre package is the core runtime package, and includes the JVM / runtime environment / whatever you need to run software written in JVM languages.\n    \n\nHere a link to the official Oracle documentation.\n\nThe binaries in jdk/bin and jdk/jre/bin are identical. According to the documentation, the PATH should point to jdk/bin.\n    \n\nHere is a link to JDK 7 and JRE 7 Installation Guide\n\n\n  If you want to run Java programs, but not develop them, download the JRE. If you want to develop Java applications, download the Java Development Kit, or JDK. The JDK includes the JRE, so you do not have to download both separately. \n\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "how can i create a java app bundle for max os x on my windows build machine?", "id": 980, "answers": [{"answer_id": 975, "document_id": 603, "question_id": 980, "text": "First make sure PIL is not installed. Download libjpeg from http://www.ijg.org/files/jpegsrc.v8c.tar.gz, unpacked it, ./configure, and make. When I tried to make install it couldn't find the directory to store the man pages so installation failed. I looked at the information on the above link and decided to\n\ncp -r ~/Downloads/jpeg-6d/ /usr/local/jpeg\n\nI suspect if the installation goes fine than that line isn't necessary.\n\nThen edit the following line in PIL's setup.py:\n\nJPEG_ROOT = None\nto\n\nJPEG_ROOT = \"/usr/local/jpeg\"\nfinally:\n", "answer_start": 1676, "answer_category": null}], "is_impossible": false}], "context": "3\n\n\nI'm sure there's a duplicate of this somewhere out there but I looked and am about at the end of my rope. I'm trying get PIL working on my mac OS X 10.8 so that I can use dev_appserver.py to test an imaging feature. First I had trouble installing PIL until I got Homebrew and installed it using brew install pil. I was under the opinion that brew installed all the necessary dependencies but when I tried to resize a jpeg in my app, it says IOError: decoder jpeg is not available. So I looked online and most places said I needed to (1) uninstall PIL, (2) install libjpeg from source and (3) reinstall PIL. So, I brew uninstall PIL, and then\n\ncurl -O www.ijg.org/files/jpegsrc.v7.tar.gz\ntar zxvf jpegsrc.v7.tar.gz\ncd jpeg-7d/\n./configure\nmake\nmake install\nand finally brew install pil. I restart dev_appserver.py and reload the page on localhost, but same error. I tested pil out from the python command-line with\n\n>>> from PIL.Image import Image\n>>> f = open(\"someimagefile\", \"rb\")\n>>> i = Image()\n>>> i.fromstring(f.read(), decoder_name=\"jpeg\")\nTraceback blah blah blah\nIOError: decoder jpeg not available\nI don't have much experience installing utilities from command-line, so I probably missed something obvious. Again, sorry if there are duplicates, but like I said, I looked and couldn't find anything that seemed to work.\n\npython\njpeg\npython-imaging-library\nlibjpeg\ninstallation\nShare\nImprove this question\nFollow\nasked Jan 3 '13 at 4:03\n\nHarrison\n83011 gold badge1414 silver badges2828 bronze badges\nAdd a comment\n2 Answers\n\n2\n\nFinally got it working! Thanks to @zgoda and this link. Here are the steps I ended up with for those of you who have the same problem:\n\nFirst make sure PIL is not installed. Download libjpeg from http://www.ijg.org/files/jpegsrc.v8c.tar.gz, unpacked it, ./configure, and make. When I tried to make install it couldn't find the directory to store the man pages so installation failed. I looked at the information on the above link and decided to\n\ncp -r ~/Downloads/jpeg-6d/ /usr/local/jpeg\n\nI suspect if the installation goes fine than that line isn't necessary.\n\nThen edit the following line in PIL's setup.py:\n\nJPEG_ROOT = None\nto\n\nJPEG_ROOT = \"/usr/local/jpeg\"\nfinally:\n\n$ python setup.py build\n$ python setup.py install\nShare\nImprove this answer\nFollow\nanswered Jan 4 '13 at 4:21\n\nHarrison\n83011 gold badge1414 silver badges2828 bronze badges\nHi, this is very helpful. Can I just clarify for other readers (since I missed it): after the copy step, you need to sudo make install. That is also described here. I did not need to also edit setup.py or execute it. \u2013 \nRacing Tadpole\n May 16 '13 at 23:56 ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Error:Unable to locate adb within SDK in Android Studio", "id": 892, "answers": [{"answer_id": 887, "document_id": 563, "question_id": 892, "text": "Simply went to Project Structure dialog (alt+ctrl+shift+s or button 1 on the screen) and then to project-> Project SDK's was selected <no SDK>", "answer_start": 3204, "answer_category": null}], "is_impossible": false}], "context": "Does anyone know what this means? When I click the \"run\" button on my simulator I get this message.\n\nThrowable: Unable to locate adb within SDK\n\nI am running the latest version, 0.8.14.\n\nangular\nandroid\nandroid-studio\ninstallation\nadb\nShare\nImprove this question\nFollow\nedited Apr 7 at 11:53\n\nDharman\n24.1k2020 gold badges6363 silver badges113113 bronze badges\nasked Dec 4 '14 at 19:05\n\nSleepsOnNewspapers\n3,20433 gold badges2121 silver badges2727 bronze badges\nClose your eclipse. you should then write in the command line or terminal if your using MAC,the following. adb kill-server and then enter. Then write adb start-server..Open eclipse again. \u2013 \nTheo\n Dec 4 '14 at 19:09\n1\nAre u using eclipse? \u2013 \nVladislav Kan\n Dec 4 '14 at 19:09\nadb is located where eclipse is installed. ie C:\\Users(your name)\\Desktop\\Android Studio\\adt-bundle-windows-x86_64-20140702\\eclipse \u2013 \nTheo\n Dec 4 '14 at 19:11\n8\nThis can also happen due to AVG antivirus detecting adb.exe as a trojan (false alarm) and adding it to quarantine. Just whitelist the file and it should be fine. \u2013 \nDzhuneyt\n Sep 20 '15 at 22:56\n@Dzhuneyt, Its not just AVG. Avast also gave me trouble. Make sure to check you antivirus. \u2013 \nNechemia Hoffmann\n Mar 15 '19 at 21:50\nAdd a comment\n28 Answers\n\n192\n\nThe ADB is now located in the Android SDK platform-tools.\n\nCheck your [sdk directory]/platform-tools directory and if it does not exist, then open the SDK manager in the Android Studio (a button somewhere in the top menu, android logo with a down arrow), switch to SDK tools tab and and select/install the Android SDK Platform-tools.\nenter image description here enter image description here enter image description here\n\nAlternatively, you can try the standalone SDK Manager: Open the SDK manager and you should see a \"Launch Standalone SDK manager\" link somewhere at the bottom of the settings window. Click and open the standalone SDK manager, then install/update the\n\"Tools > Android SDK platform tools\".\n\nIf the above does not solve the problem, try reinstalling the tools: open the \"Standalone SDK manager\" and uninstall the Android SDK platform-tools, delete the [your sdk directory]/platform-tools directory completely and install it again using the SDK manager.\n\nCheck your antivirus chest. Sometimes the Antivirus program thinks adb.exe is a virus. If you find it there please restore the file and it will start working. You can also put adb.exe as a safe file in you Antivirus program.\n\nenter image description here enter image description here\n\nHope this helps!\n\nShare\nImprove this answer\nFollow\nedited Aug 3 '19 at 22:05\n\nMindRoasterMir\n18811 gold badge11 silver badge1515 bronze badges\nanswered Dec 22 '14 at 3:39\n\nAkos K\n2,38811 gold badge1010 silver badges1414 bronze badges\nAs of Android Studio 4, you might also need to set an SDK for the project since the IDE does not seem to do this on its own. \u2013 \nStacky\n Feb 16 at 8:20 \n2\nI updated Android Studio (April 2021) and now there's a separate category called \"Android SDK Command-line Tools\". I installed those, restarted Android Studio and now it's working again. \u2013 \nLuc Bloom\n Mar 31 at 10:02 \nAdd a comment\n\n85\n\nIn my case I had no SDK selected for my project(not sure why). Simply went to Project Structure dialog (alt+ctrl+shift+s or button 1 on the screen) and then to project-> Project SDK's was selected <no SDK>. Just changed it to the latest\n\nProject Structure dialog\n\nShare\nImprove this answer\nFollow\nanswered Jun 20 '20 at 19:24\n\nArtem Luzhanovskyi\n1,31788 silver badges66 bronze badges\n12\nThis fixed my issue for me! It was a flutter project. \u2013 \nAaron Elliot\n Jan 9 at 1:25\n1\nThis fixed the issue for me. Thank you. \u2013 \nHesham Eraqi\n Jun 1 at 16:51\nHEY FLUTTERORS... try this solution! \u2013 \nanandhu\n Sep 29 at 5:23\nAdd a comment\n\n23\n\nFor anyone who is still running into this issue. I had a similar problem where I could see my device from adb on the command line using adb devices but Android Studio would not recognize when I had a device attached and would throw either:\n\nUnable to locate adb within SDK or\nUnable to obtain result of 'adb version'\n\nI had tried start/stops of adb, uninstalls, of platform-tools, and more. What I found was that inside my C:\\Users\\<UserName>\\AppData\\Local\\Android folder I had multiple sdk folders. I performed the following:\n\nUnistall Platform-Tools using Android Studio's SDK Manager\nDeleted all platform-tools\\ directories within each C:\\Users\\<UserName>\\AppData\\Local\\Android\\sdk* directory\nReinstalled Platform-Tools using Android Studio's SDK Manager\nHope this helps someone someday with their issue.\n\nShare\nImprove this answer\nFollow\nedited Jan 5 '18 at 22:07\nanswered Dec 14 '15 at 21:58\n\nNick\n73744 silver badges1111 bronze badges\n1\nC:\\Users\\myusername\\AppData\\Local\\Android\\sdk\\platform-tools \u2013 \nIberoMedia\n Jun 30 '16 at 1:48 \nIn my case I had only a single SDK folder, but uninstalling and re-installing Platform-Tools worked for me \u2013 \nfikkatra\n Jun 12 '20 at 12:54\nAdd a comment\n\n17\n\nIf you are using Android Studio and have AVG virus protection, the adb.exe file might be in the Virus Vault. This was my problem. To fix: Open AVG. Select Options (top right), then Virus Vault. If you see the adb.exe file in there, select it and then click Restore.\n\nShare\nImprove this answer\nFollow\nanswered Apr 9 '16 at 8:16\n\nHGamble\n38544 silver badges1616 bronze badges\n4\nCan confirm this to happen in Avast. \u2013 \nSuperHaker\n May 18 '17 at 17:23\n3\nyes, it also happens in Avast. Go to Protection>Virus chest> check the required checkbox adb.exe>click drop down on delete>restore>restart Android Studio \u2013 \nM.ArslanKhan\n Mar 16 '18 at 7:23\nAdd a comment\n\n16\n\nIn Android Studio, Click on 'Tools' on the top tab bar of android studio\n\nTools >> Android >> SDK Manager >> Launch Standalone Sdk manager\n\nthere you can clearly see which platform tool is missing , then just install that and your adb will start working properly.\n\nFully install at-least one Api package (Android Api 23 or 24) .\n\nShare\nImprove this answer\nFollow\nedited Dec 22 '16 at 5:22\nanswered Sep 24 '15 at 9:45\n\nLucky Rana\n99011 gold badge1111 silver badges1919 bronze badges\nAdd a comment\n\n11\n\ntry this: File->project Structure into Project Structure Left > SDKs SDK location select Android SDK location (old version use Press +, add another sdk)\n\nShare\nImprove this answer\nFollow\nanswered Dec 4 '14 at 19:10\n\nVladislav Kan\n34411 silver badge1212 bronze badges\nso you think there's something wrong with my sdk location? \u2013 \nSleepsOnNewspapers\n Dec 4 '14 at 19:31\n1\nYes, i think so. try to open sdk manager and also install all the plugins \u2013 \nVladislav Kan\n Dec 5 '14 at 3:41\nAdd a comment\n\n8\n\nFinally after several hours of investigation I think I have another solution for everyone having issues with AVD Manager \"Unable to locate adb\".\n\nI know we have the setting for the SDK in File -> Settings -> Appearance & Behavior -> System Settings -> Android SDK. This it seems is not enough! It appears that Android Studio (at least the new version 4) does not give projects a default SDK, despite the above setting.\n\nSo, you also (for each project) need to go to File -> Project Structure -> Project Settings -> Project, and select the Project SDK, which is set to [No SDK] by default.\n\nIf there's nothing in the drop-down box, then select New, select Android SDK, and navigate to your Android SDK location (normally C:\\Users[username]\\AppData\\Local\\Android\\Sdk on Windows). You will then be able to select the Android API xx Platform. You now should not get this annoying adb error.\n\nHTH", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How do I launch an application after install in a Visual Studio Setup Project", "id": 1749, "answers": [{"answer_id": 1736, "document_id": 1321, "question_id": 1749, "text": "You can find the script over on Aaron Stebner's blog at MSDN http://blogs.msdn.com/astebner/archive/2006/08/12/696833.aspx", "answer_start": 187, "answer_category": null}], "is_impossible": false}], "context": "I have created a setup project using Visual Studio 2008. After the application is finished installing, I would like to have it start up immediately. Any thoughts on how this can be done?\nYou can find the script over on Aaron Stebner's blog at MSDN http://blogs.msdn.com/astebner/archive/2006/08/12/696833.aspx\nThere's an interesting article about it on CodeProject and some good answers there also (which is where I found Aaron's article). http://www.codeproject.com/KB/install/Installation.aspx\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "mysql python installation problems on mac os x lion", "id": 1491, "answers": [{"answer_id": 1480, "document_id": 1068, "question_id": 1491, "text": "brew install python\nbrew install mysql", "answer_start": 1810, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI installed everything successfully, or so I thought:\n\n\nMySQL 5.5 for x86_64.\nPython 2.7, x86_64.\nmysql-python 1.2.3, x86_64.\n\n\nBut when I try:\n\nimport MySQLdb\n\n\nI get:\n\n    ImportError: \ndlopen(/Users/aj/.python-eggs/MySQL_python-1.2.3-py2.7-macosx-10.6-ix86_64.egg-tmp/_mysql.so, 2): \nno suitable image found.  \nDid find:   \n/Users/aj/.python-eggs/MySQL_python-1.2.3-py2.7-macosx-10.6-ix86_64.egg-tmp/_mysql.so: mach-o, \nbut wrong architecture\n\n\nWhat else can I be missing?\n\nMy system is of course 64bit version as well, uname -a gives:\n\nDarwin ajmacbook.local 11.1.0 Darwin Kernel Version 11.1.0: Tue Jul 26 16:07:11 PDT 2011; root:xnu-1699.22.81~1/RELEASE_X86_64 x86_64\n\n\nI think I have read most SO answers and Google results on the subject, can't think of anything else to try. Any suggestion would be appreciated.\n    \n\nI think there might be slight quirks with doing this on Mac 64-bit (and if you google this problem shows up a lot too).\nI've run into it, and there are a couple things you can do:\nOverride the environment\nYou can change the DYLD_LIBRARY_PATH environment variable, which tells the linker where to look for dynamic libraries (.so files and such). You said you also downloaded the 64-bit version of MySQL, so where ever it's installed, change the path you see here:\nIn a shell:\nexport DYLD_LIBRARY_PATH=/usr/local/mysql/lib/\nAnd then run python and see if you can import MySQLdb.\nIf that works, you can make this permanent by altering your shell profile (.bash_profile, most likely).\nUse homebrew\nI don't really like mucking around with making sure MySQL and Python and all that are correct architectures and installing them separately. I run homebrew, which is a sort of package manager for Mac. If you install that, you can pretty easily take care of this issue:\n\nbrew install python\nbrew install mysql\n/usr/local/share/python/easy_install mysql-python\n\nDo note that homebrew installs into /usr/local, so you should add /usr/local/bin to your PATH, ahead of /usr/bin and /bin, otherwise you'll get really confused why python is different.\nYou can add /usr/local/share/python to your PATH as well, to make it permanent.\n    \n\nWith the help of the comment from @birryree I found the problem. I would probably be better off following the procedure suggested by @birryree in his answer but I did try this before and it worked:\n\nAs suggested, I did:\n\nfile /Users/aj/.python-eggs/MySQL_python-1.2.3-py2.7-macosx-10.6-ix86_64.egg-tmp/_mysql.so\n\n\nTo get: [...]: Mach-O bundle i386\nSo, wrong architecture. From there I did the same with mysql and python just to be sure:\nfile $(which python) gave:\n\n/Library/Frameworks/Python.framework/Versions/2.7/bin/python: Mach-O universal binary with 2 architectures\n/Library/Frameworks/Python.framework/Versions/2.7/bin/python (for architecture i386):   Mach-O executable i386\n/Library/Frameworks/Python.framework/Versions/2.7/bin/python (for architecture x86_64): Mach-O 64-bit executable x86_64\n\n\nAnd file $(which mysql):\n\n/usr/local/mysql/bin/mysql: Mach-O 64-bit executable x86_64\n\n\nSo I uninstalled the mysql-python package: sudo pip uninstall mysql-python and installed it again. But doing this I realized my previous mistake while installing this package. First time I typed:\n\nsudo ARCHFLAGS='-arch ix86_64' python setup.py build (and \"install\" afterwards)\n\nThe architecture name was wrong, should be '-arch x86_64', no \"i\", so it just ignored my flag and installed the 32bit.\n\nThe proper commands to install the downloaded mysql-python package for 64bit (from the source folder):\n\nsudo ARCHFLAGS='-arch x86_64' python setup.py build\nsudo ARCHFLAGS='-arch x86_64' python setup.py install\n\n    \n\nVERY IMPORTANT!\n\nAs mentioned above, please make sure you are running the 64-bit version of mysql.  It's easy to overlook this detail especially if you've upgraded from Snow Leopard. (I certainly did).\n\nif you're not sure about removing the older version of mysql on your system, refer to this post:  http://johnmcostaiii.net/2011/removing-mysql-osx-lion/\n    \n\nI had the same problem, and a lot of headache with MySQLdb after fixing the 64bit issue (it was complaining also about where is libmysqlclient.18.dylib).\n\nI think it's time to switch to the official MysQL Python Connector?\n\nsudo pip install mysql-connector-python\n\n\nOr download it from: http://dev.mysql.com/downloads/connector/python/\n\nDocumentation: http://dev.mysql.com/doc/refman/5.5/en/connector-python.htm\n\nIt's easy to use and also compatible with PEP 249 (Python DB API version 2.0).\n    \n\nAlso make sure you are running Python 64-bit too. I was running mysql 64 bit and Python 32bit so got the 'but wrong architecture' error\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Do you use Phing?", "id": 514, "answers": [{"answer_id": 516, "document_id": 239, "question_id": 514, "text": "Of course you could write custom scripts for all of the above. However, using a specialized build tool like Phing gives you a number of benefits. You'll be using a proven framework so instead of having to worry about setting up \"infrastructure\" you can focus on the code you need to write. Using Phing will also make it easier for when new members join your team, they'll be able to understand what is going on if they've used Phing (or Ant, which is what Phing is based on) before.", "answer_start": 745, "answer_category": null}], "is_impossible": false}], "context": "Does anyone use Phing to deploy PHP applications, and if so how do you use it? We currently have a hand-written \"setup\" script that we run whenever we deploy a new instance of our project. We just check out from SVN and run it. It sets some basic configuration variables, installs or reloads the database, and generates a v-host for the site instance.\nI have often thought that maybe we should be using Phing. I haven't used ant much, so I don't have a real sense of what Phing is supposed to do other than script the copying of files from one place to another much as our setup script does. What are some more advanced uses that you can give examples of to help me understand why we would or would not want to integrate Phing into our process?\nOf course you could write custom scripts for all of the above. However, using a specialized build tool like Phing gives you a number of benefits. You'll be using a proven framework so instead of having to worry about setting up \"infrastructure\" you can focus on the code you need to write. Using Phing will also make it easier for when new members join your team, they'll be able to understand what is going on if they've used Phing (or Ant, which is what Phing is based on) before.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Installing a new version of a deployment project over old version", "id": 540, "answers": [{"answer_id": 542, "document_id": 265, "question_id": 540, "text": "This is tricky, you have to do following steps,\nSet Remove Previous Installation as True\nSet Detect new version as True\nYour C# program's version must increase with every deployment\nYou should change version of your installer to one higher version and it will ask you to change product code, select YES.\nDo not change your upgrade code, let it be same.", "answer_start": 302, "answer_category": null}], "is_impossible": false}], "context": "I have a deployment project which will not let me install over an older version. The msi file says to uninstall the program first from Add/Remove programs. This is not a good user experience. How can I do it so that the installer will simply remove the software first and then install the new version?\nThis is tricky, you have to do following steps,\nSet Remove Previous Installation as True\nSet Detect new version as True\nYour C# program's version must increase with every deployment\nYou should change version of your installer to one higher version and it will ask you to change product code, select YES.\nDo not change your upgrade code, let it be same.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "http timestamp verisign com scripts timstamp dll not available", "id": 1412, "answers": [{"answer_id": 1401, "document_id": 987, "question_id": 1412, "text": "As of 4/24/2017, Verisign knowledge base article AR185 recommends the jarsigner arguments \"-tsa http://sha256timestamp.ws.symantec.com/sha256/timestamp\". This works for certificates issued by Symantec, and presumably all of Symantec's subsidiaries, at least. http://timestamp.verisign.com/scripts/timstamp.dll does not work.\n    \n\nThis list of time servers seems to be getting regular updates: https://gist.github.com/Manouchehri/fd754e402d98430243455713efada710\n    \n\nYou can use \"http://timestamp.digicert.com\" without quotes.\nfor more info: https://knowledge.digicert.com/solution/SO912.html", "answer_start": 1567, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nWhen the following URL is not available, what other timestamp URL can I use in my setup authoring tool? The specific error I get is: SignTool Error: The specified timestamp server either could not be reached or returned an invalid response.\n\nhttp://timestamp.verisign.com/scripts/timstamp.dll\n    \n\nHonestly, I would just try again. But you can use any of the following:\n\nhttp://timestamp.globalsign.com/scripts/timstamp.dll,\nhttp://timestamp.comodoca.com/authenticode, or\nhttp://www.startssl.com/timestamp.\nhttp://timestamp.sectigo.com\n\n    \n\nTry these servers\nhttp://tsa.starfieldtech.com\nhttp://timestamp.globalsign.com/scripts/timstamp.dll\nhttp://timestamp.comodoca.com/authenticode\nhttp://www.startssl.com/timestamp\nhttp://timestamp.verisign.com/scripts/timstamp.dll\nhttp://timestamp.sectigo.com\n\nWith a retry script such as the one included here:\nAlternative timestamping services for Authenticode\n    \n\nhttp://timestamp.verisign.com/scripts/timstamp.dll has limped along for the last few years and had been working in a sort of depreciated state, but the new owners of the certificate issuing business, DigiCert, have issued a migration alert.\nThey have officially put the old services to EOL as of the back end of 2019.\n(article no no longer exists 18/11/2020)\nhttps://knowledge.digicert.com/alerts/migration-of-legacy-verisign-and-symantec-time-stamping-services.html\n\nNew services can be found at\n\nhttp://timestamp.digicert.com?alg=sha1\nhttp://timestamp.digicert.com?alg=sha256\n\nTroubleshooting time stamping\n    \n\nFor jarsigner users:\n\nAs of 4/24/2017, Verisign knowledge base article AR185 recommends the jarsigner arguments \"-tsa http://sha256timestamp.ws.symantec.com/sha256/timestamp\". This works for certificates issued by Symantec, and presumably all of Symantec's subsidiaries, at least. http://timestamp.verisign.com/scripts/timstamp.dll does not work.\n    \n\nThis list of time servers seems to be getting regular updates: https://gist.github.com/Manouchehri/fd754e402d98430243455713efada710\n    \n\nYou can use \"http://timestamp.digicert.com\" without quotes.\nfor more info: https://knowledge.digicert.com/solution/SO912.html\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Using git-flow in a multi-stage deployment", "id": 1123, "answers": [{"answer_id": 1116, "document_id": 700, "question_id": 1123, "text": "Here's what I ended up doing, this is a slight variation of what I proposed above and stems from another question I posted here: Deploy git branches", "answer_start": 312, "answer_category": null}], "is_impossible": false}], "context": "Drawing a blank with finalizing my deploy scheme here. After posting this question: Migrating a production site with no VCS at all to Git, I've got the gist of deploying to a local repo down.\nMy local development server has a git-flow repository on it that I can push to and it will update an external worktree.\nHere's what I ended up doing, this is a slight variation of what I proposed above and stems from another question I posted here: Deploy git branches\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "create rpm package from ant script under windows", "id": 1977, "answers": [{"answer_id": 1963, "document_id": 1562, "question_id": 1977, "text": "y.\n\nfpm -s dir -t rpm mydir -- or another incantation where you provide the rpm keywords. See fpm --h", "answer_start": 2002, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI need to create an RPM package from an ANT script that runs on a Windows computer.  This package must contain the result classes from the build plus some other, additional resources.\n\nI guess there should be some program somewhere that can be invoked from the command line to create this package.\n\nThe reason why I want to do this under windows is that we have a script that builds installers for several different platforms, and everything is already configured for windows so I want to avoid the pain of migrating everything to linux.\n\nDo you know how could this be achieved?\n\nEDIT:\nI've used a separate Linux machine to do this.  I copy all files using the scp task, then execute the rpm command remotely using the ssh task.  If this task returns a success code, I copy the resulting rpm back with scp.\n    \n\nAlso consider the redline-rpm java library &amp; ant task. https://github.com/craigwblake/redline\n    \n\nA possible solution is to use a pure java RPM Manipulation tool, such as the one described in:\n\nhttp://jrpm.sourceforge.net/rpmspec/index.html\n    \n\nI'm not aware of any packages to do this, but that could just be my ignorance speaking.\n\nTwo suggestions:\n\nSet up a Linux machine (separate PC or virtual machine) and kit the rpm remotely via scripts (see ssh/scp/samba). This strikes me as a safe approach that avoids full build environment migration. \n\nA more adventurous approach would be to write your own RPM files using Java. The RPM file format seems to be a header followed by one of a number of standard archive formats. Some analysis of the archive contents would be needed, so this approach might be quite time consuming in the short term (though it would probably result in faster builds).\n    \n\nChecking the Ant manual, I found the following optional task:\n\nhttp://ant.apache.org/manual/Tasks/rpm.html\n\nHowever, this only runs with linux computers\n    \n\nFreaking Package Management can do this. You should be able to run it both with JRuby and on MRI Ruby.\n\nfpm -s dir -t rpm mydir -- or another incantation where you provide the rpm keywords. See fpm --help.\n    \n\nI don't think it is possible, runtime packages built for linux won't work on windows (we don't expect dlls and exes on linux, do we!). Cygwin faces similar challenges and they clearly state that they do it by compiling source files for windows.\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Maven: Failed to read artifact descriptor", "id": 1878, "answers": [{"answer_id": 1864, "document_id": 1449, "question_id": 1878, "text": "You can always try mvn -U clean install\n-U forces a check for updated releases and snapshots on remote repositories.", "answer_start": 722, "answer_category": null}], "is_impossible": false}], "context": "I am hoping someone can help me with a problem I am struggling with.\nWhen I try to build my project from the terminal I get this error:\nFailed to read artifact descriptor for com.morrislgn.merchandising.common:test-data-utils:jar:0.3b-SNAPSHOT: Could not find artifact com.morrislgn.merchandising:merchandising:pom:0.3b-SNAPSHOT\nThe common.test-data-utils jar is created by a separate project and shared between this and another project (the other project doesn't build either, but that is down to another problem).\nI am able to build com.morrislgn.merchandising.common:test-data-utils without issue, I can see the entry it makes in the .m2 local repository on my machine. I have reindexed my repository in Eclipse also.\t\nYou can always try mvn -U clean install\n-U forces a check for updated releases and snapshots on remote repositories.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Hive installation issues: Hive metastore database is not initialized", "id": 829, "answers": [{"answer_id": 824, "document_id": 511, "question_id": 829, "text": "1.\tfind /usr/ -name hive-schema-2.0.0.derby.sql\n2.\tvi /usr/local/Cellar/hive/2.0.1/libexec/scripts/metastore/upgrade/derby/hive-schema-2.0.0.derby.sql\n3.\tcomment the 'NUCLEUS_ASCII' function and 'NUCLEUS_MATCHES' function\n4.\trerun schematool -dbType derby -initSchema, then everything goes well!", "answer_start": 633, "answer_category": null}], "is_impossible": false}], "context": "I tried to install hive on a raspberry pi 2. I installed Hive by uncompress zipped Hive package and configure $HADOOP_HOME and $HIVE_HOME manually under hduser user-group I created. When running hive, I got the following error message. Starting metastore schema initialization to 2.0.0 Initialization script hive-schema-2.0.0.derby.sql Error: FUNCTION 'NUCLEUS_ASCII' already exists. (state=X0Y68,code=30000) org.apache.hadoop.hive.metastore.HiveMetaException: Schema initialization FAILED! Metastore state would be inconsistent !!\nHowever, I can't find either metastore_db or metastore_db.tmp folder under install path, so I tried:\n1.\tfind /usr/ -name hive-schema-2.0.0.derby.sql\n2.\tvi /usr/local/Cellar/hive/2.0.1/libexec/scripts/metastore/upgrade/derby/hive-schema-2.0.0.derby.sql\n3.\tcomment the 'NUCLEUS_ASCII' function and 'NUCLEUS_MATCHES' function\n4.\trerun schematool -dbType derby -initSchema, then everything goes well!\nhope this help someone.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "What creates the directory \"app.publish\" in visual studio 2013?", "id": 1637, "answers": [{"answer_id": 1625, "document_id": 1211, "question_id": 1637, "text": "I was experimenting with ClickOnce, then decided against using it, then started noticing a MyApp.application file and app.publish folder in my bin directory. Unchecking \"Enable ClickOnce security settings\" on the Security tab did the trick for me.", "answer_start": 241, "answer_category": null}], "is_impossible": false}], "context": "I switched over visual studio 2010 express, to visual studio 2013. Now, whenever I try to test the code, he says /bin/release/app.publish access is denied, giving back a error and forcing me to manually delete the folder. Every single time.\nI was experimenting with ClickOnce, then decided against using it, then started noticing a MyApp.application file and app.publish folder in my bin directory. Unchecking \"Enable ClickOnce security settings\" on the Security tab did the trick for me.\nWhy is visual studio attempting to create that path in the first place? Whenever I go there he is empty anyways.\n\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Capistrano Attempting To Create /public Directory", "id": 1257, "answers": [{"answer_id": 1249, "document_id": 828, "question_id": 1257, "text": "You should set deploy_to correctly:\nset :deploy_to, \"/home/<user>/<domain>/<application>\"", "answer_start": 236, "answer_category": null}], "is_impossible": false}], "context": "I'm currently in the process of attempting my first Rails deployment using Capistrano, and I've run into a roadblock I haven't been able to overcome. During the cap deploy I'm getting an error \"mkdir: cannot create directory/public'`\".\nYou should set deploy_to correctly:\nset :deploy_to, \"/home/<user>/<domain>/<application>\"\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "React app is looking for static files in different location when using proxy", "id": 1328, "answers": [{"answer_id": 1318, "document_id": 897, "question_id": 1328, "text": "There can be two places where you could define relative path\n1) By default, Create React App produces a build assuming your app is hosted at the server root. To override this, specify the homepage in your package.json, for example:\n  \"homepage\": \"http://mywebsite.com/react_app\",\nThis will let Create React App correctly infer the root path to use in the generated HTML file.\n2) If you are using react-router@^4, relative path can be set using basename\n<Router history={browserHistory} basename={'react_app'}>\n  <Route path=\"/\" component={App} />\n</Router>", "answer_start": 389, "answer_category": null}], "is_impossible": false}], "context": "I have used npx create-react-app my-app to create a react app.\nThe I used npm run build to build the app and deployed it using serve -s build\nI'm using a proxy server to make my app publicly available.\nMy httpd configs looks like below,\n/react_app http://192.168.1.100:3000\nWhats really happening is once a request comes to http://<my public domain>/react_app I need to show my react app.\nThere can be two places where you could define relative path\n1) By default, Create React App produces a build assuming your app is hosted at the server root. To override this, specify the homepage in your package.json, for example:\n  \"homepage\": \"http://mywebsite.com/react_app\",\nThis will let Create React App correctly infer the root path to use in the generated HTML file.\n2) If you are using react-router@^4, relative path can be set using basename\n<Router history={browserHistory} basename={'react_app'}>\n  <Route path=\"/\" component={App} />\n</Router>\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "how to get variables corresponding to build-relevant information for Cmake?", "id": 87, "answers": [{"answer_id": 93, "document_id": 56, "question_id": 87, "text": "Packages are found with the find_package() command.  The result of using find_package() is either a set of IMPORTED targets, or a set of variables corresponding to build-relevant information.", "answer_start": 153, "answer_category": null}], "is_impossible": false}, {"question": "how to specify components of a package for cmake?", "id": 91, "answers": [{"answer_id": 97, "document_id": 56, "question_id": 91, "text": "Both types of packages also support specifying components of a package, either after the REQUIRED keyword: find_package(Qt5 5.1.0 CONFIG REQUIRED Widgets Xml Sql) or as a separate COMPONENTS list: find_package(Qt5 5.1.0 COMPONENTS Widgets Xml Sql)", "answer_start": 1304, "answer_category": null}], "is_impossible": false}, {"question": "I am using Cmake, if I don't want to search one package, what shall I do? ", "id": 93, "answers": [{"answer_id": 99, "document_id": 56, "question_id": 93, "text": "By setting the CMAKE_DISABLE_FIND_PACKAGE_<PackageName> variable to TRUE, the <PackageName> package will not be searched, and will always be NOTFOUND.", "answer_start": 1628, "answer_category": null}], "is_impossible": false}, {"question": "when using cmake to find packages, how can I know the location of the package configuration file?", "id": 95, "answers": [{"answer_id": 103, "document_id": 56, "question_id": 95, "text": "The <PackageName>_DIR cache variable is set to the location of the package configuration file.", "answer_start": 2569, "answer_category": null}], "is_impossible": false}, {"question": "what does CMakePackageConfigHelpers module  do?", "id": 103, "answers": [{"answer_id": 111, "document_id": 56, "question_id": 103, "text": "The CMakePackageConfigHelpers module provides a macro for creating a simple ConfigVersion.cmake file.  This file sets the version of the package.", "answer_start": 5352, "answer_category": null}], "is_impossible": false}, {"question": "when using cMake, which command can I use to export the targets in the ClimbingStatsTargets export-set?", "id": 105, "answers": [{"answer_id": 113, "document_id": 56, "question_id": 105, "text": "The install(EXPORT) command is used to export the targets in the ClimbingStatsTargets export-set, defined previously by the install(TARGETS) command. ", "answer_start": 5736, "answer_category": null}], "is_impossible": false}, {"question": "when using cMake, How can I know the detail message about why package could not be found?", "id": 109, "answers": [{"answer_id": 117, "document_id": 56, "question_id": 109, "text": "the ClimbingStats_NOT_FOUND_MESSAGE is set to a diagnosis that the package could not be found because an invalid component was specified", "answer_start": 7758, "answer_category": null}], "is_impossible": false}, {"question": "Can I find_package() command to find dependcies when using cMake?", "id": 111, "answers": [{"answer_id": 119, "document_id": 56, "question_id": 111, "text": "When a consumer uses the installed package, the consumer will run the appropriate find_package() commands (via the find_dependency macro described above) to find the dependencies", "answer_start": 9295, "answer_category": null}], "is_impossible": false}, {"question": "I'm using Cmake, and I want to find a package in the windows registry, what shall I do?", "id": 116, "answers": [{"answer_id": 124, "document_id": 56, "question_id": 116, "text": "On Windows the user package registry is stored in the Windows registry under a key in HKEY_CURRENT_USER. A <PackageName> may appear under registry key: HKEY_CURRENT_USER\\Software\\Kitware\\CMake\\Packages\\<PackageName>\n", "answer_start": 11405, "answer_category": null}], "is_impossible": false}, {"question": "if build trees in registry has been deleted with something being uninstalled, how to clear?", "id": 117, "answers": [{"answer_id": 125, "document_id": 56, "question_id": 117, "text": " Build trees tend to be deleted by developers and have no \"uninstall\" event that could trigger removal of their entries.  In order to keep the registries clean the find_package() command automatically removes stale entries it encounters if it has sufficient permissions.", "answer_start": 12421, "answer_category": null}], "is_impossible": false}], "context": "\n\n\n\n\n\n\ncmake-packages(7)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNavigation\n\n\nindex\n\nnext |\n\nprevious |\n\n\n\nPackages provide dependency information to CMake based buildsystems.  Packages are found with the find_package() command.  The result of using find_package() is either a set of IMPORTED targets, or a set of variables corresponding to build-relevant information.\n\n\nUsing Packages\u00b6\nCMake provides direct support for two forms of packages, Config-file Packages and Find-module Packages. Indirect support for pkg-config packages is also provided via the FindPkgConfig module.  In all cases, the basic form of find_package() calls is the same: find_package(Qt4 4.7.0 REQUIRED) # CMake provides a Qt4 find-module find_package(Qt5Core 5.1.0 REQUIRED) # Qt provides a Qt5 package config file. find_package(LibXml2 REQUIRED) # Use pkg-config via the LibXml2 find-module\n\n\nIn cases where it is known that a package configuration file is provided by upstream, and only that should be used, the CONFIG keyword may be passed to find_package(): find_package(Qt5Core 5.1.0 CONFIG REQUIRED) find_package(Qt5Gui 5.1.0 CONFIG). Similarly, the MODULE keyword says to use only a find-module: find_package(Qt4 4.7.0 MODULE REQUIRED)\n\n\nSpecifying the type of package explicitly improves the error message shown to the user if it is not found. Both types of packages also support specifying components of a package, either after the REQUIRED keyword: find_package(Qt5 5.1.0 CONFIG REQUIRED Widgets Xml Sql) or as a separate COMPONENTS list: find_package(Qt5 5.1.0 COMPONENTS Widgets Xml Sql)\n\n\nHandling of COMPONENTS and OPTIONAL_COMPONENTS is defined by the package. By setting the CMAKE_DISABLE_FIND_PACKAGE_<PackageName> variable to TRUE, the <PackageName> package will not be searched, and will always be NOTFOUND.\n\n\nA config-file package is a set of files provided by upstreams for downstreams to use. CMake searches in a number of locations for package configuration files, as described in the find_package() documentation.  The most simple way for\na CMake user to tell cmake(1) to search in a non-standard prefix for a package is to set the CMAKE_PREFIX_PATH cache variable. Config-file packages are provided by upstream vendors as part of development packages, that is, they belong with the header files and any other files provided to assist downstreams in using the package. A set of variables which provide package status information are also set automatically when using a config-file package. The <PackageName>_FOUND variable is set to true or false, depending on whether the package was found.  The <PackageName>_DIR cache variable is set to the location of the package configuration file.\n\n\nA find module is a file with a set of rules for finding the required pieces of a dependency, primarily header files and libraries.  Typically, a find module is needed when the upstream is not built with CMake, or is not CMake-aware enough to otherwise provide a package configuration file.  Unlike a package configuration file, it is not shipped with upstream, but is used by downstream to find the files by guessing locations of files with platform-specific hints. Unlike the case of an upstream-provided package configuration file, no single point of reference identifies the package as being found, so the <PackageName>_FOUND variable is not automatically set by the find_package() command.  It can still be expected to be set by convention however and should be set by the author of the Find-module.  Similarly there is no <PackageName>_DIR variable, but each of the artifacts such as library locations and header file locations provide a separate cache variable. See the cmake-developer(7) manual for more information about creating Find-module files.\n\n\n\n\n\nwhere Foo* is a case-insensitive globbing expression.  In our example the globbing expression will match <prefix>/lib/cmake/foo-1.2 and the package configuration file will be found. Once found, a package configuration file is immediately loaded.  It, together with a package version file, contains all the information the project needs to use the package.\n\n\n\nWhen the find_package() command finds a candidate package configuration file it looks next to it for a version file. The version file is loaded to test whether the package version is an acceptable match for the version requested. If the version file claims compatibility the configuration file is accepted. Otherwise it is ignored.\n\n\nVersion files are loaded in a nested scope so they are free to set any variables they wish as part of their computation. The find_package command wipes out the scope when the version file has completed and it has checked the output variables. When the version file claims to be an acceptable match for the requested version the find_package command sets the following variables for use by the project:<PackageName>_VERSIONFull provided version string <PackageName>_VERSION_MAJORMajor version if provided, else 0 <PackageName>_VERSION_MINORMinor version if provided, else 0 <PackageName>_VERSION_PATCHPatch version if provided, else 0 <PackageName>_VERSION_TWEAKTweak version if provided, else 0 <PackageName>_VERSION_COUNTNumber of version components, 0 to 4\n\n\nThe variables report the version of the package that was actually found. The <PackageName> part of their name matches the argument given to the find_package() command.\n\n\nThe CMakePackageConfigHelpers module provides a macro for creating a simple ConfigVersion.cmake file.  This file sets the version of the package.  It is read by CMake when find_package() is called to determine the compatibility with the requested version, and to set some version-specific variables<PackageName>_VERSION, <PackageName>_VERSION_MAJOR, <PackageName>_VERSION_MINOR etc.  The install(EXPORT) command is used to export the targets in the ClimbingStatsTargets export-set, defined previously by the install(TARGETS) command. This command generates the ClimbingStatsTargets.cmake file to contain IMPORTED targets, suitable for use by downstreams and arranges to install it to lib/cmake/ClimbingStats.  The generated ClimbingStatsConfigVersion.cmake and a cmake/ClimbingStatsConfig.cmake are installed to the same location, completing the package.\n\nThe generated IMPORTED targets have appropriate properties set to define their usage requirements, such as INTERFACE_INCLUDE_DIRECTORIES, INTERFACE_COMPILE_DEFINITIONS and other relevant built-in INTERFACE_ properties.  The INTERFACE variant of user-defined properties listed in COMPATIBLE_INTERFACE_STRING and other Compatible Interface Properties are also propagated to the generated IMPORTED targets.  In the above case, ClimbingStats_MAJOR_VERSION is defined as a string which must be compatible among the dependencies of any depender.  By setting this custom defined user property in this version and in the next version of ClimbingStats, cmake(1) will issue a diagnostic if there is an attempt to use version 3 together with version 4.  Packages can choose to employ such a pattern if different major versions of the package are designed to be incompatible.\n\nA NAMESPACE with double-colons is specified when exporting the targets for installation.  This convention of double colons gives CMake a hint that the name is an IMPORTED target when it is used by downstreams with the target_link_libraries() command.  This way, CMake can issue a diagnostic if the package providing it has not yet been found. In this case, when using install(TARGETS) the INCLUDES DESTINATION was specified.  This causes the IMPORTED targets to have their INTERFACE_INCLUDE_DIRECTORIES populated with the include directory in the CMAKE_INSTALL_PREFIX.  When the IMPORTED target is used by downstream, it automatically consumes the entries from that property.\n\n\n\nHere, the ClimbingStats_NOT_FOUND_MESSAGE is set to a diagnosis that the package could not be found because an invalid component was specified.  This message variable can be set for any case where the _FOUND variable is set to False, and will be displayed to the user.\n\nCreating a Package Configuration File for the Build Tree\u00b6 The export(EXPORT) command creates an IMPORTED targets\ndefinition file which is specific to the build-tree, and is not relocatable. This can similarly be used with a suitable package configuration file and package version file to define a package for the build tree which may be used without installation.  Consumers of the build tree can simply ensure that the CMAKE_PREFIX_PATH contains the build directory, or set the ClimbingStats_DIR to <build_dir>/ClimbingStats in the cache.\n\n\nThe referenced variables may contain the absolute paths to libraries and include directories as found on the machine the package was made on. This would create a package with hard-coded paths to dependencies and not suitable for relocation. Ideally such dependencies should be used through their own IMPORTED targets that have their own IMPORTED_LOCATION and usage requirement properties such as INTERFACE_INCLUDE_DIRECTORIES populated appropriately.  Those imported targets may then be used with the target_link_libraries() command for ClimbingStats: target_link_libraries(ClimbingStats INTERFACE Foo::Foo Bar::Bar)\n\n\nWith this approach the package references its external dependencies only through the names of IMPORTED targets. When a consumer uses the installed package, the consumer will run the appropriate find_package() commands (via the find_dependency macro described above) to find the dependencies and populate the imported targets with appropriate paths on their own machine. Unfortunately many modules shipped with CMake do not yet provide IMPORTED targets because their development pre-dated this approach.  This may improve incrementally over time.  Workarounds to create relocatable packages using such modules include:\n\nWhen building the package, specify each Foo_LIBRARY cache entry as just a library name, e.g. -DFoo_LIBRARY=foo.  This\ntells the corresponding find module to populate the Foo_LIBRARIES with just foo to ask the linker to search for the library instead of hard-coding a path. Or, after installing the package content but before creating the package installation binary for redistribution, manually replace the absolute paths with placeholders for substitution by the\ninstallation tool when the package is installed.\n\nThe registries are especially useful to help projects find packages in non-standard install locations or directly in their own build trees. A project may populate either the user or system registry (using its own means, see below) to refer to its location. In either case the package should store at the registered location a Package Configuration File (<PackageName>Config.cmake) and optionally a Package Version File (<PackageName>ConfigVersion.cmake).\nThe find_package() command searches the two package registries as two of the search steps specified in its documentation.  If it has sufficient permissions it also removes stale package registry entries that refer to directories that do not exist or do not contain a matching package configuration file.\n\n\nThe User Package Registry is stored in a per-user location. The export(PACKAGE) command may be used to register a project build tree in the user package registry.  CMake currently provides no interface to add install trees to the user package registry.  Installers must be manually taught to register their packages if desired. On Windows the user package registry is stored in the Windows registry under a key in HKEY_CURRENT_USER. A <PackageName> may appear under registry key: HKEY_CURRENT_USER\\Software\\Kitware\\CMake\\Packages\\<PackageName>\n\n\n CMakeLists.txt code:find_package(MyPackage) will search the registered locations for package configuration files (MyPackageConfig.cmake).  The search order among package registry entries for a single package is unspecified and the entry names (hashes in this example) have no meaning.  Registered locations may contain package version files (MyPackageConfigVersion.cmake) to tell find_package() whether a specific location is suitable for the version requested.\n\n\n\nPackage registry entries are individually owned by the project installations that they reference.  A package installer is responsible for adding its own entry and the corresponding uninstaller is responsible for removing it. The export(PACKAGE) command populates the user package registry with the location of a project build tree.  Build trees tend to be deleted by developers and have no \"uninstall\" event that could trigger removal of their entries.  In order to keep the registries clean the find_package() command automatically removes stale entries it encounters if it has sufficient permissions.  CMake provides no interface to remove an entry referencing an existing build tree once export(PACKAGE) has been invoked. However, if the project removes its package configuration file from the build tree then the entry referencing the location will be considered stale.", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Auto Deploy using Continuous Integration in TFS 2012", "id": 1243, "answers": [{"answer_id": 1236, "document_id": 815, "question_id": 1243, "text": "Turns out the problem was files were missing on the Build Server as desibried in the Answer to this question:\nhttps://stackoverflow.com/questions/2607428/msbuild-target-package-not-found/6413635#6413635", "answer_start": 367, "answer_category": null}], "is_impossible": false}], "context": "I have setup continuous integration for a WCF project and want to use the MSBuild Arguments to automatically deploy the application to a remote server but it is not deploying.\nWhen running a new Build all the Tests pass and all the projects build but the website is not being deployed. Also, I am getting no errors back from the build to say anything has gone wrong.\nTurns out the problem was files were missing on the Build Server as desibried in the Answer to this question:\nhttps://stackoverflow.com/questions/2607428/msbuild-target-package-not-found/6413635#6413635\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "The simplest way to deploy to production with builds", "id": 1119, "answers": [{"answer_id": 1112, "document_id": 696, "question_id": 1119, "text": "You can use the buildnumber plugin if you want the buildnumber in some document, e.g. CHANGES.txt", "answer_start": 291, "answer_category": null}], "is_impossible": false}], "context": "I must confess I'm new to maven world after years of living in the world of monstrous debuild/ant/makefile chimeric constructions. I just don't have yet that very feeling which helps seasoned developer to choose right decisions, and it's looks like there are plenty different ways in maven.\nYou can use the buildnumber plugin if you want the buildnumber in some document, e.g. CHANGES.txt\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to deploy heroku app with secret yaml configuration file without committing the file?", "id": 500, "answers": [{"answer_id": 503, "document_id": 227, "question_id": 500, "text": "Heroku have some guidance on this -\nhttp://devcenter.heroku.com/articles/config-vars", "answer_start": 347, "answer_category": null}], "is_impossible": false}], "context": "In other rails projects, I'd have a local database.yml and in source code repository only commit the database.sample file. When deploying, a capistrano script that would symlink a shared version of database.yml to all the releases.\nWhen deploying to heroku, git is used and they seem to override database.yml altogether and do something internal.\nHeroku have some guidance on this -\nhttp://devcenter.heroku.com/articles/config-vars\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "installing opencv for python on lion no module present in dist packages ", "id": 2012, "answers": [{"answer_id": 1998, "document_id": 1606, "question_id": 2012, "text": "On my Lion (10.8), I have java installed, and openCV complaints about cannot build the unit test and stop at 92%. It turned out that I have to manually create the build folder and put the junit jar into the lib folder in order to let opencv compile all the test case. After that, everything is ok", "answer_start": 1847, "answer_category": null}], "is_impossible": false}, {"question": "installing opencv for python in ubuntu 12 04 no module present in dist packages", "id": 2013, "answers": [{"answer_id": 1999, "document_id": 1606, "question_id": 2013, "text": "On my ubuntu (12.04, fresh installation with build-essential and all packages are up-to-date), eveything is compiled fine, except that \"No module named cv\" and there is not cv.so in the dist-packages and site-packages. Searching around and then finally, it turned out that I have to have python-dev and python-numpy", "answer_start": 2144, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nFollowing the steps given here, i have completed the installation process, however Python (IDLE) is giving me an ImportError. For which the guide suggests :\n\n\n  Python may return to you an error like \"No module named cv\" The trouble is that the python module is installed in /usr/local/lib/python2.6/site-packages. But, on Debian and on Ubuntu, Python only looks in /usr/local/lib/python2.6/dist-packages\n  \n  You can fix it using three ways (Use only one of those, the first is the best):\n  \n  \n  move the cv.so file from the site-packages to the dist-packages:\n  \n  sudo mv /usr/local/lib/python2.6/site-packages/cv.so /usr/local/lib/python2.6/dist-packages/cv.so\n  \n\n\nHowever, both the site-packages as well as dist-packages, for both 2.7 and 3.2 are empty. \n\nWhat went wrong and how do i solve it ? \n    \n\nI solved the problem by installing all the packages and dependencies again using the Software Center. OpenCV Python bindings are available for OpenCV 2.3 and Python 2.7 directly from the software center which I used the second time. \n\nOpenCv python bindings for 3.2 are not available for 12.04. They are being developed for Raring (Ubuntu 13.04) only. Hence, the solution is to use either backports, or to use OpenCv on Python 2.7 \n\nBackports also have a problem as they are available only for 32 bit OS systems and not 64 bit. \n\nSo, the only safe and stable way to run OpenCV Python on Ubuntu seems to be using OpenCV 2.3 on Python 2.7\n    \n\nI also get the empty dist-packages folder with OpenCV2.4.4 and Ubuntu 12.04. It turns out that I need to install python-dev and python-numpy in order to make the cv.so compiled into the dist-packages. \n\nsudo apt-get install python-dev python-numpy\n\n\nAfter that, using cmake to build OpenCV again and everything will be fine \n\nUPDATE\nit depends on your system pretty much.\n\n\nOn my Lion (10.8), I have java installed, and openCV complaints about cannot build the unit test and stop at 92%. It turned out that I have to manually create the build folder and put the junit jar into the lib folder in order to let opencv compile all the test case. After that, everything is ok\nOn my ubuntu (12.04, fresh installation with build-essential and all packages are up-to-date), eveything is compiled fine, except that \"No module named cv\" and there is not cv.so in the dist-packages and site-packages. Searching around and then finally, it turned out that I have to have python-dev and python-numpy\n\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Clear the .NET-downloaded application cache without Mage?", "id": 454, "answers": [{"answer_id": 463, "document_id": 187, "question_id": 454, "text": "Visual Studio isn't needed, just Mage. It comes with the Windows SDK for people that haven't gotten it via Visual Studio or the Framework SDK.", "answer_start": 193, "answer_category": null}], "is_impossible": false}], "context": "I have a .NET application that I distribute using ClickOnce and I make available online only. How do I clear the download cache from a users' machine that doesn't have Visual Studio installed?\nVisual Studio isn't needed, just Mage. It comes with the Windows SDK for people that haven't gotten it via Visual Studio or the Framework SDK.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to install Haskell on Mac OS", "id": 1178, "answers": [{"answer_id": 1171, "document_id": 755, "question_id": 1178, "text": "brew install ghc cabal-install", "answer_start": 61, "answer_category": null}], "is_impossible": false}], "context": "And I get the same messages.\nHow can I fix them ?\nPlease do:\nbrew install ghc cabal-install\nIf you prefer homebrew.\nNote: (Edited to add) Homebrew may have an older (or newer) version of ghc than the Haskell Platform download for OSX, and there might be other differences. You can check the release notes here for the version included in Haskell Platform, as well as other information of interest. brew info ghc cabal-install will give you up to date information about Homebrew's versions.\nNote: The last time I did this, I just installed haskell-stack instead, as described in Filippo's answer.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Deploy to iPhone without running", "id": 503, "answers": [{"answer_id": 505, "document_id": 229, "question_id": 503, "text": "Edit your launch Scheme, open the section titled \"Run myApp.app\" and toggle ON the option to \"Wait for myApp.app to launch\".", "answer_start": 110, "answer_category": null}], "is_impossible": false}], "context": "How do I deploy my app down to the iPhone (for testing purposes) from XCode without actually running the app? Edit your launch Scheme, open the section titled \"Run myApp.app\" and toggle ON the option to \"Wait for myApp.app to launch\".It will then just install, and not run the app. If you do run it yourself, the debugger should attach. This is useful for when you want to test how your applications works when launched via a URL, amongst other things.", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to install latest version of git on CentOS 8.x/7.x/6.x", "id": 1584, "answers": [{"answer_id": 1573, "document_id": 1161, "question_id": 1584, "text": "You can use WANDisco's CentOS repository to install Git 2.x: for CentOS 6, for CentOS 7", "answer_start": 232, "answer_category": null}], "is_impossible": false}], "context": "I used the usual:\nyum install git\nIt did not install the latest version of git on my CentOS 6. How can I update to the latest version of git for CentOS 6? The solution can be applicable to newer versions of CentOS such as CentOS 7.\nYou can use WANDisco's CentOS repository to install Git 2.x: for CentOS 6, for CentOS 7\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Deploying CherryPy (daemon)", "id": 505, "answers": [{"answer_id": 507, "document_id": 231, "question_id": 505, "text": "The cherryd is found in your:\nvirtualenv/lib/python2.7/site-packages/cherrypy/cherryd\nor in: https://bitbucket.org/cherrypy/cherrypy/src/default/cherrypy/cherryd", "answer_start": 233, "answer_category": null}], "is_impossible": false}], "context": "I've followed the basic CherryPy tutorial (http://www.cherrypy.org/wiki/CherryPyTutorial). One thing not discussed is deployment.\nHow can I launch a CherryPy app as a daemon and \"forget about it\"? What happens if the server reboots?\nThe cherryd is found in your:\nvirtualenv/lib/python2.7/site-packages/cherrypy/cherryd\nor in: https://bitbucket.org/cherrypy/cherrypy/src/default/cherrypy/cherryd\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to install PyPy in anaconda", "id": 1762, "answers": [{"answer_id": 1749, "document_id": 1334, "question_id": 1762, "text": "conda-forge now supports PyPy3.6 as the python interpreter in a conda environment (see the official blog post):\nconda config --set channel_priority strict\nconda create -n pypy pypy\nconda activate pypy", "answer_start": 448, "answer_category": null}], "is_impossible": false}], "context": "I have a Linux-64 bit machine. How do I install PyPy in my anaconda environment. I tried conda install pypy. It says there are no available packages. pip install pypy also returns a similar message. Is it that the only way to install PyPy is by downloading the tar.gz file from the website? Once installed, how do I configure my Anaconda environment in such a way so as to be able to switch from PyPy to other Python implementations when required?\nconda-forge now supports PyPy3.6 as the python interpreter in a conda environment (see the official blog post):\nconda config --set channel_priority strict\nconda create -n pypy pypy\nconda activate pypy\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Netbeans Maven Project Not adding Main Class to Manifest", "id": 1102, "answers": [{"answer_id": 1094, "document_id": 679, "question_id": 1102, "text": "You can add it into project's pom file, inside <project> tag:\n\n<build>\n    <plugins>\n        <plugin>\n          <groupId>org.apache.maven.plugins</groupId>\n          <artifactId>maven-jar-plugin</artifactId>\n          <version>2.4</version>\n          <configuration>\n              <archive>\n                  <manifest>\n                      <mainClass>your.main.class</mainClass>\n                  </manifest>\n              </archive>\n          </configuration>\n      </plugin>\n  </plugins>\n</build>", "answer_start": 507, "answer_category": null}], "is_impossible": false}], "context": "I am having a similar problem to this question. I have tried all the suggestions listed and am still at a loss. My issue is that I am trying to build a maven project and distribute it to other machines, but the jar files are not being populated with a correct Manifest. Each time I build and run I get the following error: no main manifest attribute, in myjar.jar. Is there some sort of configuration file I need to edit? I just don't know what is going on. I have attempted this fix also, but to no avail.\nYou can add it into project's pom file, inside <project> tag:\n\n<build>\n    <plugins>\n        <plugin>\n          <groupId>org.apache.maven.plugins</groupId>\n          <artifactId>maven-jar-plugin</artifactId>\n          <version>2.4</version>\n          <configuration>\n              <archive>\n                  <manifest>\n                      <mainClass>your.main.class</mainClass>\n                  </manifest>\n              </archive>\n          </configuration>\n      </plugin>\n  </plugins>\n</build>\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Clean Windows Azure Website", "id": 487, "answers": [{"answer_id": 492, "document_id": 216, "question_id": 487, "text": " When you right click and click Publish Web on the left hand side there should be a settings tab. Click this. Then expand the option under File Publish Options and check the Box for Remove addtional files at destination. (This will wipe out whats already there)", "answer_start": 299, "answer_category": null}], "is_impossible": false}], "context": "My ASP.NET MVC project pushed to a Azure website with an extra DLL which is invalid. This is keeping the project from running correctly. If a new website is created it works perfectly. Cleaning the project locally and redeploying does not fix the problem. Is there a way to perform a \"remote clean?\" When you right click and click Publish Web on the left hand side there should be a settings tab. Click this. Then expand the option under File Publish Options and check the Box for Remove addtional files at destination. (This will wipe out whats already there)", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to automatically start your service after install?", "id": 736, "answers": [{"answer_id": 737, "document_id": 424, "question_id": 736, "text": "Define serviceInstaller1 (type ServiceInstaller) in the Installer class designer and also set its ServiceName property in the designer.", "answer_start": 325, "answer_category": null}], "is_impossible": false}], "context": "How do you automatically start a service after running an install from a Visual Studio Setup Project?\nI just figured this one out and thought I would share the answer for the general good. Answer to follow. I am open to other and better ways of doing this.\nThis approach uses the Installer class and the least amount of code\uff1aDefine serviceInstaller1 (type ServiceInstaller) in the Installer class designer and also set its ServiceName property in the designer. I have to put this code in the Committed event.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How do I set register a variable to persist between plays in ansible?", "id": 1690, "answers": [{"answer_id": 1678, "document_id": 1263, "question_id": 1690, "text": "You need to keep in mind that in Ansible, the variable app_git_sha1 assigned to the host localhost is distinct from the variable app_git_sha1 assigned to the host main or any other host. If you want to access one hosts facts/variables from another host then you need to explicitly reference it via the hostvars variable.", "answer_start": 915, "answer_category": null}], "is_impossible": false}], "context": "I have an ansible playbook, where I'd like a variable I register on one machine to be available on another.\nIn my case, I'd like to run a command on localhost, in this case git rev-parse --abbrev-ref HEAD, so I can make a note of the current git branch, and sha1, and register this output, so I can refer to it later when working any machine in the main group, in the second play.\nHowever, it's not clear to me how I register a variable on localhost, so I can access it from main. When I try to access the variable in the second play I get this message:\n    TASK: [debug msg={{ app_git_sha1.stdout }}] ***********************************\n    fatal: [main] => One or more undefined variables: 'app_git_sha1' is undefined\nHere's the play I'm using. Is there anything obvious I should be doing?\nThe problem you're running into is that you're trying to reference facts/variables of one host from those of another host. You need to keep in mind that in Ansible, the variable app_git_sha1 assigned to the host localhost is distinct from the variable app_git_sha1 assigned to the host main or any other host. If you want to access one hosts facts/variables from another host then you need to explicitly reference it via the hostvars variable. There's a bit more of a discussion on this in this question.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Configure Tomcat to use properties file to load DB connection information", "id": 489, "answers": [{"answer_id": 494, "document_id": 218, "question_id": 489, "text": "We often distribute webapps by providing a WAR, and a Context XML file, which gets placed into your tomcat/conf/Catalina/localhost directory, and can load the webapp from any path. There is a reference document here: http://tomcat.apache.org/tomcat-6.0-doc/config/context.html.", "answer_start": 357, "answer_category": null}], "is_impossible": false}], "context": "What are the accepted practices for creating a Tomcat deployment that reads configuration parameters from a properties file?\nIt would be nice to be able to deliver a WAR file and specify that the client need only create or edit a properties file in a specific directory. Is this a somewhat regular way of doing things? Is there a better approach than this?\nWe often distribute webapps by providing a WAR, and a Context XML file, which gets placed into your tomcat/conf/Catalina/localhost directory, and can load the webapp from any path. There is a reference document here: http://tomcat.apache.org/tomcat-6.0-doc/config/context.html.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "installing tesseract ocr on centos 6", "id": 1464, "answers": [{"answer_id": 1453, "document_id": 1035, "question_id": 1464, "text": "Configure, compile, install Leptonica:\n\n$ tar xzvf leptonica-1.76.0.tar.gz\n$ cd leptonica-1.76.0\n$ ./configure &amp; make &amp; sudo make install\n\n\nConfigure, compile, install Tesseract:\n\n$ tar xzf tesseract-ocr-3.02.02.tar.gz\n$ cd tesseract-ocr\n$ ./autogen.sh &amp; ./configure &amp; make &amp; sudo make ins", "answer_start": 2222, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI'm trying to install Tesseract-OCR on my server however when I install all what I believe to be the correct repos. When I try to install it the package is not found\n\nI tried adding rpmforge but to no avail. Any ideas from somebody that has done before or is familiar with adding and searching through repos?\n    \n\nI used these instructions which worked correctly in Centos\n\nInstall Tesseract OCR libs from sources in Centos\n\nDownload Leptonica and Teseract sources:\n\n$ wget http://www.leptonica.org/source/leptonica-1.69.tar.gz\n$ wget https://tesseract-ocr.googlecode.com/files/tesseract-ocr-3.02.02.tar.gz\n\n\nConfigure, compile, install libs:   \n\n $ tar xzvf leptonica-1.69.tar.gz      \n $ cd leptonica-1.69      \n $ ./configure\n $ make\n $ sudo make install\n\n $ tar xzf tesseract-ocr-3.02.02.tar.gz\n $ cd tesseract-3.01\n $ ./autogen.sh\n $ ./configure\n $ make\n $ sudo make install\n $ sudo ldconfig\n\n\nDownload languages (english) and copy to tessdata folder:     \n\n$ wget http://tesseract-ocr.googlecode.com/files/tesseract-ocr-3.02.eng.tar.gz       \n$ tar xzf tesseract-ocr-3.02.eng.tar.gz       \n$ sudo cp tesseract-ocr/tessdata/* /usr/local/share/tessdata\n\n\nand enjoy it ;)\n    \n\nI recommend to try installing from rpm here: http://pkgs.org/download/tesseract\nThere are also several dependencies: libpng-devel, libjpeg-devel, libtiff-devel, zlib and leptonica.\nLast 2 can also be found on RPM site\n    \n\nI have written a bash script to install Tesseract 3.05 on Centos 7. This fetches and installs all dependencies, and also installs language files for English, Hindi, Bengali and Thai.\n\nCode available on GitHub\n\nhttps://github.com/EisenVault/install-tesseract-redhat-centos\n\nHope this helps.\n    \n\nThis worked for me :\n\n/usr/bin/yum --enablerepo epel-testing install tesseract.x86_64 tesseract-langpack-fra.noarch\n\n\ntesseract is not in the epel repository but in the epel-testing repo witch is not activated by default.\n    \n\nInstall Tesseract OCR libs from sources (UPDATED as on 14th July 2018)\n\nDownload Leptonica and Teseract sources:\n\n$ wget http://www.leptonica.com/source/leptonica-1.76.0.tar.gz\n\n$ wget https://sourceforge.net/projects/tesseract-ocr-alt/files/tesseract-ocr-3.02.02.tar.gz\n\nConfigure, compile, install Leptonica:\n\n$ tar xzvf leptonica-1.76.0.tar.gz\n$ cd leptonica-1.76.0\n$ ./configure &amp; make &amp; sudo make install\n\n\nConfigure, compile, install Tesseract:\n\n$ tar xzf tesseract-ocr-3.02.02.tar.gz\n$ cd tesseract-ocr\n$ ./autogen.sh &amp; ./configure &amp; make &amp; sudo make install &amp; sudo ldconfig\n\n\nDownload language file:\n\nI am downloading english language file(eng.traineddata) here. You can see complete list of language files here and download as per your need.\nhttps://github.com/tesseract-ocr/tesseract/wiki/Data-Files#data-files-for-version-302\n\nDownload languages (english) and copy to tessdata folder:\n\n$ wget https://sourceforge.net/projects/tesseract-ocr-alt/files/tesseract-ocr-3.02.eng.tar.gz\n$ tar xzf tesseract-ocr-3.02.eng.tar.gz\n$ sudo cp tesseract-ocr/tessdata/* /usr/local/share/tessdata\n\n\nNow your Tesseract OCR is installed and ready to use!\nExample:\n\n$tesseract /path/to/input/test.jpg /path/to/output/abc.txt -l eng\n\n\nEnjoy!!!\n    \n\nenter image description here\n\nyum install --nogpgcheck tesseract\n\nafter installation to test enter the following command:\ntesseract --version \n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Capistrano deployment problems", "id": 1639, "answers": [{"answer_id": 1627, "document_id": 1213, "question_id": 1639, "text": "It occurs that config/deploy.rb has lock '3.1.0'.\nIt was enough to change it to '3.2.0' and now it's working.", "answer_start": 0, "answer_category": null}], "is_impossible": false}], "context": "It occurs that config/deploy.rb has lock '3.1.0'.\nIt was enough to change it to '3.2.0' and now it's working.\nWhen I type\ncap production deploy\nI get\nCapfile locked at 3.1.0, but 3.2.0 is loaded\nWhen I uninstall capistrano 3.2.0 I get\nCould not find capistrano-3.2.0 in any of the sources\nRun `bundle install` to install missing gems.\nMe gemfile has\ngem 'capistrano', '~> 3.1'\ngem 'capistrano-rails', '~> 1.1'\nAnd Capfile\nrequire 'capistrano/setup'\nrequire 'capistrano/deploy'\nrequire 'capistrano/bundler'\nrequire 'capistrano/rails/assets'\nWhat to do in that case?\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Raising minimum iOS Deployment Target Version for App Update", "id": 567, "answers": [{"answer_id": 570, "document_id": 292, "question_id": 567, "text": "From my experience those updates just won't show up as available.", "answer_start": 222, "answer_category": null}], "is_impossible": false}], "context": "Let's say we have an application with a deployment target set to 3.0 and we want to raise the deployment target to 3.2. Normally, the App Store won't let the App be installed on devices with an IOS version less then this.\nFrom my experience those updates just won't show up as available.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "error installing coffeescript on mac 10 7 2", "id": 1482, "answers": [{"answer_id": 1471, "document_id": 1058, "question_id": 1482, "text": "dylan-hermans-macbook:~ sudo npm install -g coffee-script\n", "answer_start": 2761, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nNode and npm are both installed and up to date but keep getting this error when trying to install coffeescript. I am still new to programming so any advice would be greatly appreciated.\n\ntest-macbook:~ Test$ npm -v\n1.1.0-3\ntest-macbook:~ Test$ node -v\nv0.6.8\ntest-macbook:~ Test$ npm install -g coffee-script\nnpm http GET https://registry.npmjs.org/coffee-script\nnpm http 304 https://registry.npmjs.org/coffee-script\nnpm ERR! Could not create /usr/local/lib/node_modules/___coffee-script.npm\nnpm ERR! error installing coffee-script@1.2.0\n\nnpm ERR! Error: EACCES, permission denied '/usr/local/lib/node_modules/___coffee-script.npm'\nnpm ERR! \nnpm ERR! Please try running this command again as root/Administrator.\nnpm ERR! \nnpm ERR! System Darwin 11.2.0\nnpm ERR! command \"node\" \"/usr/local/bin/npm\" \"install\" \"-g\" \"coffee-script\"\nnpm ERR! cwd /Users/Dylan\nnpm ERR! node -v v0.6.8\nnpm ERR! npm -v 1.1.0-3\nnpm ERR! path /usr/local/lib/node_modules/___coffee-script.npm\nnpm ERR! code EACCES\nnpm ERR! message EACCES, permission denied '/usr/local/lib/node_modules/___coffee-script.npm'\nnpm ERR! errno {}\nnpm ERR! \nnpm ERR! Additional logging details can be found in:\nnpm ERR!     /Users/Dylan/npm-debug.log\nnpm not ok\n\n    \n\nFollowing the advice of nmp author Isaac Z. Schlueter:\n\n\n  I strongly encourage you not to do package management with sudo!\n\n\nInstead of sudo npm install ... you could instead change permissions on your /usr/local directory:\n\nsudo chown -R $USER /usr/local\n\n\nAfter doing this once, you should be able to npm install ... (without sudo).\n    \n\nI realize this is an older thread, but I just ran across this and wanted to update the last answer.\n\nChanging the owner of the entire /usr/local directory is a horrible answer to this question.  There's no reason at all that you should do that.\n\nIf you look at the error message from npm, it's being denied write permissions on /usr/local/lib/node_modules/  I know that after installing node and npm on OS X Mavericks, its default owner is a non-existent user.\n\n0 drwxr-xr-x   3 24561        wheel     102 Jan 23 14:17 node_modules\n\n\nIf you're just running node/npm on your local development machine, just change the owner of the node_modules folder to your user:\n\nsudo chown -R yourusername /usr/local/lib/node_modules\n\n\nThen you can install modules with npm without sudo and without changing the owner of /usr/lib from root, as it should be.\n    \n\nThe error message is fairly clear:\n\nnpm ERR! Error: EACCES, permission denied '/usr/local/lib/node_modules/___coffee-script.npm'\nnpm ERR! \nnpm ERR! Please try running this command again as root/Administrator.\n\n\nYou can't install it in /usr/local/lib/node_modules because you don't have the necessary permissions. Try using sudo:\n\ndylan-hermans-macbook:~ sudo npm install -g coffee-script\n\n\n\n\nThe npm author recommends not using sudo because packages can run arbitrary commands so sudo npm install is dangerous. He suggests switching the ownership of /usr/local to your user. I think that's horribly advice that just gives you a false sense of security: if a package can run arbitrary commands then it can mess with your home directory (including all your personal data, executables, config and startup files, ...) regardless of sudo or who owns /usr/local so not using sudo really doesn't do much for you. If you don't trust a package then don't install it; if you don't trust a package then how can you even use it? The /usr/local tree is still a system directory tree and OSX is still a multi-user operating system.\n\nIMO a much better solution is twofold:\n\n\nDon't install or use any packages that you don't trust. If you install it then you're trusting that code to be you (unless you're always going to run it in a jail of some sort but if you're going to those lengths you're probably better off writing the code yourself).\nLeave sudo and /usr/local alone and install it all inside your home directory. You'll be subject to most of the same dangers as using sudo or changing the /usr/local ownership but at least you won't be picking up bad habits.\n\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Print a list of all installed node.js modules", "id": 1884, "answers": [{"answer_id": 1870, "document_id": 1455, "question_id": 1884, "text": "If you are only interested in the packages installed globally without the full TREE then:\nnpm -g ls --depth=0\nor locally (omit -g) :\nnpm ls --depth=0", "answer_start": 139, "answer_category": null}], "is_impossible": false}], "context": "In a node.js script that I'm working on, I want to print all node.js modules (installed using npm) to the command line. How can I do this?\nIf you are only interested in the packages installed globally without the full TREE then:\nnpm -g ls --depth=0\nor locally (omit -g) :\nnpm ls --depth=0\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to deploy an ASP.NET Application with zero downtime", "id": 1667, "answers": [{"answer_id": 1654, "document_id": 1240, "question_id": 1667, "text": "1.\tTurn all traffic on Server 2\n2.\tDeploy on Server 1\n3.\tTest Server 1\n4.\tTurn all traffic on Server 1\n5.\tDeploy on Server 2\n6.\tTest Server 2\n7.\tTurn traffic on both servers", "answer_start": 280, "answer_category": null}], "is_impossible": false}], "context": "This process is all scripted, and happens quite quickly, but there can still be a 10-20 second downtime when the old files are being deleted, and the new files being deployed.\nAny suggestions on a 0 second downtime method?\nYou need 2 servers and a load balancer. Here's in steps:\n1.\tTurn all traffic on Server 2\n2.\tDeploy on Server 1\n3.\tTest Server 1\n4.\tTurn all traffic on Server 1\n5.\tDeploy on Server 2\n6.\tTest Server 2\n7.\tTurn traffic on both servers\nThing is, even in this case you will still have application restarts and loss of sessions if you are using \"sticky sessions\". If you have database sessions or a state server, then everything should be fine.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How do I set an ADO.NET Entity Framework connection string via the Windows Azure (Preview) Managemen", "id": 576, "answers": [{"answer_id": 582, "document_id": 301, "question_id": 576, "text": "You should select \"Custom\" instead of \"SQL Azure\" from the \"SQL Azure / SQL Server / MySQL / Custom\" selector for the Entity Framework connection string, even though the database does run on SQL Azure.", "answer_start": 221, "answer_category": null}], "is_impossible": false}], "context": "I currently set the connection string for my ADO.NET Entity Framework connection via Web.Release.Config, but I want to set it via the Management Portal, but no matter what I use, I always end up with the following error:\nYou should select \"Custom\" instead of \"SQL Azure\" from the \"SQL Azure / SQL Server / MySQL / Custom\" selector for the Entity Framework connection string, even though the database does run on SQL Azure.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to install J Base system including Qt IDE?", "id": 1000, "answers": [{"answer_id": 995, "document_id": 611, "question_id": 1000, "text": "   sudo ijconsole\nThen enter one of:\n\n   install 'all'           NB. install all addons\n   install 'qtide'         NB. install only the Qt IDE\n   install 'slim'          NB. install only the Qt IDE (slim version)", "answer_start": 1744, "answer_category": null}], "is_impossible": false}], "context": "System/Installation/Linux\n< System\u200e | Installation\nThese instructions install a full system (base plus all addons) using a Debian package for Linux.\n\nThe 64-bit Linux system is AVX only. For 64-bit Linux non-AVX, use the System/Installation/Zips install.\n\n\nContents\n1\tThe Name of the J console binary\n2\tInstall the J Base System\n3\t(optional) Install addons, including Qt IDE\n4\t(optional) Change Default Directory Names\n5\t(optional) Qt IDE Setup\nThe Name of the J console binary\nBy default, the J console binary is named jconsole. Unfortunately this is also the name of the Java console program. Usually this is not a problem, but if there is a conflict, then either install J outside the path, or rename the binary to ijconsole and make corresponding changes to scripts that call it. The Debian installer described below does install the binary as ijconsole.\n\nInstall the J Base System\nDownload the appropriate file:\n\ndeb for 32-bit Linux\ndeb for 64-bit Linux\ndeb for 32-bit Raspberry Pi\ndeb for 64-bit Raspberry Pi\nand MUST install with sudo dpkg -i {filename}, for example:\n\n  sudo dpkg -i j807_amd64.deb\nThis uses the following directories:\n\n  /usr/bin                      ijconsole binary\n  /usr/lib/x86_64-linux-gnu/    j engine (libj.so.8.07)\n  /usr/share/j/8.07             system files\n  /etc/j/8.07                   profile scripts\nJ will NOT work if deb is extracted and installed in other directories. Use the System/Installation/Zips to install to any location you wanted.\n\n(optional) Install addons, including Qt IDE\nTo install any addons, first make sure you are connected to the Internet, then start jconsole as described above. Note that if you are installing to a shared folder, then you need to start jconsole as root. eg.\n\n   sudo ijconsole\nThen enter one of:\n\n   install 'all'           NB. install all addons\n   install 'qtide'         NB. install only the Qt IDE\n   install 'slim'          NB. install only the Qt IDE (slim version)\nIt will echo messages\n\nUpdating server catalog...\nInstalling 1 package\nDownloading base library...\nInstalling base library...\nDone.\nInstalling JQt binaries...\ncd /usr/bin && tar --no-same-owner --no-same-permissions -xzf ...\nFinished install of JQt binaries.\nExit and restart J using jqt\nIf you later want to customize the addons, do one of the following:\n\nin Jqt, select Tools|Package Manager from the menu, select the addons to be installed (or Select All to get them all), and press Install.\nin the J console, use the Package Manager console interface.\n(optional) Change Default Directory Names\nJ needs directories to store temporary files, configuration files and so on. By default these are in a folder under the User's home directory. To change the folders J uses, follow the instructions here.\n\n(optional) Qt IDE Setup\nSee Qt IDE/Install for more information on setting up the Qt IDE.", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to make rpm auto install dependencies", "id": 1577, "answers": [{"answer_id": 1566, "document_id": 1154, "question_id": 1577, "text": "Create a (local) repository and use yum to have it resolve the dependencies for you.\nThe CentOS wiki has a nice page providing a how-to on this. CentOS wiki http://wiki.centos.org/HowTos/CreateLocalRepos", "answer_start": 221, "answer_category": null}], "is_impossible": false}], "context": "I have built two RPM packages.\nThe installation of proj1 fails due to a missing dependency.\nI did try the --aid option with rpm -i as described here but it didn't work for me.\nIs there any other way?\nThanks for any help.\nCreate a (local) repository and use yum to have it resolve the dependencies for you.\nThe CentOS wiki has a nice page providing a how-to on this. CentOS wiki http://wiki.centos.org/HowTos/CreateLocalRepos\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Register file extensions / mime types in Linux", "id": 699, "answers": [{"answer_id": 703, "document_id": 390, "question_id": 699, "text": "You need to register a new file type and then create a desktop entry for your application", "answer_start": 398, "answer_category": null}], "is_impossible": false}], "context": "I'm developing a Linux application that has its own file format. I want my app to open when you double-click on those files.\nHow can I register a file extension and associate it with my application on Linux? I'm looking for a way that is standard (works with GNOME and KDE based systems) and can be done automatic when my program is installed or run for the first time.There are two parts to this. You need to register a new file type and then create a desktop entry for your application. The desktop entry associates your application with your new mime type.\nI thought that both Gnome and KDE (maybe only 4+?) used the freedesktop shared mime info spec, but I may well be wrong.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Capistrano: linked file database.yml does not exist on my.server.ipadress", "id": 1259, "answers": [{"answer_id": 1251, "document_id": 830, "question_id": 1259, "text": "Just create /home/deploy/myrailsapp/shared/config/database.yml file manually and adjust it.\nCapistrano doesn't create (or manage) configuration file out of the box. So, you should do it manually or automate use own Capistrano scripts, Puppet, Chef, Ansible tools.", "answer_start": 100, "answer_category": null}], "is_impossible": false}], "context": "I tried this tut https://www.gorails.com/deploy/ubuntu/14.04, this is my first try with capistrano.\nJust create /home/deploy/myrailsapp/shared/config/database.yml file manually and adjust it.\nCapistrano doesn't create (or manage) configuration file out of the box. So, you should do it manually or automate use own Capistrano scripts, Puppet, Chef, Ansible tools.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Best way to deploy Visual Studio application that can run without installing", "id": 1684, "answers": [{"answer_id": 1671, "document_id": 1257, "question_id": 1684, "text": "1.\t\"Publish\" the application\n2.\tBut instead of using the produced installer, find the produced files \n3.\tZip that folder and provide it to the users.", "answer_start": 667, "answer_category": null}], "is_impossible": false}], "context": "I wrote a fairly simple application with C#/.NET and can't figure out a good way to publish it. It's a sort of a \"tool\" that users would only run once, or run every few months. Because of this, I'm hoping that there is a way I could deploy it where it wouldn't need installing to run (it could just be run by double-clicking an EXE file straight after downloading).\nHowever, it still needs (somehow) to include the correct version of .NET, libraries, etc. so it will run correctly. I know this is included when using ClickOnce, but that still installs the application onto the user's computer.\nIs there a way this can be done?\nIt is possible and is deceptively easy:\n1.\t\"Publish\" the application\n2.\tBut instead of using the produced installer, find the produced files \n3.\tZip that folder and provide it to the users.\nAn added advantage is that, as a ClickOnce application, it does not require administrative privileges to run (if your application follows the normal guidelines for which folders to use for application data, etc.).\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Why is php5isapi.dll missing after installing PHP for Windows?", "id": 1812, "answers": [{"answer_id": 1797, "document_id": 1383, "question_id": 1812, "text": "You should try to download the binaries (rather than the installer). I found php5isapi.dll in those zips, but the .exe/.msi doesn't seem to install that dll.", "answer_start": 335, "answer_category": null}], "is_impossible": false}], "context": "It's been a while since I've installed PHP for Windows, but every guide I've seen online tells me to set IIS to recognize .PHP files with php5isapi.dll. However, I can't seem to find php5isapi.dll anywhere after installing PHP 5.3.0 and PHP 5.2.10.\nIf I recall correctly it should be in C:\\InstallDir\nAm I missing something important?\nYou should try to download the binaries (rather than the installer). I found php5isapi.dll in those zips, but the .exe/.msi doesn't seem to install that dll.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to enable @rpath in CMake 2.8.12 and newer?", "id": 3, "answers": [{"answer_id": 3, "document_id": 16, "question_id": 3, "text": "This was enabled by setting the target property MACOSX_RPATH. ", "answer_start": 117, "answer_category": null}], "is_impossible": false}, {"question": "How to use old behavior when policy is not set in  CMake version 3.0?", "id": 4, "answers": [{"answer_id": 4, "document_id": 16, "question_id": 4, "text": " Use the cmake_policy() command to set it to OLD or NEW explicitly", "answer_start": 605, "answer_category": null}], "is_impossible": false}, {"question": "Does CMake 2.8.14 support @rpath in a target's install name?", "id": 5, "answers": [{"answer_id": 5, "document_id": 16, "question_id": 5, "text": "CMake 2.8.12 and newer has support for using @rpath in a target's install name.", "answer_start": 36, "answer_category": null}], "is_impossible": false}], "context": "MACOSX_RPATH is enabled by default.\nCMake 2.8.12 and newer has support for using @rpath in a target's install name.  This was enabled by setting the target property MACOSX_RPATH.  The @rpath in an install name is a more flexible and powerful mechanism than @executable_path or @loader_path CMake 3.0 and later prefer this property to be ON by default.  Projects wanting @rpath in a target's install name may remove any setting of the INSTALL_NAME_DIR and CMAKE_INSTALL_NAME_DIR This policy was introduced in CMake version 3.0.  CMake version 3.21.3 warns when the policy is not set and uses OLD behavior.  Use the cmake_policy() command to set it to OLD or NEW explicitly.The OLD behavior of a policy is and may be removed in a future version of CMake.\n\u00a9 Copyright 2000-2021 Kitware, Inc. and Contributors.", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to install xgboost in python on MacOS?", "id": 1768, "answers": [{"answer_id": 1754, "document_id": 1339, "question_id": 1768, "text": "If you installed Anaconda for Python 2.7, then you should have no troubles installing XGBoost with:\nconda install -c aterrel xgboost=0.4.0", "answer_start": 220, "answer_category": null}], "is_impossible": false}], "context": "I am a newbie and learning python. Can someone help me- how to install xgboost in python. Im using Mac 10.11. I read online and did the below mentioned step, but not able to decode what to do next:\npip install xgboost -\nIf you installed Anaconda for Python 2.7, then you should have no troubles installing XGBoost with:\nconda install -c aterrel xgboost=0.4.0\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "installing python2 6", "id": 1975, "answers": [{"answer_id": 1961, "document_id": 1560, "question_id": 1975, "text": "Check that you have these:\n\ndpkg -l libreadline-dev\ndpkg -l zlib1g-dev\ndpkg -l libssl-dev", "answer_start": 1211, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI'm trying to install python 2.6 on LinuxMint. I've came across a few issues.\n\nThe first thing I did was to download Python2.6.8 from the python website\n\nThen, I've extracted the files, ran a \n\n./configure --prefix=/opt/python-2.7.3 --with-threads --with-signal-module --with-pydebug\n\n\nI found this here  which I found here\n\nWhen I run the make command, I get these errors:\n\nFailed to find the necessary bits to build these modules:\n_bsddb             _curses            _curses_panel   \n_hashlib           _sqlite3           _ssl            \nbsddb185           bz2                dbm             \ndl                 gdbm               imageop         \nlinuxaudiodev      ossaudiodev        readline        \nsunaudiodev                                           \nTo find the necessary bits, look in setup.py in detect_modules() for the module's name.\n\n\nFailed to build these modules:\ncrypt              nis                                \n\n\nI have installed all the packages mentioned in the Cheater's page.\nI successfully installed python2.6 without all the options that I mentioned, but I can't get the bz2 module to work.\n    \n\nI think you're probably missing a few development packages. Check that you have these:\n\ndpkg -l libreadline-dev\ndpkg -l zlib1g-dev\ndpkg -l libssl-dev\n\n\nAlso, older versions of python don't look for files in the new locations where ubuntu (and I assume Mint by extension) installs them. You need to open up setup.py and look for the place where it defines the various library directories, eg this patch was needed to compile python2.4 on new ubuntus (and I see a fix like this is still necessary on 2.6):\n\ndiff -urNad python2.4-2.4.6-natty~/setup.py python2.4-2.4.6-natty/setup.py\n--- python2.4-2.4.6-natty~/setup.py 2011-07-27 14:42:03.000000000 +0200\n+++ python2.4-2.4.6-natty/setup.py  2011-07-27 15:03:35.000000000 +0200\n@@ -269,6 +269,7 @@\n         lib_dirs = self.compiler.library_dirs + [\n             '/lib64', '/usr/lib64',\n             '/lib', '/usr/lib',\n+            '/usr/lib/i386-linux-gnu', '/usr/lib/x86_64-linux-gnu',\n             ]\n         inc_dirs = self.compiler.include_dirs + ['/usr/include']\n         exts = []\n\n\nBut what I would personally do is grab the debian sources, and attempt to build the package from source.\n\nOr you could just use the dead snakes ppa\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "maven failed to install metadata project could not parse metadata maven metada", "id": 1937, "answers": [{"answer_id": 1924, "document_id": 1516, "question_id": 1937, "text": " \n\nGo to the the path where the maven-metadata-local.xml is. Delete the project folder along with the xml and build the proj", "answer_start": 629, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nGot this error when I was trying to build a project which I just downloaded from SVN.\n\n\n  Failed to execute goal\n  org.apache.maven.plugins:maven-install-plugin:2.4:install\n  (default-install) on project : Failed to install metadata\n  project:1.0-SNAPSHOT/maven-metadata.xml: Could not parse metadata\n  C:\\Users.m2\\project\\1.0-SNAPSHOT\\maven-metadata-local.xml: only\n  whitespace content allowed before start tag and not \\u0 (position:\n  START_DOCUMENT seen \\u0... @1:1) -&gt; [Help 1]\n\n    \n\nI just wanted to document this error on the internet. There was no much help or I didn't search properly. \n\nAnswer : \n\nGo to the the path where the maven-metadata-local.xml is. Delete the project folder along with the xml and build the project. \n\nIt worked for me!\n    \n\nDeleting the project folder and the maven-metadata-local.xml in the local repository solves the problem as stated by eugene-s's answer.\n\nEven though the problem in the question does not seem to be caused by concurrent usage of the local maven repository, this problem often occurs in a CI environment when the various jobs use the same maven local repository system.\n\nYou can either go with the solution (by user2818782) to always user a fresh repository, or you can start using https://github.com/takari/takari-local-repository which is a replacement to the local maven repository that is safe for concurrent access.\n    \n\nBy adding the \n\n\" -Dmaven.repo.local=$WORKSPACE/.repository\" \n\nargument to your mvn install command it forces Maven to use a local private repository, instead of the central one (~/.m2/repository).\n\nEvery Maven build will start with a fresh repository, thus avoiding such conflicts. This trick does however come with the caveat that it now needs to download EvERYTHING. So expect dramatically increased network traffic (not a problem if you use a local Nexus as a mirror), and slightly longer build times.\n    \n\nWe got the same problem in a project right after modifying the pom.xml to add some plugins and configurations. \nDeleting the project folders from maven local repository cache solved it. \n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Wix: How to set permissions for folder and all sub folders", "id": 1149, "answers": [{"answer_id": 1142, "document_id": 726, "question_id": 1149, "text": "First of all, I would recommend you using PermissionEx instead. It is a standard WiX extension and it has one really huge advantage over Permission", "answer_start": 123, "answer_category": null}], "is_impossible": false}], "context": "However I need the permissions to be applied to all subfolders as well. Is this possible with out listing all the folders? First of all, I would recommend you using PermissionEx instead. It is a standard WiX extension and it has one really huge advantage over Permission. Can you provide an example with PermissionEx? I'm using it inside a CreateFolder tag but I receive the error \"The required attribute SDDL is missing\". I've also the User and GenericAll attributes with an \"attribute is not declared\" error. Thanks.", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "\"Unable to update dependencies of the project\" after committing to Subversion", "id": 1680, "answers": [{"answer_id": 1667, "document_id": 1253, "question_id": 1680, "text": "Closing VS2010 and then re-opening it has always worked for me :)", "answer_start": 196, "answer_category": null}], "is_impossible": false}], "context": "I have a setup project in .NET. When I save the project and the other projects to subversion, the setup project no longer compiles. I get the error \"Unable to update dependencies of the project.\" Closing VS2010 and then re-opening it has always worked for me :). The fact that I googled this problem, came here and saw that I had already upvoted this answer told me that this was probably gonna work for me. And it did.", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "npm install: WARN on dependency", "id": 1189, "answers": [{"answer_id": 1182, "document_id": 765, "question_id": 1189, "text": " run npm with --loglevel=error", "answer_start": 110, "answer_category": null}], "is_impossible": false}], "context": "Are you running on Windows or Linux? If so, fsevents cannot be installed, as it is an OSX-only package.\nIf you run npm with --loglevel=error, you should no longer see this or any other warnings. As far as I know, there is no way to selectively suppress warnings in npm.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "What are the basic steps for deploying java and JavaFX applications?", "id": 431, "answers": [{"answer_id": 440, "document_id": 180, "question_id": 431, "text": "1.\tDecide how you want users to access and run your application.\nApplications can be deployed on a user's desktop, embedded in a web page, or launched from a browser.\n2.\tCreate the application package.\nThe application package consists of the JAR files needed to run your application, and the deployment descriptor or JNLP file for applications that are embedded in a web page or are launched from a browser. If your application is embedded in a web page or is launched from a browser, the JAR files must be signed with a valid signing certificate. A self-contained application package also includes the JRE needed.\n3.\tSet up the web page, if your application is embedded in a web page or is launched from a browser.\nThe web page needs either HTML elements or JavaScript code to run an application embedded in the page. JavaScript code is needed to launch an application from the browser. The Java packaging tools generate JavaScript code for both types of execution, which you can copy into your web page.\n4.\tCopy the package to the location from which you want users to access it.\nA web server is typically used for applications embedded in a web page or launched from a browser. Desktop applications can be delivered directly to users or made available through an app store.", "answer_start": 585, "answer_category": null}], "is_impossible": false}, {"question": "How to create java package using NetBeans IDE?", "id": 432, "answers": [{"answer_id": 441, "document_id": 180, "question_id": 432, "text": "Open Project Properties to specify preferred dimensions for your JavaFX application scene. Build the project with Clean and Build. The application package is generated in the dist folder. ", "answer_start": 1947, "answer_category": null}], "is_impossible": false}, {"question": "When deploying java app, I need to create web page, what\u2019s the difference between Java Plug-in and Java Web Start?", "id": 433, "answers": [{"answer_id": 442, "document_id": 180, "question_id": 433, "text": "The Java Plug-in is used to run an application embedded in the web page. Java Web Start is used to run an application that is launched from the browser.", "answer_start": 2617, "answer_category": null}], "is_impossible": false}, {"question": "How to Distribute my Application when deploying java application?", "id": 434, "answers": [{"answer_id": 443, "document_id": 180, "question_id": 434, "text": "If your application is embedded in a web page or launched from a browser, copy your package and the web page to the web server from which they will be loaded.\n\u2022\tIf your application is a desktop application, copy the application to the location from which users will download it. Self-contained applications provide installable packages and the required JRE, which makes it easier for users to install and run your application.", "answer_start": 3751, "answer_category": null}], "is_impossible": false}], "context": "Skip to Content\n \n\u2022\tJava Software\n\u2022\tJava SE Downloads\n\u2022\tJava SE 8 Documentation\nSearch\nJava Platform, Standard Edition Deployment Guide\nContents    Previous    Next\n________________________________________\n2 Getting Started\nThis topic describes the basics of deploying your Java and JavaFX applications.\nThis topic contains the following sections:\n\u2022\tBasic Steps\n\u2022\tChoose the Execution Environment\n\u2022\tCreate the Package\n\u2022\tCreate the Web Page\n\u2022\tDistribute Your Application\n\u2022\tBeyond the Basics\n2.1 Basic Steps\nHave an application ready to publish? Follow these steps for basic deployment:\n1.\tDecide how you want users to access and run your application.\nApplications can be deployed on a user's desktop, embedded in a web page, or launched from a browser.\n2.\tCreate the application package.\nThe application package consists of the JAR files needed to run your application, and the deployment descriptor or JNLP file for applications that are embedded in a web page or are launched from a browser. If your application is embedded in a web page or is launched from a browser, the JAR files must be signed with a valid signing certificate. A self-contained application package also includes the JRE needed.\n3.\tSet up the web page, if your application is embedded in a web page or is launched from a browser.\nThe web page needs either HTML elements or JavaScript code to run an application embedded in the page. JavaScript code is needed to launch an application from the browser. The Java packaging tools generate JavaScript code for both types of execution, which you can copy into your web page.\n4.\tCopy the package to the location from which you want users to access it.\nA web server is typically used for applications embedded in a web page or launched from a browser. Desktop applications can be delivered directly to users or made available through an app store.\n2.3.1.1 NetBeans IDE\nIf you use Netbeans IDE, then much of the work is done for you. Open Project Properties to specify preferred dimensions for your JavaFX application scene. Build the project with Clean and Build. The application package is generated in the dist folder. To test your application, open this folder in Windows Explorer and double-click the HTML, JNLP, or JAR file, depending on your execution environment.\nTo package a self-contained application, customize the build.xml script in the NetBeans IDE. For more information, see Section 7.3.2, \"Basic Build.\"\n2.4 Create the Web Page\nIf your application is embedded in a web page or launched from a browser, you need to set up the web page that provides users with access to your application. The Java Plug-in is used to run an application embedded in the web page. Java Web Start is used to run an application that is launched from the browser.\nYou can use an <applet> or <object> element or JavaScript code for applications that are embedded in a web page and use the Java Plug-In to run. Use JavaScript code to create the link or button that calls Java Web Start to launch an application from the browser. The Java packaging tools generate JavaScript code for both types of execution, which you can copy into your web page.\nThe HTML page generated by the packaging tools is a simple test page for your application. It includes sample JavaScript code to launch and embed your application, which you can copy to your own web page. To avoid manual copying, consider using HTML templates for application packaging to insert the JavaScript code into an existing web page. For more information, see Section 5.7.4, \"Web Page Templates.\"\n2.5 Distribute Your Application\nWhen you have your application package and any web pages that you are using, copy them to the appropriate location to make your application available to users.\n\u2022\tIf your application is embedded in a web page or launched from a browser, copy your package and the web page to the web server from which they will be loaded.\n\u2022\tIf your application is a desktop application, copy the application to the location from which users will download it. Self-contained applications provide installable packages and the required JRE, which makes it easier for users to install and run your application.\n\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "mysql data directory location", "id": 743, "answers": [{"answer_id": 744, "document_id": 431, "question_id": 743, "text": "If the software is Sequel pro the default install mysql on Mac OSX has data located here:\n/usr/local/var/mysql\n", "answer_start": 289, "answer_category": null}], "is_impossible": false}], "context": "I installed mysql in Mac after downloding its dmg file version 64 bit. While trying to create a database it gave me error 1006 -- can't create database. After browsing a number of website, it seems due to user ownership setting of mysql \"data directory\" location that needs to be changed. If the software is Sequel pro the default install mysql on Mac OSX has data located here:\n/usr/local/var/mysql\n\nWhere is mysql default \"data directory\"? I could not find /var/lib/mysql in localhost. As suggested, I edited this message to place a proper answer.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "What does Chef need?", "id": 274, "answers": [{"answer_id": 284, "document_id": 125, "question_id": 274, "text": "ChefDK\nChef Infra Client\nChef Supermarket\nAn install script for Chef Infra Client", "answer_start": 10538, "answer_category": null}], "is_impossible": false}, {"question": "How to install chef sever on Red Hat?", "id": 294, "answers": [{"answer_id": 304, "document_id": 125, "question_id": 294, "text": "sudo rpm -Uvh /tmp/chef-server-core-<version>.rpm\n", "answer_start": 12269, "answer_category": null}], "is_impossible": false}, {"question": "How to install chef sever on Ubuntu?", "id": 295, "answers": [{"answer_id": 305, "document_id": 125, "question_id": 295, "text": "sudo dpkg -i /tmp/chef-server-core-<version>.deb", "answer_start": 12335, "answer_category": null}], "is_impossible": false}, {"question": "How to install chefDK?", "id": 299, "answers": [{"answer_id": 309, "document_id": 125, "question_id": 299, "text": "Use the appropriate tool to run the installer:\ndpkg -i chefdk_3.2.30-1_amd64.deb\n\n\n\nUse the chef generate repo command to generate your Chef repo:\nchef generate repo chef-repo\n\n\n\nWithin your Chef repo, create a .chef directory:\nmkdir /chef-repo/.chef\n\n\n\nCopy the USER.pem and ORGANIZATION.pem files from the server, and move them into the .chef directory.\nscp ssh-user@chef-server.example.com:/path/to/pem/files /chef-repo/.chef/", "answer_start": 14286, "answer_category": null}], "is_impossible": false}, {"question": "What Rubby gems do Chef Supermaket need?", "id": 288, "answers": [{"answer_id": 298, "document_id": 125, "question_id": 288, "text": "mixlib-install\nmixlib-shellout\nmixlib-versioning\nartifactory", "answer_start": 11040, "answer_category": null}], "is_impossible": false}, {"question": "What Rubby gems do Chef Private Supermaket need?", "id": 302, "answers": [{"answer_id": 312, "document_id": 125, "question_id": 302, "text": "mixlib-install\nmixlib-shellout\nmixlib-versioning\nartifactory", "answer_start": 18187, "answer_category": null}], "is_impossible": false}, {"question": "What is the minimum memory requirement for Chef Infra Server?", "id": 304, "answers": [{"answer_id": 314, "document_id": 125, "question_id": 304, "text": " 1 GB", "answer_start": 18730, "answer_category": null}], "is_impossible": false}, {"question": "How to start Chef server?", "id": 308, "answers": [{"answer_id": 318, "document_id": 125, "question_id": 308, "text": " sudo chef-server-ctl reconfigure", "answer_start": 12498, "answer_category": null}], "is_impossible": false}, {"question": "How long will Chef Infra Server finish start?", "id": 309, "answers": [{"answer_id": 319, "document_id": 125, "question_id": 309, "text": "a few minutes ", "answer_start": 12673, "answer_category": null}], "is_impossible": false}, {"question": "How to reconfigure Chef Infra Server?", "id": 312, "answers": [{"answer_id": 322, "document_id": 125, "question_id": 312, "text": "sudo chef-server-ctl reconfigure", "answer_start": 19734, "answer_category": null}], "is_impossible": false}], "context": "\n\n\n\n\n\n\nInstall Chef in an air-gapped environment\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLearn Chef\n\n\n\n\n\n\n\n\n\nTutorials\n\n\nSkills Library\n\n\nDocs\n\n\nTraining\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOverview\n\n\n\n\n\nPlatform Overview\n\n\n\n\n\nCommunity\n\n\n\n\n\nAbout the Community\n\n\n\n\nContributing\n\n\n\n\nGuidelines\n\n\n\n\nDocs Style Guide\n\n\n\n\nSend Feedback\n\n\n\n\n\n\nPackages & Platforms\n\n\n\n\n\nPackages\n\n\n\n\nPlatforms\n\n\n\n\nSupported Versions\n\n\n\n\nOmnitruck API\n\n\n\n\nLicensing\n\n\n\n\n\nAbout Licensing\n\n\n\n\nAccepting License\n\n\n\n\n\n\n\n\n\n\nChef Infra\n\n\n\n\n\nGetting Started\n\n\n\n\n\nChef Infra Overview\n\n\n\n\nInstall ChefDK\n\n\n\n\nConfigure ChefDK\n\n\n\n\nQuick Start\n\n\n\n\nSystem Requirements\n\n\n\n\nChef on Azure Guide\n\n\n\n\n\nInstalling Chef on Windows\n\n\n\n\nMicrosoft Azure\n\n\n\n\nChef Workstation on Azure Cloud Shell\n\n\n\n\nAzure Powershell_Cmdlets\n\n\n\n\nKnife Azure\n\n\n\n\nKnife Azurerm\n\n\n\n\n\n\nChef on Windows Guide\n\n\n\n\n\nChef for Microsoft Windows\n\n\n\n\nChef Workstation and ChefDK on Windows\n\n\n\n\nChef Infra Client on Windows\n\n\n\n\nKnife Windows\n\n\n\n\n\n\nGlossary\n\n\n\n\nUninstall\n\n\n\n\n\n\nConcepts\n\n\n\n\n\nChefDK\n\n\n\n\nChef Infra Client Overview\n\n\n\n\nChef Infra Server Overview\n\n\n\n\nchef-repo\n\n\n\n\nCookbooks\n\n\n\n\nCustom Resources\n\n\n\n\nNodes\n\n\n\n\nPolicy\n\n\n\n\n\nAbout Policy\n\n\n\n\nData Bags\n\n\n\n\nRun-lists\n\n\n\n\nEnvironments\n\n\n\n\nRoles\n\n\n\n\n\n\nSecrets\n\n\n\n\nAuthentication\n\n\n\n\nAuthorization\n\n\n\n\nEnvironment Variables\n\n\n\n\nSupermarket\n\n\n\n\n\nSupermarket\n\n\n\n\nPublic Supermarket\n\n\n\n\nPrivate Supermarket\n\n\n\n\nShare Cookbooks\n\n\n\n\n\n\n\n\nFeatures\n\n\n\n\n\nFIPS\n\n\n\n\nHandlers\n\n\n\n\nManagement Console\n\n\n\n\n\nAbout the Management Console\n\n\n\n\nConfigure SAML\n\n\n\n\nClients\n\n\n\n\nCookbooks\n\n\n\n\nData Bags\n\n\n\n\nEnvironments\n\n\n\n\nNodes\n\n\n\n\nRoles\n\n\n\n\nUsers\n\n\n\n\nmanage.rb\n\n\n\n\nchef-manage-ctl\n\n\n\n\n\n\nPush Jobs\n\n\n\n\nSearch\n\n\n\n\n\n\nTroubleshooting\n\n\n\n\n\nSetup\n\n\n\n\n\nChefDK\n\n\n\n\nNodes\n\n\n\n\n\nInstall via Bootstrap\n\n\n\n\nInstall via Install Script\n\n\n\n\nchef-client (executable)\n\n\n\n\nclient.rb\n\n\n\n\nUpgrades\n\n\n\n\nSecurity\n\n\n\n\n\n\nChef Infra Server\n\n\n\n\n\nHosted Chef Server\n\n\n\n\nInstall Chef Infra Server\n\n\n\n\nInstall Standalone\n\n\n\n\nChef Infra Server Prerequisites\n\n\n\n\nTiered Installation\n\n\n\n\nInstall High Availability\n\n\n\n\n\n\nWorking with Proxies\n\n\n\n\nAir-gapped Installation\n\n\n\n\nFIPS-mode\n\n\n\n\nIntegrations\n\n\n\n\n\nAWS Marketplace\n\n\n\n\nGoogle Cloud Platform\n\n\n\n\nVMware\n\n\n\n\n\n\nSupermarket\n\n\n\n\n\nPublic Supermarket\n\n\n\n\nInstall Private Supermarket\n\n\n\n\nCustomize Supermarket\n\n\n\n\nsupermarket.rb Settings\n\n\n\n\nBackup and Restore\n\n\n\n\nLog Files\n\n\n\n\nMonitoring\n\n\n\n\nknife supermarket\n\n\n\n\nsupermarket-ctl\n\n\n\n\nSupermarket API\n\n\n\n\n\n\nManagement Console\n\n\n\n\nPush Jobs\n\n\n\n\n\n\nCookbook Reference\n\n\n\n\n\nAbout Cookbooks\n\n\n\n\nAttributes\n\n\n\n\nFiles\n\n\n\n\nLibraries\n\n\n\n\nRecipes\n\n\n\n\n\nAbout Recipes\n\n\n\n\nDebug Recipes, Client Runs\n\n\n\n\n\n\nRecipe DSL\n\n\n\n\n\nDSL Overview\n\n\n\n\nattribute?\n\n\n\n\ncookbook_name\n\n\n\n\ndata_bag\n\n\n\n\ndata_bag_item\n\n\n\n\ndeclare_resource\n\n\n\n\ndelete_resource\n\n\n\n\ndelete_resource!\n\n\n\n\nedit_resource\n\n\n\n\nedit_resource!\n\n\n\n\nfind_resource\n\n\n\n\nfind_resource!\n\n\n\n\nplatform?\n\n\n\n\nplatform_family?\n\n\n\n\nreboot_pending?\n\n\n\n\nrecipe_name\n\n\n\n\nresources\n\n\n\n\nsearch\n\n\n\n\nshell_out\n\n\n\n\nshell_out!\n\n\n\n\ntag, tagged?, untag\n\n\n\n\nvalue_for_platform\n\n\n\n\nvalue_for_platform_family\n\n\n\n\nwith_run_context\n\n\n\n\nWindows Platform\n\n\n\n\nregistry_data_exists?\n\n\n\n\nregistry_get_subkeys\n\n\n\n\nregistry_get_values\n\n\n\n\nregistry_has_subkeys?\n\n\n\n\nregistry_key_exists?\n\n\n\n\nregistry_value_exists?\n\n\n\n\nWindows Platform Helpers\n\n\n\n\nLog Entries\n\n\n\n\n\n\nCustom Resources DSL\n\n\n\n\nResources\n\n\n\n\n\nAbout Resources\n\n\n\n\nCommon Functionality\n\n\n\n\nMigrating from Definitions\n\n\n\n\nCustom Resources\n\n\n\n\nAll Resources (Single Page)\n\n\n\n\napt_package\n\n\n\n\napt_preference\n\n\n\n\napt_repository\n\n\n\n\napt_update\n\n\n\n\narchive_file\n\n\n\n\nbash\n\n\n\n\nbatch\n\n\n\n\nbff_package\n\n\n\n\nbreakpoint\n\n\n\n\nbuild_essential\n\n\n\n\ncab_package\n\n\n\n\nchef_gem\n\n\n\n\nchef_handler\n\n\n\n\nchef_sleep\n\n\n\n\nchocolatey_config\n\n\n\n\nchocolatey_feature\n\n\n\n\nchocolatey_package\n\n\n\n\nchocolatey_source\n\n\n\n\ncookbook_file\n\n\n\n\ncron\n\n\n\n\ncron_d\n\n\n\n\ncron_access\n\n\n\n\ncsh\n\n\n\n\ndirectory\n\n\n\n\ndmg_package\n\n\n\n\ndnf_package\n\n\n\n\ndpkg_package\n\n\n\n\ndsc_resource\n\n\n\n\ndsc_script\n\n\n\n\nexecute\n\n\n\n\nfile\n\n\n\n\nfreebsd_package\n\n\n\n\ngem_package\n\n\n\n\ngit\n\n\n\n\ngroup\n\n\n\n\nhomebrew_cask\n\n\n\n\nhomebrew_package\n\n\n\n\nhomebrew_tap\n\n\n\n\nhostname\n\n\n\n\nhttp_request\n\n\n\n\nifconfig\n\n\n\n\nips_package\n\n\n\n\nkernel_module\n\n\n\n\nksh\n\n\n\n\nlaunchd\n\n\n\n\nlink\n\n\n\n\nlocale\n\n\n\n\nlog\n\n\n\n\nmacos_userdefaults\n\n\n\n\nmacports_package\n\n\n\n\nmdadm\n\n\n\n\nmount\n\n\n\n\nmsu_package\n\n\n\n\nohai_hint\n\n\n\n\nohai\n\n\n\n\nopenbsd_package\n\n\n\n\nopenssl_dhparam\n\n\n\n\nopenssl_ec_private_key\n\n\n\n\nopenssl_ec_public_key\n\n\n\n\nopenssl_rsa_private_key\n\n\n\n\nopenssl_rsa_public_key\n\n\n\n\nopenssl_x509_certificate\n\n\n\n\nopenssl_x509_crl\n\n\n\n\nopenssl_x509_request\n\n\n\n\nosx_profile\n\n\n\n\npackage\n\n\n\n\npacman_package\n\n\n\n\npaludis_package\n\n\n\n\nperl\n\n\n\n\nportage_package\n\n\n\n\npowershell_package\n\n\n\n\npowershell_package_source\n\n\n\n\npowershell_script\n\n\n\n\npython\n\n\n\n\nreboot\n\n\n\n\nreference\n\n\n\n\nregistry_key\n\n\n\n\nremote_directory\n\n\n\n\nremote_file\n\n\n\n\nrhsm_errata_level\n\n\n\n\nrhsm_errata\n\n\n\n\nrhsm_register\n\n\n\n\nrhsm_repo\n\n\n\n\nrhsm_subscription\n\n\n\n\nroute\n\n\n\n\nrpm_package\n\n\n\n\nruby\n\n\n\n\nruby_block\n\n\n\n\nscript\n\n\n\n\nservice\n\n\n\n\nsmartos_package\n\n\n\n\nsnap_package\n\n\n\n\nsolaris_package\n\n\n\n\nssh_known_hosts_entry\n\n\n\n\nsubversion\n\n\n\n\nsudo\n\n\n\n\nswap_file\n\n\n\n\nsysctl\n\n\n\n\nsystemd_unit\n\n\n\n\ntemplate\n\n\n\n\ntimezone\n\n\n\n\nuser\n\n\n\n\nwindows_ad_join\n\n\n\n\nwindows_auto_run\n\n\n\n\nwindows_certificate\n\n\n\n\nwindows_dfs_folder\n\n\n\n\nwindows_dfs_namespace\n\n\n\n\nwindows_dfs_server\n\n\n\n\nwindows_dns_record\n\n\n\n\nwindows_dns_zone\n\n\n\n\nwindows_env\n\n\n\n\nwindows_feature\n\n\n\n\nwindows_feature_dism\n\n\n\n\nwindows_feature_powershell\n\n\n\n\nwindows_firewall_rule\n\n\n\n\nwindows_font\n\n\n\n\nwindows_package\n\n\n\n\nwindows_pagefile\n\n\n\n\nwindows_path\n\n\n\n\nwindows_printer\n\n\n\n\nwindows_printer_port\n\n\n\n\nwindows_service\n\n\n\n\nwindows_share\n\n\n\n\nwindows_shortcut\n\n\n\n\nwindows_task\n\n\n\n\nwindows_uac\n\n\n\n\nwindows_workgroup\n\n\n\n\nyum_package\n\n\n\n\nyum_repository\n\n\n\n\nzypper_package\n\n\n\n\nzypper_repository\n\n\n\n\n\n\nTemplates\n\n\n\n\nCookbook Repo\n\n\n\n\nmetadata.rb\n\n\n\n\nCookbook Versioning\n\n\n\n\nRuby Guide\n\n\n\n\n\n\nChefDK\n\n\n\n\n\nAbout ChefDK\n\n\n\n\nBerkshelf\n\n\n\n\nchef-shell (executable)\n\n\n\n\nchef (executable)\n\n\n\n\n\nchef env\n\n\n\n\nchef exec\n\n\n\n\nchef gem\n\n\n\n\nchef generate attribute\n\n\n\n\nchef generate cookbook\n\n\n\n\nchef generate file\n\n\n\n\nchef generate recipe\n\n\n\n\nchef generate repo\n\n\n\n\nchef generate resource\n\n\n\n\nchef generate template\n\n\n\n\nchef shell-init\n\n\n\n\n\n\nchef-apply (executable)\n\n\n\n\nChef Solo\n\n\n\n\n\nAbout Chef Solo\n\n\n\n\nchef-solo (executable)\n\n\n\n\nsolo.rb\n\n\n\n\n\n\nchef-vault\n\n\n\n\nChefSpec\n\n\n\n\nconfig.rb (knife.rb)\n\n\n\n\nOptional config.rb Settings\n\n\n\n\ncookstyle\n\n\n\n\nDelivery CLI\n\n\n\n\nFoodcritic\n\n\n\n\nTest Kitchen\n\n\n\n\n\nAbout Test Kitchen\n\n\n\n\nkitchen (executable)\n\n\n\n\nkitchen.yml\n\n\n\n\nkitchen-vagrant\n\n\n\n\n\n\nKnife\n\n\n\n\n\nAbout Knife\n\n\n\n\nSetting up Knife\n\n\n\n\nKnife Common Options\n\n\n\n\nconfig.rb (knife.rb)\n\n\n\n\nknife bootstrap\n\n\n\n\nknife client\n\n\n\n\nknife configure\n\n\n\n\nknife cookbook\n\n\n\n\nknife cookbook site\n\n\n\n\nknife data bag\n\n\n\n\nknife delete\n\n\n\n\nknife deps\n\n\n\n\nknife diff\n\n\n\n\nknife download\n\n\n\n\nknife edit\n\n\n\n\nknife environment\n\n\n\n\nknife exec\n\n\n\n\nknife list\n\n\n\n\nknife node\n\n\n\n\nknife raw\n\n\n\n\nknife recipe list\n\n\n\n\nknife role\n\n\n\n\nknife search\n\n\n\n\nknife serve\n\n\n\n\nknife show\n\n\n\n\nknife ssh\n\n\n\n\nknife ssl_check\n\n\n\n\nknife ssl_fetch\n\n\n\n\nknife status\n\n\n\n\nknife supermarket\n\n\n\n\nknife tag\n\n\n\n\nknife upload\n\n\n\n\nknife user\n\n\n\n\nknife xargs\n\n\n\n\nknife opc\n\n\n\n\n\n\nOhai\n\n\n\n\n\nAbout Ohai\n\n\n\n\nohai (executable)\n\n\n\n\n\n\nPolicyfile\n\n\n\n\n\nAbout Policyfile\n\n\n\n\nPolicyfile.rb\n\n\n\n\n\n\npush-jobs-client (executable)\n\n\n\n\n\n\nChef Infra Server\n\n\n\n\n\nRunbook (Single Page)\n\n\n\n\nBackup & Restore\n\n\n\n\nBackend Failure Recovery\n\n\n\n\nFirewalls & Ports\n\n\n\n\nActive Directory & LDAP\n\n\n\n\nLog Files\n\n\n\n\nMonitor\n\n\n\n\nOrganizations & Groups\n\n\n\n\nSecurity\n\n\n\n\nServices\n\n\n\n\nTuning\n\n\n\n\nUpgrades\n\n\n\n\nUpgrade HA Cluster\n\n\n\n\nUsers\n\n\n\n\nchef-server-ctl\n\n\n\n\nchef-backend-ctl\n\n\n\n\nchef-server.rb\n\n\n\n\nChef Infra Server Optional Settings\n\n\n\n\nopscode-expander-ctl\n\n\n\n\nChef Infra Server API\n\n\n\n\nPush Jobs\n\n\n\n\n\nknife push jobs\n\n\n\n\npush-jobs-client\n\n\n\n\npush-jobs-client.rb\n\n\n\n\npush-jobs-server.rb\n\n\n\n\nPush Jobs API\n\n\n\n\nChef Infra Server Sent Events\n\n\n\n\n\n\n\n\nRelease Notes\n\n\n\n\n\nChef Infra Client\n\n\n\n\nChefDK\n\n\n\n\nChef Infra Server\n\n\n\n\nChef Push Jobs\n\n\n\n\n\n\nDeprecations\n\n\n\n\n\n\n\nChef Workstation\n\n\n\n\n\nChef Workstation\n\n\n\n\n\nDocumentation\n\n\n\n\n\n\n\nChef Habitat\n\n\n\n\n\nDocumentation\n\n\n\n\n\nTutorials\n\n\n\n\n\n\n\nChef InSpec\n\n\n\n\n\nDocumentation\n\n\n\n\n\nTutorials\n\n\n\n\n\n\n\nChef Automate\n\n\n\n\n\nChef Automate Documentation\n\n\n\n\n\n\n\nLegacy\n\n\n\n\n\nWorkflow\n\n\n\n\n\nWorkflow Basics\n\n\n\n\n\nWorkflow Overview\n\n\n\n\nConfigure a Pipeline\n\n\n\n\nConfigure a Project\n\n\n\n\nConfigure Data Collection\n\n\n\n\nData Collection with Chef HA\n\n\n\n\nData Collection without Chef Infra Server\n\n\n\n\nAudit Cookbook\n\n\n\n\n\n\nManaging Workflow\n\n\n\n\n\nbuild-cookbook (cookbook)\n\n\n\n\ndelivery-truck (cookbook)\n\n\n\n\nManage Dependencies\n\n\n\n\nManage Secrets\n\n\n\n\nPublish to Multiple Chef Infra Servers\n\n\n\n\nRunners\n\n\n\n\nWorkflow w/Bitbucket\n\n\n\n\nWorkflow w/Email (SMTP)\n\n\n\n\nWorkflow w/GitHub\n\n\n\n\nWorkflow w/Slack\n\n\n\n\nUsers and Roles\n\n\n\n\nAuthentication w/LDAP\n\n\n\n\nAuthentication w/SAML\n\n\n\n\nElasticsearch and Kibana Auth\n\n\n\n\nTuning\n\n\n\n\n\n\nReference\n\n\n\n\n\nDelivery CLI\n\n\n\n\ndelivery.rb\n\n\n\n\nWorkflow DSL\n\n\n\n\n\n\nAWS OpsWorks for Chef Automate\n\n\n\n\nChef Automate for Microsoft Azure\n\n\n\n\n\n\n\n\nExtension APIs\n\n\n\n\n\nCompliance DSL\n\n\n\n\n\nHandlers\n\n\n\n\n\nCustom Handlers\n\n\n\n\nHandler DSL\n\n\n\n\nCommunity Handlers\n\n\n\n\n\n\nKnife Plugins\n\n\n\n\n\nCloud Plugins\n\n\n\n\nWriting Custom Plugins\n\n\n\n\n\n\nOhai Plugins\n\n\n\n\n\nCustom Plugins\n\n\n\n\nCommunity Plugins\n\n\n\n\n\n\n\n\n\nAvailable on GitHub\n\n\n\n\n\nGet Chef\n\n\n\n\n\nSend Feedback\n\n\n\n\n\nSupport\n\n\n\n\n\nSite Map\n\n\n\n\n\nArchive\n\n\n\n\n\n\n\n\n\n\n\n\nTable Of Contents\n\nInstall Chef in an air-gapped environment\nPrerequisites\nRequired cookbooks\nRequired Gems\nCreate an install script\n\n\nChef server\nChef Workstation\nInstall ChefDK\nCreate a bootstrap template\nConfigure knife\n\n\nPrivate Supermarket\nRequirements\nConfigure credentials\nCreate a Wrapper\nDefine Attributes\nBootstrap Supermarket\nConnect to Supermarket\nConfiguration updates\nKnife\nBerkshelf\n\n\nUpload cookbooks to Supermarket\n\n\n\n\n\n\n\n\n\n\n\nInstall Chef in an air-gapped environment\u00b6\n[edit on GitHub]\nThis guide will show you how to run a fully functional Chef environment within an air-gapped network.\n\nPrerequisites\u00b6\nSince a variety of different practices are used to create an air-gapped network, this guide focuses solely on the implementation of Chef software - as such, it makes the following assumptions:\n\nYou have a way to get packages to your air-gapped machines\nMachines on your air-gapped network are able to resolve each other via DNS\nA server\u2019s Fully Qualified Domain Name (FQDN) is the name that will be used by other servers to access it\nYou have a private Ruby gem mirror to supply gems as needed\nYou have an artifact store for file downloads. At a minimum, it should have the following packages available:\nChefDK\nChef Infra Client\nChef Supermarket\nAn install script for Chef Infra Client\n\n\n\n\nRequired cookbooks\u00b6\nThis guide will link to the required cookbooks for each piece of software in that software\u2019s respective section, but this is a full list of the cookbooks required to complete the entire guide:\nFor Chef Supermarket:\n\nsupermarket-omnibus-cookbook\nchef-ingredient\nhostsfile\n\n\n\nRequired Gems\u00b6\nThe following Ruby gems are required to install private Supermarket via the supermarket-omnibus-cookbook:\n\nmixlib-install\nmixlib-shellout\nmixlib-versioning\nartifactory\n\nThese should be accessible from your Gem mirror.\n\n\nCreate an install script\u00b6\nAn install script is used to install Chef Infra Client when bootstrapping a new node. It simply pulls the Chef Infra Client package from your artifact store, and then installs it. For example, on Debian-based Linux systems, it would look similar to this:\n#!/bin/bash\n\ncd /tmp/\nwget http://packages.example.com/chef_13.2.20-1_amd64.deb\ndpkg -i chef_13.2.20-1_amd64.deb\n\n\nThe install script should be accessible from your artifact store.\n\n\n\nChef server\u00b6\nIn this section you\u2019ll install the Chef Infra Server, and create your organization and user.  Note that in order to configure Supermarket later in this guide, you will need a user that is a member of the admins group.\n\nDownload the package from https://downloads.chef.io/chef-server/.\n\nUpload the package to the machine that will run the Chef Infra Server, and then record its location on the file system. The rest of these steps assume this location is in the /tmp directory.\n\nAs a root user, install the Chef Infra Server package on the server, using the name of the package provided by Chef. For Red Hat Enterprise Linux and CentOS:\n$ sudo rpm -Uvh /tmp/chef-server-core-<version>.rpm\n\n\nFor Ubuntu:\n$ sudo dpkg -i /tmp/chef-server-core-<version>.deb\n\n\nAfter a few minutes, the Chef Infra Server will be installed.\n\nRun the following to start all of the services:\n$ sudo chef-server-ctl reconfigure\n\n\nBecause the Chef Infra Server is composed of many different services that work together to create a functioning system, this step may take a few minutes to complete.\n\nRun the following command to create an administrator:\n$ sudo chef-server-ctl user-create USER_NAME FIRST_NAME LAST_NAME EMAIL 'PASSWORD' --filename FILE_NAME\n\n\nAn RSA private key is generated automatically. This is the user\u2019s private key and should be saved to a safe location. The --filename option will save the RSA private key to the specified absolute path.\nFor example:\n$ sudo chef-server-ctl user-create janedoe Jane Doe janed@example.com 'abc123' --filename /path/to/janedoe.pem\n\n\n\nRun the following command to create an organization:\n$ sudo chef-server-ctl org-create short_name 'full_organization_name' --association_user user_name --filename ORGANIZATION-validator.pem\n\n\nFor example:\n$ sudo chef-server-ctl org-create 4thcoffee 'Fourth Coffee, Inc.' --association_user janedoe --filename /path/to/4thcoffee-validator.pem\n\n\nThe name must begin with a lower-case letter or digit, may only contain lower-case letters, digits, hyphens, and underscores, and must be between 1 and 255 characters. For example: 4thcoffee.\nThe full name must begin with a non-white space character and must be between 1 and 1023 characters. For example: 'Fourth Coffee, Inc.'.\nThe --association_user option will associate the user_name with the admins security group on the Chef Infra Server.\nAn RSA private key is generated automatically. This is the chef-validator key and should be saved to a safe location. The --filename option will save the RSA private key to the specified absolute path.\n\n\n\n\nChef Workstation\u00b6\n\nInstall ChefDK\u00b6\n\nYour workstation should have a copy of ChefDK installer package. Use the appropriate tool to run the installer:\ndpkg -i chefdk_3.2.30-1_amd64.deb\n\n\n\nUse the chef generate repo command to generate your Chef repo:\nchef generate repo chef-repo\n\n\n\nWithin your Chef repo, create a .chef directory:\nmkdir /chef-repo/.chef\n\n\n\nCopy the USER.pem and ORGANIZATION.pem files from the server, and move them into the .chef directory.\nscp ssh-user@chef-server.example.com:/path/to/pem/files /chef-repo/.chef/\n\n\n\n\n\n\nCreate a bootstrap template\u00b6\nBy default, knife bootstrap uses the chef-full template to bootstrap a node. This template contains a number of useful features, but it also attempts to pull an installation script from downloads.chef.io. In this section, you\u2019ll copy the contents of the chef-full template to a custom template, and then modify the package and Ruby gem sources.\n\nNavigate to the .chef directory, and create a bootstrap directory within it:\nmkdir bootstrap\n\n\n\nMove to the bootstrap directory and create a blank template file; this example will use airgap.erb for the template name:\ntouch airgap.erb\n\n\n\nStill in the bootstrap directory, issue the following command to copy the chef-full configuration to your new template:\nfind /opt/chefdk/embedded/lib/ruby -type f -name chef-full.erb -exec cat {} \\; > airgap.erb\n\n\nThis command searches for the chef-full template file under /opt/chefdk/embedded/lib/ruby, and then outputs the contents of the file to airgap.erb. If you used a different template file name, be sure to replace airgap.erb with the template file you created during the last step.\n\nUpdate airgap.erb to replace omnitruck.chef.io with the URL of install.sh on your artifact store:\ninstall_sh=\"<%= knife_config[:bootstrap_url] ? knife_config[:bootstrap_url] : \"http://packages.example.com/install.sh\" %>\"\n\n\n\nStill in your text editor, locate the following line near the bottom of your airgap.erb file:\ncat > /etc/chef/client.rb <<'EOP'\n<%= config_content %>\nEOP\n\n\nBeneath it, add the following, replacing gems.example.com with the URL of your gem mirror:\ncat >> /etc/chef/client.rb <<'EOP'\nrubygems_url \"http://gems.example.com\"\nEOP\n\n\nThis appends the appropriate rubygems_url setting to the /etc/chef/client.rb file that is created during bootstrap, which ensures that your nodes use your internal gem mirror.\n\n\n\n\nConfigure knife\u00b6\nWithin the .chef directory, create a config.rb file and replace USER and ORGANIZATION with the user and organization that you created on your Chef Infra Server; replace chef-server.example.com with your Chef Infra Server URL:\ncurrent_dir = File.dirname(__FILE__)\nlog_level                :info\nlog_location             STDOUT\nnode_name                'USER'\nclient_key               \"#{current_dir}/USER.pem\"\nvalidation_client_name   'ORGANIZATION-validator'\nvalidation_key           \"#{current_dir}/ORGANIZATION.pem\"\nchef_server_url          'https://chef-server.example.com/organizations/ORGANIZATION'\ncache_type               'BasicFile'\ncache_options( :path => \"#{ENV['HOME']}/.chef/checksums\" )\ncookbook_path            [\"#{current_dir}/../cookbooks\"]\nknife[:bootstrap_template] = \"#{current_dir}/bootstrap/airgap.erb\"\n\n\nThe knife[:bootstrap_template] option in this example allows you to specify the template that knife bootstrap will use by default when bootstrapping a node. It should point to your custom template within the bootstrap directory.\nNow that knife is configured, copy the SSL certificates from your Chef Infra Server to your trusted certificates:\nknife ssl fetch\n\n\n\n\n\nPrivate Supermarket\u00b6\nPrivate Supermarket allows you to host your own internal version of the Chef Supermarket within your air-gapped network.\n\nRequirements\u00b6\nIn this section, you will use a wrapper around the supermarket-omnibus-cookbook to install private Supermarket. The Supermarket cookbook depends upon the following cookbooks:\n\nchef-ingredient\nhostsfile\n\nThe following Gems must be accessible via your Gem mirror:\n\nmixlib-install\nmixlib-shellout\nmixlib-versioning\nartifactory\n\nYour cookbooks directory must have all three of these cookbooks installed before you will be able to use the Supermarket cookbook wrapper. In addition the necessary cookbooks, a private Chef Supermarket has the following requirements:\n\nAn operational Chef Infra Server (version 12.0 or higher) to act as the OAuth 2.0 provider\nA user account on the Chef Infra Server with admins privileges\nA key for the user account on the Chef server\nAn x86_64 compatible Linux host with at least 1 GB memory\nSystem clocks synchronized on the Chef Infra Server and Supermarket hosts\nSufficient disk space to meet project cookbook storage capacity or credentials to store cookbooks in an Amazon Simple Storage Service (S3) bucket\n\n\n\nConfigure credentials\u00b6\nFirst, you\u2019ll configure Chef Identity credentials for Supermarket. Chef Identity is an OAuth 2.0 service packaged with the Chef Infra Server, that allows you to use the same credentials to access both server and Supermarket.\n\nLog on to the Chef Infra Server via SSH and elevate to an admin-level user. If running a multi-node Chef Infra Server cluster, log on to the node acting as the primary node in the cluster.\n\nUpdate the /etc/opscode/chef-server.rb configuration file.\nTo define OAuth 2 information for Chef Supermarket, create a Hash similar to:\noc_id['applications'] ||= {}\noc_id['applications']['supermarket'] = {\n'redirect_uri' => 'https://supermarket.mycompany.com/auth/chef_oauth2/callback'\n}\n\n\n\nReconfigure the Chef Infra Server.\n$ sudo chef-server-ctl reconfigure\n\n\n\nRetrieve Supermarket\u2019s OAuth 2.0 client credentials:\nDepending on your Chef Infra Server version and configuration (see chef-server.rb), this can be retrieved via chef-server-ctl oc-id-show-app supermarket or is located in /etc/opscode/oc-id-applications/supermarket.json:\n{\n\"name\": \"supermarket\",\n\"uid\": \"0bad0f2eb04e935718e081fb71asdfec3681c81acb9968a8e1e32451d08b\",\n\"secret\": \"17cf1141cc971a10ce307611beda7ffadstr4f1bc98d9f9ca76b9b127879\",\n\"redirect_uri\": \"https://supermarket.mycompany.com/auth/chef_oauth2/callback\"\n}\n\n\n\n\n\n\nCreate a Wrapper\u00b6\n\nGenerate the cookbook:\n$ chef generate cookbook my_supermarket_wrapper\n\n\n\nChange directories into that cookbook:\n$ cd my_supermarket_wrapper\n\n\n\nDefines the wrapper cookbook\u2019s dependency on the supermarket-omnibus-cookbook cookbook. Open the metadata.rb file of the newly-created cookbook, and then add the following line:\ndepends 'supermarket-omnibus-cookbook'\n\n\n\nSave and close the metadata.rb file.\n\nOpen the /recipes/default.rb recipe located within the newly-generated cookbook and add the following content:\ninclude_recipe 'supermarket-omnibus-cookbook'\n\n\nThis ensures that the default.rb file in the supermarket-omnibus-cookbook is run.\n\n\n\n\nDefine Attributes\u00b6\nDefine the attributes for the Chef Supermarket installation and how it connects to the Chef Infra Server. One approach would be to hard-code attributes in the wrapper cookbook\u2019s default.rb recipe. A better approach is to place these attributes in a data bag, and then reference them from the recipe. For example, the data bag could be named apps and then a data bag item within the data bag could be named supermarket. The following attributes are required:\n\nchef_server_url: the url for your chef server.\nchef_oauth2_app_id: the Chef Identity uid from /etc/opscode/oc-id-applications/supermarket.json\nchef_oauth2_secret: The Chef Identity secret from /etc/opscode/oc-id-applications/supermarket.json\npackage_url: The location of the Supermarket package on your artifact store\n\nTo define these attributes, do the following:\n\nOpen the recipes/default.rb file and add the following, before the include_recipe line that was added in the previous step. This example uses a data bag named apps and a data bag item named supermarket:\napp = data_bag_item('apps', 'supermarket')\n\n\n\nSet the attributes from the data bag:\nnode.override['supermarket_omnibus']['chef_server_url'] = app['chef_server_url']\nnode.override['supermarket_omnibus']['chef_oauth2_app_id'] = app['chef_oauth2_app_id']\nnode.override['supermarket_omnibus']['chef_oauth2_secret'] = app['chef_oauth2_secret']\nnode.override['supermarket_omnibus']['package_url'] = app['package_url']\n\n\nNote that the ['package_url'] setting points to the location of the Supermarket package on your artifact store. When finished, the /recipes/default.rb file should have code similar to:\napp = data_bag_item('apps', 'supermarket')\n\nnode.override['supermarket_omnibus']['chef_server_url'] = app['chef_server_url']\nnode.override['supermarket_omnibus']['chef_oauth2_app_id'] = app['chef_oauth2_app_id']\nnode.override['supermarket_omnibus']['chef_oauth2_secret'] = app['chef_oauth2_secret']\n\ninclude_recipe 'supermarket-omnibus-cookbook'\n\n\nAlternatively, if you chose not to use a data bag to store these values, your default.rb should look similar to this:\nnode.override['supermarket_omnibus']['chef_server_url'] = 'https://chef-server.example.com:443'\nnode.override['supermarket_omnibus']['chef_oauth2_app_id'] = '0bad0f2eb04e935718e081fb71asdfec3681c81acb9968a8e1e32451d08b'\nnode.override['supermarket_omnibus']['chef_oauth2_secret'] = '17cf1141cc971a10ce307611beda7ffadstr4f1bc98d9f9ca76b9b127879'\nnode.override['supermarket_omnibus']['package_url'] = 'http://packages.example.com/supermarket_3.1.22-1_amd64.deb'\n\n\ninclude_recipe 'supermarket-omnibus-cookbook'\n\n\n\nSave and close the recipes/default.rb file.\n\nUpload all of your cookbooks to the Chef Infra Server:\nknife cookbook upload -a\n\n\n\n\n\n\nBootstrap Supermarket\u00b6\nBootstrap the node on which Chef Supermarket is to be installed. For example, to bootstrap a node running Ubuntu on Amazon Web Services (AWS), the command is similar to:\n$ knife bootstrap ip_address -N supermarket-node -x ubuntu --sudo\n\n\nwhere:\n\n-N defines the name of the Chef Supermarket node: supermarket-node\n-x defines the (ssh) user name: ubuntu\n--sudo ensures that sudo is used while running commands on the node during the bootstrap operation\n\nWhen the bootstrap operation is finished, do the following:\n\nAdd the wrapper cookbook\u2019s /recipes/default.rb recipe to the run-list:\n$ knife node run_list set supermarket-node recipe[my_supermarket_wrapper::default]\n\n\nwhere supermarket-node is the name of the node that was just bootstrapped.\n\nStart Chef Infra Client on the newly-bootstrapped Chef Supermarket node. For example, using SSH:\n$ ssh ubuntu@your-supermarket-node-public-dns\n\n\n\nAfter accessing the Chef Supermarket node, run Chef Infra Client:\n$ sudo chef-client\n\n\n\n\n\n\nConnect to Supermarket\u00b6\nTo reach the newly spun up private Chef Supermarket, the hostname must be resolvable from a workstation. For production use, the hostname should have a DNS entry in an appropriate domain that is trusted by each user\u2019s workstation.\n\nVisit the Chef Supermarket hostname in the browser. A private Chef Supermarket will generate and use a self-signed certificate, if a certificate is not supplied as part of the installation process (via the wrapper cookbook).\nIf an SSL notice is shown due to your self-signed certificate while connecting to Chef Supermarket via a web browser, accept the SSL certificate. A trusted SSL certificate should be used for  private Chef Supermarket that is used in production.\nAfter opening Chef Supermarket in a web browser, click the Create Account link. A prompt to log in to the Chef Infra Server is shown. Authorize the Chef Supermarket to use the Chef Infra Server account for authentication.\n\n\nNote\nThe redirect URL specified for Chef Identity MUST match the FQDN hostname of the Chef Supermarket server. The URI must also be correct: /auth/chef_oauth2/callback. Otherwise, an error message similar to The redirect uri included is not valid. will be shown.\n\n\n\nConfiguration updates\u00b6\n\nKnife\u00b6\nUpdate the config.rb file on your workstation to use your private Supermarket:\nknife[:supermarket_site] = 'https://supermarket.example.com'\n\n\n\n\nBerkshelf\u00b6\nIf you\u2019re using Berkshelf, update your Berksfile to replace https://supermarket.chef.io with the URL of your private Supermarket:\nsource 'https://supermarket.example.com'\n\n\n\n\n\nUpload cookbooks to Supermarket\u00b6\nTo upload new cookbooks to your private Supermarket, use the knife supermarket share command on your workstation:\nknife supermarket share chef-ingredient\n\n\n\n\n\n\n\n\u00a9 Copyright: 2019 Chef Software, Inc.\nThis page is about: Current version of Chef.\nProvide feedback on Chef documentation.\nPrivacy policy.\n\n\n\n\n\n\n\n\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Gradle Implementation vs API configuration", "id": 1839, "answers": [{"answer_id": 1825, "document_id": 1410, "question_id": 1839, "text": "Gradle compile keyword was deprecated in favor of the api and implementation keywords to configure dependencies.\nUsing api is the equivalent of using the deprecated compile, so if you replace all compile with api everything will works as always.", "answer_start": 429, "answer_category": null}], "is_impossible": false}], "context": "I'm trying to figure out what is the difference between api and implementation configuration while building my dependencies.\nIn the documentation, it says that implementation has better build time, but, seeing this comment in a similar question I got to wonder if is it true.\nSince I'm no expert in Gradle, I hope someone can help. I've read the documentation already but I was wondering about an easy-to-understand explanation.\nGradle compile keyword was deprecated in favor of the api and implementation keywords to configure dependencies.\nUsing api is the equivalent of using the deprecated compile, so if you replace all compile with api everything will works as always.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "\"The installer cannot proceed with the current Internet Connection proxy\n  settings. Please check the Installation Notes for more information.\", what is wrong and what to do?", "id": 368, "answers": [{"answer_id": 375, "document_id": 163, "question_id": 368, "text": "If you see this message, check your proxy settings: From the Start menu select Settings, click Control Panel, double-click Internet Options, select the Connections tab, and click the LAN Settings button.", "answer_start": 1552, "answer_category": null}], "is_impossible": false}, {"question": "which kind of JRE installer shall I download for windows?", "id": 369, "answers": [{"answer_id": 376, "document_id": 163, "question_id": 369, "text": "Windows x86 Online: jre-8version-windows-i586-iftw.exe (The letters iftw mean \"install from the web.\")\n\nWindows x86 Offline: jre-8version-windows-i586.exe\n\nWindows x64: jre-8version-windows-x64.exe", "answer_start": 2636, "answer_category": null}], "is_impossible": false}, {"question": "how to download the installer for windows?", "id": 370, "answers": [{"answer_id": 377, "document_id": 163, "question_id": 370, "text": "Click the JRE Download link for the installer you want to use. A dialog box opens. Depending on your browser, click Save or Save File to save the JRE installer without installing it. Verify that you have downloaded the entire file by comparing the size of the file you downloaded with the expected size shown on the download page. Alternatively, (depending on your browser) click Run or Open to run the JRE installer from your browser.", "answer_start": 3595, "answer_category": null}], "is_impossible": false}, {"question": "what to do if JDK is not registered?", "id": 371, "answers": [{"answer_id": 378, "document_id": 163, "question_id": 371, "text": "You must set the PATH environment variable to point to JAVA_HOME\\bin (where JAVA_HOME is the location where you installed the public JRE) to register the JRE. ", "answer_start": 4787, "answer_category": null}], "is_impossible": false}], "context": "6 JRE Installation for Microsoft Windows\nThis page describes how to install and uninstall JRE 8 for Windows.\n\nThe page has these topics:\n\n\"System Requirements\"\n\n\"Proxy Settings and Authentication\"\n\n\"Installation Instructions Notation\"\n\n\"Installation Instructions\"\n\n\"Uninstalling the JRE\"\n\n\"Java Start Menu\"\n\n\"Java Plug-in\"\n\n\"Java Web Start\"\n\n\"Java Update Feature\"\n\n\"Option to Disable the \"JRE out of date\" Warning\"\n\n\"Installation of JRE on 64-Bit Windows Computers\"\n\nSee \"JDK 8 and JRE 8 Installation Start Here\" for general information about installing JDK 8 and JRE 8.\n\nSystem Requirements\nSee http://www.oracle.com/technetwork/java/javase/certconfig-2095354.html for information about supported platforms, operating systems, and browsers.\n\nSee \"Windows System Requirements for JDK and JRE\" for minimum processor, disk space, and memory requirements.\n\nIf you have any difficulties, see http://docs.oracle.com/javase/8/docs/technotes/guides/troubleshoot/index.html, \"Windows Online Installation and Java Update FAQ\", or submit a bug report at http://bugreport.java.com/bugreport/.\n\nProxy Settings and Authentication\nTo use the Windows Online Installer, you must be connected to the Internet. If you are running behind a proxy server, you must have your proxy settings correctly configured. If they are not configured, or are incorrectly configured, the installer will terminate with the following message:\n\n  The installer cannot proceed with the current Internet Connection proxy\n  settings. Please check the Installation Notes for more information.\nIf you see this message, check your proxy settings: From the Start menu select Settings, click Control Panel, double-click Internet Options, select the Connections tab, and click the LAN Settings button.\n\nIf you do not know what the correct settings should be, check with your Internet provider or system administrator.\n\nInstallation Instructions Notation\nFor any text in this document that contains the following notation, you must substitute the appropriate update version number:\n\nversion\nFor example, if you were downloading the JRE installer for 32-bit systems for update 1.8.0_01, the file name: jre-8version-windows-i586.exe would become jre-8u1-windows-i586.exe.\n\nSimilarly, if you were downloading the JRE installer for 64-bit systems for update 1.8.0_01, the file name jre-8version-windows-x64.exe would become jre-8u1-windows-x64.exe.\n\nInstallation Instructions\nInstalling the JRE consists of two main steps:\n\n\"Downloading the Installer\"\n\n\"Running the Installer\"\n\nDownloading the Installer\nYou have a choice of the following kinds of JRE installers that you can download:\n\nWindows x86 Online: jre-8version-windows-i586-iftw.exe (The letters iftw mean \"install from the web.\")\n\nWindows x86 Offline: jre-8version-windows-i586.exe\n\nWindows x64: jre-8version-windows-x64.exe\n\nThe Windows x86 Online Installer is a small program that will download more installer files based on your system configuration. Using this installer can help you avoid downloading large amounts of unnecessary files. For more information, see \"Windows Online Installation and Java Update FAQ\".\n\nThe Windows x86 Offline Installer, as well as the installers for 64-bit systems, contain everything needed to install the JRE.\n\n\nNote:\n\nThe Microsoft Windows Installer (MSI) Enterprise JRE Installer is also available, which enables you to install the JRE across your enterprise. It requires a commercial license for use in production. For more information, see the MSI Enterprise JRE Installer Guide at https://docs.oracle.com/javacomponents/msi-jre8/install-guide.\nClick the JRE Download link for the installer you want to use. A dialog box opens. Depending on your browser, click Save or Save File to save the JRE installer without installing it. Verify that you have downloaded the entire file by comparing the size of the file you downloaded with the expected size shown on the download page. Alternatively, (depending on your browser) click Run or Open to run the JRE installer from your browser.\n\nRunning the Installer\n\nNote:\n\nYou must have administrative permissions in order to install the JRE.\nIf you saved the JRE installer to your computer, run the installer by double-clicking it. Follow the instructions the installer provides. The installer notifies you if Java content is disabled in web browsers, and provides instructions for enabling it. If you previously chose to hide some of the security prompts for applets and Java Web Start applications, the installer provides an option for restoring the prompts. When you are finished with the installation, you can delete the downloaded file to recover disk space.\n\nNote the following:\n\nThe public JRE installed with the JDK is not registered. (This also applies to the 64-bit version of the JDK.) You must set the PATH environment variable to point to JAVA_HOME\\bin (where JAVA_HOME is the location where you installed the public JRE) to register the JRE. See \"Private Versus Public JRE\" for more information about the public JRE.\n\nBy default, Java Access Bridge is disabled. To enable it, see http://docs.oracle.com/javase/8/docs/technotes/guides/access/enable_and_test.html#enabling_jab.\n\nAfter installation, use the Java item in the Windows Start menu to get access to essential Java information and functions, including help, the Java Control Panel, and checking for updates.\n\nUninstalling the JRE\nTo uninstall the JRE, use the Java Uninstall tool, which you can access in the following ways:\n\nIf the JRE is version 8u20 or later, uninstall it with the \"Add/Remove Programs\" utility in the Microsoft Windows Control Panel. The Java Removal Tool is integrated with the uninstallation process, and it will guide you through the removal of older JREs.\n\nUse the online Java Uninstall tool:\n\nhttps://www.java.com/en/download/uninstallapplet.jsp\n\nThe Java Uninstall tool helps you improve your computer security by simplifying the process of finding and uninstalling older versions of Java. The Uninstall tool shows you a list of the Java versions on your computer and then removes those that are out-of-date.\n\n\nNote:\n\nThe Java Uninstall tool will not run if your system administrator specified a deployment rule set in your organization.\nA deployment rule set enables enterprises to manage their Java desktop environment directly and continue using legacy business applications in an environment of ever-tightening Java applet and Java Web Start application security policies. A deployment rule set enables administrators to specify rules for applets and Java Web Start applications; these rules may specify that a specific JRE version must be used. Consequently, the Java Uninstall tool will not run if it detects a deployment rule set to ensure that no required JREs are uninstalled.\n\nSee http://docs.oracle.com/javase/8/docs/technotes/guides/deploy/deployment_rules.html for more information about the Deployment Rule Set feature.\n\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "What's the difference between dependencies, devDependencies and peerDependencies in npm package.json file?", "id": 1898, "answers": [{"answer_id": 1885, "document_id": 1469, "question_id": 1898, "text": "If you do not want to install devDependencies you can use npm install \u2013production", "answer_start": 269, "answer_category": null}], "is_impossible": false}], "context": "This documentation answers my question very poorly. I didn't understand those explanations. Can someone say in simpler words? Maybe with examples if it's hard to choose simple words?\nEDIT also added peerDependencies, which is closely related and might cause confusion.\nIf you do not want to install devDependencies you can use npm install \u2013production\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "create a windows xp installer on a linux machine", "id": 2004, "answers": [{"answer_id": 1990, "document_id": 1593, "question_id": 2004, "text": "InstallJammer can build any of its supported platforms from any other platform, so it should do exactly what you need.", "answer_start": 1167, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI need to create an installer for my software for Windows XP and newer. Is there any mechanism to do that on a Linux machine alone? (I'm running ubuntu, but I'd guess is not a show stopper).\n    \n\nThe NSIS (Nullsoft Scriptable Install System) is a free and open source installer system that allows you to create native Windows installers. \n\nIt uses ascript files to define all aspects of the setup procedure and with a compiler you generate the resulting setup package. You can find the sources here. The Installer system runs on Windows and POSIX compliant systems. There is also an Eclipse plugin available.\n\nThe Nullsoft installer is widely used for open source projects and even commercial products.\n\nUPDATE: There is a new alpha release on December 24, 2013, so as of late 2013 this project is still active.\n\nUPDATE 2: Beginning of April 2016 a new version 2.51 was released together with a release candidate for NSIS 3.0.\n    \n\nTake a look at InstallJammer.  It's a free, open source installer that is cross platform and can easily do what you want.  I have many users who build strictly for Windows but use Linux as their build platforms.\n\nInstallJammer can build any of its supported platforms from any other platform, so it should do exactly what you need.\n    \n\nFlexera Software (makers of InstallShield) have a cross-platform installation authoring utility called InstallAnywhere:\n\nhttp://www.flexerasoftware.com/products/installanywhere.htm\n\n\n  From a single project file and build\n  environment, InstallAnywhere enables\n  developers to create reliable\n  installations for the broadest range\n  of current platforms including\n  Windows, Linux, Mac OS X, Solaris, AIX\n  , HP-UX, and IBM iSeries.\n\n\n\n\n\n  InstallAnywhere runs on the latest\n  versions of these operating systems,\n  fully updated with the most recent\n  patches and service packs:\n  \n  \u2022Windows 7 (32-bit x86) \u2022Windows Vista\n  \u2022Windows XP \u2022Windows 2000 \u2022Mac OS X\n  10.4, 10.5, and 10.6 with Java 1.6 \u2022Red Hat Enterprise Linux 4 and 5\n  (32-bit x86) \u2022SUSE Linux 9, 10, and\n  11.2 (32-bit x86) \u2022Ubuntu 9.10 (32-bit x86) \u2022Solaris Solaris 9 and 10 (SPARC)\n  \u2022HP-UX 11i (PA-RISC) \u2022AIX 5.2, 5.3,\n  and 6.1 (Power/PowerPC)\n\n    \n\nOur software BitRock InstallBuilder also allows building Windows installers from Linux You can do so from the command line to make it easier to integrate with nightly builds, ANT, etc.\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Service already exists (when it clearly doesn't)", "id": 1794, "answers": [{"answer_id": 1780, "document_id": 1366, "question_id": 1794, "text": "You should run in command line (adm mode):\nsc delete service_name\nService names with spaces must be quoted.\nUpdate: Try this:\nsc query type= service > services.txt\nand verify that the service does not appear with another name.", "answer_start": 564, "answer_category": null}], "is_impossible": false}], "context": "I'm trying to create an installer for a Windows Service I developed. This installer has a custom UI at one point and it's the first time I do something like that so I installed and uninstalled the service a few times to make sure everything was like I wanted to in the installer.\nNow my issue is that when I try to install the service, it fails with Error 1001: Specified service already exists, but the service is listed nowhere in the registry, the services.msc console, or by sc query.\nCan anyone give me a clue of what's happening and how to fix it? Thank you\nYou should run in command line (adm mode):\nsc delete service_name\nService names with spaces must be quoted.\nUpdate: Try this:\nsc query type= service > services.txt\nand verify that the service does not appear with another name.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Deploy Symfony2 application to prod environment causes post-install-cmd exception", "id": 1273, "answers": [{"answer_id": 1265, "document_id": 844, "question_id": 1273, "text": "If you get a \"class not found\" error during this step, you may need to run export SYMFONY_ENV=prod before running this command so that the post-install-cmd scripts run in the prod environment.", "answer_start": 154, "answer_category": null}], "is_impossible": false}], "context": "I have read Symfony2 docs for How to deploy a Symfony2 application but I'm having some issues|warnings. As said here the first command I run is this one:\nIf you get a \"class not found\" error during this step, you may need to run export SYMFONY_ENV=prod before running this command so that the post-install-cmd scripts run in the prod environment.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to update code from Git to a Docker container?", "id": 584, "answers": [{"answer_id": 590, "document_id": 309, "question_id": 584, "text": "There are a couple of approaches you can use.You can use docker build --no-cache to avoid using the cache of the Git clone.The startup command calls git pull. So instead of running python manage.py, you'd have something like CMD cd /repo && git pull && python manage.py or use a start script if things are more complex.", "answer_start": 289, "answer_category": null}], "is_impossible": false}], "context": "I have a Docker file trying to deploy Django code to a container.\nAnd then I build my code as docker build -t dockerhubaccount/demo:v1 ., and this pulls my code from Bitbucket to the container. I run it as docker run -p 8000:8080 -td felixcheruiyot/demo:v1 and things appear to work fine.\nThere are a couple of approaches you can use.You can use docker build --no-cache to avoid using the cache of the Git clone.The startup command calls git pull. So instead of running python manage.py, you'd have something like CMD cd /repo && git pull && python manage.py or use a start script if things are more complex.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "is it possible to automate a clickonce deployment", "id": 1972, "answers": [{"answer_id": 1958, "document_id": 1557, "question_id": 1972, "text": "You can build a standard MSI installer and deploy it via Group Policy, but then updates are a little trickier. As a hybrid, you could build an MSI installer (deployed via Group Policy) that just installs a shortcut to the correct ClickOnce address on the desktop. In this way, the user always runs the ClickOnce version, which is guaranteed to be up-to-date when they run it.", "answer_start": 1987, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI work on a project consisting of a server and a client application deployed via ClickOnce. The client is installed the first time a user clicks a http://...file.application link, and the interaction with the user during installation are minimal (just the standard ClickOnce install/don't install dialog box).\n\nOne of our client wants to be able to automate the installation of the client on the users' machines. Is there a way to install the ClickOnce application without any user interaction, in order to automate the deployment process?\n\nDoes software providing this kind of deployment exist?\n\nMy problem comes from the fact I don't know how the ClickOnce deployment works under the hood (I don't even know if it is possible to run an ClickOnce installer from the command line...), and as ClickOnce applications are not packaged at all like Windows Installer, I am not sure of anything.\n    \n\nThere are many third-party solutions available for automated deployment. You can also create your own scripts, but you need some way of invoking them. If your clients' machines are configured to look for network located start up scripts then this would be an ideal method. Typically, automated deployments work with a standard deployment package, i.e. containing a setup.exe and required files, so this would probably be the best way to package your application.\n\nClickOnce is specifically designed for manual deployment by an end user and only confuses things when the goal is automated deployment. Obviously you can keep your ClickOnce deployment in place in case someone wants to install your application manually, but it will make things easier if you package it separately for automated deployment.\n    \n\nIt's possible to automate it.  The trick is the way the manifests are generated and signed.\n\nOnce an application manifest has been built and the deployed files are renamed, you have a set of files and folder that you can just copy to your install source.\n    \n\nYou can build a standard MSI installer and deploy it via Group Policy, but then updates are a little trickier. As a hybrid, you could build an MSI installer (deployed via Group Policy) that just installs a shortcut to the correct ClickOnce address on the desktop. In this way, the user always runs the ClickOnce version, which is guaranteed to be up-to-date when they run it.\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Can't upgrade Android SDK Tools", "id": 1141, "answers": [{"answer_id": 1134, "document_id": 718, "question_id": 1141, "text": "make a copy of the tools directory and call this new directory tools2.\n2.\tDO NOT USE 'SDK Setup.exe'.\n3.\tInstead open up a cmd.exe window as administrator and run the following from this new tools2 directory (obviously the full path on your local machine will be different): J:\\android-sdk-windows>tools2\\android.bat update sdk", "answer_start": 369, "answer_category": null}], "is_impossible": false}], "context": "I tried disabling my antivirus (as this problem has been reported by some people in 2009), and it didn't work. A fresh reboot didn't work, either. I'm encountering this problem on two different Windows 7 machines.\nI did not encounter this problem upgrading from previous revisions all the way up to revision 15.\nAnybody knows how I can circumvent this\nINSTRUCTIONS:\n1.\tmake a copy of the tools directory and call this new directory tools2.\n2.\tDO NOT USE 'SDK Setup.exe'.\n3.\tInstead open up a cmd.exe window as administrator and run the following from this new tools2 directory (obviously the full path on your local machine will be different): J:\\android-sdk-windows>tools2\\android.bat update sdk\nBasically, as noted before in this thread, 'SDK Setup.exe' invokes tools\\android.bat, which makes it impossible for it to rename the tools directory thereafter. It's a bit goofy and should never have passed QA validation..\nI'm trying to upgrade the Android SDK tools from revision 15 to revision 16. The update fails, claiming that a \"folder failed to be moved\":\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Clearscript files cannot be found on host", "id": 1051, "answers": [{"answer_id": 1046, "document_id": 632, "question_id": 1051, "text": "you have to install on the hosting server the Visual C++ Redistributable for Visual Studio 2012 or above: http://www.microsoft.com/en-gb/download/details.aspx?id=30679\nAnd Make sure you place ClearScript.dll in website's bin\\ folder, and ClearScriptV8-64.dll and v8-x64.dll into bin\\ClearScript.V8.", "answer_start": 98, "answer_category": null}], "is_impossible": false}], "context": "Like a lot of others I'm receiving the following error when deploying my ASP.Net MVC application.\nyou have to install on the hosting server the Visual C++ Redistributable for Visual Studio 2012 or above: http://www.microsoft.com/en-gb/download/details.aspx?id=30679\nAnd Make sure you place ClearScript.dll in website's bin\\ folder, and ClearScriptV8-64.dll and v8-x64.dll into bin\\ClearScript.V8.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Best strategy to deploy static site to s3 on github push?", "id": 474, "answers": [{"answer_id": 483, "document_id": 207, "question_id": 474, "text": "Rather than using an AWS service directly (as you say they nearly all expect a much more complicated setup, deploying to EC2 etc), you might be better off using a CI provider such as Shippable, Codeship or Wercker.", "answer_start": 513, "answer_category": null}], "is_impossible": false}], "context": "I'd like to automate deploying our site to AWS S3. I've written a node script to automate building and uploading the site, but I'd like to have the script automatically run whenever the master branch of our repo is updated on github.\nI looked into AWS CodeDeploy, but it looks like that's for specifically deploying to EC2. I've also looked at AWS Lambda, but there doesn't seem to be a clear way to pull a copy of the repo using git so I can run the script.\nAny services (preferably tied to AWS) that I can use?\nRather than using an AWS service directly (as you say they nearly all expect a much more complicated setup, deploying to EC2 etc), you might be better off using a CI provider such as Shippable, Codeship or Wercker.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Combine --user with --prefix error with setup.py install", "id": 647, "answers": [{"answer_id": 652, "document_id": 340, "question_id": 647, "text": "You can simply run pip install --user . , no prefix args required.", "answer_start": 871, "answer_category": null}], "is_impossible": false}], "context": "I was trying to install Python packages a system I recently gained access to. I was trying to take advantage of Python's relatively new per user site-packages directory, and the new option --user. (The option is currently undocumented, however it exists for Python 2.6+; you can see the help by running python setup.py install --help.)\nWhen I tried running\npython setup.py install --user\non any package I downloaded, I always got the following error:\nerror: can't combine user with with prefix/exec_prefix/home or install_(plat)base\nThe error was extremely perplexing because, as you can see, I wasn't providing the --prefix, --exec-prefix, --install-base, or --install-platbase flags as command line options. I wasted a lot of time trying to figure out what the problem was. I document my answer below, in hopes to spare some other poor soul a few hours of yak shaving.\nYou can simply run pip install --user . , no prefix args required.\nThis is better anyway because it will default to python3 if your pip is configured to use Python 3. (I forgot to enter python3 setup.py and it installed a 3-only package under 2.7)\n\nAs has been noted in the comments, the accepted answer (by @gotgenes, who, presumably, has genes) can lead to unexpected consequences.\n@rogeleaderr says, \"Note that keeping this file like this will make Python think that / is your root python library directory, leading to confusing issues if you try to install other new packages.\"\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "What is an easy way to deploy database changes using SQL Server?", "id": 1074, "answers": [{"answer_id": 1067, "document_id": 652, "question_id": 1074, "text": "You should take a look at this blog post. I've used this type of single update script from any DB version on a couple projects and it works pretty nicely.\nhttp://blogs.msdn.com/danhardan/archive/2007/03/30/database-change-scripts-mambo-style.aspx", "answer_start": 239, "answer_category": null}], "is_impossible": false}], "context": "The software system I work on is a medical billing system, large amounts of data and data tables, and stored procedures.\nI was reading the article \"12 Steps to Better Code\" and in The Joel Test #2 states: Can you make a build in one step?\nYou should take a look at this blog post. I've used this type of single update script from any DB version on a couple projects and it works pretty nicely.\nhttp://blogs.msdn.com/danhardan/archive/2007/03/30/database-change-scripts-mambo-style.aspx\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "JBoss AS 7 not accepting remote connections", "id": 1713, "answers": [{"answer_id": 1701, "document_id": 1286, "question_id": 1713, "text": "edit standalone.xml and insert the tag any-address instead of inet-address bound to 127.0.0.1\n<interfaces>\n    <interface name=\"management\">\n        <inet-address value=\"127.0.0.1\"/>\n    </interface>\n    <interface name=\"public\">\n       <any-ipv4-address/>\n    </interface>\n</interfaces>", "answer_start": 355, "answer_category": null}], "is_impossible": false}], "context": "I am using JBoss AS 7 and trying to connect to my application using the IP (from a computer in the intranet). It is not working. If I test from the computer which has the server I can see the system running if I go through localhost (http://localhost:8080/MySystem....) but not If I try with the IP (http://:8080/MySystem....).\nAny help?\nThe answer is to edit standalone.xml and insert the tag any-address instead of inet-address bound to 127.0.0.1\n<interfaces>\n    <interface name=\"management\">\n        <inet-address value=\"127.0.0.1\"/>\n    </interface>\n    <interface name=\"public\">\n       <any-ipv4-address/>\n    </interface>\n</interfaces>\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to start Sidekiq worker on Ubuntu VPS", "id": 1300, "answers": [{"answer_id": 1291, "document_id": 870, "question_id": 1300, "text": "You have to run the following command from your Rails root:\nbundle exec sidekiq -d -L sidekiq.log -q mailers,5 -q default -e production", "answer_start": 228, "answer_category": null}], "is_impossible": false}], "context": "I already setup Redis, Sidekiq and Rails app, I can access it form //url/sidekiq, but how do I start the Sidekiq worker on a VPS? On my local I do:\nbundle exec sidekiq -q carrierwave,5 default\nWhat should I do on a VPS hosting?\nYou have to run the following command from your Rails root:\nbundle exec sidekiq -d -L sidekiq.log -q mailers,5 -q default -e production\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Creating a Java application that downloads its own dependencies", "id": 1267, "answers": [{"answer_id": 1259, "document_id": 838, "question_id": 1267, "text": "You can distribute your file using the web start technology (aka distribute a jnlp file). i believe this will handle most of this functionality for you, including updatability.", "answer_start": 331, "answer_category": null}], "is_impossible": false}], "context": "I'm interested in how to distribute a Java application that has a lot of dependencies (specified in a pom.xml in Maven).\nObviously it would be possible to just package everything in one big .jar file. However that seems wasteful, since an update of the application would require sending a new copy of all the dependencies as well.\nYou can distribute your file using the web start technology (aka distribute a jnlp file). i believe this will handle most of this functionality for you, including updatability.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "app.config Transformations", "id": 460, "answers": [{"answer_id": 469, "document_id": 193, "question_id": 460, "text": "You can use the XML transformation functionality with any XML file - we do this all the time. It's available via an MSBuild task.", "answer_start": 370, "answer_category": null}], "is_impossible": false}], "context": "I'm a huge fan of the addition of web.config transformations in Visual Studio 2010. See also Scott Hanselman's recent talk at MIX2011.\nWhat sucks is that this functionality (appears at least) to only be available to web projects.\nIn our solution we have several Windows Services that connect to a different database dependant on the environment they are deployed under.\nYou can use the XML transformation functionality with any XML file - we do this all the time. It's available via an MSBuild task.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Why does npm install say I have unmet dependencies?", "id": 1901, "answers": [{"answer_id": 1888, "document_id": 1472, "question_id": 1901, "text": "You can see https://github.com/npm/npm/issues/1341#issuecomment-20634338", "answer_start": 542, "answer_category": null}], "is_impossible": false}], "context": "I have a node package. When I run npm install from the package root, it installs a bunch of things, but then prints several error messages that look like this:\nnpm WARN unmet dependency /Users/seanmackesey/google_drive/code/explore/generator/node_modules/findup-sync/node_modules/glob requires graceful-fs@'~1.2.0' but will load\nI must be confused about what exactly npm install does. If it detects a dependency, shouldn't it install it? Under what conditions does it give me error messages like this, and how can I resolve the dependencies?\nYou can see https://github.com/npm/npm/issues/1341#issuecomment-20634338\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to set the target architecture in CMake?", "id": 9, "answers": [{"answer_id": 9, "document_id": 28, "question_id": 9, "text": "The -A <arch>", "answer_start": 385, "answer_category": null}], "is_impossible": false}, {"question": "How to set the directory location of the toolset?", "id": 11, "answers": [{"answer_id": 11, "document_id": 28, "question_id": 11, "text": "The -T <toolset> option", "answer_start": 594, "answer_category": null}], "is_impossible": false}], "context": "Generates Green Hills MULTI project files (experimental, work-in-progress). Customizations are available through the following cache variables:GHS_CUSTOMIZATION,GHS_GPJ_MACROS.\n\nNew in version 3.14: The buildsystem has predetermined build-configuration settings that can be controlled via the CMAKE_BUILD_TYPE variable.\n\nCustomizations that are used to pick toolset and target system: The -A <arch> can be supplied for setting the target architecture.<arch> usually is one of arm, ppc, 86, etcetera.If the target architecture is not specified then the default architecture of arm will be used. The -T <toolset> option can be used to set the directory location of the toolset. Both absolute and relative paths are valid. Relative paths use GHS_TOOLSET_ROOT as the root. If the toolset is not specified then the latest toolset found in GHS_TOOLSET_ROOT will be used.\n\nCache variables that are used for toolset and target system customization: GHS_TARGET_PLATFORM\n\nNew in version 3.14.The following properties are available:GHS_INTEGRITY_APP,GHS_NO_SOURCE_GROUP_FILE.\n\nNote\nThis generator is deemed experimental as of CMake 3.21.3 and is still a work in progress.  Future versions of CMake\nmay make breaking changes as the generator matures.\n\n\n\n\n\n\n\n\n\n\n\n\n\nNavigation\n\n\nindex\n\nnext |\n\nprevious |\n\n\n\n\nCMake \u00bb\n\n\n3.21.3\nDocumentation \u00bb\n\ncmake-generators(7) \u00bb\nGreen Hills MULTI\n\n\n\n\u00a9 Copyright 2000-2021 Kitware, Inc. and Contributors.\nCreated using Sphinx 4.0.1.\n\n\n\n\n\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Base versus Active versus Deployment target", "id": 471, "answers": [{"answer_id": 480, "document_id": 204, "question_id": 471, "text": "The difference between the Base and Active SDK is that the former is the default SDK set for the project and the latter is the SDK you are currently building against. So it is possible for your Active SDK to be the Base SDK, at which point XCode will use the SDK you specified for the project.", "answer_start": 209, "answer_category": null}], "is_impossible": false}], "context": "I know that parts of this question was asked in several variation but I want to make sure I got it right.\nHere are my assumptions and understandings which I want to know if they are correct before submitting.\nThe difference between the Base and Active SDK is that the former is the default SDK set for the project and the latter is the SDK you are currently building against. So it is possible for your Active SDK to be the Base SDK, at which point XCode will use the SDK you specified for the project.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How do I run a Node.js application as its own process?", "id": 1653, "answers": [{"answer_id": 1641, "document_id": 1227, "question_id": 1653, "text": "Use Forever. It runs Node.js programs in separate processes and restarts them if any dies.\nUsage:\n\u2022\tforever start example.js to start a process.\n\u2022\tforever list to see list of all processes started by forever\n\u2022\tforever stop example.js to stop the process, or forever stop 0 to stop the process with index 0 (as shown by forever list)", "answer_start": 238, "answer_category": null}], "is_impossible": false}], "context": "What is the best way to deploy Node.js?\nI have a Dreamhost VPS (that's what they call a VM), and I have been able to install Node.js and set up a proxy. This works great as long as I keep the SSH connection that I started node with open.\nUse Forever. It runs Node.js programs in separate processes and restarts them if any dies.\nUsage:\n\u2022\tforever start example.js to start a process.\n\u2022\tforever list to see list of all processes started by forever\n\u2022\tforever stop example.js to stop the process, or forever stop 0 to stop the process with index 0 (as shown by forever list).\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "What is the difference between Docker Host and Container?", "id": 12, "answers": [{"answer_id": 13, "document_id": 30, "question_id": 12, "text": "n people say \u201cDocker\u201d they typically mean Docker Engine, the\nclient-server application made up of the Docker daemon, a REST API that\nspecifies interfaces for interacting with the daemon, and a command line\ninterface (CLI) client that talks to the daemon (through the REST API wrapper).\nDocker Engine accepts docker commands from the CLI, such as\ndocker run <image>, docker ps to list running containers, docker image ls\nto list images, and so on.\n\nD", "answer_start": 3140, "answer_category": null}], "is_impossible": false}], "context": "Docker Machine overview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSearch\n\n\n\n\n\n\n\nToggle navigation\n\n\n\n\n\n\n\nHomeGuidesProduct manualsReferenceSamples\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDocker Machine overviewEstimated reading time: 4 minutesYou can use Docker Machine to:\n\nInstall and run Docker on Mac or Windows\nProvision and manage multiple remote Docker hosts\nProvision Swarm clusters\n\nWhat is Docker Machine?\ud83d\udd17\nDocker Machine is a tool that lets you install Docker Engine on virtual hosts,\nand manage the hosts with docker-machine commands. You can use Machine to\ncreate Docker hosts on your local Mac or Windows box, on your company network,\nin your data center, or on cloud providers like Azure, AWS, or DigitalOcean.\nUsing docker-machine commands, you can start, inspect, stop, and restart a\nmanaged host, upgrade the Docker client and daemon, and configure a Docker\nclient to talk to your host.\nPoint the Machine CLI at a running, managed host, and you can run docker\ncommands directly on that host. For example, run docker-machine env default to\npoint to a host called default, follow on-screen instructions to complete\nenv setup, and run docker ps, docker run hello-world, and so forth.\nMachine was the only way to run Docker on Mac or Windows previous to Docker\nv1.12. Starting with the beta program and Docker v1.12,\nDocker Desktop for Mac and\nDocker Desktop for Windows are available as native apps and the\nbetter choice for this use case on newer desktops and laptops. We encourage you\nto try out these new apps. The installers for Docker Desktop for Mac and Docker Desktop for\nWindows include Docker Machine, along with Docker Compose.\nIf you aren\u2019t sure where to begin, see Get Started with Docker,\nwhich guides you through a brief end-to-end tutorial on Docker.\nWhy should I use it?\ud83d\udd17\nDocker Machine enables you to provision multiple remote Docker hosts on various\nflavors of Linux.\nAdditionally, Machine allows you to run Docker on older Mac or Windows systems,\nas described in the previous topic.\nDocker Machine has these two broad use cases.\n\n\nI have an older desktop system and want to run Docker on Mac or Windows\n\nIf you work primarily on an older Mac or Windows laptop or desktop that doesn\u2019t meet the requirements for the new Docker Desktop for Mac and Docker Desktop for Windows apps, then you need Docker Machine to run Docker Engine locally.\n\n\nI want to provision Docker hosts on remote systems\n\n\n\nDocker Engine runs natively on Linux systems. If you have a Linux box as your\nprimary system, and want to run docker commands, all you need to do is\ndownload and install Docker Engine. However, if you want an efficient way to\nprovision multiple Docker hosts on a network, in the cloud or even locally,\nyou need Docker Machine.\nWhether your primary system is Mac, Windows, or Linux, you can install Docker\nMachine on it and use docker-machine commands to provision and manage large\nnumbers of Docker hosts. It automatically creates hosts, installs Docker\nEngine on them, then configures the docker clients. Each managed host\n(\u201cmachine\u201d) is the combination of a Docker host and a configured client.\nWhat\u2019s the difference between Docker Engine and Docker Machine?\ud83d\udd17\nWhen people say \u201cDocker\u201d they typically mean Docker Engine, the\nclient-server application made up of the Docker daemon, a REST API that\nspecifies interfaces for interacting with the daemon, and a command line\ninterface (CLI) client that talks to the daemon (through the REST API wrapper).\nDocker Engine accepts docker commands from the CLI, such as\ndocker run <image>, docker ps to list running containers, docker image ls\nto list images, and so on.\n\nDocker Machine is a tool for provisioning and managing your Dockerized hosts\n(hosts with Docker Engine on them). Typically, you install Docker Machine on\nyour local system. Docker Machine has its own command line client\ndocker-machine and the Docker Engine client, docker. You can use Machine to\ninstall Docker Engine on one or more virtual systems. These virtual systems can\nbe local (as when you use Machine to install and run Docker Engine in VirtualBox\non Mac or Windows) or remote (as when you use Machine to provision Dockerized\nhosts on cloud providers). The Dockerized hosts themselves can be thought of,\nand are sometimes referred to as, managed \u201cmachines\u201d.\n\nWhere to go next\ud83d\udd17\n\nInstall Docker Machine\nCreate and run a Docker host on your local system using VirtualBox\nProvision multiple Docker hosts on your cloud provider\nGetting started with swarm mode\nUnderstand Machine concepts\nDocker Machine driver reference\nDocker Machine subcommand reference\nMigrate from Boot2Docker to Docker Machine\n\ndocker, machine, amazonec2, azure, digitalocean, google, openstack, rackspace, softlayer, virtualbox, vmwarefusion, vmwarevcloudair, vmwarevsphere, exoscaleRate this page:\u00a0129\u00a091\u00a0i\n\n\n\n\n\nDocker EngineOverviewInstallInstallation per distroInstall on CentOSInstall on DebianInstall on FedoraInstall on UbuntuInstall binariesOptional post-installation stepsRelease notesPrevious versionsEngine 18.09 release notesEngine 18.06 release notesEngine 18.05 release notesEngine 18.04 release notesEngine 18.03 release notesEngine 18.02 release notesEngine 18.01 release notesEngine 17.12 release notesEngine 17.11 release notesEngine 17.10 release notesEngine 17.09 release notesEngine 17.07 release notesEngine 17.06 release notesEngine 17.05 release notesEngine 17.04 release notesEngine 17.03 release notesEngine 1.13 and earlierDeprecated featuresDocker AppDocker BuildxDocker ContextDocker ScanDocker ComposeOverview of Docker ComposeInstall ComposeRelease notesGetting startedEnvironment variables in ComposeEnvironment fileExtend services in ComposeNetworking in ComposeUsing Compose in productionControl startup orderSample apps with ComposeDocker Compose release notesDocker DesktopOverviewMacInstall Docker Desktop for MacUser manualDeploy on KubernetesLeveraging Multi-CPU architecture supportNetworkingDisk utilizationLogs and troubleshootingFAQsStable release notesEdge release notesWindowsInstall Docker Desktop for WindowsInstall Docker Desktop on Windows HomeUser manualDeploy on KubernetesNetworkingLogs and troubleshootingFAQsStable release notesEdge release notesDocker Desktop WSL 2 backendDashboardOpen source licensingDocker HubQuickstartDocker ID accountsSecurity and authenticationManage access tokensTwo-factor authenticationEnable two-factor authenticationDisable two-factor authenticationRecover your Docker Hub accountGenerate a new recovery codeTeams and organizationsRepositoriesVulnerability scanningOfficial imagesAutomated buildsSet up automated buildsTesting in automated buildsAdvanced automated buildsLink to GitHub and BitBucketWebhooksSlack integrationDownload rate limitAdministrationConvert an account into an organizationDeactivate an account or an organizationBillingOverviewUpgrade your planDowngrade your planAdd seatsRemove seatsPublisher & certified contentOverviewSubmit a product for Docker HubUser FAQsPublisher FAQsCertify images & pluginsCertify logging pluginsTrust chainBring Your Own License (BYOL)Release notesOpen-source projectsNotaryGet started with NotaryUse the Notary clientUnderstand the service architectureRun a Notary serviceNotary changelogConfiguration filesServer configurationSigner configurationClient configurationCommon Server and signer configurationsDocker RegistryRegistry overviewUnderstand the RegistryDeploy a registry serverConfigure a registryWork with notificationsRecipesRecipes overviewAuthenticating proxy with apacheAuthenticating proxy with nginxMirroring Docker HubRunning on macOSGarbage collectionTesting an insecure registryDeprecated featuresCompatibilityGetting helpRelease notesSuperseded products and toolsDocker MachineMachine overviewInstall MachineRelease notesGet started with a local VMProvision hosts in the cloudLearn by exampleProvision Digital Ocean DropletsProvision AWS EC2 instancesMachine concepts and helpMachine (docker-machine) CLIMachine CLI overviewMachine command-line completionactiveconfigcreateenvhelpinspectipkilllsmountprovisionregenerate-certsrestartrmscpsshstartstatusstopupgradeurlMachine driversDrivers overviewDriver options and operating system defaultsAmazon Web ServicesDigital OceanExoscaleGenericGoogle Compute EngineIBM SoftlayerMicrosoft AzureMicrosoft Hyper-VOpenStackOracle VirtualBoxRackspaceVMware FusionVMware vCloud AirVMware vSphereMigrate from Boot2Docker to MachineDocker Toolbox (deprecated)KitematicKitematic user guide: intro & overviewSet up an Nginx web serverSet up a Minecraft ServerCreating a local RethinkDB database for developmentFrequently asked questionsKnown issuesTroubleshooting\n\n\n\n\n\n\n\nEdit this page Request docs changes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn this page:\n\nWhat is Docker Machine?\nWhy should I use it?\nWhat\u2019s the difference between Docker Engine and Docker Machine?\nWhere to go next\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhy Docker?\nWhat is a Container?\n\n\n\n\nProducts\nDocker Desktop\nDocker Hub\nFeatures\nContainer Runtime\nDeveloper Tools\nKubernetes\n\n\n\n\nDevelopers\nUse Cases\nPlay with Docker\nCommunity\nOpen Source\nDocker Captains\n\n\n\n\nCompany\nAbout Us\nBlog\nCustomers\nPartners\nNewsroom\nCareers\nContact Us\n\n\n\n\n\n\nStatus\nSecurity\nLegal\nContact\n\n\n\n\n\n\n\nCopyright \u00a9 2013-2020 Docker Inc. All rights reserved.\n\n\n\nTwitter\nYoutube\nGitHub\nLinkedin\nFacebook\nSlideshare\nReddit\n\n\n\n\n\n\n\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "ASP.NET MVC on IIS6", "id": 461, "answers": [{"answer_id": 470, "document_id": 194, "question_id": 461, "text": "You need to rework the base routing to include .aspx so that they will be routed through the ASP.NET ISAPI filter.", "answer_start": 461, "answer_category": null}], "is_impossible": false}], "context": "Where can I find some good pointers on best practices for running ASP.NET MVC on IIS6?\nI haven't seen any realistic options for web-hosts who provide IIS7-hosting yet. Mostly because I don't live in the U.S.\nSo I was wondering on how you best build applications in ASP.NET MVC and make it easily available to deploy on both IIS6 and IIS7. Keep in mind that this is for standard web-hosts, so there is no access to ISAPI-filters or special settings inside IIS6.\nYou need to rework the base routing to include .aspx so that they will be routed through the ASP.NET ISAPI filter.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Conda version pip install -r requirements.txt --target ./lib", "id": 1541, "answers": [{"answer_id": 1530, "document_id": 1118, "question_id": 1541, "text": "You can run conda install --file requirements.txt instead of the loop, but there is no target directory in conda install. conda install installs a list of packages into a specified conda environment.", "answer_start": 244, "answer_category": null}], "is_impossible": false}], "context": "What is the conda version of this?\npip install -r requirements.txt --target ./lib\nI've found these commands:\nwhile read requirement; do conda install --yes $requirement; done < requirements.txt\nBut it doesn't tell how to specify --target ./lib\nYou can run conda install --file requirements.txt instead of the loop, but there is no target directory in conda install. conda install installs a list of packages into a specified conda environment.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Excel Add-In - ClickOnce - VSTOInstaller.exe.config file, what is it?", "id": 1077, "answers": [{"answer_id": 1069, "document_id": 654, "question_id": 1077, "text": " You can see Microsoft support forum where the current progress can be found.", "answer_start": 603, "answer_category": null}], "is_impossible": false}], "context": "My buddy Google gave me a number of suggestions: We had the client try a repair of MS VSTO Tools for Office and a repair of Office, and had the IE cache cleared, but continued to get the same error. Finally, we tried re-naming the VSTOInstaller.exe.Config to 'hide' it from the installation and it appears to be working.\nI don't believe that our Add-In or installation produces this file, and there is no dependancies on this file that I'm aware of (it appears to be related to a SharePoint assembly)\nIt seems to be a known issue which affects many users. Someone from Microsoft has jumped on the train. You can see Microsoft support forum where the current progress can be found.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "creating an installer via command line through jenkins execute batch command", "id": 1471, "answers": [{"answer_id": 1460, "document_id": 1044, "question_id": 1471, "text": "We use Nullsoft Scriptable Install System (NSIS), and have Jenkins call a batch file in the repository to build our installer. The NSIS binaries are also stored in the repository, no need to install anything on the build server. The NSIS configuration is a text file, looking something like this (from NSIS Simple Tutorials):\n\n# name the installer\noutFile \"Installer.exe\"\n\n# default section start; every NSIS script has at least one section.\nsection\n\n# default section end\nsectionEnd", "answer_start": 975, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nWe use Jenkins for CI.\n\nI have a project that is built in Windows, using CMake 2.8.4 and VS2010 (NMake Makefiles). Once the build is complete, we manually zip up the artefacts to give to people. I would like to create an installation package via Jenkins if possible, instead of having to zip up everything. \n\nDoes anyone know of an installer that can work completely command line, so I can put the command in the Jenkins \"Execute Batch Command\" window? Has anyone done this? What installer-creator are you using? Hopefully looking for something in the free/open-source arena.\n    \n\nI've used Inno Setup through Jenkins with much success. It is a free installer for Windows. Installation files are created via scripts and command-line arguments can be used when executing these scripts to create installation files. Inno Setup scripts can be added to your source control repository for tracking changes, while also making them accessible to Jenkins.\n    \n\nWe use Nullsoft Scriptable Install System (NSIS), and have Jenkins call a batch file in the repository to build our installer. The NSIS binaries are also stored in the repository, no need to install anything on the build server. The NSIS configuration is a text file, looking something like this (from NSIS Simple Tutorials):\n\n# name the installer\noutFile \"Installer.exe\"\n\n# default section start; every NSIS script has at least one section.\nsection\n\n# default section end\nsectionEnd\n\n\nSo far it has been working perfectly. The configuration language is a little unusual, but it is not too hard to grasp and the result is stable and predictable. There are plenty of examples and tutorials in the installation and on the web site.\n    \n\nWe used WiX Toolset to create an Windows Installer package (MSI). It is open source.\n\nAll the tools are command line and we called them from an Ant script. MSI can be installed without user being involved, which could be used for testing.\n\nWiX Toolset also has a plugin to Visual Studio\u00a0\u2013 Votive.\n\nThere's a good tutorial on how to create installs using WiX.\n    \n\nWe run InstallShield as a batch command to build a Windows installer. The setup process for the packaging is entirely GUI driven, but you can build the installer package from the command line. InstallShield is not free though. \n\nThe InstallShield configuration is a binary file, so it's hard to see what's changed from build to build. \n\n\n\nUpdate: In our implementation (using InstallShield 2011), there's a Jenkins job with two parameters, ProjectConfig and ReleaseConfig. The Jenkins job checks out the InstallShield project including the Project.ism and runs:\n\n\"\\Program Files\\InstallShield\\2011\\System\\IsCmdBld.exe\" -a \"%ProjectConfig%\" -r \"%ReleaseConfig%\" -b \"%BuildDir%\" -p Project.ism\n\n\nYou can get more details on the command line options by running IsCmdBld -? or in the InstallShield documentation.  \n    \n\nYou can integrate both Inno Setup and NSIS installation systems with Jenkins using MSBuild Plugin.\n\nThere is Visual &amp; Installer (http://www.visual-installer.com) extension which allows you to create installation project (compatible with MSBuild) in Visual Studio and compile it from command line (or directly in VS). \n\nAll parameters and arguments are stored in Visual Studio solution/project files so there is no need to pass them manually in batch.\n\nAlso there is support for multiple platforms, targets and configurations everything can be defined and you can switch among them (The same work as in regular Visual Studio).\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "how to specify which Java Plug-in is used to run for browsers?", "id": 362, "answers": [{"answer_id": 369, "document_id": 164, "question_id": 362, "text": "To specify this, run Java Control Panel, go to the Advanced tab, Java Plug-in node, and toggle the check box labeled Enable the next-generation Java Plug-in. See Java Control Panel http://docs.oracle.com/javase/8/docs/technotes/guides/deploy/jcp.html#JSDPG759 for more information.", "answer_start": 2187, "answer_category": null}], "is_impossible": false}, {"question": "if I want to Upgrade java web start, do I need to uninstall the previous one?", "id": 363, "answers": [{"answer_id": 370, "document_id": 164, "question_id": 363, "text": "If you have a previous version of Java Web Start, do not uninstall it. Uninstalling it will cause the download cache to be cleared, and all previously installed Java Web Start application data will have to be downloaded again. This new version will write over previous installations and automatically update browsers to use this new version.", "answer_start": 3583, "answer_category": null}], "is_impossible": false}, {"question": "I meet \"You do not have sufficient access to remove Java-Application: name App from the Add or Remove Program list. Please contact your system administrator.\" , what shall I do?", "id": 364, "answers": [{"answer_id": 371, "document_id": 164, "question_id": 364, "text": "To avoid seeing this misleading message, either press F5 or close and reopen the dialog box. Any Java Web Start application that was downloaded and cached with the JDK or JRE will no longer appear in the list of currently installed programs.\n", "answer_start": 5548, "answer_category": null}], "is_impossible": false}, {"question": "how to disable the \"JRE out of date\" Warning?", "id": 365, "answers": [{"answer_id": 372, "document_id": 164, "question_id": 365, "text": "Starting from 7u40, a new deployment property, deployment.expiration.check.enabled is available. This property can be used to disable the JRE out of date warning.\n\nTo suppress this specific warning message, add the following entry in the deployment properties file:\n\ndeployment.expiration.check.enabled=false", "answer_start": 6328, "answer_category": null}], "is_impossible": false}, {"question": "how  to determine whether you are using a 32-bit or 64-bit version of Internet Explorer?", "id": 366, "answers": [{"answer_id": 373, "document_id": 164, "question_id": 366, "text": "Start Internet Explorer.\n\nIn the menu bar, click Help.\n\nSelect About Internet Explorer, which will open an information window.", "answer_start": 8039, "answer_category": null}], "is_impossible": false}, {"question": "how to  to determine whether you are using a 32-bit or 64-bit version of  Firefox?", "id": 367, "answers": [{"answer_id": 374, "document_id": 164, "question_id": 367, "text": "Check the About Firefox panel.\n\nType about:support in the browser address.", "answer_start": 8268, "answer_category": null}], "is_impossible": false}], "context": "6 JRE Installation for Microsoft Windows\nThis page describes how to install and uninstall JRE 8 for Windows.\n\nThe page has these topics:\n\n\"System Requirements\"\n\n\"Proxy Settings and Authentication\"\n\n\"Installation Instructions Notation\"\n\n\"Installation Instructions\"\n\n\"Uninstalling the JRE\"\n\n\"Java Start Menu\"\n\n\"Java Plug-in\"\n\n\"Java Web Start\"\n\n\"Java Update Feature\"\n\n\"Option to Disable the \"JRE out of date\" Warning\"\n\n\"Installation of JRE on 64-Bit Windows Computers\"\n\nJava Start Menu\nStarting with JDK 7u40 release, Java menu items are added to the Windows Start Menu to provide easy access to Java resources.\n\nDuring JRE install, a Java folder is created in the Windows Start Menu, which contains the following items:\n\nAbout Java: Opens an About Java window that shows the latest JRE version installed on the system.\n\nCheck for Updates: Opens the Java Control Panel with focus on the Update tab.\n\nConfigure Java: Opens the Java Control Panel with focus on the General tab.\n\nGet Help: Opens the URL http://java.com/en/download/help/index.xml.\n\nVisit Java.com: Opens the URL http://java.com/en/.\n\nDuring JRE install and uninstall processes, the appropriate start menu items are updated to be associated with the latest JRE version on the system.\n\nJava Plug-in\nJava Plug-in technology, included as part of the JRE, establishes a connection between popular browsers and the Java platform. This connection enables applets on web sites to be run within a browser on the desktop. Java Plug-in is automatically enabled for supported web browsers during installation of the JRE. No user intervention is necessary. See http://docs.oracle.com/javase/8/docs/technotes/guides/deploy/applet_dev_guide.html for more information about Java Plug-in technology.\n\n\nNote:\n\nIn Java SE 8, the version of Java Plug-in that is available in versions of the JRE prior to Java SE 6 Update 10 has been deprecated. However, this earlier version of Java Plug-in is still shipped with Java SE 8 for compatibility purposes but is no longer fully supported. It will be removed in a future release. For backward compatibility purposes, you can specify which Java Plug-in is used to run applets in the Java Control Panel. To specify this, run Java Control Panel, go to the Advanced tab, Java Plug-in node, and toggle the check box labeled Enable the next-generation Java Plug-in. See Java Control Panel http://docs.oracle.com/javase/8/docs/technotes/guides/deploy/jcp.html#JSDPG759 for more information. In addition, you can use the Java Control Panel to manage multiple JRE versions used for applet execution.\nJava Web Start\nJava Web Start is an application-deployment technology that gives you the power to run full-featured applications with a single click from your Web browser. You can now download and run applications, such as a complete spreadsheet program or an Internet chat client, without going through complicated installation procedures. With Java Web Start, you run applications simply by clicking a web page link. If the application is not present on your computer, Java Web Start automatically downloads all necessary files. It then caches the files on your computer so the application is always ready to be run anytime you want - either from an icon on your desktop or from the browser link. No matter which method you use to run the application, the most current, available version of the application is always presented to you. See Java Web Start for more information.\n\nThis topic covers the following topics:\n\n\"Upgrading from Previous Versions\"\n\n\"Uninstallation\"\n\nUpgrading from Previous Versions\nIf you have a previous version of Java Web Start, do not uninstall it. Uninstalling it will cause the download cache to be cleared, and all previously installed Java Web Start application data will have to be downloaded again. This new version will write over previous installations and automatically update browsers to use this new version. The configuration files and the program files folder used by Java Web Start have changed, but all your settings will remain intact after the upgrade because Java Web Start will translate your settings to the new form.\n\nUninstallation\nThe only way to uninstall Java Web Start is to uninstall the JDK/JRE. Uninstalling the JDK/JRE will not, however, remove the cache for previous versions of Java Web Start. Previous releases have separate uninstall instructions for Java Web Start.\n\nYou may see a misleading message if you do the following:\n\nDownload and cache a Java Web Start application with the JDK or JRE.\n\nRemove the JDK or JRE using \"Add or Remove Programs\" from the Windows Control Panel.\n\nRemove the Java Web Start application using \"Add or Remove Programs.\"\n\nWhen you remove the application, you see an \"Uninstaller Error\" dialog box saying \"An error occurred while trying to remove Java-Application: name App. It may have already been uninstalled. Would you like to remove Java-Application: name App from the Add or Remove program list?\" If you say yes to this, then you will see another \"Uninstaller Error\" dialog box saying \"You do not have sufficient access to remove Java-Application: name App from the Add or Remove Program list. Please contact your system administrator.\" This is the misleading message. It implies that the problem is due to privileges. It is not. The problem is that you have already removed the Java Web Start application when you removed JDK or JRE, but this is not reflected in the \"Add or Remove Programs\" dialog box until it is refreshed by pressing F5 or it is closed and reopened.\n\nTo avoid seeing this misleading message, either press F5 or close and reopen the dialog box. Any Java Web Start application that was downloaded and cached with the JDK or JRE will no longer appear in the list of currently installed programs.\n\nJava Update Feature\nFor issues related to the Java Update feature, see http://docs.oracle.com/javase/8/docs/technotes/guides/deploy/jcp.html#JSDPG759.\n\nOption to Disable the \"JRE out of date\" Warning\nWhen the installed JRE (7u10 or later), falls below the security baseline or passes it's built-in expiration date, an additional warning is shown to users to update their installed JRE to the latest version. For businesses that manage the update process centrally, users attempting to update their JRE individually, may cause problems.\n\nStarting from 7u40, a new deployment property, deployment.expiration.check.enabled is available. This property can be used to disable the JRE out of date warning.\n\nTo suppress this specific warning message, add the following entry in the deployment properties file:\n\ndeployment.expiration.check.enabled=false\nFor more information, see Deployment Configuration File and Properties.\n\nTo disable automatic updates, un-check the Check for Updates Automatically check box in the Update tab of the Java Control Panel.\n\nSilent Installation\nTo install the JRE silently or non-interactively, which is useful for installing on multiple computers, see \"Command-Line Installation\".\n\nInstallation of JRE on 64-Bit Windows Computers\n64-bit Windows operating systems (which may be Vista, Windows 7, or Windows 8) come with a 32-bit Internet Explorer (IE) browser as the standard (default) for viewing web pages. These operating systems also include a 64-bit Internet Explorer browser. However, using it is optional, and it must be explicitly selected to view web pages. Note that because some web content may not work properly in a 64-bit browser, it is recommended that you use the default 32-bit browser and install the 32-bit JRE.\n\nThe following topics can help you determine which JRE to install:\n\n\"Verifying Your Version of Windows\"\n\n\"Verifying Your Browser\"\n\n\"Choosing Which JRE Installer to Download\"\n\nVerifying Your Version of Windows\nSee http://windows.microsoft.com/en-us/windows/32-bit-and-64-bit-windows#1TC=windows-7 to determine if your system is running a 64-bit version of Windows.\n\nVerifying Your Browser\nFollow these steps to determine whether you are using a 32-bit or 64-bit version of Internet Explorer:\n\nStart Internet Explorer.\n\nIn the menu bar, click Help.\n\nSelect About Internet Explorer, which will open an information window.\n\nFollow one of the following methods to determine whether you are using a 64-bit version of Firefox:\n\nCheck the About Firefox panel.\n\nType about:support in the browser address.\n\nIf you running 64-bit version of Firefox, it may be indicated as 64-bit (for example, Win64); otherwise, it is a 32-bit version of Firefox.\n\n\nNote:\n\nThe 64-bit JRE is presented as a download option automatically for 64-bit Internet Explorer and 64-bit Firefox for Windows users.\nChoosing Which JRE Installer to Download\nDownload a JRE installer, depending on which browser you have installed:\n\n32-bit browser: download the 32-bit JRE installer (use either the Windows x86 Offline or Online installer).\n\n64-bit browser: download the 64-bit JRE installer.\n\nBoth 32-bit and 64-bit browsers: download both the 32-bit and 64-bit JRE installers, respectively.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Git - multiple users using the same working dir: permissions issues in .git meta-files", "id": 1292, "answers": [{"answer_id": 1284, "document_id": 863, "question_id": 1292, "text": "There are a number of ways to address this problem. Personally, I'd recommend the following in your existing repository:\n# Create a group named \"gitshare\" that will share the repository.\nsudo groupadd gitshare\n# Add yourself and as many others as you want to the group.\nsudo adduser \"$LOGNAME\" gitshare\n# Everything else needs to run from the top level of your Git repository.\ncd /path/to/repository\n# Add group permissions for *new* modifications to repository.\ngit init --shared=group\n# Fix permissions for existing repository objects.\nchown -R :gitshare \"$PWD\"\nchmod -R g+swX \"$PWD\"", "answer_start": 1454, "answer_category": null}], "is_impossible": false}], "context": "The Issue: when multiple users have access to the same working directory, permissions issues can occur in the meta-data if any git operations are performed.\nThere are some git operations that we perform as part of the script. This includes some tagging of commits (stuff gets tagged as QA-current, QA-previous, etc...). So I guess we aren't actually read-only entirely. Due to the nature of the build script, we run it sudo'ed as a common user (let's call that user DevAdmin). I realize this might be a bad practice, and is perhaps the source of the pain, since it forces us to use a shared repo checkout.\nThis would all be fine, if we were always sudo'ed when in that working dir. The issue is that on occasion, one of us will do a git pull or something similar by accident, without being sudo'ed as DevAdmin. So most of the files in .git are owned by DevAdmin (who performed the initial clone), but whenever we do this, we end up with dir's in .git/objects that contain files owned by a specific user. And these are created as group-unwritable. I've also noticed ORIG_HEAD with wrong ownership, for example. So when we try to do something as DevAdmin, we get issues.\nIs there anything that we can do to fix this issue? Right now, we have to recognize that it has happened, and then get a server admin to chown .git back to DevAdmin. Or have the user delete the meta-files in question, or at least chmod them to group-writable. This all seems very bad.\nThere are a number of ways to address this problem. Personally, I'd recommend the following in your existing repository:\n# Create a group named \"gitshare\" that will share the repository.\nsudo groupadd gitshare\n# Add yourself and as many others as you want to the group.\nsudo adduser \"$LOGNAME\" gitshare\n# Everything else needs to run from the top level of your Git repository.\ncd /path/to/repository\n# Add group permissions for *new* modifications to repository.\ngit init --shared=group\n# Fix permissions for existing repository objects.\nchown -R :gitshare \"$PWD\"\nchmod -R g+swX \"$PWD\"\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "What is the difference in maven between dependency and plugin tags in pom xml?", "id": 1897, "answers": [{"answer_id": 1884, "document_id": 1468, "question_id": 1897, "text": "You can't use compiler-plugin as a dependency since that will only add the plugin to the classpath, and will not trigger any compilation. The Jar files to be added to the classpath while compiling the file, will be specified as a dependency.", "answer_start": 240, "answer_category": null}], "is_impossible": false}], "context": "I'm new to the maven tool, I have made a project with Spring and Hibernate and they are configured in pom.xml as plugins, but JUnit is tagged under dependency. My question is what is the logic behind one as a plugin and one as dependency ?\nYou can't use compiler-plugin as a dependency since that will only add the plugin to the classpath, and will not trigger any compilation. The Jar files to be added to the classpath while compiling the file, will be specified as a dependency.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How do I deploy Node.js applications as a single executable file?", "id": 1666, "answers": [{"answer_id": 1653, "document_id": 1239, "question_id": 1666, "text": "Meanwhile I have found the (for me) perfect solution: nexe, which creates a single executable from a Node.js application including all of its modules.", "answer_start": 333, "answer_category": null}], "is_impossible": false}], "context": "Supposed I have written a Node.js application, and I now would like to distribute it. Of course, I want to make it easy for the user, hence I do not want him to install Node.js, run npm install and then manually type node app.js.\nWhat I'd prefer was a single executable file, e.g. an .exe file on Windows.\nHow could I approach this?\nMeanwhile I have found the (for me) perfect solution: nexe, which creates a single executable from a Node.js application including all of its modules.\nIt's the next best thing to an ideal solution.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Installing R on Mac - Warning messages: Setting LC_CTYPE failed, using \"C\"", "id": 1593, "answers": [{"answer_id": 1582, "document_id": 1169, "question_id": 1593, "text": "You should do the following steps:\n1.Open Terminal\n2.Write or paste in: defaults write org.R-project.R force.LANG en_US.UTF-8\n3.Close Terminal (including any RStudio window)\n4.Start R", "answer_start": 268, "answer_category": null}], "is_impossible": false}], "context": "I would like install R on my laptop Mac OS X version 10.7.3\nI downloaded the last version and I double click on it and it was installed, when i start up I get the following error, I searched in internet but I could not solve the problem, any help would be appreciated\nYou should do the following steps:\n1.Open Terminal\n2.Write or paste in: defaults write org.R-project.R force.LANG en_US.UTF-8\n3.Close Terminal (including any RStudio window)\n4.Start R\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "\"Python version 2.7 required, which was not found in the registry\" error when attempting to install netCDF4 on Windows 8", "id": 1146, "answers": [{"answer_id": 1139, "document_id": 723, "question_id": 1146, "text": "Windows 64 bit support is still work in progress at this time. The superpack will certainly not work on a 64-bits Python (but it should work fine on a 32 bits Python, even on Windows 64 bit).", "answer_start": 454, "answer_category": null}], "is_impossible": false}], "context": "I use Anaconda 1.7, 32 bit. I downloaded the correct version of the netCDF4 installer from here.\nI attempted to copy the HKEY_LOCAL_MACHINE\\SOFTWARE\\Python folder into HKEY_LOCAL_MACHINE\\SOFTWARE\\Wow6432Node. No luck.\nDoes anyone have any idea why this might be happening? Anaconda installed in the default location, C:/.\nYes, I know Anaconda has netCDF4 in the packages list - but if you look closely, it's only offered for Mac and Linux.\nShort answer: Windows 64 bit support is still work in progress at this time. The superpack will certainly not work on a 64-bits Python (but it should work fine on a 32 bits Python, even on Windows 64 bit).\nThe main issue with Windows 64 bit is that building with mingw-w64 is not stable at this point: it may be our's (NumPy developers) fault, Python's fault or mingw-w64. Most likely a combination of all those :). So you have to use proprietary compilers: anything other than the Microsoft compiler crashes NumPy randomly; for the Fortran compiler, ifort is the one to use. As of today, both NumPy and SciPy source code can be compiled with Visual Studio 2008 and ifort (all tests passing), but building it is still quite a pain, and not well supported by the NumPy build infrastructure.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Removing files when uninstalling WiX", "id": 843, "answers": [{"answer_id": 838, "document_id": 522, "question_id": 843, "text": "RemoveFile Element\nDescription\nRemove a file(s) if the parent component is selected for installation or removal. Multiple files can be removed by specifying a wildcard for the value of the Name attribute.", "answer_start": 0, "answer_category": null}], "is_impossible": false}], "context": "RemoveFile Element\nDescription\nRemove a file(s) if the parent component is selected for installation or removal. Multiple files can be removed by specifying a wildcard for the value of the Name attribute. By default, the source directory of the file is the directory of the parent component. This can be overridden by specifying the Directory attribute with a value corresponding to the Id of the source directory, or by specifying the Property attribute with a value corresponding to a property that will have a value that resolves to the full path to the source directory.\nWindows Installer references\nRemoveFile Table\nParents\nComponent\nInner Text\nNone\nChildren\nNone\nAttributes\nName\tType\tDescription\tRequired\nId\tString\tPrimary key used to identify this particular entry.\tYes\nDirectory\tString\tOverrides the directory of the parent component with a specific Directory. This Directory must exist in the installer database at creation time. This attribute cannot be specified in conjunction with the Property attribute.\t \nLongName\tWildCardLongFileNameType\tThis attribute has been deprecated; please use the Name attribute instead.\t \nName\tWildCardLongFileNameType\tThis value should be set to the localizable name of the file(s) to be removed. All of the files that match the wild card will be removed from the specified directory. The value is a filename that may also contain the wild card characters \"?\" for any single character or \"*\" for zero or more occurrences of any character. In prior versions of the WiX toolset, this attribute specified the short file name. This attribute's value may now be either a short or long file name. If a short file name is specified, the ShortName attribute may not be specified. If a long file name is specified, the LongName attribute may not be specified. Also, if this value is a long file name, the ShortName attribute may be omitted to allow WiX to attempt to generate a unique short file name. However, if you wish to manually specify the short file name, then the ShortName attribute may be specified.\tYes\nOn\tInstallUninstallType\tThis value determines the time at which the file(s) may be removed. For 'install', the file will be removed only when the parent component is being installed (msiInstallStateLocal or msiInstallStateSource); for 'uninstall', the file will be removed only when the parent component is being removed (msiInstallStateAbsent); for 'both', the file will be removed in both cases.\tYes\nProperty\tString\tOverrides the directory of the parent component with the value of the specified property. The property should have a value that resolves to the full path of the source directory. The property does not have to exist in the installer database at creation time; it could be created at installation time by a custom action, on the command line, etc. This attribute cannot be specified in conjunction with the Directory attribute.\t \nShortName\tWildCardShortFileNameType\tThe short file name of the file in 8.3 format. This attribute should only be set if you want to manually specify the short file name.\t \nSee Also\nWix Schema, CopyFile", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Create Windows Installer for Java Programs", "id": 826, "answers": [{"answer_id": 821, "document_id": 508, "question_id": 826, "text": "You can use WiX to create Windows Installer package. The package will include the JRE binaries and your compiled application (jar file).", "answer_start": 348, "answer_category": null}], "is_impossible": false}], "context": "I'm a Java beginner.\nI already created a simple GUI application that display will \"hello world\" label.\nBut, how can I create an installer from .java or .jar for windows. Let's say that I have created a useful application and want to share it with my friends to install it in their PC without they need to know what is JRE, or how to download JRE. \nYou can use WiX to create Windows Installer package. The package will include the JRE binaries and your compiled application (jar file). Upon installation, the installer unpacks your files to user's computer, creates shortcut that starts your application (app-path\\jre\\bin\\javaw.exe -jar app-path\\your-app.jar).\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "What is Google Snappy?", "id": 200694, "answers": [{"answer_id": 239867, "document_id": 357786, "question_id": 200694, "text": "a very fast compressor/decompressor", "answer_start": 2906, "answer_category": null}], "is_impossible": false}, {"question": "What does database_dir store?", "id": 200674, "answers": [{"answer_id": 239719, "document_id": 357786, "question_id": 200674, "text": "Specifies location of CouchDB database files", "answer_start": 1610, "answer_category": null}], "is_impossible": false}], "context": "\n\n\n3.2. Base Configuration\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApache CouchDB\u00ae\n\n\n\n\n\n\n\nstable\n\n\n\n\n\n\n\n\n\n\nTable of Contents\nUser Guides\n\n1. Introduction\n2. Replication\n3. Design Documents\n4. Best Practices\n\nAdministration Guides\n\n1. Installation\n2. Setup\n3. Configuration\n3.1. Introduction To Configuring\n3.2. Base Configuration\n3.2.1. Base CouchDB Options\n\n\n3.3. Configuring Clustering\n3.4. couch_peruser\n3.5. CouchDB HTTP Server\n3.6. Authentication and Authorization\n3.7. Compaction\n3.8. Background Indexing\n3.9. IO Queue\n3.10. Logging\n3.11. Replicator\n3.12. Query Servers\n3.13. Miscellaneous Parameters\n3.14. Resharding\n\n\n4. Cluster Management\n5. Maintenance\n6. Fauxton\n7. Experimental Features\n\nReference Guides\n\n1. API Reference\n2. JSON Structure Reference\n3. Query Server\n4. Partitioned Databases\n\nOther\n\n1. Release Notes\n2. Security Issues / CVEs\n3. Reporting New Security Problems with Apache CouchDB\n4. License\n5. Contributing to this Documentation\n\nQuick Reference Guides\n\nAPI Quick Reference\nConfiguration Quick Reference\n\nMore Help\n\nCouchDB Homepage\nMailing Lists\nRealtime Chat\nIssue Tracker\nDownload Docs\n\n\n\n\n\n\n\n\nApache CouchDB\u00ae\n\n\n\n\n\nDocs \u00bb\n3. Configuration \u00bb\n3.2. Base Configuration\n\nEdit on GitHub\n\n\n\n\n\n\n\n3.2. Base Configuration\u00b6\n\n3.2.1. Base CouchDB Options\u00b6\n\n\n[couchdb]\u00b6\n\n\nattachment_stream_buffer_size\u00b6\nHigher values may result in better read performance due to fewer read\noperations and/or more OS page cache hits. However, they can also\nincrease overall response time for writes when there are many\nattachment write requests in parallel.\n[couchdb]\nattachment_stream_buffer_size = 4096\n\n\n\n\n\ndatabase_dir\u00b6\nSpecifies location of CouchDB database files (*.couch named). This\nlocation should be writable and readable for the user the CouchDB\nservice runs as (couchdb by default).\n[couchdb]\ndatabase_dir = /var/lib/couchdb\n\n\n\n\n\ndefault_security\u00b6\n\nChanged in version 3.0: admin_only is now the default.\n\nDefault security object for databases if not explicitly set. When set\nto everyone, anyone can performs reads and writes. When set to\nadmin_only, only admins can read and write. When set to\nadmin_local, sharded databases can be read and written by anyone\nbut the shards can only be read and written by admins.\n\n[couchdb]\ndefault_security = admin_only\n\n\n\nenable_database_recovery\u00b6\nEnable this to only \u201csoft-delete\u201d databases when\nDELETE /{db} DELETE  requests are made. This will place\na .recovery directory in your data directory and move deleted\ndatabases/shards there instead. You can then manually delete these\nfiles later, as desired.\nDefault is false.\n[couchdb]\nenable_database_recovery = false\n\n\n\n\n\nfile_compression\u00b6\n\nChanged in version 1.2: Added Google Snappy compression algorithm.\n\nMethod used to compress everything that is appended to database and\nview index files, except for attachments (see the\nattachments section). Available methods are:\n\nnone: no compression\nsnappy: use Google Snappy, a very fast compressor/decompressor\ndeflate_N: use zlib\u2019s deflate; N is the compression level\nwhich ranges from 1 (fastest, lowest compression ratio) to 9\n(slowest, highest compression ratio)\n\n[couchdb]\nfile_compression = snappy\n\n\n\n\n\nmaintenance_mode\u00b6\nA CouchDB node may be put into two distinct maintenance modes by setting\nthis configuration parameter.\n\ntrue: The node will not respond to clustered requests from other\nnodes and the /_up endpoint will return a 404 response.\nnolb: The /_up endpoint will return a 404 response.\nfalse: The node responds normally, /_up returns a 200 response.\n\nIt is expected that the administrator has configured a load balancer\nin front of the CouchDB nodes in the cluster. This load balancer should\nuse the /_up endpoint to determine whether or not to send HTTP requests\nto any particular node. For HAProxy, the following config is\nappropriate:\nhttp-check disable-on-404\noption httpchk GET /_up\n\n\n\n\n\nmax_dbs_open\u00b6\nThis option places an upper bound on the number of databases that can\nbe open at once. CouchDB reference counts database accesses internally\nand will close idle databases as needed. Sometimes it is necessary to\nkeep more than the default open at once, such as in deployments where\nmany databases will be replicating continuously.\n[couchdb]\nmax_dbs_open = 100\n\n\n\n\n\nmax_document_size\u00b6\n\nChanged in version 3.0.0.\n\nLimit maximum document body size. Size is calculated based on the\nserialized Erlang representation of the JSON document body, because\nthat reflects more accurately the amount of storage consumed on disk.\nIn particular, this limit does not include attachments.\nHTTP requests which create or update documents will fail with error\ncode 413 if one or more documents is larger than this configuration\nvalue.\nIn case of _update handlers, document size is checked after the\ntransformation and right before being inserted into the database.\n[couchdb]\nmax_document_size = 8000000 ; bytes\n\n\n\nWarning\nBefore version 2.1.0 this setting was implemented by simply checking\nhttp request body sizes. For individual document updates via PUT\nthat approximation was close enough, however that is not the case\nfor _bulk_docs endpoint. After 2.1.0 a separate configuration\nparameter was defined: httpd/max_http_request_size,\nwhich can be used to limit maximum http request sizes. After upgrade,\nit is advisable to review those settings and adjust them accordingly.\n\n\n\n\nos_process_timeout\u00b6\nIf an external process, such as a query server or external process,\nruns for this amount of milliseconds without returning any results, it\nwill be terminated. Keeping this value smaller ensures you get\nexpedient errors, but you may want to tweak it for your specific\nneeds.\n[couchdb]\nos_process_timeout = 5000 ; 5 sec\n\n\n\n\n\nsingle_node\u00b6\n\nNew in version 3.0.0.\n\nWhen this configuration setting is set to true, automatically\ncreate the system databases on startup. Must be set false for a\nclustered CouchDB installation.\n\n\n\nuri_file\u00b6\nThis file contains the full URI that can be used to access this\ninstance of CouchDB. It is used to help discover the port CouchDB is\nrunning on (if it was set to 0 (e.g. automatically assigned any\nfree one). This file should be writable and readable for the user that\nruns the CouchDB service (couchdb by default).\n[couchdb]\nuri_file = /var/run/couchdb/couchdb.uri\n\n\n\n\n\nusers_db_security_editable\u00b6\n\nNew in version 3.0.0.\n\nWhen this configuration setting is set to false, reject any attempts\nto modify the _users database security object. Modification of this\nobject is deprecated in 3.x and will be completely disallowed in CouchDB\n4.x.\n\n\n\nusers_db_suffix\u00b6\nSpecifies the suffix (last component of a name) of the system database\nfor storing CouchDB users.\n[couchdb]\nusers_db_suffix = _users\n\n\n\nWarning\nIf you change the database name, do not forget to remove or clean\nup the old database, since it will no longer be protected by\nCouchDB.\n\n\n\n\nutil_driver_dir\u00b6\nSpecifies location of binary drivers (icu, ejson, etc.). This\nlocation and its contents should be readable for the user that runs the\nCouchDB service.\n[couchdb]\nutil_driver_dir = /usr/lib/couchdb/erlang/lib/couch-1.5.0/priv/lib\n\n\n\n\n\nuuid\u00b6\n\nNew in version 1.3.\n\nUnique identifier for this CouchDB server instance.\n[couchdb]\nuuid = 0a959b9b8227188afc2ac26ccdf345a6\n\n\n\n\n\nview_index_dir\u00b6\nSpecifies location of CouchDB view index files. This location should be\nwritable and readable for the user that runs the CouchDB service\n(couchdb by default).\n[couchdb]\nview_index_dir = /var/lib/couchdb\n\n\n\n\n\n\n\n\n\n\nNext\nPrevious\n\n\n\n\n\u00a9 Copyright 2020, Apache Software Foundation. CouchDB\u00ae is a registered trademark of the Apache Software Foundation.\n\n\nRevision 3f39035f.\n\n\n\nBuilt with Sphinx using a theme provided by Read the Docs.\n\n\n\n\n\n\n\n\nRead the Docs\nv: stable\n\n\n\n\nVersions\nmaster\nlatest\nstable\n3.1.1\n2.3.1\n1.6.1\n\n\nDownloads\npdf\nhtml\nepub\n\n\nOn Read the Docs\n\nProject Home\n\n\nBuilds\n\n\n\nFree document hosting provided by Read the Docs.\n\n\n\n\n\n\n\n\n\n\n\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Emacs markdown-mode error on preview: \"bin / bash: markdown: command not found\"", "id": 1159, "answers": [{"answer_id": 1152, "document_id": 736, "question_id": 1159, "text": "Install any markdown generating tool as you like, for example pandoc.\nThen add the following line to your .emacs file:\n(custom-set-variables\n '(markdown-command \"/usr/local/bin/pandoc\"))", "answer_start": 308, "answer_category": null}], "is_impossible": false}], "context": "I read that this is probably an issue with the path variable, so I compared the env variable by typing $ env at the command line and Esc-! env RET in emacs. I found the path variable description is the same in both cases.\nWhat is this error? How can I fix it and execute markdown previews from within emacs?\nInstall any markdown generating tool as you like, for example pandoc.\nThen add the following line to your .emacs file:\n(custom-set-variables\n '(markdown-command \"/usr/local/bin/pandoc\"))\nJust a reminder: In my case, after installing the pandoc using apt-get install , command for markdown is just ` \" pandoc\"\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": " which  variavle removes symbols information from generated binaries?", "id": 43, "answers": [{"answer_id": 45, "document_id": 57, "question_id": 43, "text": "The CMAKE_STRIP variable will contain the platform's strip utility, which removes symbols information from generated binaries", "answer_start": 554, "answer_category": null}], "is_impossible": false}], "context": "\n\n\n\n\n\n\nNinja\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNavigation\n\n\nindex\n\nnext |\n\nprevious |\n\n\n\n\nCMake \u00bb\n\n\n3.21.3\nDocumentation \u00bb\n\ncmake-generators(7) \u00bb\nNinja\n\n\n\n\n\n\n\nNinja\u00b6\nGenerates build.ninja files.\nA build.ninja file is generated into the build tree.  Use the ninja program to build the project through the all target and install the project through the install (or install/strip) target. For each subdirectory sub/dir of the project, additional targets are generated.\n\n\nNew in version 3.7: Runs the install step in the subdirectory followed by a CMAKE_STRIP command, if any.The CMAKE_STRIP variable will contain the platform's strip utility, which removes symbols information from generated binaries. CMAKE_STRIP is the file path to the platform's strip utility (e.g. /usr/bin/strip on Linux). The $<$<CONFIG:cfg>:str> generator expression will expand to str when building the configuration cfg, and to an empty string otherwise. In this scenario, this directly means \"call strip when building in Release, and do nothing otherwise\". Note that the CONFIG generator is case insensitive on your build type, and will work when using multi-config generators.\n\n\nThe Ninja generator conditionally supports Fortran when the ninjatool is at least version 1.10 (which has the required features).The Swift support is experimental, not considered stable, and may change in future releases of CMake. New in version 3.17: The Ninja Multi-Config generator is similar to the Ninja generator, but generates multiple configurations at once.\n\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "how do i find out the expiry date of a sql server 2008 trial install instance?", "id": 1009, "answers": [{"answer_id": 1004, "document_id": 615, "question_id": 1009, "text": "From installation logs\nFrom Management Studio", "answer_start": 814, "answer_category": null}], "is_impossible": false}, {"question": "how do i find out the expiry date of a sql server 2008 trial install instance from log files?", "id": 1010, "answers": [{"answer_id": 1005, "document_id": 615, "question_id": 1010, "text": "A summary.txt file is get created during the installation of SQL Server.  So this summary.txt can give you the exact date and time of the SQL Server installation.  And the trial software will automatically expire after six months.  You can find summary.txt file under\n<Drive>\\Program Files\\Microsoft SQL Server\\90\\Setup Bootstrap\\LOG\\Summary.txt\nAt the end of the file you can see something like\n\nSetup succeeded with the installation, inspect the log file completely for status on all the components.", "answer_start": 883, "answer_category": null}], "is_impossible": false}, {"question": "how do i find out the expiry date of a sql server 2008 trial install instance from Management Studio?", "id": 1011, "answers": [{"answer_id": 1006, "document_id": 615, "question_id": 1011, "text": "Open the Management Studio, click on the \u201cHelp\u201d of Menu Bar and then on \u201cAbout\u201d.  A new window will appear, where you will see some thing like:\n", "answer_start": 1580, "answer_category": null}], "is_impossible": false}, {"question": "What will happen when the install of Evaluation SQL expire?", "id": 1012, "answers": [{"answer_id": 1007, "document_id": 615, "question_id": 1012, "text": "the SQL Server services stop functioning", "answer_start": 552, "answer_category": null}], "is_impossible": false}, {"question": "How long will SQL SERVER 2008 expire?", "id": 1013, "answers": [{"answer_id": 1008, "document_id": 615, "question_id": 1013, "text": "180 days", "answer_start": 360, "answer_category": null}], "is_impossible": false}], "context": "SQL MASTER\n\nPAGES\nHome\nAbout Me\nSEARCH THIS BLOG\nPAGEVIEWS\n671306\n\n \nADS\n\n \nFRIDAY, MAY 22, 2009\nExpiration Date of SQL Server Evaluation Edition\nIf you have installed the Evaluation / Trial Version of SQL Server, you may like to find out what is the expiry date of the Trial Version.  The Trial Edition or Evaluation Edition of SQL Server will expire exactly 180 days after the install date.  Using it after the 180 days period is violation of MICROSOFT evaluation license terms.  Anyways after 180 days when an install of Evaluation Edition Expires, the SQL Server services stop functioning.  So you must purchase the license for SQL Server and upgrade the Evaluation copy before the expiry date of the Evaluation.\n\nBut then how to find out the exact expiry date of Trial Version?  There are 2 ways to find out\n\nFrom installation logs\nFrom Management Studio\nFrom installation logs\nA summary.txt file is get created during the installation of SQL Server.  So this summary.txt can give you the exact date and time of the SQL Server installation.  And the trial software will automatically expire after six months.  You can find summary.txt file under\n<Drive>\\Program Files\\Microsoft SQL Server\\90\\Setup Bootstrap\\LOG\\Summary.txt\nAt the end of the file you can see something like\n\nSetup succeeded with the installation, inspect the log file completely for status on all the components.\n\nTime : Tue Dec 30 15:59:55 2008\n\nSo after 180 days counting from 30 Dec 2008 i.e. 28 Jun 2009 the SQL Server Evaluation Edition will be expired.\n\nFrom Management Studio\nNow this is very simple. Open the Management Studio, click on the \u201cHelp\u201d of Menu Bar and then on \u201cAbout\u201d.  A new window will appear, where you will see some thing like:\n\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "bundle install doesn't work from Capistrano", "id": 479, "answers": [{"answer_id": 486, "document_id": 210, "question_id": 479, "text": "You should do the following steps:\n1) Installing the capistrano-rvm gem and adding\nrequire 'capistrano/rvm'\n2) Adding my deployment user to the rvm group on the server:\n# usermod deploy -a -G rvm\n3) Creating two directories in my deployment user's home folder: .rvm and .rvm/bin\n4) Adding this line to my deploy.rb file:\nset :default_env, { rvm_bin_path: '~/.rvm/bin' }", "answer_start": 159, "answer_category": null}], "is_impossible": false}], "context": "I want to deploy my simple rails 4.0 application via capistrano 3.0.\nI use bundler 1.3.5 so I add capistrano-bundler gem to integrate bundler with capistrano.\nYou should do the following steps:\n1) Installing the capistrano-rvm gem and adding\nrequire 'capistrano/rvm'\n2) Adding my deployment user to the rvm group on the server:\n# usermod deploy -a -G rvm\n3) Creating two directories in my deployment user's home folder: .rvm and .rvm/bin\n4) Adding this line to my deploy.rb file:\nset :default_env, { rvm_bin_path: '~/.rvm/bin' }\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Separate back-end and front-end apps on same domain?", "id": 575, "answers": [{"answer_id": 581, "document_id": 300, "question_id": 575, "text": "You are gonna to dig yourself... deep :)\nSimplest and most clean approach with no any doubt is creating a single application serving data for both, BE and FE, where you differ response (JSON vs HTML) by the URL, pseudo routes:\nGET  /products/:id          controllers.Frontend.productHtml(id)\nGET  /backend/products/:id  controllers.Backend.productJson(id)", "answer_start": 315, "answer_category": null}], "is_impossible": false}], "context": "We are building a fully RESTful back-end with the Play Framework. We are also building a separate web front-end with a different technology stack that will call the RESTful API.\nHow do we deploy both apps so they have the same domain name, with some url's used for the backend API and some for the front-end views?\nYou are gonna to dig yourself... deep :)\nSimplest and most clean approach with no any doubt is creating a single application serving data for both, BE and FE, where you differ response (JSON vs HTML) by the URL, pseudo routes:\nGET  /products/:id          controllers.Frontend.productHtml(id)\nGET  /backend/products/:id  controllers.Backend.productJson(id)\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "does intellij idea work on a 64 bit windows system", "id": 1359, "answers": [{"answer_id": 1348, "document_id": 927, "question_id": 1359, "text": "You also need to add a new environment variable IDEA_JDK_64 pointing to your 64-bit JDK so IntelliJ can use a 64-bit JDK.", "answer_start": 543, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI have downloaded and installed IDEA 13.1. However, even having a 64-bit system, Windows has automatically installed the program in the folder Program Files (x86), which is dedicated for 32-bit programs. \n\nIs there another download for the 64-bit version or maybe a special setting to make it suitable for a 64-bit system?\n    \n\nIn C:\\Program Files (x86)\\JetBrains\\${Intellij Idea version}\\bin, there is an executable called idea64.exe. This is for 64-bit systems. You can simply change your IntelliJ shortcut path to it.\n\nYou also need to add a new environment variable IDEA_JDK_64 pointing to your 64-bit JDK so IntelliJ can use a 64-bit JDK.\n\nidea64.exe uses this JDK search sequence:\n\n\nIDEA_JDK_64 environment variable\n..\\jre64 directory\nsystem Registry\nJDK_HOME environment variable\nJAVA_HOME environment variable\n\n\nMore can be found in a related IntelliJ support article.\n    \n\nIf installing latest version as of today 2016.3.2, while installing the installation wizard prompts to either choose 32-bit or 64-bit.\n    \n\nYou need to install the Java SDK 64bit so it shows the options to install IntelliJ 64bit.\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Web application monitoring best practices", "id": 1686, "answers": [{"answer_id": 1674, "document_id": 1259, "question_id": 1686, "text": "1.\tIdentify the important work-flows in your system. \n2.\t Prioritize the work-flows, and build out higher-priority tests first. \n3.\tBuild UI tests using one of the available UI testing frameworks. \n4.\tSetup at least one remote location from which to run tests. \n5.\tMake sure your solution includes both reporting and notification.", "answer_start": 1377, "answer_category": null}], "is_impossible": false}], "context": "We are finishing up our web application and planning for deployment. Very important aspect of deployment to production is monitoring the health of the system. Having a small team of developers/support makes it very critical for us to get the early notifications of potential problems and resolve them before they have impact on users.\nUsing Nagios seams like a good option, but wanted to get more opinions on what are the best monitoring tools/practices for web application in general and specifically for Django app? Also would welcome recommendations on what should be monitored aside from the obvious CPU, memory, disk space, database connectivity.\nOur web app is written in Django, we are running on Linux (Ubuntu) under Apache + Fast CGI with PostgreSQL database.\nEDIT We have a completely virtualized environment under Linode.\nEDIT We are using django-logging so we have a way separate info, errors, critical issues, etc.\nIf I had to pick one type of testing it would be to test the end-user functionality of the system. The important thing to consider is the user. While testing things like database availability, server up-time, etc, are all important, testing work-flows through your system via a remote UI testing system covers all these bases. If you know that the critical parts of your system are available to the end-user, then you know your system is prolly Ok.\n1.\tIdentify the important work-flows in your system. \n2.\t Prioritize the work-flows, and build out higher-priority tests first. \n3.\tBuild UI tests using one of the available UI testing frameworks. \n4.\tSetup at least one remote location from which to run tests. \n5.\tMake sure your solution includes both reporting and notification.\nThis end-user testing should not eliminate monitoring of system in your data-center, but I want to reiterate that end-user testing is the most important type of testing you can do for a web application.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Solving the 'npm WARN saveError ENOENT: no such file or directory, open '/Users/<username>/package.json'' error", "id": 725, "answers": [{"answer_id": 728, "document_id": 415, "question_id": 725, "text": "You don't have a package.json > use npm init\n\u2022\tYou are in the wrong directory > cd to the folder where your package.json is, like so:\ncd C://Dev/MySolution/MyWebProject and then try again.", "answer_start": 491, "answer_category": null}], "is_impossible": false}], "context": "I'm a newbie so please include links to URLs or explain terminologies so I can understand.\nI've managed to install 'npm' on a Mac OS (10.13.3) via the terminal, and have installed some packages like SASS using it.\nI'm now trying to install sass-mq using npm. I think I've managed to install it, but I'd like a second opinion on what I might have done that was incomplete, or wrong while doing it.\nnpm WARN saveError ENOENT: no such file or directory, open '/Users/{username}/package.json'\n\u2022\tYou don't have a package.json > use npm init\n\u2022\tYou are in the wrong directory > cd to the folder where your package.json is, like so:\ncd C://Dev/MySolution/MyWebProject and then try again.\nAbulifa's answer explains that your project could be published as it's own npm package... In that scenario, these warnings would help warn that your package.json is missing some fields.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How do I arrange a silent (unattended) Java 8 installation with command line?", "id": 155, "answers": [{"answer_id": 162, "document_id": 96, "question_id": 155, "text": "Command-Line Installation\nThis section describes the command-line options for the JRE Windows Offline Installer. Run the installer as follows:\n\njre [INSTALLCFG=configuration_file_path] [options]", "answer_start": 935, "answer_category": null}], "is_impossible": false}, {"question": "How do I arrange a silent (unattended) Java 7 installation with command line?", "id": 156, "answers": [{"answer_id": 163, "document_id": 96, "question_id": 156, "text": "Command Line Install\nThe Java SE 7 Windows Offline Installer command has the following syntax:\n\n<jre>.exe [/s] [INSTALLDIR=<drive>:\\<JRE_install_path>] [STATIC=1] [WEB_JAVA=0/1] [WEB_JAVA_SECURITY_LEVEL=VH/H/M] [SPONSORS=0]", "answer_start": 1758, "answer_category": null}], "is_impossible": false}, {"question": "how to Creat a Log File when I install java?", "id": 157, "answers": [{"answer_id": 164, "document_id": 96, "question_id": 157, "text": "The following is an example of creating a log file:\n\njre-8-windows-i586.exe /s /L C:\\<path>setup.log", "answer_start": 4379, "answer_category": null}], "is_impossible": false}], "context": "This article applies to:\nPlatform(s): Windows 10, Windows 2008 Server, Windows 7, Windows 8, Windows Server 2012, Windows Vista, Windows XP\nJava version(s): 7.0, 8.0\nThis page describes options for installation of the Java SE Runtime Environment (JRE) on Windows 32-bit platform. It is intended for:\n\nSystem administrators deploying the JRE with Java Plug-in and Java Web Start technologies on multiple PCs in their Intranet without user interaction.\n\nVendors having products requiring the JRE. The JRE can be silently (non-interactively from the command line) installed with their product.\n\nJRE installers are built using Microsoft Window Installer (MSI) 2.0 technology. MSI contains built-in support for silent installations or unattended installations. This document explains how to manually install the JRE using the .exe file that runs the MSI.\n\nJava 8\nSee Windows JRE 8 installer options (docs.oracle.com) for more information.\n\nCommand-Line Installation\nThis section describes the command-line options for the JRE Windows Offline Installer. Run the installer as follows:\n\njre [INSTALLCFG=configuration_file_path] [options]\n\njre refers to the JRE Windows Offline Installer base file name (for example, jre-8u05-windows-i586.exe).\nINSTALLCFG=configuration_file_path specifies the path of the installer configuration file.\nSee Installing With a Configuration File (docs.oracle.com) for more information.\noptions are options with specified values separated by spaces. Use the same options as listed in Table 20-1, Configuration File Options (docs.oracle.com). In addition, you may use the option /s for the JRE Windows Offline Installer to perform a silent installation.\nJava 7\nSee JRE 7 Windows installer options (docs.oracle.com) for more information.\n\nCommand Line Install\nThe Java SE 7 Windows Offline Installer command has the following syntax:\n\n<jre>.exe [/s] [INSTALLDIR=<drive>:\\<JRE_install_path>] [STATIC=1] [WEB_JAVA=0/1] [WEB_JAVA_SECURITY_LEVEL=VH/H/M] [SPONSORS=0]\n\nNote\n<jre>.exe is the single executable installer for the JRE.\n/s if used, indicates a silent installation.\nINSTALLDIR if used, specifies the drive and path of the JRE. If INSTALLDIR is not specified, the installation will go into C:\\Program Files\\java\\jre (default location).\nSTATIC=1 if used, specifies a static installation. For more information about static installations, see Static Installation in Patch-in-Place and Static JRE Installation (docs.oracle.com).\nWEB_JAVA=0 if used, disables any Java application from running in the browser. WEB_JAVA=1 the default, enables Java applications in the browser. This field is available as of the 7u10 release. For more information, see Setting the Security Level of the Java Client (docs.oracle.com).\nWEB_JAVA_SECURITY_LEVEL if used, sets the security level of unsigned Java apps running in a browser. The possible values for this field are VH (very high), H (high, the default), or M (medium). This field is available as of the 7u10 release. For more information, see Setting the Security Level of the Java Client (docs.oracle.com).\nSPONSORS=0 if used, entirely bypasses sponsor offers such as browser add-ons. This field is available as of the 7u55 release. Note that sponsor offers, and therefore this functionality, is only applicable to online 32 bit JRE installers and Auto Update mechanisms.\nExample 1\nSuppose the JRE installer is jre-7-windows-i586.exe and you want to install the following configuration:\nPerform a Windows Installation\nInstall the JRE core, additional fonts, colors, and Soundbank\nThe command to install the above mentioned configuration is as follows:\njre-7-windows-i586.exe /s\n\nExample 2\nSuppose the JRE installer is jre-7-windows-i586.exe and you want to install the following configuration:\nPerform a Windows Offline Installation and install the JRE on D drive at java\\jre\nHave all features of the JRE installed\nThe command to install the above mentioned configuration is as follows:\njre-7-windows-i586.exe /s INSTALLDIR=D:\\java\\jre\n\nNote: To keep the MS-DOS window open, until the installation of Java is complete use the start /w command as follows:\nstart /w jre-7-windows-i586.exe /s\n\nCreating a Log File\nUse a log file to verify that an installation succeeded. To create a log file describing the installation, append /L C:\\<path>setup.log to the install command and scroll to the end of the log file to verify.\n\nThe following is an example of creating a log file:\n\njre-8-windows-i586.exe /s /L C:\\<path>setup.log\n\nThis example causes the log to be written to the setup.log file.", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "install a vimball from the command line", "id": 1370, "answers": [{"answer_id": 1359, "document_id": 939, "question_id": 1370, "text": "hide the messiness in a script that always should have existed", "answer_start": 1273, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nAs this post points out you can install Vimballs using the normal:\n\nvim somevimball.vba\n:so %\n:q\n\n\nBut if you want to install a from the command line how do you do it? I ran a 'man vim' and it seems like the best \"from source install\" option was the '-S' option so I tried to install haskellmode with it:\n\nwget 'http://projects.haskell.org/haskellmode-vim/vimfiles/haskellmode-20090430.vba'\nvim -S haskellmode-20090430.vba    \n\n\nand that failed to work. It gave me the following error:\n\n\n  Error detected while processing function vimball#Vimball:\n  line   10:\n  (Vimball) The current file does not appear to be a Vimball!\n  press ENTER or type command to continue  \n\n\nIt should be noted that using the first method I was able to successfully install the vimball. I have tried the second method on a few other vimballs and it has failed every time. Is there a way to install a vimball from the command line? It seems like a useful sort of task. \n\nOh, and I am running the following version of vim:\n\nVersion: 2:7.2.330-1ubuntu3\n\n\nThanks.\n    \n\nUse one of the following commands:\n\nvim -c 'so %' -c 'q' somevimball.vba\n\n\nor:\n\nvim -c 'so % | q' somevimball.vba\n\n\nFor more information, see:\n\n:help -c\n:help :bar\n\n    \n\nHaving seen this solution I decided to hide the messiness in a script that always should have existed, vim-install: http://github.com/robertmassaioli/vim-install\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How do I delete a Git branch locally and remotely?", "id": 1846, "answers": [{"answer_id": 1832, "document_id": 1417, "question_id": 1846, "text": "To delete the local branch you should use one of the following:\n$ git branch -d <branch_name>\n$ git branch -D <branch_name>", "answer_start": 165, "answer_category": null}], "is_impossible": false}], "context": "I want to delete a branch both locally and remotely.\nWhat should I do differently to successfully delete the remotes/origin/bugfix branch both locally and remotely?\nTo delete the local branch you should use one of the following:\n$ git branch -d <branch_name>\n$ git branch -D <branch_name>\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Installing JDK without administrator privileges", "id": 1131, "answers": [{"answer_id": 1124, "document_id": 708, "question_id": 1131, "text": "download the \"Server JRE\" from Java download site\n\u2022\textract the .tar.gz\n\u2022\tadd the bin subdirectory to your PATH", "answer_start": 500, "answer_category": null}], "is_impossible": false}], "context": "I am trying to install JDK at office laptop but it says I need administrator privileges. I have only access to my own account at work.\nHow can I install the Java Development Kit without administrator rights?\nStarting with Java SE 7u21, Oracle offers a so-called Server JRE for download. This is a standard .tar.gz archive that you can extract anywhere on your computer. Although the download is called JRE, it contains a \"normal\" JDK subdirectory (including the javac compiler etc.).\nInstructions:\n\u2022\tdownload the \"Server JRE\" from Java download site\n\u2022\textract the .tar.gz\n\u2022\tadd the bin subdirectory to your PATH\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "how to specify verified site to a chrome app", "id": 2001, "answers": [{"answer_id": 1987, "document_id": 1587, "question_id": 2001, "text": "'none';\n        }\n      &lt;/script&gt;\n    &lt;/div&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n\n    \n\nIt's not about your display name.\n\nYou need to:\n\n\nGo to the Webmaster Tools.\nAdd the site to your sites.\nObtain and embed a verification code into your site.\nComplete verification in Webmaster Tools.\nGo to your Developer Dashboard (must be", "answer_start": 1913, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI'm trying to enable inline installs, but continue to get the following error\n\nInline installs can only be initiated for Chrome Web Store items that have one or more verified sites\n\n\nI've verified ownership of my test site via both a name tag and a .html file.  Is it a problem that this test site is also owned by my other email (for analytics)?  This is all being tested in Canary, where I'm signed in with a work email.\n\nIn the Developer Dashboard, I've changed my display name to be the site I verified (e.g. www.example.com).  I've also enabled the checkbox for This item uses inline install.\n\nIt's not clear how I associate the site I own with the app I publish on the chrome web store.\n\nIt seems like I'd need to specify the site I own in the manifest file?\n\nIn my www.mydomain.com/testfile.html:\n\n&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n  &lt;head&gt;\n    &lt;title&gt;blu&lt;/title&gt;\n    &lt;meta charset=\"UTF-8\" /&gt;\n    &lt;link rel=\"shortcut icon\" href=\"./img/favicon.png\"&gt;\n\n    &lt;link rel=\"chrome-webstore-item\" href=\"https://chrome.google.com/webstore/detail/myAppIdNumber\"&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n    &lt;div style=\"width: 600px; margin: 40px auto;\"&gt;\n      &lt;h1&gt;\n        Example Install Page\n      &lt;/h1&gt;\n\n      &lt;p&gt;\n        You can initiate app and extension installations \"inline\" from your site. These apps and extensions are still hosted in the &lt;a href=\"https://chrome.google.com/webstore/category/apps\"&gt;Chrome Web Store&lt;/a&gt;, but users no longer have to leave your site to install them.\n      &lt;/p&gt;\n\n      &lt;button onclick=\"chrome.webstore.install(undefined, undefined, function(message) { console.log(message); });\" id=\"install-button\"&gt;\n        Add to Chrome\n      &lt;/button&gt;\n\n      &lt;script&gt;\n        if (chrome.app.isInstalled) {\n          document.getElementById('install-button').style.display = 'none';\n        }\n      &lt;/script&gt;\n    &lt;/div&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n\n    \n\nIt's not about your display name.\n\nYou need to:\n\n\nGo to the Webmaster Tools.\nAdd the site to your sites.\nObtain and embed a verification code into your site.\nComplete verification in Webmaster Tools.\nGo to your Developer Dashboard (must be under the same Google account) and edit your Web Store item.\nSelect your site in \"Verify that this is an official item for a website you own:\"\n\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Unable to locate package mongodb-org", "id": 641, "answers": [{"answer_id": 646, "document_id": 334, "question_id": 641, "text": "sudo apt-get install -y mongodb", "answer_start": 574, "answer_category": null}], "is_impossible": false}], "context": "I am trying to download mongodb and I am following the steps on this link.But when I get to the step:sudo apt-get install -y mongodb-org.I get the following error:Reading package lists... DoneBuilding dependency tree       Reading state information... Done E: Unable to locate package mongodb-org  //This is the error. The true problem here may be if you have a 32-bit system. MongoDB 3.X was never made to be used on a 32-bit system, so the repostories for 32-bit is empty (hence why it is not found). Installing the default 2.X Ubuntu package might be your best bet with:\nsudo apt-get install -y mongodb .Another workaround, if you nevertheless want to get the latest version of Mongo:\nYou can go to https://www.mongodb.org/downloads and use the drop-down to select \"Linux 32-bit legacy\"\nBut it comes with severe limitations...\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "wheel file installation?", "id": 898, "answers": [{"answer_id": 893, "document_id": 565, "question_id": 898, "text": "To install a wheel file, use pip:\n\n$ pip install someproject-1.5.0-py2-py3-none.whl", "answer_start": 1966, "answer_category": null}], "is_impossible": false}], "context": "User Guide\nBuilding Wheels\nBuilding wheels from a setuptools based project is simple:\n\npython setup.py bdist_wheel\nThis will build any C extensions in the project and then package those and the pure Python code into a .whl file in the dist directory.\n\nIf your project contains no C extensions and is expected to work on both Python 2 and 3, you will want to tell wheel to produce universal wheels by adding this to your setup.cfg file:\n\n[bdist_wheel]\nuniversal = 1\nIncluding license files in the generated wheel file\nSeveral open source licenses require the license text to be included in every distributable artifact of the project. By default, wheel conveniently includes files matching the following glob patterns in the .dist-info directory:\n\nAUTHORS*\nCOPYING*\nLICEN[CS]E*\nNOTICE*\nThis can be overridden by setting the license_files option in the [metadata] section of the project\u2019s setup.cfg. For example:\n\n[metadata]\nlicense_files =\n   license.txt\n   3rdparty/*.txt\nNo matter the path, all the matching license files are written in the wheel in the .dist-info directory based on their file name only.\n\nBy specifying an empty license_files option, you can disable this functionality entirely.\n\nNote\n\nThere used to be an option called license_file (singular). As of wheel v0.32, this option has been deprecated in favor of the more versatile license_files option.\n\nConverting Eggs to Wheels\nThe wheel tool is capable of converting eggs to the wheel format. It works on both .egg files and .egg directories, and you can convert multiple eggs with a single command:\n\nwheel convert blah-1.2.3-py2.7.egg foo-2.0b1-py3.5.egg\nThe command supports wildcard expansion as well (via iglob()) to accommodate shells that do not do such expansion natively:\n\nwheel convert *.egg\nBy default, the resulting wheels are written to the current working directory. This can be changed with the --dest-dir option:\n\nwheel convert --dest-dir /tmp blah-1.2.3-py2.7.egg\nInstalling Wheels\nTo install a wheel file, use pip:\n\n$ pip install someproject-1.5.0-py2-py3-none.whl", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "ValueError: numpy.dtype has the wrong size, try recompiling", "id": 683, "answers": [{"answer_id": 688, "document_id": 376, "question_id": 683, "text": "For me (Mac OS X Maverics, Python 2.7)\neasy_install --upgrade numpy\nhelped. After this you can install up-to-date packages pandas, scikit-learn, e.t.c. using pip:\npip install pandas", "answer_start": 1361, "answer_category": null}], "is_impossible": false}], "context": "Numpy developers follow in general a policy of keeping a backward compatible binary interface (ABI). However, the ABI is not forward compatible.\nWhat that means:\nA package, that uses numpy in a compiled extension, is compiled against a specific version of numpy. Future version of numpy will be compatible with the compiled extension of the package (for exception see below). Distributers of those other packages do not need to recompile their package against a newer versions of numpy and users do not need to update these other packages, when users update to a newer version of numpy.\nHowever, this does not go in the other direction. If a package is compiled against a specific numpy version, say 1.7, then there is no guarantee that the binaries of that package will work with older numpy versions, say 1.6, and very often or most of the time they will not.\nThe binary distribution of packages like pandas and statsmodels, that are compiled against a recent version of numpy, will not work when an older version of numpy is installed. Some packages, for example matplotlib, if I remember correctly, compile their extensions against the oldest numpy version that they support. In this case, users with the same old or any more recent version of numpy can use those binaries.\nThe error message in the question is a typical result of binary incompatibilities.\nFor me (Mac OS X Maverics, Python 2.7)\neasy_install --upgrade numpy\nhelped. After this you can install up-to-date packages pandas, scikit-learn, e.t.c. using pip:\npip install pandas\n\nThe solution is to get a binary compatible version, either by updating numpy to at least the version against which pandas or statsmodels were compiled, or to recompile pandas and statsmodels against the older version of numpy that is already installed.\nBreaking the ABI backward compatibility:\nSometimes improvements or refactorings in numpy break ABI backward compatibility. This happened (unintentionally) with numpy 1.4.0. As a consequence, users that updated numpy to 1.4.0, had binary incompatibilities with all other compiled packages, that were compiled against a previous version of numpy. This requires that all packages with binary extensions that use numpy have to be recompiled to work with the ABI incompatible version.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "[Microsoft][ODBC Driver Manager] Data source name not found and no default driver specified", "id": 686, "answers": [{"answer_id": 691, "document_id": 379, "question_id": 686, "text": "The Problem might be from the driver name for example instead of DRIVER={MySQL ODBC 5.3 Driver} try DRIVER={MySQL ODBC 5.3 Unicode Driver} you can see the name of the driver from administration tool.", "answer_start": 857, "answer_category": null}], "is_impossible": false}], "context": "I am trying to open a program for the first time on Windows XP Pro that uses PostgreSQL 9. I'm getting an error message that says :\nA problem was encountered while trying to log into or create the production database. Details: [Microsoft][ODBC Driver Manager] Data source name not found and no default driver specified\nIn my ODBC manager, I have a list of User DSN's and System DSN's. I tried installing a postgres odbc driver to see if that would help, but it didn't.\nThere is a connect.dat file in the program file with a line saying \"OLE DB Provider = MSDASQL\". Changing this entry alters the error message I get to \"Provider cannot be found, it may not be properly installed\".\nI don't know what provider name to insert to get this to work properly. I have done extensive research on this error to no avail. Any suggestions would be greatly appreciated. The Problem might be from the driver name for example instead of DRIVER={MySQL ODBC 5.3 Driver} try DRIVER={MySQL ODBC 5.3 Unicode Driver} you can see the name of the driver from administration tool.The problem is the above driver only is 32 bit. I had switched visual studio testsettings file to 64 bit to test a 64-bit-only application.Switching back to 32 bit in the testsettings file fixed the issue.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Capistrano and deployment of a website from Github", "id": 1256, "answers": [{"answer_id": 1248, "document_id": 827, "question_id": 1256, "text": "To disable asset timestamps updates, simply add:\n set :normalize_asset_timestamps, false", "answer_start": 290, "answer_category": null}], "is_impossible": false}], "context": "So, I had what I thought was a fairly simple Capistrano use case: I want to deploy a PHP site from Github. But I'm running into a lot of problems. When I run cap deploy, Capistrano is able to clone the Github repo (the deploy:update_code step), but then in the deploy:finalize_update step.\nTo disable asset timestamps updates, simply add:\n set :normalize_asset_timestamps, false\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Circular import dependency in Python", "id": 1822, "answers": [{"answer_id": 1808, "document_id": 1393, "question_id": 1822, "text": "You may defer the import, for example in a/__init__.py:\ndef my_function():\n    from a.b.c import Blah\n    return Blah()", "answer_start": 280, "answer_category": null}], "is_impossible": false}], "context": "In the a package's __init__.py, the c package is imported. But c_file.py imports a.b.d.\nThe program fails, saying b doesn't exist when c_file.py tries to import a.b.d. (And it really doesn't exist, because we were in the middle of importing it.)\nHow can this problem be remedied?\nYou may defer the import, for example in a/__init__.py:\ndef my_function():\n    from a.b.c import Blah\n    return Blah()\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How do you deploy a website to your webservers?", "id": 1268, "answers": [{"answer_id": 1260, "document_id": 839, "question_id": 1268, "text": "You should consider using branching and merging for individual projects (on the same codebase), if they make huge changes to the shared codebase.", "answer_start": 668, "answer_category": null}], "is_impossible": false}], "context": "At my company we have a group of 8 web developers for our business web site (entirely written in PHP, but that shouldn't matter). Everyone in the group is working on different projects at the same time and whenever they're done with their task, they immediately deploy it (cause business is moving fast these days).\nCurrently the development happens on one shared server with all developers working on the same code base (using RCS to \"lock\" files away from others). When deployment is due, the changed files are copied over to a \"staging\" server and then a sync script uploads the files to our main webserver from where it is distributed over to the other 9 servers.\nYou should consider using branching and merging for individual projects (on the same codebase), if they make huge changes to the shared codebase.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Prerequisites for installing WxHaskell on windows?", "id": 1027, "answers": [{"answer_id": 1022, "document_id": 618, "question_id": 1027, "text": "You will need wx-config for Windows, MinGW 5.1.6 and MSYS 0.1.11 (these are the latest versions at the time of writing). You must select the C++ compiler option when installing MinGW.\nMinGW 5.1.6 can be downloaded from MinGW\nMSYS 0.1.11 can be downloaded from MSYS\nwx-config can be downloaded from wx-config", "answer_start": 1366, "answer_category": null}], "is_impossible": false}, {"question": "Prerequisites for installing WxHaskell on MAC OS X?", "id": 1028, "answers": [{"answer_id": 1023, "document_id": 618, "question_id": 1028, "text": "you need to install the gcc compiler, which is part of the Apple Developer Tools. These tools are shipped with Panther and are installed by invoking Applications/Installers/Developer Tools/Developer.mdmg.\nYou should install the current stable version (2.8.10) of wxWidgets for your platform - versions older than 2.8.1 are no longer supported. Build instructions are given for Windows - on most other platforms you should be able to obtain wxWidgets using your package manager (you may need -developer packages).", "answer_start": 1684, "answer_category": null}], "is_impossible": false}, {"question": "How to get  wxWidgets on windows?", "id": 1029, "answers": [{"answer_id": 1024, "document_id": 618, "question_id": 1029, "text": "You can use the wxPack prebuilt binaries (the MinGW ones)\nAlternatively, you can build using the MinGW compiler - instructions below", "answer_start": 2403, "answer_category": null}], "is_impossible": false}, {"question": "How to get  wxWidgets on MAC OS?", "id": 1030, "answers": [{"answer_id": 1025, "document_id": 618, "question_id": 1030, "text": "Tiger - do not use the wxMac 2.5 that comes pre-installed.\nLeopard - wxHaskell now supports wxWidgets 2.8", "answer_start": 2546, "answer_category": null}], "is_impossible": false}, {"question": "How to get  wxWidgets on Linux?", "id": 1031, "answers": [{"answer_id": 1026, "document_id": 618, "question_id": 1031, "text": "the wxWidgets that ships with your system (as long as it's the 2.8 one and not the 2.6 one) should work.\nBackport of wxWidgets 2.8 for Debian and Ubuntu\nwxGTK - wxHaskell supports wxGTK.\nwxX11 - wxHaskell does not support wxX11 now. Use wxGTK instead.\nSee the wxWidgets site for more details", "answer_start": 2661, "answer_category": null}], "is_impossible": false}, {"question": "How to build wxWidgets on windows?", "id": 1032, "answers": [{"answer_id": 1027, "document_id": 618, "question_id": 1032, "text": "Using the MSYS shell, and making sure that you have the C++ compiler option installed for MinGW as it is 'not' the default:\n\n> cd /c/path/to/wxWidgets-2.8.10/build/msw\n> mingw32-make -f makefile.gcc BUILD=release MONOLITHIC=1 SHARED=1 UNICODE=1\nThis will take quite some time and generate lots of text in the MSYS shell. Assuming it is successful, a DLL is generated in /c/path/to/wxWidgets/gcc_dll, which you will need to copy to a location where it can be found when running your executables", "answer_start": 3490, "answer_category": null}], "is_impossible": false}, {"question": "How to build wxWidgets on Unix system?", "id": 1033, "answers": [{"answer_id": 1028, "document_id": 618, "question_id": 1033, "text": "On most Unix platforms, wxWidgets build goes something like: create a mybuild directory in the wxWidgets directory, and run configure and make from that directory", "answer_start": 4249, "answer_category": null}], "is_impossible": false}, {"question": " I've found that if you put wx-config in some system directories, such as c:\\windows\\system32, 'cabal install wx' will complain \"wx-config: does not exist\"", "id": 1034, "answers": [{"answer_id": 1029, "document_id": 618, "question_id": 1034, "text": " To put it in another directory in %PATH% will solve this problem", "answer_start": 5050, "answer_category": null}], "is_impossible": false}, {"question": "Compiling wxcore out of memory error?", "id": 1035, "answers": [{"answer_id": 1030, "document_id": 618, "question_id": 1035, "text": "Try something like:\n\n> export GHCRTS='-M512m'\nto allocate 512 MB to the GHC RTS. This has been reported to be sufficient", "answer_start": 6401, "answer_category": null}], "is_impossible": false}, {"question": "How to test wxhaskell?", "id": 1036, "answers": [{"answer_id": 1031, "document_id": 618, "question_id": 1036, "text": "> cd samples/wx\n> ghc -package wx -o helloworld HelloWorld.hs\n> ./helloworld", "answer_start": 7077, "answer_category": null}], "is_impossible": false}, {"question": "How test wxhaskell on Unix?", "id": 1037, "answers": [{"answer_id": 1032, "document_id": 618, "question_id": 1037, "text": "> ghci -package wx BouncingBalls.hs\n> main", "answer_start": 7381, "answer_category": null}], "is_impossible": false}], "context": "Quick start\nGetting wxHaskell is easy on MacOS X and Linux. It's slightly less easy on Windows.\n\nBasically,\n\nGet g++ (MinGW on Windows, Developer Tools on Mac)\nGet wxWidgets\nBuild wxHaskell\n> cabal install wx\nSee the platform specific quick starts for more details:\n\nWindows\nMacOS X\nLinux\nMuch of the information on the rest of this page is obsolete\n\nSupported Configurations\nThe build process has been tested against three main platforms:\n\nunix-gtk. General Unix systems with GTK.\nmacosx. Mac OS X.\nwindows. General Windows systems (i.e. Windows 95 to Windows 8.1).\n Please note that we have discontinued support for Microsoft compilers in the build system.\nwxHaskell has been built successfully on (at least) the following configurations:\n\nwindows. Windows 98, 2000, XP, and 7, using wxMSW 2.8.x. Windows 8.1, using wxWidgets 2.9 and 3.0\nmacosx. Mac OS X 10.5 (Leopard) and 10.6 with wxMAC 2.8.x.\nunix-gtk. Red Hat Linux 10 (Fedora), FreeBSD, and Gentoo Linux, using wxGTK 2.8.x.\n(Unfortunately, there are still build problems on Sun Solaris \u2013 we are looking for build volunteers :-)\n\nPrerequisites\nEnsure you have a recent GHC compiler \u2013 version 6.10.3 or higher is recommended. In principle, any Haskell 98 compiler that supports the standard FFI libraries along with cabal install should work. We have discontinued support for building without cabal.\n\nWindows: You will need wx-config for Windows, MinGW 5.1.6 and MSYS 0.1.11 (these are the latest versions at the time of writing). You must select the C++ compiler option when installing MinGW.\nMinGW 5.1.6 can be downloaded from MinGW\nMSYS 0.1.11 can be downloaded from MSYS\nwx-config can be downloaded from wx-config\nMac OS X: you need to install the gcc compiler, which is part of the Apple Developer Tools. These tools are shipped with Panther and are installed by invoking Applications/Installers/Developer Tools/Developer.mdmg.\nYou should install the current stable version (2.8.10) of wxWidgets for your platform - versions older than 2.8.1 are no longer supported. Build instructions are given for Windows - on most other platforms you should be able to obtain wxWidgets using your package manager (you may need -developer packages).\n\nWe assume in this guide that the variable $wxwin points to your wxWidgets installation directory, for example: ~/dev/wxGTK-2.8.10.\n\nGetting wxWidgets\nwxHaskell 0.12.x.x supports wxWidgets > 2.8.\n\nWindows:\nYou can use the wxPack prebuilt binaries (the MinGW ones)\nAlternatively, you can build using the MinGW compiler - instructions below.\nMac OS X\nTiger - do not use the wxMac 2.5 that comes pre-installed.\nLeopard - wxHaskell now supports wxWidgets 2.8.\nLinux - the wxWidgets that ships with your system (as long as it's the 2.8 one and not the 2.6 one) should work.\nBackport of wxWidgets 2.8 for Debian and Ubuntu\nwxGTK - wxHaskell supports wxGTK.\nwxX11 - wxHaskell does not support wxX11 now. Use wxGTK instead.\nSee the wxWidgets site for more details.\n\nBuilding wxWidgets (usually optional)\nOn Windows\nNote: Microsoft compilers are no longer supported. The cabal build system cannot understand the output of wx-config for Microsoft compilers.\n\nNote: The Haskell Platform 2010.1.0.0 installer shipped without C++ support. If you want to build wxHaskell on Windows using this Haskell Platform, please see [1]\n\nNote: At the time of this writing there is a MinGW bug that may prevent you from building wxWidgets. The solution for now is to use tdm-gcc. Check the wxWidgets wiki for details.\n\nUsing the MSYS shell, and making sure that you have the C++ compiler option installed for MinGW as it is 'not' the default:\n\n> cd /c/path/to/wxWidgets-2.8.10/build/msw\n> mingw32-make -f makefile.gcc BUILD=release MONOLITHIC=1 SHARED=1 UNICODE=1\nThis will take quite some time and generate lots of text in the MSYS shell. Assuming it is successful, a DLL is generated in /c/path/to/wxWidgets/gcc_dll, which you will need to copy to a location where it can be found when running your executables.\n\nOn Unix systems\nYou should follow the instructions on the wxWidgets website if there are no suitable packages for your environment. You probably want to use GCC to build wxWidgets, as this makes it more likely that cabal will understand the output of wx-config.\n\nOn most Unix platforms, wxWidgets build goes something like: create a mybuild directory in the wxWidgets directory, and run configure and make from that directory (and take the time to get some coffee :-).\n\n> cd $wxwin\n> mkdir mybuild\n> cd mybuild\n> ../configure --enable-unicode\n> make\n> make install\n> cd ../contrib/src\n> make\n> make install\nTesting wxWidgets build\nNow try out a few samples of wxWidgets to see if it all works correctly:\n\n> cd ../../samples/controls\n> make\n> ./controls\nNote that you build from the local samples directory that resides in the mybuild directory.\n\nAnd also try if wx-config works too:\n\n> wx-config -v\nChengWei: I've found that if you put wx-config in some system directories, such as c:\\windows\\system32, 'cabal install wx' will complain \"wx-config: does not exist\". To put it in another directory in %PATH% will solve this problem. Tested on Windows 7, Haskell platform 2010 2.0.0.0.\n\nBuilding wxHaskell\nSince wxHaskell 0.12.1.1, we only support building using Cabal.\n\nFor all platforms installation is straightforward, provided that:\n\nwx-config is somewhere in the path\nThe WXWIN environment variable points to the root of your wxWidgets installation\nWINDOWS PLATFORMS ONLY: You may also need to set WXCFG=gcc_dll\\mswu\nYou can then install wxhaskell as follows (in a Windows cmd.exe shell, not an MSys shell):\n\n> cabal update\n> cabal install wx\nNote that on Windows 7 machines, your command window must be running as Administrator.\n\nNote that on Unix systems, you may prefer something like\n\n> sudo cabal install --global wx\nWindows developers who don't update environment variables can do something like this in an MS-DOS shell:\n\n> Set CPLUS_INCLUDE_PATH=C:\\MinGW\\include\\c++\\3.4.5;C:\\MinGW\\include\\c++\\3.4.5\\mingw32\\\n> Set WXWIN=C:\\path\\to\\wxWidgets-2.8.10 \n> Set WXCFG=gcc_dll\\mswu\n> cabal install wx\nThe Windows installation is global by default.\n\nOut of memory errors\nThere have been reports of out of memory errors when compiling wxcore on some machines, notably 64 bit Linux hosts. Wxcore contains some very large auto-generated source files, and increasing the memory available to the GHC runtime can help.\n\nTry something like:\n\n> export GHCRTS='-M512m'\nto allocate 512 MB to the GHC RTS. This has been reported to be sufficient.\n\nSource Release\nYou can obtain a source release of wxHaskell from the GitHub repository. Git creates a wxHaskell directory for you; we assume in the following example that your $wxHaskell directory will be ~/dev/wxhaskell.\n\n> cd ~/dev\n> git clone https://github.com/wxHaskell/wxHaskell\nYou then build each of the key components in order:\n\n> cd ~/dev/wxhaskell/wxdirect\n> cabal install\n> cd ../wxc\n> cabal install\n> cd ../wxcore\n> cabal install\n> cd ../wx\n> cabal install\nTest wxHaskell\nIf everything succeeded, you should be able to run a test program.\n\n> cd samples/wx\n> ghc -package wx -o helloworld HelloWorld.hs\n> ./helloworld\nNotes:\n\nmacosx: wxHaskell programs need to be turned into application bundles before they can be run. Look at the Mac OS X notes for more information. You can also run the examples from GHCi \u2013 a great development environment!\n\n> ghci -package wx BouncingBalls.hs\n> main\nNotes:\n\nwxHaskell programs are not always properly re\u00efnitialized when started the second time from a GHCi prompt. This is due static variables in the wxWidgets C++ code, and we are working with wxWidgets developers to remove those bugs. Currently, GHCi only works well with wxWidgets 2.4.2. gtk: Unfortunately, one can only start a wxWidgets program once with GHCi on GTK (rendering it useless). macosx. You need to use a special command to run wxHaskell applications, see the Mac OS X notes from more information. Have fun!", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to uninstall Ruby from /usr/local?", "id": 1215, "answers": [{"answer_id": 1208, "document_id": 791, "question_id": 1215, "text": "Create a symlink at /usr/bin named 'ruby' and point it to the latest installed ruby.\nYou can use something like ln -s /usr/bin/ruby /to/the/installed/ruby/binary", "answer_start": 626, "answer_category": null}], "is_impossible": false}], "context": "Everything was working fine , until we decided to upgrade ruby to 1.8.7 from 1.8.6, and thats when all hell broke loose. When we compiled Ruby 1.8.7 from source it got installed into /usr/local/bin and Ruby 1.8.6 stayed in /usr/bin. Currently, we've uninstalled ruby 1.8.6 and by some stroke we deleted the ruby 1.8.7 files from /usr/local.\nwhen we try \"which ruby\" it points to /usr/local. If anybody could help us out what we need to do get back on track , we would be very grateful.and also any idea how we can uninstall ruby from /usr/local. we tried yum remove ruby , which removed ruby from /usr/bin.Thanks and Cheers !\nCreate a symlink at /usr/bin named 'ruby' and point it to the latest installed ruby.\nYou can use something like ln -s /usr/bin/ruby /to/the/installed/ruby/binary\nHope this helps.\n\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Dependency Walker: missing dlls", "id": 1826, "answers": [{"answer_id": 1812, "document_id": 1397, "question_id": 1826, "text": "You're not missing anything. These are API-sets - essentially, an extra level of call indirection introduced gradually since windows 7. Dependency walker development seemingly halted long before that, and it can't handle API sets properly", "answer_start": 536, "answer_category": null}], "is_impossible": false}], "context": "I have been trying to resolve .dll dependencies for the executable file with Dependency Walker. Currently, I am getting missing .dlls in the following form:\nAPI-MS-WIN-XXX\nEXT-MS-WIN-XXX\nDoes anybody have any ideas on how to resolve these? Any help will be greatly appreciated!\nAdditional info: I compiled the executable using Visual Studio 2013. The most interesting thing is that I did not receive any errors during compilation. However, I cannot run it due to missing dependencies. I also attached a screenshot of dependency walker:\nYou're not missing anything. These are API-sets - essentially, an extra level of call indirection introduced gradually since windows 7. Dependency walker development seemingly halted long before that, and it can't handle API sets properly\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to install mongoDB on windows?", "id": 1572, "answers": [{"answer_id": 1561, "document_id": 1149, "question_id": 1572, "text": "Are you ready for the installation \u2026 and use \u2026 Technically, it\u2019s not an installation it\u2019s just Downloading\u2026\nI. Download the zip file http://www.mongodb.org/downloads\nII. Extract it and copy the files into your desired location.\nIII. Start the DB engine.\nIV. Test the installation and use it.", "answer_start": 320, "answer_category": null}], "is_impossible": false}], "context": "I am trying to test out mongoDB and see if it is anything for me. I downloaded the 32bit windows version, but have no idea on how to continue from now on.\nI normally use the WAMP services for developing on my local computer. Can i run mongoDB on Wamp?\nHowever, what's the best (easiest!) way to make it work on windows?\nAre you ready for the installation \u2026 and use \u2026 Technically, it\u2019s not an installation it\u2019s just Downloading\u2026\nI. Download the zip file http://www.mongodb.org/downloads\nII. Extract it and copy the files into your desired location.\nIII. Start the DB engine.\nIV. Test the installation and use it.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "I have my project set up, it is updating correctly from subversion, and building ok. ", "id": 328, "answers": [{"answer_id": 336, "document_id": 140, "question_id": 328, "text": "Install WebDeploy\nEnable Web config transforms\nConfigure TeamCity BuildRunner\nConfigure TeamCity Build Dependencies", "answer_start": 86, "answer_category": null}], "is_impossible": false}], "context": "I have my project set up, it is updating correctly from subversion, and building ok. \nInstall WebDeploy\nEnable Web config transforms\nConfigure TeamCity BuildRunner\nConfigure TeamCity Build Dependencies\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Embedding/deploying custom font in .NET app", "id": 1281, "answers": [{"answer_id": 1273, "document_id": 852, "question_id": 1281, "text": "You just specify the font-file, the font-size and the font-style, and the font-type is returned.", "answer_start": 395, "answer_category": null}], "is_impossible": false}], "context": "Is there an official way to distribute (deploy) a specific font with a .NET application?\nWe have a (public domain) \"LED font\" that prints numbers with the retro LED instrumentface look. This is a standard True Type or Open Type font like any other except it looks funky.\nI use a custom font for my custom graphics-library on an asp.net site, but this should also work on winform without issues. You just specify the font-file, the font-size and the font-style, and the font-type is returned.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "installing requests module in python 2 7 windows", "id": 1939, "answers": [{"answer_id": 1926, "document_id": 1518, "question_id": 1939, "text": "Get virtualenv set up. Each virtual environment you create will automatically have pip.\nGet pip set up globally.\nLearn how to install Python packages manually\u2014in most cases it's as simple as download, unzip, python setup.py install, but not always.\nUse Christoph Gohlke's binary installers.", "answer_start": 1540, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI am facing issues while installing request module (python 2.7) on windows. \n\nTried the below steps as per documentation:\n\n1\n\npip install requests\n\nerror\n\n'pip' is not recognized as an internal or external command,\noperable program or batch file.\n\n2\n\neasy_install requests\n\nerror\n\n'easy_install' is not recognized as an internal or external command,\noperable program or batch file.\n\n3\n\nsetup.py\n\nerror\n\nC:\\Location\\Python\\lib\\distutils\\dist.py:267: UserWarning: Unknown distribution option: 'zip_safe' warnings.warn(msg)\nC:\\Location\\Python\\lib\\distutils\\dist.py:267: UserWarning: Unknown distribution option: 'include_package_data' warnings.warn(msg)\nC:\\Location\\Python\\lib\\distutils\\dist.py:267: UserWarning: Unknown distribution option: 'install_requires' warnings.warn(msg)\nusage: setup.py [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]\n   or: setup.py --help [cmd1 cmd2 ...]\n   or: setup.py --help-commands\n   or: setup.py cmd --help\n\nerror: no commands supplied\n\n\nCan anyone please advise how to install the module on windows , without downloading any new stuff.\n    \n\nIf you want to install requests directly you can use the \"-m\" (module) option available to python. \n\npython.exe -m pip install requests\n\nYou can do this directly in PowerShell, though you may need to use the full python path (eg. C:\\Python27\\python.exe) instead of just python.exe.\n\nAs mentioned in the comments, if you have added Python to your path you can simply do:\n\npython -m pip install requests\n    \n\nThere are four options here:\n\n\nGet virtualenv set up. Each virtual environment you create will automatically have pip.\nGet pip set up globally.\nLearn how to install Python packages manually\u2014in most cases it's as simple as download, unzip, python setup.py install, but not always.\nUse Christoph Gohlke's binary installers.\n\n    \n\nOn windows 10 run cmd.exe with admin rights\nthen type : \n\n1) cd \\Python27\\scripts\n\n2) pip install requests\n\nIt should work. My case was with python 2.7\n    \n\n\nDownload the source code(zip or rar package).\nRun the setup.py inside.\n\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Unable to set default python version to python3 in ubuntu?", "id": 919, "answers": [{"answer_id": 914, "document_id": 573, "question_id": 919, "text": "sudo apt-install python3.7 Install the latest version of python you want\ncd /usr/bin Enter the root directory where python is installed\nsudo unlink python or sudo unlink python3 . Unlink the current default python\nsudo ln -sv /usr/bin/python3.7 python Link the new downloaded python version", "answer_start": 8653, "answer_category": null}], "is_impossible": false}], "context": "Unable to set default python version to python3 in ubuntu\nAsked 4 years, 9 months ago\nActive 1 month ago\nViewed 504k times\n\n244\n\n\n70\nI was trying to set default python version to python3 in Ubuntu 16.04. By default it is python2 (2.7). I followed below steps :\n\nupdate-alternatives --remove python /usr/bin/python2\nupdate-alternatives --install /usr/bin/python python /usr/bin/python3\nbut I'm getting the following error for the second statement,\n\nrejeesh@rejeesh-Vostro-1015:~$ update-alternatives --install /usr/bin/python python /usr/bin/python3\nupdate-alternatives: --install needs <link> <name> <path> <priority>\n\nUse 'update-alternatives --help' for program usage information.   \nI'm new to Ubuntu and Idon't know what I'm doing wrong.\n\npython\npython-3.x\nubuntu\ninstallation\nubuntu-16.04\nShare\nImprove this question\nFollow\nasked Feb 1 '17 at 17:57\n\nRejeeshChandran\n3,63833 gold badges2222 silver badges3030 bronze badges\n3\nAs stated in the warning, you are missing priority. \u2013 \ngreedy52\n Feb 1 '17 at 17:59\n12\nTake care not to remove Python 2.7 as it will cripple many facilities of you OS (from experience :( ) \u2013 \nJacques de Hooge\n Feb 1 '17 at 18:36\n1\nA word of warning: It sounds like a bad idea to me to change python to Python 3. The default way to invoke scripts written in Python 2 is python my-script-p2.py, while it's python3 my-script-p3.py. I would expect many system scripts to rely on this. \u2013 \nJan Groth\n Sep 26 '19 at 2:20\n1\nFor those who're interested in the topic I'd recommend to pay attention to the virtual environment: docs.python.org/3/tutorial/venv.html My Ubuntu 18 LTS still uses Python 2.7 and, for example, I use the virtual environment for using Python 3.X and be up-to-date in my Django projects. \u2013 \nVictor Bjorn\n Jan 20 '20 at 10:41\n1\nThis link might have the answer ;) unix.stackexchange.com/questions/410579/\u2026 \u2013 \nmr_azad\n Jun 1 '20 at 22:27\nShow 3 more comments\n22 Answers\n\n512\n\nThe second line mentioned can be changed to\n\n[sudo] update-alternatives --install /usr/bin/python python /usr/bin/python3 10\nThis gives a priority of 10 for the path of python3.\n\nThe disadvantage of alternatively editing .bashrc is that using the commands with sudo will not work.\n\nShare\nImprove this answer\nFollow\nedited Jul 20 at 10:02\n\nNico Schl\u00f6mer\n41.4k2121 gold badges149149 silver badges202202 bronze badges\nanswered May 14 '18 at 13:10\n\nPardhu\n5,27822 gold badges99 silver badges2323 bronze badges\n5\nGood and easy way out. \u2013 \nPrakashG\n Jan 11 '19 at 9:05\n9\nGood and right to the point. \" <priority>\" in the error message already suggested it. BTW, \"sudo\" is typically needed to run this install command. \u2013 \nywu\n May 7 '19 at 20:20 \n4\nLike ywu said, I had to run \"sudo update-alternatives --install /usr/bin/python python /usr/bin/python3 10\" \u2013 \nRoyal\n Sep 4 '19 at 11:51\n3\nThis is the right way to do it for sure, but it's worth noting that changing the systemwide default is likely to break some things. For instance, I had to go and apply a fix to terminator, which only works with python2. \u2013 \nDale Anderson\n Sep 13 '19 at 18:09\n2\nDoesn't work completely -- after this command python runs python3, but python-config still runs python2-config and general breakage ensues \u2013 \nChris Dodd\n Dec 1 '19 at 17:28\nShow 4 more comments\n\n182\n\nEDIT:\n\nI wrote this when I was young and naive, update-alternatives is the better way to do this. See @Pardhu's answer.\n\nOutdated answer:\n\nOpen your .bashrc file nano ~/.bashrc. Type alias python=python3 on to a new line at the top of the file then save the file with ctrl+o and close the file with ctrl+x. Then, back at your command line type source ~/.bashrc. Now your alias should be permanent.\n\nShare\nImprove this answer\nFollow\nedited Jul 20 at 10:00\n\nNico Schl\u00f6mer\n41.4k2121 gold badges149149 silver badges202202 bronze badges\nanswered Feb 1 '17 at 18:17\n\nSteampunkery\n3,67322 gold badges1717 silver badges2828 bronze badges\n86\nThis is the wrong answer. Editing your bashrc does not do the same thing as update-alternatives. For example, scripts that begin with #!/usr/bin/env python will not use the version in bashrc. Please use @Pardhu's answer. \u2013 \nstonewareslord\n Mar 28 '19 at 19:45\nI wrote this answer a long time ago, and I am aware that update alternatives is not the same as changing bashrc. I can edit the answer if you'd like. \u2013 \nSteampunkery\n Mar 29 '19 at 20:17\n1\nIt is more of a warning to users with this question that changing the alias does not do the same thing. Up to you if you want to edit. \u2013 \nstonewareslord\n Apr 1 '19 at 18:35\nThis is the only answer that helped me. I tried doing sudo update-alternatives --install /usr/bin/python python /usr/bin/python3.5.2. But I got: update-alternatives: --install needs <link> <name> <path> <priority> Use 'update-alternatives --help' for program usage information. \u2013 \nalexchenco\n Jan 10 '20 at 10:14\n4\nI did this and it broke things, like virtualenvs. This answer is creating more problems \u2013 \nKuzeko\n Jul 19 '20 at 14:29\nShow 2 more comments\n\n77\n\nTo change Python 3.6.8 as the default in Ubuntu 18.04 to Python 3.7.\n\nInstall Python 3.7\n\nSteps to install Python3.7 and configure it as the default interpreter.\n\nInstall the python3.7 package using apt-get\n\nsudo apt-get install python3.7\n\nAdd Python3.6 & Python 3.7 to update-alternatives\n\nsudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.6 1\nsudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.7 2\nUpdate Python 3 to point to Python 3.7\n\nsudo update-alternatives --config python3 Enter 2 for Python 3.7\n\nTest the version of python\n\npython3 --version\nPython 3.7.1 \nShare\nImprove this answer\nFollow\nedited Oct 20 '20 at 11:20\n\nAlvin Sartor\n1,51333 gold badges1515 silver badges3030 bronze badges\nanswered Aug 24 '19 at 23:03\n\nPurushottam Prabhakar\n79955 silver badges33 bronze badges\n6\nYou might want to include a warning that this can break packaged software. Python 3.6 is the distributed default and any bundled software packages will also assume this version. \u2013 \nTim\n Aug 25 '19 at 0:27\nCan I replace the python to version 3.7 instead of python3? \u2013 \nWee Hong\n Mar 9 '20 at 17:10\n@Tim, what's the best way to take care of that? \u2013 \ndoobeedoobee\n May 27 '20 at 13:22\n@Wee Hong, yes you can. Instead of $ sudo update-alternative --install /usr/bin/python3 python3 .... you just replace to $ sudo update-alterative --install /usr/bin/python python ..... and after: sudo update-alternatives --config python. \u2013 \nArthur Zennig\n Jul 1 '20 at 18:05\nWorked for me, but I realized that I had some old packages in the system that had python2 pre-remove scripts. So I couldn't deinstall them - had to switch back to python2 to be able to remove them... \u2013 \nRoman\n Mar 4 at 14:17\nAdd a comment\n\n61\n\nIf you have Ubuntu 20.04 LTS (Focal Fossa) you can install python-is-python3:\n\nsudo apt install python-is-python3\nwhich replaces the symlink in /usr/bin/python to point to /usr/bin/python3.\n\nShare\nImprove this answer\nFollow\nedited Sep 11 '20 at 9:48\nanswered May 18 '20 at 20:12\n\nsilviot\n4,19544 gold badges3535 silver badges4949 bronze badges\n2\nMinor typo: Ubuntu Focal is 20.04, not 20.20 \u2013 \nConchylicultor\n Aug 6 '20 at 17:07\n1\nIt's what I wanted, not just an alias. Thanks :) \u2013 \nTien Do\n Aug 13 '20 at 10:41\n2\nBecause this is about the latest distro, I'd suggest using apt instead of apt-get for install subcommand. \u2013 \nShiplu Mokaddim\n Sep 8 '20 at 15:24\nAdd a comment\n\n29\n\nTo change to python3, you can use the following command in terminal alias python=python3.\n\nShare\nImprove this answer\nFollow\nanswered Feb 1 '17 at 18:00\n\nDanteVoronoi\n98311 gold badge1010 silver badges2020 bronze badges\n6\nBut that only work for the current running process in terminal. If I close and open the terminal it will change back to python2. \u2013 \nRejeeshChandran\n Feb 1 '17 at 18:03\n2\n@RejeeshChandran Look at Steampunkery answer \u2013 \nSeraf\n Aug 31 '17 at 16:39\nAdd a comment\n\n19\n\nA simple safe way would be to use an alias. Place this into ~/.bashrc file: if you have gedit editor use\n\ngedit ~/.bashrc\n\nto go into the bashrc file and then at the top of the bashrc file make the following change.\n\nalias python=python3\n\nAfter adding the above in the file. run the below command\n\nsource ~/.bash_aliases or source ~/.bashrc\n\nexample:\n\n$ python --version\n\nPython 2.7.6\n\n$ python3 --version\n\nPython 3.4.3\n\n$ alias python=python3\n\n$ python --version\n\nPython 3.4.3\n\nShare\nImprove this answer\nFollow\nedited Jun 20 '20 at 9:12\n\nCommunityBot\n111 silver badge\nanswered Feb 9 '18 at 10:32\n\nKhan\n1,09688 silver badges1010 bronze badges\nAdd a comment\n\n11\n\nJust follow these steps to help change the default python to the newly upgrade python version. Worked well for me.\n\nsudo apt-install python3.7 Install the latest version of python you want\ncd /usr/bin Enter the root directory where python is installed\nsudo unlink python or sudo unlink python3 . Unlink the current default python\nsudo ln -sv /usr/bin/python3.7 python Link the new downloaded python version\npython --version Check the new python version and you're good to go\nShare\nImprove this answer\nFollow\nanswered Dec 30 '19 at 9:19\n\nShorya Sharma\n36922 silver badges1010 bronze badges\nAdd a comment\n\n10\n\nAt First Install python3 and pip3\n\nsudo apt-get install python3 python3-pip\nthen in your terminal run\n\nalias python=python3\nCheck the version of python in your machine.\n\npython --version\nShare\nImprove this answer\nFollow\nanswered Nov 25 '19 at 18:32\nuser9744623\nAdd a comment\n\n9\n\nAs an added extra, you can add an alias for pip as well (in .bashrc or bash_aliases):\n\nalias pip='pip3'\n\nYou many find that a clean install of python3 actually points to python3.x so you may need:\n\nalias pip='pip3.6'\nalias python='python3.6'", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Why does Nginx return a 403 even though all permissions are set properly?", "id": 852, "answers": [{"answer_id": 847, "document_id": 532, "question_id": 852, "text": "setsebool -P httpd_enable_homedirs=1", "answer_start": 2813, "answer_category": null}], "is_impossible": false}], "context": "View Full Version : [SOLVED] SELinux prevents httpd from reading homes - intrusion attempt?\n\n\n \n\nfom_hd\n30th August 2010, 04:49 PM\nHello,\n\nthe following security alert made me checking my httpd.conf:\n\n\n\nSummary:\n\nSELinux is preventing the http daemon from reading users' home directories.\n\nDetailed Description:\n\nSELinux has denied the http daemon access to users' home directories. Someone is\nattempting to access your home directories via your http daemon. If you have not\nsetup httpd to share home directories, this probably signals an intrusion\nattempt.\n\n\nEven though in httpd.conf there is a line that reads\n\n\n\nLoadModule userdir_module modules/mod_userdir.so\n\n\nin the same conf-file the access to home-dirs is disabled:\n\n\n\n<IfModule mod_userdir.c>\n#\n# UserDir is disabled by default since it can confirm the presence\n# of a username on the system (depending on home directory\n# permissions).\n#\nUserDir disabled\n\n#\n# To enable requests to /~user/ to serve the user's public_html\n# directory, remove the \"UserDir disabled\" line above, and uncomment\n# the following line instead:\n#\n#UserDir public_html\n\n</IfModule>\n\n\n\nSo i wonder, if the warnings really indicate an intrusion attempt.\n\nWhat can i check to find out what's going on here?\n\nI use Fedora 13.\n\ngreetings\nfrank\n \nking sabri\n30th August 2010, 06:05 PM\nif you want to close SELinux\n\nOpen\n\nvim /etc/selinux/config\n\nchange\n\nSELINUX=enforcing\nTo\n\nSELINUX=disabled\n\nand reboot your machine\n\n\nif you want to solve it and keep SElinux working\n\nplease show us the error from AV\n\n\nsealert -a /var/log/audit/audit.log\n\nNote: Execute the previous line after you get the apache error\nfom_hd\n30th August 2010, 06:52 PM\nI want to keep SELinux and wonder why this warning is coming up.\n\nFollowing is the output of sealert. Sorry, that i didn't include it in my question.\n\n\nsealert -a /var/log/audit/audit.log\n77% donetype=AVC msg=audit(1282564054.643:117): avc: denied { read write search } for pid=8679 comm=\"gdm-session-wor\" name=\"/\" dev=dm-4 ino=2 scontext=system_u:system_r:xdm_t:s0-s0:c0.c1023 tcontext=system_u:object_r:file_t:s0 tclass=dir\n\n**** Invalid AVC dontaudited in current policy. 'semodule -B' will turn on dontaudit rules. ***\n\n100% donefound 2 alerts in /var/log/audit/audit.log\n--------------------------------------------------------------------------------\n\n\nSummary:\n\nSELinux is preventing the http daemon from reading users' home directories.\n\nDetailed Description:\n\nSELinux has denied the http daemon access to users' home directories. Someone is\nattempting to access your home directories via your http daemon. If you have not\nsetup httpd to share home directories, this probably signals an intrusion\nattempt.\n\nAllowing Access:\n\nIf you want the http daemon to share home directories you need to turn on the\nhttpd_enable_homedirs boolean: \"setsebool -P httpd_enable_homedirs=1\" You may\nneed to also label the content that you wish to share. The man page\nhttpd_selinux will have further information. 'man httpd_selinux'.\n\nFix Command:\n\nsetsebool -P httpd_enable_homedirs=1\n\nAdditional Information:\n\nSource Context unconfined_u:system_r:httpd_t:s0\nTarget Context unconfined_u:object_r:user_home_dir_t:s0\nTarget Objects /home/martin [ dir ]\nSource httpd\nSource Path /usr/sbin/httpd\nPort <Unknown>\nHost <Unknown>\nSource RPM Packages httpd-2.2.16-1.fc13\nTarget RPM Packages\nPolicy RPM selinux-policy-3.7.19-49.fc13\nSelinux Enabled True\nPolicy Type targeted\nEnforcing Mode Enforcing\nPlugin Name httpd_enable_homedirs\nHost Name conny\nPlatform Linux conny 2.6.33.8-149.fc13.i686.PAE #1 SMP Tue\nAug 17 22:39:27 UTC 2010 i686 i686\nAlert Count 2\nFirst Seen Sun Aug 22 22:53:21 2010\nLast Seen Sun Aug 22 23:09:33 2010\nLocal ID 689d716f-d318-45b5-9f8f-b23f75d755be\nLine Numbers 1794, 1795, 1805, 1806\n\nRaw Audit Messages\n\ntype=AVC msg=audit(1282511373.20:68): avc: denied { search } for pid=19928 comm=\"httpd\" name=\"martin\" dev=dm-3 ino=8177 scontext=unconfined_u:system_r:httpd_t:s0 tcontext=unconfined_u:object_r:user_home_dir_t:s0 tclass=dir\n\ntype=SYSCALL msg=audit(1282511373.20:68): arch=40000003 syscall=195 success=no exit=-13 a0=247b200 a1=bff73f50 a2=b16ff4 a3=8000 items=0 ppid=19922 pid=19928 auid=0 uid=48 gid=489 euid=48 suid=48 fsuid=48 egid=489 sgid=489 fsgid=489 tty=(none) ses=1 comm=\"httpd\" exe=\"/usr/sbin/httpd\" subj=unconfined_u:system_r:httpd_t:s0 key=(null)\n\n\n\n--------------------------------------------------------------------------------\n\n\nSummary:\n\nSELinux is preventing the http daemon from reading users' home directories.\n\nDetailed Description:\n\nSELinux has denied the http daemon access to users' home directories. Someone is\nattempting to access your home directories via your http daemon. If you have not\nsetup httpd to share home directories, this probably signals an intrusion\nattempt.\n\nAllowing Access:\n\nIf you want the http daemon to share home directories you need to turn on the\nhttpd_enable_homedirs boolean: \"setsebool -P httpd_enable_homedirs=1\" You may\nneed to also label the content that you wish to share. The man page\nhttpd_selinux will have further information. 'man httpd_selinux'.\n\nFix Command:\n\nsetsebool -P httpd_enable_homedirs=1\n\nAdditional Information:\n\nSource Context system_u:system_r:httpd_t:s0\nTarget Context unconfined_u:object_r:user_home_dir_t:s0\nTarget Objects /home/martin [ dir ]\nSource httpd\nSource Path /usr/sbin/httpd\nPort <Unknown>\nHost <Unknown>\nSource RPM Packages httpd-2.2.16-1.fc13\nTarget RPM Packages\nPolicy RPM selinux-policy-3.7.19-49.fc13\nSelinux Enabled True\nPolicy Type targeted\nEnforcing Mode Enforcing\nPlugin Name httpd_enable_homedirs\nHost Name conny\nPlatform Linux conny 2.6.33.8-149.fc13.i686.PAE #1 SMP Tue\nAug 17 22:39:27 UTC 2010 i686 i686\nAlert Count 16\nFirst Seen Mon Aug 23 00:45:03 2010\nLast Seen Mon Aug 30 13:35:02 2010\nLocal ID bf16d7d8-ce4c-4a4d-a2a8-edde2d0bafdc\nLine Numbers 1959, 1960, 2105, 2106, 2288, 2289, 2767, 2768,\n2801, 2802, 2870, 2871, 3053, 3054, 3093, 3094,\n3162, 3163, 3184, 3185, 3220, 3221, 3276, 3277,\n3315, 3316, 3427, 3428, 3459, 3460, 3486, 3487\n\nRaw Audit Messages\n\ntype=AVC msg=audit(1283168102.984:39): avc: denied { search } for pid=5494 comm=\"httpd\" name=\"martin\" dev=dm-3 ino=8177 scontext=system_u:system_r:httpd_t:s0 tcontext=unconfined_u:object_r:user_home_dir_t:s0 tclass=dir\n\ntype=SYSCALL msg=audit(1283168102.984:39): arch=40000003 syscall=195 success=no exit=-13 a0=27cd238 a1=bfa79bb0 a2=30aff4 a3=8000 items=0 ppid=1795 pid=5494 auid=4294967295 uid=48 gid=489 euid=48 suid=48 fsuid=48 egid=489 sgid=489 fsgid=489 tty=(none) ses=4294967295 comm=\"httpd\" exe=\"/usr/sbin/httpd\" subj=system_u:system_r:httpd_t:s0 key=(null)\njpollard\n30th August 2010, 08:31 PM\n \nYou need to enable home directories in SELinux. You can enable this using\nSystem > Administration > SELinux Administration and select \"Boolean\"\noptions. apache has a number of flags that can be turned on, most shouldn't be\nunless you actually need them. Just scroll down to the line \"Allow httpd to read\nhome directories\" and enable. It will take a bit of time (10 seconds or so) but\nthat is all that needs changing.\n\nIn addition, the users public directory ($HOME/public_html) must be labeled\n\nunconfined_u:object_r:httpd_user_content_t:s0\n\nand have world rx permissions to give apache access. The user can make this\nchange (chcon -t httpd_user_content_t public_html).\n\nThis multi-layered protection really works - if the facility/administrator decides that\nusers should have publication rights in their home directory (not the best thing but\nreasonable in some circumstances) then this flag can be turned off.\n \nking sabri\n30th August 2010, 09:44 PM\nno problem ;)\n\nthe answer in what you ware rise now\n\n\nsetsebool -P httpd_enable_homedirs=1\n\nalso please consider \"jpollard\" answer his true\n\n\nalways thinking about 2 things\n\n1. is the selinux accept this process or not\n2. the SELinux permission to the related file/folder/user/process\n\nhopefully this working well\nfom_hd\n30th August 2010, 09:46 PM\n \nThanks for your answers.\n\nbut actually i don't want to allow http to read home directories (and the http daemon is configured not to use them).\n\nAlso my question is not about simply getting rid of the security warning - i want to know WHY the warning occurs, and if there is maybe an intrusion attempt (as the alert message indicates) ?\n\nOr does http try to read from the home directories in all cases??\n\nThanks in advance for your patience.\n\ngreetings\nfrank\n \nking sabri\n30th August 2010, 11:27 PM\nWHY the warning occurs, and if there is maybe an intrusion attempt (as the alert message indicates)\nI'm answering \"why\" by the error massages (I dont know another way ).\nplease check\n/var/log/messages\n/var/log/httpd/\n\nand there is no intrusion . even there is intrusion, it's prevented by SELinux\nmotnahp00\n31st August 2010, 04:52 AM\nI have a similar issue:\n\nI have a PHP script that creates an empty file in a particular directory.\n\nWhen I run the script, SELINUX fires a warning:\n\n***\nSELinux is preventing the http daemon from reading users' home directories.\n\nSELinux has denied the http daemon access to users' home directories. Someone is\nattempting to access your home directories via your http daemon. If you have not\nsetup httpd to share home directories, this probably signals an intrusion\nattempt.\n***\n\nI modified SELINUX with the following:\n\nsetsebool -P httpd_enable_homedirs=on\n\nWhat else am I missing?\n\nHere's the script:\n\n<?php\n$filename = \"/test.txt\";\n$newfile = fopen($filename, \"w+\") or die(\"Couldn't create file.\");\nfclose($newfile);\n$msg = \"<P>File created!</P>\";\n?>\n\n<HTML>\n<HEAD>\n<TITLE>Creating a New File</TITLE>\n</HEAD>\n<BODY>\n<?php echo \"$msg\"; ?>\n</BODY>\n</HTML>\nEdit/Delete Message\ndomg472\n31st August 2010, 03:36 PM\nHello,\n\nthe following security alert made me checking my httpd.conf:\n\n\n\nSummary:\n\nSELinux is preventing the http daemon from reading users' home directories.\n\nDetailed Description:\n\nSELinux has denied the http daemon access to users' home directories. Someone is\nattempting to access your home directories via your http daemon. If you have not\nsetup httpd to share home directories, this probably signals an intrusion\nattempt.\n\n\nEven though in httpd.conf there is a line that reads\n\n\n\nLoadModule userdir_module modules/mod_userdir.so\n\n\nin the same conf-file the access to home-dirs is disabled:\n\n\n\n<IfModule mod_userdir.c>\n#\n# UserDir is disabled by default since it can confirm the presence\n# of a username on the system (depending on home directory\n# permissions).\n#\nUserDir disabled\n\n#\n# To enable requests to /~user/ to serve the user's public_html\n# directory, remove the \"UserDir disabled\" line above, and uncomment\n# the following line instead:\n#\n#UserDir public_html\n\n</IfModule>\n\n\n\nSo i wonder, if the warnings really indicate an intrusion attempt.\n\nWhat can i check to find out what's going on here?\n\nI use Fedora 13.\n\ngreetings\nfrank\n\nThis is not the first time i hear about this issue. What is causing it i do not know\n\nTo test, i just started my httpd service and so far it did not try to search the user home space.\n\nSo again, i have heard of this issue before but i cannot reproduce it here so far.\n\nYou can view your apache logs, and other logs.\njpollard\n31st August 2010, 04:25 PM\n \nDouble check the apache configuration - it is possible to have it enabled in one place\nand disabled in another. Whichever is last is the one that is used.\n\nIt is possible for this situation to occur - usually when importing a configuration from\nan earlier installation.\nCiaW\n31st August 2010, 04:58 PM\nI suspect the reason you're getting this alert is because of this line you showed in the first post, in the httpd.conf file:\n\nLoadModule userdir_module modules/mod_userdir.so\n\nSince you said you're not using home dir's for your setup, just comment the line (i.e. put a # in front of it) so it looks like this:\n#LoadModule userdir_module modules/mod_userdir.so\n\nAnd hopefully that fixes it. I think / suspect it's due to this module trying to load that the selinux message is being generated.\nfom_hd\n2nd September 2010, 06:19 PM\n \nI went the road commenting out all modules and inserting them stepwise to the httpd.conf file. Finally figured out that the module \"mod_dnssd.conf\" in /etc/httpd/conf.d/ causes the alert message.\n\nDocumentation of the module says (among other things):\n\"The module publishes all configured virtual hosts and the mod_userdir directories of all local users. For mod_userdir to work you need to load that module and configure it for the path ~/public_html/.\"\n\nInserted \"DNSSDAutoRegisterUserDir off\" in the file and alert with starting httpd is gone :)\n\nSo - to answer my own question - there was no intrusion attempt :D\n\nThanks for the encouraging feedback!\nmotnahp00\n3rd September 2010, 01:52 AM\nInteresting. I'll give that a try and see if my PHP script works without setting off alerts. Thanks fom_hd.", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How can I find out if I have Xcode commandline tools installed?", "id": 911, "answers": [{"answer_id": 906, "document_id": 570, "question_id": 911, "text": "1. To get install Xcode Version\n\n xcodebuild -version", "answer_start": 3319, "answer_category": null}], "is_impossible": false}, {"question": "How to check the swift language version?", "id": 912, "answers": [{"answer_id": 907, "document_id": 570, "question_id": 912, "text": "swift --version", "answer_start": 3415, "answer_category": null}], "is_impossible": false}, {"question": "How can I find out if I have Xcode commandline tools installed?", "id": 913, "answers": [{"answer_id": 908, "document_id": 570, "question_id": 913, "text": "/usr/bin/xcodebuild -version", "answer_start": 1071, "answer_category": null}], "is_impossible": false}], "context": "I need to use gdb.\n\nps-MacBook-Air:AcoustoExport pi$ gdb\n-bash: gdb: command not found\n\nps-MacBook-Air:AcoustoExport pi$ sudo find / -iname \"*gdb*\"\nPassword:\n/usr/local/share/gdb\n/usr/local/Cellar/isl/0.12.1/share/gdb\n:\nand:\n\nps-MacBook-Air:AcoustoExport pi$ ls -la /usr/local/share/gdb\nlrwxr-xr-x   1 pi    admin   30 14 Jan 22:01 gdb -> ../Cellar/isl/0.12.1/share/gdb\nNot quite sure what to make this, clearly it is something installed by homebrew. I don't know why it's there, I don't know whether I could use it instead. It isn't in the search path.\n\nSo I figure I need Xcode commandline tools.\n\nXcode is not currently available from the Software Update server\n\n^ my current problem exactly. Comment on that question says \"you can get this error if you have them already\"\n\nBut how do I check whether I have them already?\n\nxcode\ncommand-line\ninstallation\nShare\nImprove this question\nFollow\nedited May 23 '17 at 11:47\n\nCommunityBot\n111 silver badge\nasked Jan 22 '14 at 1:28\n\nP i\n26k3333 gold badges141141 silver badges238238 bronze badges\nAdd a comment\n7 Answers\n\n244\n\n/usr/bin/xcodebuild -version\nwill give you the xcode version, run it via Terminal command\n\nShare\nImprove this answer\nFollow\nedited Nov 4 '16 at 7:22\n\nDanh\n5,56677 gold badges2727 silver badges4141 bronze badges\nanswered Nov 4 '16 at 5:45\n\ncrujzo\n2,65011 gold badge1111 silver badges1414 bronze badges\n4\nTo get only the version use the following command: /usr/bin/xcodebuild -version | sed -En 's/Xcode[[:space:]]+([0-9\\.]*)/\\1/p' \u2013 \nWerner Altewischer\n Sep 20 '18 at 6:54\n1\nI am getting error with the mentioned command. \"-bash: /usr/build/xcodebuild: No such file or directory.\" I have confirmed that I have xcode by running command gcc --version, which gives the gcc details. I am on Mac OS X 10.13.6. \u2013 \nGAURAV SRIVASTAVA\n Sep 26 '18 at 21:37 \n@GAURAV SRIVASTAVA: it's /usr/bin, not /usr/build. In fact xcodebuild should be enough since /usr/bin is usually in your PATH. \u2013 \nSamuel Leli\u00e8vre\n Jul 25 '20 at 19:18\n6\nI get this xcode-select: error: tool 'xcodebuild' requires Xcode, but active developer directory '/Library/Developer/CommandLineTools' is a command line tools instance. i think \u2013 \nGulam Hussain\n Aug 31 at 1:51\nAdd a comment\n\n48\n\nFirst of all, be sure that you have downloaded it or not. Open up your terminal application, and enter $ gcc if you have not installed it you will get an alert. You can verify that you have installed it by\n\n$ xcode-select -p\n/Library/Developer/CommandLineTools\nAnd to be sure then enter $ gcc --version\n\nYou can read more about the process here: Xcode command line tools for Mavericks\n\nShare\nImprove this answer\nFollow\nedited Jan 22 '14 at 2:25\nanswered Jan 22 '14 at 2:03\n\nSiavash Alp\n1,3221414 silver badges1414 bronze badges\n1\n@Pi What do you get when you enter $gcc --version? Do you get something like this: --prefix=/Library/Developer/CommandLineTools/usr --with-gxx-include-dir=/usr/include/c++/4.2.1 Apple LLVM version 5.0 (clang-500.2.79) (based on LLVM 3.3svn) \u2013 \nSiavash Alp\n Jan 22 '14 at 4:47 \nNo, I get --prefix=/Applications/Xcode.app/Contents/Developer/usr. The remainder is the same as your printout. \u2013 \nP i\n Jan 22 '14 at 13:58\nAdd a comment\n\n38\n\nif you want to know the install version of Xcode as well as Swift language current version:\n\nUse below simple command by using Terminal:\n\n1. To get install Xcode Version\n\n xcodebuild -version\n2. To get install Swift language Version\n\nswift --version\nShare\nImprove this answer\nFollow\nedited Jan 10 '18 at 7:50\nanswered Jul 21 '17 at 7:33\n\nKiran Jadhav\n2,8592222 silver badges2727 bronze badges\nAdd a comment\n\n32\n\nI was able to find my version of Xcode on maxOS Sierra using this command:\n\npkgutil --pkg-info=com.apple.pkg.CLTools_Executables | grep version\nas per this answer.\n\nShare\nImprove this answer\nFollow\nedited Apr 13 '17 at 12:45\n\nCommunityBot\n111 silver badge\nanswered Feb 9 '17 at 18:55\n\nAlexG\n9,25855 gold badges5656 silver badges6666 bronze badges\nAdd a comment\n\n13\n\nThanks to the folks on Freenode's #macdev, here is some information:\n\nIn the old days before Xcode was on the app-store, it included commandline tools.\n\nNow you get it from the store, and with this new mechanism it can't install extra things outside of the Xcode.app, so you have to manually do it yourself, by:\n\nxcode-select --install\nOn Xcode 4.x you can check to see if they are installed from within the Xcode UI:\n\nenter image description here\n\nOn Xcode 5.x it is now here:\n\nenter image description here\n\nMy problem of finding gcc/gdb is that they have been superseded by clang/lldb: GDB missing in OS X v10.9 (Mavericks)\n\nAlso note that Xcode contains compiler and debugger, so one of the things installing commandline tools will do is symlink or modify $PATH. It also downloads certain things like git.\n\nShare\nImprove this answer\nFollow\nedited Oct 14 '19 at 10:03\n\nJeroen Ooms\n29.7k3434 gold badges121121 silver badges194194 bronze badges\nanswered Jan 22 '14 at 1:50\n\nP i\n26k3333 gold badges141141 silver badges238238 bronze badges\nAdd a comment\n\n3\n\nIf for some reason xcode is not installed under\n\n/usr/bin/xcodebuild\n\nexecute the following command\n\nwhich xcodebuild\n\nand if it is installed, you'll be prompted with it's location.\n\nShare\nImprove this answer\nFollow\nanswered Feb 13 '20 at 11:41\n\nSylvesterAbreuLoreto\n34433 silver badges1616 bronze badges\nAdd a comment\n\n0\n\nFor macOS catalina try this : open Xcode. if not existing. download from App store (about 11GB) then open Xcode>open developer tool>more developer tool and used my apple id to download a compatible command line tool. Then, after downloading, I opened Xcode>Preferences>Locations>Command Line Tool and selected the newly downloaded command line tool from downloads.", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Why does Azure deployment take so long?", "id": 1662, "answers": [{"answer_id": 1649, "document_id": 1235, "question_id": 1662, "text": "Check out this PDC10 video by Mark Russinovich. He goes into great detail on what's going on inside Azure with some insights into the (admittedly slow) deployment process", "answer_start": 399, "answer_category": null}], "is_impossible": false}], "context": "I'm trying to understand why it can take from 20-60min to deploy a small application to Azure (using the configuration/package upload method, not from within VS).\nI've read through this situation and this one but I'm still a little unclear - is there a weird non-technology ritual that occurs while the instances are distributing, like somebody over at Microsoft lighting a candle or doing a dance?\nCheck out this PDC10 video by Mark Russinovich. He goes into great detail on what's going on inside Azure with some insights into the (admittedly slow) deployment process.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Skipping acquire of configured file 'main/binary-i386/Packages'", "id": 756, "answers": [{"answer_id": 756, "document_id": 443, "question_id": 756, "text": "You must change the line of /etc/apt/sources.list.d/pgdg.list to\ndeb [arch=amd64] http://apt.postgresql.org/pub/repos/apt/ focal-pgdg main", "answer_start": 480, "answer_category": null}], "is_impossible": false}], "context": "Good afternoon, please tell me what I'm doing wrong. I just installed the Linux Ubuntu on my computer and still don\u2019t understand anything about it. I tried to install PostreSQL and pgAdmin. I installed on this video tutorial https://www.youtube.com/watch?v=Vdzb7JTPnGk I get this error.\nText of Error: Skipping acquire of configured file 'main/binary-i386/Packages' as repository 'http://apt.postgresql.org/pub/repos/apt focal-pgdg InRelease' doesn't support architecture 'i386'. You must change the line of /etc/apt/sources.list.d/pgdg.list to\ndeb [arch=amd64] http://apt.postgresql.org/pub/repos/apt/ focal-pgdg main\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Can't install python-dev on centos 6.5", "id": 798, "answers": [{"answer_id": 794, "document_id": 481, "question_id": 798, "text": "So you should use\nsudo yum install python-devel\nto install them on your CentOS system", "answer_start": 213, "answer_category": null}], "is_impossible": false}], "context": "Can't install python-dev on centos 6.5. Yum says \"No package python-dev available.\". No graphical solutions please. 112\nOn CentOS, the python development libraries are under the name python-devel, not python-dev.\nSo you should use\nsudo yum install python-devel\nto install them on your CentOS system.\nYou can search the repositories available to you using the yum search xxxxx command, where xxxxx is the name or part of the name of the package you are looking for.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Error: \"The configuration section 'system.webServer/management/delegation' cannot be read because it is missing schema", "id": 1672, "answers": [{"answer_id": 1659, "document_id": 1245, "question_id": 1672, "text": "Remove Web Deploy 3.6 and reinstall 3.5 and see if it solves your issue.You can also try @james-wilkins comment below: \"In my case I installed Web Deploy BEFORE installing the management service...\"", "answer_start": 899, "answer_category": null}], "is_impossible": false}], "context": "I am trying to publish a website from my computer to an IIS web server via web deploy.\nI have 3 servers. All servers have installed the same things (Webdeploy etc.) and the services are started (Web Deployment Agent Service and Web Management Service).\nOn the first server I have no problems on connecting. But the other two servers give me some error when I \"Validate Connection\" in Visual Studio to the IIS. When I look at the logs on the IIS server, there are my accesses listet with html code 200 (ok).\nAn error ocurred when the request was processed on the remote computer. Filename: MACHINE/WEBROOT\nError: The configuration section 'system.webServer/management/delegation' cannot be read because it is missing schema\nI really have no idea what could solve this problem and hope you are able to help me. I had the exact same problem, it turned out I had installed Web Deploy 3.6 on top of 3.5. Remove Web Deploy 3.6 and reinstall 3.5 and see if it solves your issue.You can also try @james-wilkins comment below: \"In my case I installed Web Deploy BEFORE installing the management service...\"\n\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "how to not make install step when building external project with cmake", "id": 2010, "answers": [{"answer_id": 1996, "document_id": 1602, "question_id": 2010, "text": "ect-build.\n\nYou probably also want to exclude the CommonBaseProject from the \"all\" target with EXCLUDE_FR", "answer_start": 1367, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI'm building dependency project with cmake ExternalProject_Add command:\n\ninclude(ExternalProject)\n...\nset(COMMON_BASE_PROJECT_DIR ${CMAKE_CURRENT_SOURCE_DIR}/../CommonBase)\n\nExternalProject_Add(CommonBaseProject\n  SOURCE_DIR ${COMMON_BASE_PROJECT_DIR}\n  BINARY_DIR ${COMMON_BASE_PROJECT_DIR}/build\n  INSTALL_COMMMAND \"\"\n)   \n\ninclude_directories(${CMAKE_CURRENT_SOURCE_DIR}/include)\ninclude_directories(${COMMON_BASE_PROJECT_DIR}/include)\n\nadd_library(\n    ${LIBRARY_NAME}\n    SHARED\n    ${SRC_FILES}\n    ${INCLUDE_FILES}\n)\n\ntarget_link_libraries (Bios ${COMMON_BASE_PROJECT_DIR}/build/libCommonBase.dll)\nadd_dependencies(Bios CommonBaseProject)\n\n\nbut i get error:\n\n[100%] Linking CXX shared library libCommonBase.dll\n[100%] Built target CommonBase\n[ 50%] Performing install step for 'CommonBaseProject'\nmake[3]: *** No rule to make target 'install'.  Stop.\n\n\nI don't need to make install step, so my question is: how to disable it?\n    \n\nYou almost had it: Instead of INSTALL_COMMAND \"\" put something like\n\n    INSTALL_COMMAND cmake -E echo \"Skipping install step.\"\n\n    \n\nYou can generate a target for the build step with STEP_TARGETS build and add dependency on this particular target. The step targets are named &lt;external-project-name&gt;-&lt;step-name&gt; so in this case the target representing the build step will be named CommonBaseProject-build.\n\nYou probably also want to exclude the CommonBaseProject from the \"all\" target with EXCLUDE_FROM_ALL TRUE.\n\nExternalProject_Add(CommonBaseProject\n  SOURCE_DIR ${COMMON_BASE_PROJECT_DIR}\n  BINARY_DIR ${COMMON_BASE_PROJECT_DIR}/build\n  STEP_TARGETS build\n  EXCLUDE_FROM_ALL TRUE\n)\n\nadd_dependencies(Bios CommonBaseProject-build)\n\n    \n\nNot relevant to your question, which it was already answered, but in my case I had the following ExternalProject_Add directive:\n\nExternalProject_Add(external_project\n    # [...]\n    # Override build/install command\n    BUILD_COMMAND \"\"\n    INSTALL_COMMAND\n        \"${CMAKE_COMMAND}\"\n        --build .\n        --target INSTALL    # Wrong casing for \"install\" target\n       --config ${CMAKE_BUILD_TYPE}\n)\n\n\nIn this case cmake quits with very similar error (*** No rule to make target 'INSTALL'), but in this case it's the external project that is looking for incorrect uppercase INSTALL target: correct case is install instead. Apparently, that worked in Windows with MSVC but fails in unix operating systems.\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How can I deploy an iPhone application from Xcode to a real iPhone device?", "id": 1658, "answers": [{"answer_id": 1646, "document_id": 1232, "question_id": 1658, "text": "Download ldid from Cydia and then use it like so: ldid -S /Applications/AccelerometerGraph.app/AccelerometerGraph\nAlso be sure that the binary is marked as executable: chmod +x /Applications/AccelerometerGraph.app/AccelerometerGraph", "answer_start": 159, "answer_category": null}], "is_impossible": false}], "context": "How can I deploy an iPhone application from Xcode to real iPhone device without having a US$99 Apple certificate? It sounds like the application isn't signed. Download ldid from Cydia and then use it like so: ldid -S /Applications/AccelerometerGraph.app/AccelerometerGraph\nAlso be sure that the binary is marked as executable: chmod +x /Applications/AccelerometerGraph.app/AccelerometerGraph\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Moto 360 Deploying from Android Studio extremely slow", "id": 1316, "answers": [{"answer_id": 1306, "document_id": 885, "question_id": 1316, "text": "To setup an emulator, you can use the following tutorials. http://www.binpress.com/tutorial/how-to-create-a-custom-android-wear-watch-face/120 https://developer.android.com/training/wearables/apps/creating.html", "answer_start": 459, "answer_category": null}], "is_impossible": false}], "context": "Trying to run a wear counterpart of an Android app directly on Moto 360, after having it connected and listed as a target device.\nADB can see it and communicates fine.\nHowever, the deployment message is not progressing for a long time, about 10 mins.\nEventually, the app gets installed, but this makes continuous development on the device almost impossible.\nIs there any way to speed up this installation? I really doubt that the Bluetooth link is that slow.\nTo setup an emulator, you can use the following tutorials. http://www.binpress.com/tutorial/how-to-create-a-custom-android-wear-watch-face/120 https://developer.android.com/training/wearables/apps/creating.html\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How do I uninstall Java on my Mac?", "id": 158, "answers": [{"answer_id": 165, "document_id": 97, "question_id": 158, "text": "Click on the Finder icon located in your dock\nClick on the Utilities folder\nDouble-click on the Terminal icon\nIn the Terminal window Copy and Paste the commands below:\nsudo rm -fr /Library/Internet\\ Plug-Ins/JavaAppletPlugin.plugin\nsudo rm -fr /Library/PreferencePanes/JavaControlPanel.prefPane\nsudo rm -fr ~/Library/Application\\ Support/Oracle/Java", "answer_start": 312, "answer_category": null}], "is_impossible": false}, {"question": "Can I restore Apple Java 6 after uninstalling Oracle Java?", "id": 159, "answers": [{"answer_id": 166, "document_id": 97, "question_id": 159, "text": "Yes, see the instructions on the Apple website Restore Apple Java 6. If you have JDK 7 or later versions installed on your system and you want to restore Apple Java 6, then those JDK versions need to be uninstalled first.", "answer_start": 976, "answer_category": null}], "is_impossible": false}], "context": "This article applies to:\nPlatform(s): Mac OS X\nJava version(s): 7.0, 8.0\nUninstall Oracle Java using the Terminal\nNote: To uninstall Java, you must have Administrator privileges and execute the remove command either as root or by using the sudo tool.\n\nRemove one directory and one file (a symlink), as follows:\n\nClick on the Finder icon located in your dock\nClick on the Utilities folder\nDouble-click on the Terminal icon\nIn the Terminal window Copy and Paste the commands below:\nsudo rm -fr /Library/Internet\\ Plug-Ins/JavaAppletPlugin.plugin\nsudo rm -fr /Library/PreferencePanes/JavaControlPanel.prefPane\nsudo rm -fr ~/Library/Application\\ Support/Oracle/Java\nDo not attempt to uninstall Java by removing the Java tools from /usr/bin. This directory is part of the system software and any changes will be reset by Apple the next time you perform an update of the OS.\n\nNote: After successfully uninstalling Java, you may remove Java Deployment cache using these instructions.Yes, see the instructions on the Apple website Restore Apple Java 6. If you have JDK 7 or later versions installed on your system and you want to restore Apple Java 6, then those JDK versions need to be uninstalled first. See the instructions to Uninstall JDK.\n\nMORE TECHNICAL INFORMATION\n\nUninstall JDK\nTo uninstall JDK 7 and later versions, you must have Administrator privileges and execute the remove command either as root or by using sudo. See the instructions to Uninstall JDK.", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "mysql configuration stops at \"starting server\"", "id": 1155, "answers": [{"answer_id": 1148, "document_id": 732, "question_id": 1155, "text": "The issue is that the installer cannot handle spaces in the windows service name (the name I used was Something MySQL). Simply change the name to one without spaces.", "answer_start": 467, "answer_category": null}], "is_impossible": false}], "context": "I was installing MySQL installer on my windows 8 machine. During the server configuration process, it gets stuck at \"starting server\" and doesn't move any further. I had the same issue of the installer hanging on \"Starting Server\". However, after looking at the windows Event Viewer under Windows Logs -> Application, I discovered the error\n\"Too many arguments (first extra is 'Something').For more information, see Help and Support Center at http://www.mysql.com.\".\nThe issue is that the installer cannot handle spaces in the windows service name (the name I used was Something MySQL). Simply change the name to one without spaces.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "an error occurred building visual studio cordova app using multi device hybrid a", "id": 1517, "answers": [{"answer_id": 1506, "document_id": 1093, "question_id": 1517, "text": "is bad . -*-\n    \n\nGO to Command Prompt and type:\n\nnpm install -g glob\n\n\nNow try running your app, It should work.\n    \n\nTry to following these steps in order:\n\n\nEnsure that the Android SDK is installed at %", "answer_start": 6917, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI am getting 3 fatal errors.  I installed the android SDK 19 as well. Any ideas???\n\nAn error occurred while listing Android targets\n\nC:\\Project\\AngularJS_ToDo_Sample_for_Multi-Device_Hybrid_Apps\\JavaScript\\AngularJSTodo\\EXEC 1   1 AngularJSToDo\n\nC:\\Users\\wil\\.cordova\\lib\\android\\cordova\\3.4.0\\bin\\create.bat: Command failed with exit code 8\n\nC:\\Project\\AngularJS_ToDo_Sample_for_Multi-Device_Hybrid_Apps\\JavaScript\\AngularJSTodo\\EXEC 1   1 AngularJSToDo\n\nThe command \"\"C:\\Users\\wil\\AppData\\Roaming\\npm\\node_modules\\vs-mda\\vs-cli\" prepare --platform Android --configuration Debug --projectDir . --projectName \"AngularJSToDo\"\" exited with code 8.\n\nC:\\Users\\wil\\AppData\\Roaming\\npm\\node_modules\\vs-mda-targets\\Microsoft.MDA.targets  115 5   AngularJSToDo\n\n\nI tried @Freddy's answer, everything looked right but it didn't work. I got this:\n\n1&gt;------ Build started: Project: BlankCordovaApp2, Configuration: Debug Android ------\n1&gt;C:\\Program Files (x86)\\MSBuild\\Microsoft\\VisualStudio\\v12.0\\TypeScript\\Microsoft.TypeScript.targets(90,5): warning : The TypeScript Compiler was given no files for compilation, so it will skip compiling.\n1&gt;  Your environment has been set up for using Node.js 0.10.22 (x64) and npm.\n1&gt;  ------ Ensuring correct global installation of package from source package directory: C:\\Program Files (x86)\\Microsoft Visual Studio 12.0\\Common7\\IDE\\Extensions\\ojeaygbd.tal\\packages\\vs-mda\n1&gt;  ------ Name from source package.json: vs-mda\n1&gt;  ------ Version from source package.json: 0.1.1\n1&gt;  ------ Current globally installed version : 0.1.1\n1&gt;  ------ Build settings:\n1&gt;  ------    buildCommand: prepare\n1&gt;  ------    platform: Android\n1&gt;  ------    cordovaPlatform: android\n1&gt;  ------    configuration: Debug\n1&gt;  ------    cordovaConfiguration: Debug\n1&gt;  ------    projectName: BlankCordovaApp2\n1&gt;  ------    projectSourceDir: C:\\Project\\vsPhoneGap\\BlankCordovaApp2\n1&gt;  ------ Creating app at C:\\Project\\vsPhoneGap\\BlankCordovaApp2\\bld\\Debug\n1&gt;  Creating a new cordova project with name \"HelloCordova\" and id \"io.cordova.hellocordova\" at location \"C:\\Project\\vsPhoneGap\\BlankCordovaApp2\\bld\\Debug\"\n1&gt;  Using stock cordova hello-world application.\n1&gt;  cordova library for \"www\" already exists. No need to download. Continuing.\n1&gt;  Copying stock Cordova www assets into \"C:\\Project\\vsPhoneGap\\BlankCordovaApp2\\bld\\Debug\\www\"\n1&gt;  ------ Copying app files to www\n1&gt;  ------ Done copying app files to www\n1&gt;  ------ Copying res files\n1&gt;  ------ Creating directory: res\n1&gt;  ------ Done copying res files\n1&gt;  ------ Adding platform: android\n1&gt;  cordova library for \"android\" already exists. No need to download. Continuing.\n1&gt;  Checking if platform \"android\" passes minimum requirements...\n1&gt;  Creating android project...\n1&gt;  Running command: C:\\Users\\wil\\.cordova\\lib\\android\\cordova\\3.4.0\\bin\\create.bat --cli C:\\Project\\vsPhoneGap\\BlankCordovaApp2\\bld\\Debug\\platforms\\android io.cordova.BlankCordovaApp2 BlankCordovaApp2\n1&gt;  \n1&gt;  C:\\Users\\wil\\.cordova\\lib\\android\\cordova\\3.4.0\\bin\\node_modules\\q\\q.js:126\n1&gt;                      throw e;\n1&gt;                            ^\n1&gt;EXEC : error : An error occurred while listing Android targets\n1&gt;      at C:\\Users\\wil\\.cordova\\lib\\android\\cordova\\3.4.0\\bin\\lib\\check_reqs.js:87:29\n1&gt;      at _rejected (C:\\Users\\wil\\.cordova\\lib\\android\\cordova\\3.4.0\\bin\\node_modules\\q\\q.js:808:24)\n1&gt;      at C:\\Users\\wil\\.cordova\\lib\\android\\cordova\\3.4.0\\bin\\node_modules\\q\\q.js:834:30\n1&gt;      at Promise.when (C:\\Users\\wil\\.cordova\\lib\\android\\cordova\\3.4.0\\bin\\node_modules\\q\\q.js:1079:31)\n1&gt;      at Promise.promise.promiseDispatch (C:\\Users\\wil\\.cordova\\lib\\android\\cordova\\3.4.0\\bin\\node_modules\\q\\q.js:752:41)\n1&gt;      at C:\\Users\\wil\\.cordova\\lib\\android\\cordova\\3.4.0\\bin\\node_modules\\q\\q.js:574:44\n1&gt;      at flush (C:\\Users\\wil\\.cordova\\lib\\android\\cordova\\3.4.0\\bin\\node_modules\\q\\q.js:108:17)\n1&gt;      at process._tickCallback (node.js:415:13)\n1&gt;  Command finished with error code 8: C:\\Users\\wil\\.cordova\\lib\\android\\cordova\\3.4.0\\bin\\create.bat --cli,C:\\Project\\vsPhoneGap\\BlankCordovaApp2\\bld\\Debug\\platforms\\android,io.cordova.BlankCordovaApp2,BlankCordovaApp2\n1&gt;  \n1&gt;  C:\\Users\\wil\\AppData\\Roaming\\npm\\node_modules\\vs-mda\\node_modules\\q\\q.js:126\n1&gt;                      throw e;\n1&gt;                            ^\n1&gt;EXEC : error : C:\\Users\\wil\\.cordova\\lib\\android\\cordova\\3.4.0\\bin\\create.bat: Command failed with exit code 8\n1&gt;      at ChildProcess.whenDone (C:\\Users\\wil\\AppData\\Roaming\\npm\\node_modules\\vs-mda\\node_modules\\cordova\\src\\superspawn.js:126:23)\n1&gt;      at ChildProcess.EventEmitter.emit (events.js:98:17)\n1&gt;      at maybeClose (child_process.js:735:16)\n1&gt;      at Process.ChildProcess._handle.onexit (child_process.js:802:5)\n1&gt;C:\\Users\\wil\\AppData\\Roaming\\npm\\node_modules\\vs-mda-targets\\Microsoft.MDA.targets(115,5): error MSB3073: The command \"\"C:\\Users\\wil\\AppData\\Roaming\\npm\\node_modules\\vs-mda\\vs-cli\" prepare --platform Android --configuration Debug --projectDir . --projectName \"BlankCordovaApp2\"\" exited with code 8.\n========== Build: 0 succeeded, 1 failed, 0 up-to-date, 0 skipped ==========\n========== Deploy: 0 succeeded, 0 failed, 0 skipped ==========\n\n\n\n00:03.210 - Failed  - Debug Android - BlankCordovaApp2.jsproj\n\nTotal build time: 00:00.000\n\n========== : 0 succeeded or up-to-date, 1 failed, 0 skipped, Completed at 5/17/2014 12:01:25 AM ==========\n\n    \n\nI was getting the same errors.  There are three environment variables that must be defined:\n\n1) %JAVA_HOME% -- C:\\Program Files (x86)\\Java\\jdk1.7.0_55\n\n2) %ADT_HOME% -- C:\\Users\\YOUR_NAME_GOES_HERE\\AppData\\Local\\Android\\android-sdk\n\n3) %ANT_HOME%  -- C:\\apache-ant-1.9.3\n\nNOTE -- The paths for these programs may be totally different on your PC - You have to figure out the install paths for each.\n\nOnce you have defined these variables then copy the entry below into the Path variable for your user account (I have read elsewhere that it is best to add these to the beginning of the entry and not the end - thats what I did and it worked for me).  Reboot PC after making all changes.\n\n%JAVA_HOME%\\bin;%ADT_HOME%\\tools;%ADT_HOME%\\platform-tools;%ANT_HOME%\\bin;\n\n\n*Taken from the \"Installing 3rd party software manually\" section in the official documentation.\n    \n\ni had same problem, after installing Android SDK Build-tool from Android SDK Manager\nthe problem was solved\n\n\ngo to directory of Android SDK\nas ...\\AppData\\Local\\Android\\android-sdk\nopen SDK Manager.exe\nin list of packages \nAndroid SDK Build tools package not install\nand sdk manager will auto select lastest Rev. for you to install it\nclick Install Packages\nwait until it done\ntry to build project again\n\n\nand i got apk files at directory\n.........\\bin\\Android\\Debug\nand i can run it in BlueStacks\n\nsorry my english language is bad . -*-\n    \n\nGO to Command Prompt and type:\n\nnpm install -g glob\n\n\nNow try running your app, It should work.\n    \n\nTry to following these steps in order:\n\n\nEnsure that the Android SDK is installed at %localappdata%/Android/android-sdk\nEnsure that Android is updated to the latest API (API 19 at the time of writing)\nEnsure the Android SDK path is setup correctly:\n\n\nAdd ADT_HOME as an environment variable and point it to the android-sdk folder\nAdd to System Path: \"%ADT_HOME%\\tools;%ADT_HOME%\\platform-tools\"\n\nReboot your machine\n\n\nThis should resolve the issue.\n\n*Taken from the \"Installing 3rd party software manually\" section in the official documentation.\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Android Studio not deploying changes to app", "id": 1670, "answers": [{"answer_id": 1657, "document_id": 1243, "question_id": 1670, "text": "Just go to \"File -> Settings -> Build, Execution, Deployement -> Instant Run\" and just disable it. With this Android Studio builds from scratch each time but it's better than not building it right", "answer_start": 195, "answer_category": null}], "is_impossible": false}], "context": "I've got the same problem. This thread popped up first when I've searched for it. Solved it (hoping so) by not using the Instant Run feature. Might not be the best solution but it works for now.\nJust go to \"File -> Settings -> Build, Execution, Deployement -> Instant Run\" and just disable it. With this Android Studio builds from scratch each time but it's better than not building it right.\nMini rant: Almost each version of Android Studio comes with an annoying bug. They're about to release the version 2 but its beta is still buggy. I hope they stabilize the IDE in the near future.\nSometimes this scenario occurs when developing. I would make a change in my source code, hit save all and then run but the change wouldn't be apparently not reflected in the app -(I'm using a device for testing). I can even uninstall the app on my device, and hit run again and the newly installed app still hasn't reflected the change in the source code. When this happens I have to edit the source, hit run and maybe then a new version with the changes I expected will be on the device.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to install CL-Opengl with SBCL?", "id": 977, "answers": [{"answer_id": 972, "document_id": 600, "question_id": 977, "text": "You should install quicklisp. Then to load all the packages you only need to do (ql:quickload '<system-name>)", "answer_start": 7758, "answer_category": null}], "is_impossible": false}], "context": "Quicklisp beta\nQuicklisp is a library manager for Common Lisp. It works with your existing Common Lisp implementation to download, install, and load any of over 1,500 libraries with a few simple commands.\n\nQuicklisp is easy to install and works with ABCL, Allegro CL, Clasp, Clozure CL, CLISP, CMUCL, ECL, LispWorks, MKCL, SBCL, and Scieneer CL, on Linux, Mac OS X, and Windows. The libraries were last updated on October 21, 2021.\n\nTo get started with the Quicklisp beta, download and load https://beta.quicklisp.org/quicklisp.lisp\n\nPGP signature of quicklisp.lisp\nsha256 of quicklisp.lisp = 4a7a5c2aebe0716417047854267397e24a44d0cce096127411e9ce9ccfeb2c17\n\nQuicklisp is provided as-is without warranty of any kind. See the release notes for known problems.\n\nInstallation\nLoading After Installation\nBasic Commands\nFrequently Asked Questions\nLicense information\nTo get news and announcements, follow @quicklisp on twitter or read the Quicklisp blog. If you have any questions or comments, please email me, use the Quicklisp discussion group mailing list, or visit #quicklisp on freenode. Quicklisp code is hosted on GitHub.\n\nInstallation\nTo install Quicklisp, download quicklisp.lisp and load it.\n\nTo verify the integrity of quicklisp.lisp, you can download its detached PGP signature file and verify it against the Quicklisp release signing key, which has a fingerprint of D7A3 489D DEFE 32B7 D0E7 CC61 3079 65AB 028B 5FF7, an id of 028B5FF7, and an email of release@quicklisp.org.\n\nYou only need to load quicklisp.lisp once to install Quicklisp. See Loading After Installation for how to load the installed Quicklisp into your Lisp session after the initial installation.\n\nHere's an example installation session on MacOS X with SBCL. In this example, $ is the shell prompt and * is the Lisp REPL prompt.\n\n$ curl -O https://beta.quicklisp.org/quicklisp.lisp\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100 49843  100 49843    0     0  33639      0  0:00:01  0:00:01 --:--:-- 50397\n\n$ curl -O https://beta.quicklisp.org/quicklisp.lisp.asc\n...\n$ gpg --verify quicklisp.lisp.asc quicklisp.lisp\ngpg: Signature made Sat Feb  1 09:25:28 2014 EST using RSA key ID 028B5FF7\ngpg: Good signature from \"Quicklisp Release Signing Key \"\n$ sbcl --load quicklisp.lisp\nThis is SBCL 1.0.42.52, an implementation of ANSI Common Lisp.\nMore information about SBCL is available at <http://www.sbcl.org/>.\n\nSBCL is free software, provided as is, with absolutely no warranty.\nIt is mostly in the public domain; some portions are provided under\nBSD-style licenses.  See the CREDITS and COPYING files in the\ndistribution for more information.\n\n  ==== quicklisp quickstart loaded ====\n\n    To continue, evaluate: (quicklisp-quickstart:install)\n\n* (quicklisp-quickstart:install)\n\n; Fetching #<URL \"http://beta.quicklisp.org/quickstart/asdf.lisp\">\n; 144.48KB\n==================================================\n147,949 bytes in 0.64 seconds (226.11KB/sec)\n; Fetching #<URL \"http://beta.quicklisp.org/quickstart/quicklisp.tar\">\n; 160.00KB\n==================================================\n163,840 bytes in 0.76 seconds (211.36KB/sec)\n; Fetching #<URL \"http://beta.quicklisp.org/quickstart/setup.lisp\">\n; 2.78KB\n==================================================\n2,846 bytes in 0.001 seconds (2779.30KB/sec)\nUpgrading ASDF package from version 2.004 to version 2.009\n; Fetching #<URL \"http://beta.quicklisp.org/dist/quicklisp.txt\">\n; 0.40KB\n==================================================\n408 bytes in 0.003 seconds (132.81KB/sec)\n\n  ==== quicklisp installed ====\n\n    To load a system, use: (ql:quickload \"system-name\")\n\n    To find systems, use: (ql:system-apropos \"term\")\n\n    To load Quicklisp every time you start Lisp, use: (ql:add-to-init-file)\n\n    For more information, see http://www.quicklisp.org/beta/\n\nNIL\n* (ql:system-apropos \"vecto\")\n\n; Fetching #<URL \"http://beta.quicklisp.org/dist/quicklisp/2010-10-07/systems.txt\">\n; 45.30KB\n==================================================\n46,386 bytes in 0.50 seconds (89.70KB/sec)\n; Fetching #<URL \"http://beta.quicklisp.org/dist/quicklisp/2010-10-07/releases.txt\">\n; 83.49KB\n==================================================\n85,490 bytes in 0.53 seconds (157.22KB/sec)\n#<SYSTEM cl-vectors / cl-vectors-20101006-git / quicklisp 2010-10-07>\n#<SYSTEM lispbuilder-sdl-cl-vectors / lispbuilder-20101006-svn / quicklisp 2010-10-07>\n#<SYSTEM lispbuilder-sdl-cl-vectors-examples / lispbuilder-20101006-svn / quicklisp 2010-10-07>\n#<SYSTEM lispbuilder-sdl-vecto / lispbuilder-20101006-svn / quicklisp 2010-10-07>\n#<SYSTEM lispbuilder-sdl-vecto-examples / lispbuilder-20101006-svn / quicklisp 2010-10-07>\n#<SYSTEM static-vectors / static-vectors-20101006-git / quicklisp 2010-10-07>\n#<SYSTEM vecto / vecto-1.4.3 / quicklisp 2010-10-07>\nNIL\n* (ql:quickload \"vecto\")\nTo load \"vecto\":\n  Install 5 Quicklisp releases:\n    cl-vectors salza2 vecto zpb-ttf zpng\n; Fetching #<URL \"http://beta.quicklisp.org/archive/salza2/2010-10-06/salza2-2.0.7.tgz\">\n; 14.84KB\n==================================================\n15,197 bytes in 0.12 seconds (125.77KB/sec)\n; Fetching #<URL \"http://beta.quicklisp.org/archive/zpng/2010-10-06/zpng-1.2.tgz\">\n; 38.59KB\n==================================================\n39,521 bytes in 0.37 seconds (103.47KB/sec)\n; Fetching #<URL \"http://beta.quicklisp.org/archive/zpb-ttf/2010-10-06/zpb-ttf-1.0.tgz\">\n; 42.59KB\n==================================================\n43,611 bytes in 0.39 seconds (110.33KB/sec)\n; Fetching #<URL \"http://beta.quicklisp.org/archive/cl-vectors/2010-10-06/cl-vectors-20101006-git.tgz\">\n; 40.40KB\n==================================================\n41,374 bytes in 0.37 seconds (109.20KB/sec)\n; Fetching #<URL \"http://beta.quicklisp.org/archive/vecto/2010-10-06/vecto-1.4.3.tgz\">\n; 75.71KB\n==================================================\n77,526 bytes in 0.49 seconds (153.57KB/sec)\n; Loading \"vecto\"\n..................................................\n[package zpb-ttf].................................\n[package salza2]..................................\n[package zpng]....................................\n[package net.tuxee.paths].........................\n[package net.tuxee.aa]............................\n[package net.tuxee.aa-bin]........................\n[package net.tuxee.vectors].......................\n[package vecto]........\n(\"vecto\")\n* (ql:add-to-init-file)\nI will append the following lines to #P\"/Users/quicklisp/.sbclrc\":\n\n  ;;; The following lines added by ql:add-to-init-file:\n  #-quicklisp\n  (let ((quicklisp-init (merge-pathnames \"quicklisp/setup.lisp\"\n                                         (user-homedir-pathname))))\n    (when (probe-file quicklisp-init)\n      (load quicklisp-init)))\n\nPress Enter to continue.\n\n\n#P\"/Users/quicklisp/.sbclrc\"\n* (quit)\n$ \nAlthough this example uses a Unix shell, the same procedure would work for installation in the REPL of a Windows or Mac GUI-based CL implementation.\n\nLoading After Installation\nYou only need to install Quicklisp with quicklisp.lisp once. To load Quicklisp into your Common Lisp session after the initial installation, load the file quicklisp/setup.lisp with the Common Lisp load function.\n\nFor example, if you installed Quicklisp in the default location in your home directory, the following command suffices to load Quicklisp into a running session:\n\n    (load \"~/quicklisp/setup.lisp\")\nThe path must be adjusted if Quicklisp is loaded in a different location, or if your implementation does not support the ~ pathname syntax (i.e. CMUCL).\n\nThe function ql:add-to-init-file will add code to do this to your init file automatically, so Quicklisp will load whenever your Common Lisp session starts.\nYou should install quicklisp. Then to load all the packages you only need to do (ql:quickload '<system-name>)\nBasic Commands\nTo load software, use:\n\n    (ql:quickload system-name)\nFor example:\n\n    CL-USER> (ql:quickload \"vecto\")\n    To load \"vecto\":\n      Install 5 Quicklisp releases:\n\tcl-vectors salza2 vecto zpb-ttf zpng\n    loading output here\nQuicklisp will automatically download any supporting software it needs to load the system.\n\nPlease note that some systems have different names than their projects. For example, to load cl-yacc's system, run (ql:quickload \"yacc\"), not (ql:quickload \"cl-yacc\").\n\nql:quickload can also be used to load local systems that aren't part of Quicklisp. If the local system depends on software that is available through Quicklisp, the dependencies will be automatically loaded.\n\nBy default, ql:quickload hides most compilation and loading output, including warnings, and shows progess as a series of dots. You can show full compilation and loading output by passing :verbose t as arguments to ql:quickload. This output can be especially helpful when reporting and troubleshooting problems.\n\nTo remove software, use:\n\n    (ql:uninstall system-name)\nFor example:\n\n    CL-USER> (ql:uninstall \"vecto\")\n    T\nAn uninstall does the following:\n\nDeletes the system's tarball archive and unpacked source files\nDeletes Quicklisp metadata files associated with the system\nClears ASDF's system cache via asdf:clear-system\nUninstalling does not alter the current Lisp session in any other way; if the given system has been loaded, it remains loaded and accessible until the session is ended.\n\nTo find out what's available in Quicklisp, use:\n\n    (ql:system-apropos substring)\nFor example:\n\n    CL-USER> (ql:system-apropos \"xml\")\n    #<SYSTEM bknr.xml / bknr-datastore-20100916-git / quicklisp 2010-09-16>\n    #<SYSTEM cl-libxml2 / cl-libxml2-0.3.4 / quicklisp 2010-09-16>\n    #<SYSTEM cxml / cxml-20100916-git / quicklisp 2010-09-16>\n    #<SYSTEM cxml-dom / cxml-20100916-git / quicklisp 2010-09-16>\n    #<SYSTEM cxml-klacks / cxml-20100916-git / quicklisp 2010-09-16>\n    #<SYSTEM cxml-rng / cxml-rng-2008-11-30 / quicklisp 2010-09-16>\n    #<SYSTEM cxml-rpc / cxml-rpc-20100916-git / quicklisp 2010-09-16>\n    #<SYSTEM cxml-stp / cxml-stp-2008-11-30 / quicklisp 2010-09-16>\n    #<SYSTEM cxml-stp-test / cxml-stp-2008-11-30 / quicklisp 2010-09-16>\n    #<SYSTEM cxml-test / cxml-20100916-git / quicklisp 2010-09-16>\n    #<SYSTEM cxml-xml / cxml-20100916-git / quicklisp 2010-09-16>\n    #<SYSTEM pithy-xml / pithy-xml-20100916-git / quicklisp 2010-09-16>\n    #<SYSTEM s-xml / s-xml-20100916-http / quicklisp 2010-09-16>\n    #<SYSTEM s-xml-rpc / s-xml-rpc-20100916-http / quicklisp 2010-09-16>\n    #<SYSTEM xml-render / cl-typesetting-20100916-svn / quicklisp 2010-09-16>\n    #<SYSTEM xmls / xmls-1.3 / quicklisp 2010-09-16>\nThe system name is the word after \"SYSTEM\" and before the first \"/\", e.g. \"cxml-stp\". System names are used by ql:quickload and for expressing dependency information in system definitions.\n\nTo load Quicklisp when you start Lisp, use:\n\n    (ql:add-to-init-file)\nQuicklisp will append code to your Lisp's init file that will load Quicklisp on startup.\n\nTo install and configure SLIME, use:\n\n    (ql:quickload \"quicklisp-slime-helper\")\nThen follow the directions it displays. quicklisp-slime-helper will create a file you can load in Emacs that configures the right load-path for loading Quicklisp's installation of SLIME.\n\nTo get updated software, use:\n\n    (ql:update-dist \"quicklisp\")\nSoftware updates are usually available about once per month.\n\nTo update the Quicklisp client, use:\n\n    (ql:update-client)\nQuicklisp client updates are usually available a few times per year.\nTo go back to a previous set of libraries, see the blog post Going back in (dist) time. This can be useful if your project depends on the state (APIs, etc) of the Quicklisp library universe from a particular point in time.\n\nTo see what systems depend on a particular system, use:\n\n   (ql:who-depends-on system-name)\nLicense information\n  Copyright \u00a9 2010 Zachary Beane <zach@quicklisp.org>\n\n  Permission is hereby granted, free of charge, to any person obtaining a copy\n  of this software and associated documentation files (the \"Software\"), to deal\n  in the Software without restriction, including without limitation the rights\n  to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n  copies of the Software, and to permit persons to whom the Software is\n  furnished to do so, subject to the following conditions:\n\n  The above copyright notice and this permission notice shall be included in\n  all copies or substantial portions of the Software.\n\n  THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n  FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n  OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n  THE SOFTWARE.\nPortions of Quicklisp are derived from Pierre Mai's Deflate library, which is available under the following terms:\n\n  Deflate --- RFC 1951 Deflate Decompression\n\n  Copyright (C) 2000-2009 PMSF IT Consulting Pierre R. Mai.\n\n  Permission is hereby granted, free of charge, to any person obtaining\n  a copy of this software and associated documentation files (the\n  \"Software\"), to deal in the Software without restriction, including\n  without limitation the rights to use, copy, modify, merge, publish,\n  distribute, sublicense, and/or sell copies of the Software, and to\n  permit persons to whom the Software is furnished to do so, subject to\n  the following conditions:\n\n  The above copyright notice and this permission notice shall be\n  included in all copies or substantial portions of the Software.\n\n  THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n  EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n  MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n  IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY CLAIM, DAMAGES OR\n  OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\n  ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\n  OTHER DEALINGS IN THE SOFTWARE.\n\n  Except as contained in this notice, the name of the author shall\n  not be used in advertising or otherwise to promote the sale, use or\n  other dealings in this Software without prior written authorization\n  from the author.\nZach Beane 2015-04-30", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "assertion error from pip list in virtualenv", "id": 1941, "answers": [{"answer_id": 1928, "document_id": 1520, "question_id": 1941, "text": "pip install --upgrade distribute", "answer_start": 1490, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nPip list is throwing an Assertion error and I'm not sure how to resolve. This has just happened after building 2 packages (PyUblas-2013.1 and boost_1_54_0) from source. I am using virtualenv.\n\nError below;\n\n(virtenv)[user@xyz ~]$ pip list\nbeautifulsoup4 (4.2.1)\nbiopython (1.61)\ndistribute (0.6.35)\nmethylpy (0.1.0)\nMySQL-python (1.2.4)\nnumpy (1.7.1)\npip (1.4)\npy (1.4.15)\npytest (2.3.5)\nPyUblas (2013.1)\nException:\nTraceback (most recent call last):\n  File \"/home/user/virtenv/lib/python2.7/site-packages/pip/basecommand.py\", line 134, in main\n    status = self.run(options, args)\n  File \"/home/user/virtenv/lib/python2.7/site-packages/pip/commands/list.py\", line 80, in run\n    self.run_listing(options)\n  File \"/home/user/virtenv/lib/python2.7/site-packages/pip/commands/list.py\", line 127, in run_listing\n    self.output_package_listing(installed_packages)\n  File \"/home/user/virtenv/lib/python2.7/site-packages/pip/commands/list.py\", line 136, in output_package_listing\n    if dist_is_editable(dist):\n  File \"/home/user/virtenv/lib/python2.7/site-packages/pip/util.py\", line 347, in dist_is_editable\n    req = FrozenRequirement.from_dist(dist, [])\n  File \"/home/user/virtenv/lib/python2.7/site-packages/pip/__init__.py\", line 194, in from_dist\n    assert len(specs) == 1 and specs[0][0] == '=='\nAssertionError\n\n\nCan anyone help me troubleshoot???\n\nThanks,\n    \n\nI think it is because the distribute package is out of date. Certainly the following fixed it for me:\n\npip install --upgrade distribute\n\n    \n\nYour pip may be outdated. Even in Ubuntu 14.04 LTS, the pip version it installed using apt-get install python-pip was 1.5.4. Try updating pip manually, and possibly the new packages again as well.\n\npip --version # 1.5.4\ncurl -O https://bootstrap.pypa.io/get-pip.py\nsudo python get-pip.py\npip --version # 6.0.8\n\n\nhttps://pip.pypa.io/en/latest/installing.html\n    \n\nI know it's old but since I had the same problem and the fix didn't helped. Guess does not harm adding my solution.\n\nJust updated pip, from inside of my virtualenv. \n\npip install --upgrade pip\n\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "What installation product to use? InstallShield, WiX, Wise, Advanced Installer, ", "id": 750, "answers": [{"answer_id": 751, "document_id": 438, "question_id": 750, "text": "You shall check out the freeware Inno Setup: for the long time I'm using it", "answer_start": 328, "answer_category": null}], "is_impossible": false}], "context": "I'm currently doing some investigation on moving off of the installation package we currently use (Wise Installer 9) and moving to something that will handle things like Windows Vista, Windows 7 and 64-bit systems. Localization of the installers would be of benefit since we do have a number of French Canadian clients as well. You shall check out the freeware Inno Setup: for the long time I'm using it, it never disapointed me!", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "laravel permission denied in laravel blade file", "id": 2005, "answers": [{"answer_id": 1991, "document_id": 1595, "question_id": 2005, "text": "for me.\n\nsudo chmod -R 754", "answer_start": 2223, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI have the following issue when trying to install Laravel http://laravel.com\n\nAny help, would be greatly appreciated.\n\n\n  Unhandled Exception\n  \n  Message:\n  \n  file_put_contents(/Users/alvincrespo/Sites/apollo/storage/views/26bdebca7505781c753aa21663170a1b)\n  [function.file-put-contents]: failed to open stream: Permission denied\n    Location:\n  \n  /Users/alvincrespo/Sites/apollo/laravel/blade.php on line 63    Stack\n  Trace:\n  \n  #0 /Users/alvincrespo/Sites/apollo/laravel/laravel.php(40):\n  Laravel\\Error::native(2, 'file_put_conten...', '/Users/alvincre...',\n  63)   #1 [internal function]: Laravel{closure}(2,\n  'file_put_conten...', '/Users/alvincre...', 63, Array)    #2\n  /Users/alvincrespo/Sites/apollo/laravel/blade.php(63):\n  file_put_contents('/Users/alvincre...', '...')    #3 [internal\n  function]: Laravel{closure}(Object(Laravel\\View))    #4\n  /Users/alvincrespo/Sites/apollo/laravel/event.php(199):\n  call_user_func_array(Object(Closure), Array)  #5\n  /Users/alvincrespo/Sites/apollo/laravel/event.php(138):\n  Laravel\\Event::fire('laravel.view.en...', Array, true)    #6\n  /Users/alvincrespo/Sites/apollo/laravel/view.php(325):\n  Laravel\\Event::until('laravel.view.en...', Array)     #7\n  /Users/alvincrespo/Sites/apollo/laravel/view.php(546):\n  Laravel\\View-&gt;render()    #8\n  /Users/alvincrespo/Sites/apollo/laravel/response.php(246):\n  Laravel\\View-&gt;__toString()    #9\n  /Users/alvincrespo/Sites/apollo/laravel/laravel.php(138):\n  Laravel\\Response-&gt;render()    #10\n  /Users/alvincrespo/Sites/apollo/public/index.php(34):\n  require('/Users/alvincre...')     #11 {main}\n\n    \n\nI would check that the permissions on:\n\n/Users/alvincrespo/Sites/apollo/storage/views/26bdebca7505781c753aa21663170a1b\n\n\nAllow your application to write to this directory.\n\nAssuming you are on a linux box you could run ls -l to see what the permissions are, and if it is set to read only, change the permissions with chmod.\n    \n\nAs you can see in the very good documentation under install, you have to make the directory storage/views writeable.\n\nhttp://laravel.com/docs/install#installation\n    \n\nAlthough it\u2019s an old question but I hope this might help someone, here is what solved the problem for me.\n\nsudo chmod -R 754 storage/\n\n    \n\ncheck the permissions of /storage directory in the laravel installation. The views directory must have write permisison for the webserver that is it.\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Error: gdal-config not found while installing R dependent packages whereas gdal is installed", "id": 658, "answers": [{"answer_id": 663, "document_id": 351, "question_id": 658, "text": "sudo apt-get install gdal-bin proj-bin libgdal-dev libproj-dev\nThen install rgdal by\ninstall.packages(\"rgdal\")\nLoad rgdal by\nlibrary(rgdal)", "answer_start": 227, "answer_category": null}], "is_impossible": false}], "context": "Please point out the point that I am missing: openSUSE 11.3. his happens because the configuration failed for package \u2018rgdal\u2019 so we have to install necessary dependencies.\nThe packages libgdal-dev and libproj-dev are required:\nsudo apt-get install gdal-bin proj-bin libgdal-dev libproj-dev\nThen install rgdal by\ninstall.packages(\"rgdal\")\nLoad rgdal by\nlibrary(rgdal)\nJust a tip for others, pay attention to the terminal output after running it, as there may be other things you need to do (I had to install something else before gdal would install, and I had to link it after installation - but brew let me know how to do this in the output)\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to avoid the \"Windows Defender SmartScreen prevented an unrecognized app from starting warning\"", "id": 763, "answers": [{"answer_id": 763, "document_id": 450, "question_id": 763, "text": "Right click on installer(.exe)\n2.\tSelect properties option.\n3.\tClick on checkbox to check Unblock at the bottom of Properties.", "answer_start": 500, "answer_category": null}], "is_impossible": false}], "context": "My company distributes an installer to customers via our website. Recently when I download via the website and try to run the installer I get the warning message:\nWindows protected your PC\nWindows Defender SmartScreen prevented an unrecognized app from starting. Running this app might put your PC at risk. After clicking on Properties of any installer(.exe) which block your application to install (Windows Defender SmartScreen prevented an unrecognized app ) for that issue i found one solution\n1.\tRight click on installer(.exe)\n2.\tSelect properties option.\n3.\tClick on checkbox to check Unblock at the bottom of Properties.\nThis solution work for Heroku CLI (heroku-x64) installer(.exe)\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "pthread not working in php on LINUX?", "id": 939, "answers": [{"answer_id": 934, "document_id": 585, "question_id": 939, "text": "1) Download PHP sources and Unpack PHP\n\n2) Download PEAR\n     wget http://pear.php.net/go-pear.phar\n     php go-pear.phar\n\n3) Download pthreads\nGet PECL extension (PECL is a repository for PHP Extensions)\n\n# pecl install pthread-0.4.4\n\n4) Unpack pthreads\ncopy pthread-0.4.4  to  php/ext\n(for ./configure allow  add option --enable-pthreads)\n\n# mv build/php-src-master/ext/pthreads-master    build/php-src-master/ext/pthreads\n\n5)  Reconfigure sources\n# ./buildconf --force\n# ./configure --help | grep pthreads\n\nYou should see the appropriate --enable-pthreads option listed as a result, if you do not, then\n\n# rm -rf aclocal.m4\n# rm -rf autom4te.cache/\n# ./buildconf --force\n\n6) Build PHP\nCompile PHP source code\nAdd:\n# ./configure --enable-debug --enable-maintainer-zts --enable-pthreads\n\n7) Installing PHP\n# make\n# sudo make install\n\n8) Update php.ini\nAdd in php.ini\nextension=pthreads.so\nInclude_path = \u201c/usr/local/lib/php\u201d\n\n9) Check Modules\nphp -m (check pthread loaded)\n\n10) Test Thread Class\n# php SimpleTest.php", "answer_start": 5816, "answer_category": null}], "is_impossible": false}, {"question": "pthread not working in php on windows?", "id": 938, "answers": [{"answer_id": 933, "document_id": 585, "question_id": 938, "text": "1.  Find out what is your 'PHP Extension Build' version by using phpinfo(). You can use this - http://localhost/?phpinfo=1\n\n2.  Download the pthreads that matches your php version (32 bit or 64 bit) and php extension build (currently used VC11). Use this link for download - http://windows.php.net/downloads/pecl/releases/pthreads/\n\n3.  Extract the zip -\n      Move php_pthreads.dll to the 'bin\\php\\ext\\' directory.\n      Move pthreadVC2.dll to the 'bin\\php\\' directory.\n      Move pthreadVC2.dll to the 'bin\\apache\\bin' directory.\n      Move pthreadVC2.dll to the 'C:\\windows\\system32' directory.\n\n4.  Open php\\php.ini and add\n      extension=php_pthreads.dll", "answer_start": 610, "answer_category": null}], "is_impossible": false}], "context": "Installation \u00b6\npthreads releases are hosted by PECL and the source code by \u00bb github, the easiest route to installation is the normal PECL route: \u00bb https://pecl.php.net/package/pthreads.\n\nWindows users can download prebuilt release binaries from the \u00bb PECL website.\n\nCaution\nWindows users need to take the additional step of adding pthreadVC2.dll (distributed with Windows releases) to their PATH.\n\nadd a note add a note\nUser Contributed Notes 13 notes\nup\ndown\n73zahid dot smz at gmail dot com \u00b67 years ago\nFor Wampp (Windows)\n-----------------------------------------------------------------------------------\n1.  Find out what is your 'PHP Extension Build' version by using phpinfo(). You can use this - http://localhost/?phpinfo=1\n\n2.  Download the pthreads that matches your php version (32 bit or 64 bit) and php extension build (currently used VC11). Use this link for download - http://windows.php.net/downloads/pecl/releases/pthreads/\n\n3.  Extract the zip -\n      Move php_pthreads.dll to the 'bin\\php\\ext\\' directory.\n      Move pthreadVC2.dll to the 'bin\\php\\' directory.\n      Move pthreadVC2.dll to the 'bin\\apache\\bin' directory.\n      Move pthreadVC2.dll to the 'C:\\windows\\system32' directory.\n\n4.  Open php\\php.ini and add\n      extension=php_pthreads.dll\n\nNow restart server and you are done. Thanks.\nup\ndown\n50Michel Phillipe Luca \u00b66 years ago\nHere is how I got it working under Linux Ubuntu distro - WITHOUT USE PECL:\n\nWe will download both, PHP and Pthread without PECL\n\n1 - Get PHP version\nFor this example we will use version: 5.4.36\n\n# wget http://www.php.net/distributions/php-5.4.36.tar.gz\n\n2- Get Pthreads version:\nI'm using an old version but, you could take any one\n\n# wget http://pecl.php.net/get/pthreads-1.0.0.tgz\n\nExtract both, php and pthreads versions\n\n#tar zxvf php-5.4.36.tar.gz\n#tar zxvf pthreads-1.0.0.tgz\n\n3- Move Pthreads to php/ext folder. Inside version of PHP downloaded at item 1.\n\n4- Reconfigure sources\n# ./buildconf --force\n# ./configure --help | grep pthreads\n\nYou have to see --enable-pthreads listed. If do not, clear the buidls with this commands:\n\n# rm -rf aclocal.m4\n# rm -rf autom4te.cache/\n# ./buildconf --force\n\n5 - Inside php folder run configure command to set what we need:\n# ./configure --enable-debug --enable-maintainer-zts --enable-pthreads --prefix=/usr --with-config-file-path=/etc\n\n6 - Install PHP\nWe will run make clear just to be sure that no other crashed build will mess our new one.\n\n# make clear\n# make\n# make install\n\n7 - Copy configuration file of PHP and add local lib to include path\n# cp php.ini-development /etc/php.ini\n\nEdit php.ini and set Include_path to be like this:\n\nInclude_path = \u201c/usr/local/lib/php\u201d\n\n9 - Check Modules\n# php -m (check pthread loaded)\n\nYou have to see pthreads listed\n\n10 - If pthread is not listed, update php.ini\n# echo \"extension=pthreads.so\" >> /etc/php.ini\nup\ndown\n43Jimmy Christensen \u00b68 years ago\nOn Windows the installation is as follows:\n\nDownload the pthreads that matches your php version.\nI found mine at: http://windows.php.net/downloads/pecl/releases/pthreads/\n(I used version 0.44 wich is the newest at the time of writing this, and then downloaded the one for php 5.3 which is the version I am using).\n\nExtract the zip.\nMove php_pthreads.dll to the php\\ext\\ directory.\nMove pthreadVC2.dll to the php\\ directory.\n\nOpen php\\php.ini and add\nextension=php_pthreads.dll\n\nYou are done.\nup\ndown\n31Pedro Proenca \u00b67 years ago\nI haven't found any proper instructions on how to install pthreads in linux, so I'll leave the steps I followed:\n\n# Required libraries\nsudo apt-get install gcc make libzzip-dev libreadline-dev libxml2-dev \\\nlibssl-dev libmcrypt-dev libcurl4-openssl-dev lib32bz2-dev\n\n# Download PHP\ncd /usr/local/src\n\nwget http://www.php.net/distributions/php-<version>.tar.gz\n( e.g. wget http://www.php.net/distributions/php-5.5.8.tar.gz )\n\n# Extract\ntar zxvf php-<version>.tar.gz\n(e.g. tar zxvf php-5.5.8.tar.gz )\n\n# Configure\ncd /usr/local/src/php-<version>\n( e.g. cd /usr/local/src/php-5.5.8 )\n\n./configure --prefix=/usr --with-config-file-path=/etc --enable-maintainer-zts\n\n# Compile\nmake && make install\n( make -j3 && make -j3 install) -> Faster building\n\n# Copy configuration\ncp php.ini-development /etc/php.ini\n\n# Install pthreads\npecl install pthreads\necho \"extension=pthreads.so\" >> /etc/php.ini\n\n# Check installation\nphp -m | grep pthreads\nup\ndown\n1bens at effortlessis dot com \u00b61 year ago\nSadly, this extension seems to be in poor repair. After spending a few hours trying to get it to fly on Fedora 32 with PHP 7.4, I noticed that it's not really maintained since about 2016.\n\nAttempting to compile it with PHP 7.4 results in screens full of nasty compile errors.\nup\ndown\n5pedro dot proenca at shapedfor dot me \u00b67 years ago\nI haven't found any proper instructions on how to install pthreads in linux, so I'll leave the steps I followed:\n\n# Required libraries\nsudo apt-get install gcc make libzzip-dev libreadline-dev libxml2-dev \\\nlibssl-dev libmcrypt-dev libcurl4-openssl-dev lib32bz2-dev\n\n# Download PHP\ncd /usr/local/src\n\nwget http://www.php.net/distributions/php-<version>.tar.gz\n( e.g. wget http://www.php.net/distributions/php-5.5.8.tar.gz )\n\n# Extract\ntar zxvf php-<version>.tar.gz\n(e.g. tar zxvf php-5.5.8.tar.gz )\n\n# Configure\ncd /usr/local/src/php-<version>\n( e.g. cd /usr/local/src/php-5.5.8 )\n\n./configure --prefix=/usr --with-config-file-path=/etc --enable-maintainer-zts\n\n# Compile\nmake && make install\n( make -j3 && make -j3 install) -> Faster building\n\n# Copy configuration\ncp php.ini-development /etc/php.ini\n\n# Install pthreads\npecl install pthreads\necho \"extension=pthreads.so\" >> /etc/php.ini\n\n# Check installation\nphp -m | grep pthreads\nup\ndown\n9matias dot zumbo at gmail dot com \u00b68 years ago\nHOW TO INSTALL IN LINUX SYSTEM'S:\n------------------------------------\n\n1) Download PHP sources and Unpack PHP\n\n2) Download PEAR\n     wget http://pear.php.net/go-pear.phar\n     php go-pear.phar\n\n3) Download pthreads\nGet PECL extension (PECL is a repository for PHP Extensions)\n\n# pecl install pthread-0.4.4\n\n4) Unpack pthreads\ncopy pthread-0.4.4  to  php/ext\n(for ./configure allow  add option --enable-pthreads)\n\n# mv build/php-src-master/ext/pthreads-master    build/php-src-master/ext/pthreads\n\n5)  Reconfigure sources\n# ./buildconf --force\n# ./configure --help | grep pthreads\n\nYou should see the appropriate --enable-pthreads option listed as a result, if you do not, then\n\n# rm -rf aclocal.m4\n# rm -rf autom4te.cache/\n# ./buildconf --force\n\n6) Build PHP\nCompile PHP source code\nAdd:\n# ./configure --enable-debug --enable-maintainer-zts --enable-pthreads\n\n7) Installing PHP\n# make\n# sudo make install\n\n8) Update php.ini\nAdd in php.ini\nextension=pthreads.so\nInclude_path = \u201c/usr/local/lib/php\u201d\n\n9) Check Modules\nphp -m (check pthread loaded)\n\n10) Test Thread Class\n# php SimpleTest.php\nup\ndown\n1horica78 at yahoo dot com \u00b62 years ago\nThey can still be used in web server if used with exec('php script.php') and php-cli.ini .\nup\ndown\n0agnelvishal at gmail dot com \u00b63 years ago\nTo compile PHP 7.2 with pthreads on Ubuntu 16.04 or Ubuntu 18.04 or Debian 9 Stretch, use this bash file at https://gist.github.com/agnelvishal/24f42c65af2f6cace1e9387617a0182a\nup\ndown\n-4jason at jason-rush dot com \u00b68 years ago\nThe files are in the format:\nphp_pthreads-X.XX-Y.Y-ts-vcZZ-x86.zip\n\nwhere\nX.XX is the version of the phpthreads-win32\nY.Y is the base version of PHP that you are running\nZZ is ?? (version of compiler?)\n\nCurrently, 0.44 is the latest release of phpthreads-win32, so X.XX = 0.44\nIf you are running PHP 5.3.13, then Y.Y = 5.3\nIf you are running PHP 5.5.0, then Y.Y=5.5\netc\n\nphp_pthreads-0.44-5.3-ts-vc9-x86.zip\nphp_pthreads-0.44-5.4-ts-vc9-x86.zip\nphp_pthreads-0.44-5.5-ts-vc11-x64.zip\nphp_pthreads-0.44-5.5-ts-vc11-x86.zip\nup\ndown\n-1and dot webdev at gmail dot com \u00b64 years ago\nAlso, possible by phpbrew\n\n1. phpbrew install php-5.5.30 -- --enable-maintainer-zts\n2. phpbrew use php-5.5.30\n3. phpbrew ext install pthreads 2.0.10\nup\ndown\n-16rbfowler9lfc at hotmail dot com \u00b68 years ago\nFor those using XAMPP or other apache-php bundles, it's necessary to add LoadFile \"C:/xampp/php/pthreadVC2.dll\" to httpd-xampp.conf if you want to utilize pthreads in a browser window.\nup\ndown\n-8sawyer dot lin at gmail dot com \u00b66 years ago\nTo execute pecl install pthreads\nThe library autoconf shoud be installed at first\nhttp://stackoverflow.com/questions/9322742/php-autoconf-errors-on-mac-os-x-10-7-3-when-trying-to-install-pecl-extensions\nlike\napt-get install pthreads", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to Install Windows Phone 8 SDK on Windows 7", "id": 785, "answers": [{"answer_id": 781, "document_id": 468, "question_id": 785, "text": "You can install it by first extracting all the files from the ISO and then overwriting those files with the files from the ZIP. Then you can run the batch file as administrator to do the installation. ", "answer_start": 0, "answer_category": null}], "is_impossible": false}], "context": "You can install it by first extracting all the files from the ISO and then overwriting those files with the files from the ZIP. Then you can run the batch file as administrator to do the installation. Most of the packages install on windows 7, but I haven't tested yet how well they work. There could be several reasons to install WP 8 in Windows 7. Like I have purchased a Wndows 7 recently and I don't want to make shift to Windows 8 right now, because I don't feel comfortable with its UI. Its more for a tablet or touch screen PC.\nI have set-up all my workspace and other projects on Windows 7, I don't want to waste my time in setting up on Windows 8, I don't even know that how they gona perform on Windows 8. So there are so many genuine reasons.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "how  to add parameters to the application package when deploying java application?", "id": 599, "answers": [{"answer_id": 605, "document_id": 324, "question_id": 599, "text": "Static named parameters can be added to the application package using <fx:param> and unnamed parameters can be added using <fx:argument>.", "answer_start": 453, "answer_category": null}], "is_impossible": false}, {"question": "How to pass parameters to Java and JavaFX applications from the web page?", "id": 600, "answers": [{"answer_id": 606, "document_id": 324, "question_id": 600, "text": "Parameters can also be passed to Java and JavaFX applications from the web page that hosts the application using <fx:htmlParam>. ", "answer_start": 670, "answer_category": null}], "is_impossible": false}], "context": "5.8 Packaging Cookbook\nThis section presents several examples for typical deployment tasks.\nThe examples use Ant APIs, but in most cases the same result can be achieved using the Java Packager tool. See Chapter 10, \"JavaFX Ant Tasks\" and Chapter 9, \"The Java Packager Tool.\"\n5.8.1 Passing Parameters to the Application\nJava and JavaFX applications support two types of application parameters: named and unnamed (see the API for Application.Parameters).\nStatic named parameters can be added to the application package using <fx:param> and unnamed parameters can be added using <fx:argument>. They are applicable to all execution modes, including standalone applications.\nParameters can also be passed to Java and JavaFX applications from the web page that hosts the application using <fx:htmlParam>. <fx:htmlParam> is applicable to both embedded applications and Java Web Start applications.\nPassing parameters from the HTML page is most useful if the parameters are dynamic. To use this technique, the following approach is recommended:\n\u2022\tUse a web page template (see Section 5.7.4, \"Web Page Templates\") to provide JavaScript code to prepare dynamic parameters.\n\u2022\tPass JavaScript code as a value for <fx:htmlParam>, and specify escape=\"false\". The code is evaluated at runtime\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How can I have linked dependencies in a git repo?", "id": 1843, "answers": [{"answer_id": 1829, "document_id": 1414, "question_id": 1843, "text": "You can do this with submodules in git. In your repository, do:\ngit submodule add path_to_repo path_where_you_want_it", "answer_start": 323, "answer_category": null}], "is_impossible": false}], "context": "In my scripts, I often use libraries (mine or others') that have their own repos. I don't want to duplicate those in my repo and get stuck with updating them every time a new version comes out. However, when somebody clones the repo, it should still work locally and not have broken links.\nAny ideas about what I could do?\nYou can do this with submodules in git. In your repository, do:\ngit submodule add path_to_repo path_where_you_want_it\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "installing anaconda over existing python system", "id": 1390, "answers": [{"answer_id": 1379, "document_id": 963, "question_id": 1390, "text": "Do not uninstall Python from our existing System. The Anaconda installer install Python if necessary\n\nYou can check what the anaconda packages includes: http://docs.continuum.io/anaconda/pkg-docs", "answer_start": 1608, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI found an old windows xp machine running Python 2.5.2. I would like to use Anaconda instead. Can I just install Anaconda on it and do I have to uninstall Python 2.5.2? Similarly, I have a Mac system with Python 2.7.9 working with some NLT libraries and I'd like to get Anaconda running on it too. What's the best course of action to get Anaconda over an existing system that already has python?\n    \n\nSimply install. \n\nAnaconda manages Python for you and creates the appropriate bin directory containing the executable and pkgs directory containing installed packages. All this in a directory structure named anaconda (or anaconda3 if using Python 3). Additionally, it alters the search path so the Python inside the anaconda/bin/ directory is the one used when the command python is issued.\n\nOn Ubuntu, it looks like this:\n\n# added by Anaconda 2.3.0 installer\nexport PATH=\"/home/jim/anaconda/bin:$PATH\"\n\n\nBy adding the new path in the beginning of PATH it assures the anaconda bin/python will be located first.\n\nWarning:\n\n\n  do I have to uninstall Python 2.5.2?\n\n\nIn general never remove the 'original' Python unless explicitly allowed by official sources. In many operating systems Python is a dependency; it must stay around. I can't speak for old versions of Windows but in general if you're not sure if it is needed or not leave it. \n\nRemoving it might break some completely unrelated things.\n    \n\nThe Python installation on a Mac is not affected at all when installing Anaconda. However, Anaconda manipulates the $PATH environment variable. No need to uninstall Python.\n    \n\n1 - Do not uninstall Python from our existing System. The Anaconda installer install Python if necessary\n\nYou can check what the anaconda packages includes: http://docs.continuum.io/anaconda/pkg-docs\n\nThis also applies to Mac OS, from both Python 2.x or 3.x\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How Do You Secure database.yml?", "id": 574, "answers": [{"answer_id": 580, "document_id": 299, "question_id": 574, "text": "The way I have tackled this is to put the database password in a file with read permissions only for the user I run my application as. Then, in database.yml I use ERB to read the file.", "answer_start": 381, "answer_category": null}], "is_impossible": false}], "context": "When I deploy my Rails applications I have an after deploy callback in my Capistrano recipe that creates a symbolic link within the application's /config directory to the database.yml file. The file itself is stored in a separate directory that's outside the standard Capistrano /releases directory structure. I chmod 400 the file so it's only readable by the user who created it. The way I have tackled this is to put the database password in a file with read permissions only for the user I run my application as. Then, in database.yml I use ERB to read the file.", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Deployment of war file on Tomcat", "id": 1072, "answers": [{"answer_id": 1065, "document_id": 650, "question_id": 1072, "text": "You just need to copy the war file into the $TOMCAT_HOME/webapps/ directory. Tomcat will deploy the war file by automatically exploding it. FYI - If you want you can make updates directly to the exploded directory, which is useful for development.", "answer_start": 111, "answer_category": null}], "is_impossible": false}], "context": "Is there a way to deploy a given war file on Tomcat server? I want to do this without using the web interface.\nYou just need to copy the war file into the $TOMCAT_HOME/webapps/ directory. Tomcat will deploy the war file by automatically exploding it. FYI - If you want you can make updates directly to the exploded directory, which is useful for development.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to fix visual studio error \"cannot find one or more components .Please re-install the application\" in windows 8.1?", "id": 1756, "answers": [{"answer_id": 1743, "document_id": 1328, "question_id": 1756, "text": "You should open command prompt as administrator\ncd C:\\Program Files (x86)\\Microsoft Visual Studio 12.0\\Common7\\IDE\ndevenv /resetuserdata", "answer_start": 464, "answer_category": null}], "is_impossible": false}], "context": "As Alex suggests find devenv.exe and run it . when search for devenv.exe then I found it's there . When I run this file getting error \"cannot find one or more components .Please reinstall the application\" .I re-installed visual studio thrice still getting same error . How to fix this error . It's visual studio ultimate with update 3 . I don't have ISO file , file automatically downloaded by installer.It's got installed smoothly i.e. without giving any error .\nYou should open command prompt as administrator\ncd C:\\Program Files (x86)\\Microsoft Visual Studio 12.0\\Common7\\IDE\ndevenv /resetuserdata\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to sign the JAR files using NetBeans IDE when deploying?", "id": 447, "answers": [{"answer_id": 456, "document_id": 184, "question_id": 447, "text": "Enable signing by selecting the Request unrestricted access check box in the project properties. To sign with a specific certificate, click Edit next to the check box.", "answer_start": 921, "answer_category": null}], "is_impossible": false}, {"question": "How to assemble the distributable package using NetBeans IDE when deploying?", "id": 448, "answers": [{"answer_id": 457, "document_id": 184, "question_id": 448, "text": "Creates a distributable package every time the project is built. Packaging options are set in the project's properties.", "answer_start": 2508, "answer_category": null}], "is_impossible": false}, {"question": "What detailed information do I need when Configure the Deployment Descriptor?\n", "id": 449, "answers": [{"answer_id": 458, "document_id": 184, "question_id": 449, "text": "\u2022\tEntry points: the main application class, JavaFX preloader class, and other details\nDefined as attributes of the <fx:application> tag.\n\u2022\tParameters to be passed to the application\nDefined using <fx:param> and <fx:htmlParam> tags under <fx:application>.\n\u2022\tThe preferred application stage size\nIt is crucial to reserve the display area for embedded content.\nWidth and height are defined using width and height attributes in the <fx:deploy> tag for Ant tasks, the javapackager -deploy command in the javapackager tool for Windows or Solaris, Linux, and OS X, or in the Run category of NetBeans project properties.\n\u2022\tA description of the application to be used in dialog boxes that the user sees during application startup\nDefined using the <fx:info> tag.\n\u2022\tPlatform requirements, including the required version of the JRE and JVM settings\nDefined using the <fx:platform> tag. For an example, see <fx:platform> Parameter to Specify JVM Options.\n\u2022\tDesktop integration preferences of the application, such as adding a shortcut to the desktop or a reference to the Start menu.\nDefined using the optional <fx:preferences> tag. See <fx:preferences> Usage Examples.\n\u2022\tPermissions needed to run the application.\nBy default, web applications run in the sandbox. To request elevated permissions, use the <fx:permissions> tag.", "answer_start": 2812, "answer_category": null}], "is_impossible": false}, {"question": "How  To specify callbacks when  I Package Custom JavaScript Actions?", "id": 450, "answers": [{"answer_id": 459, "document_id": 184, "question_id": 450, "text": "list them in the <fx:callbacks> tag under <fx:deploy>. Add an <fx:callback> entry for every callback that you want to install, and specify the name of the hook in the name attribute. ", "answer_start": 6530, "answer_category": null}], "is_impossible": false}, {"question": "How to Package Custom JavaScript Action using NetBeans IDE when deploying?", "id": 451, "answers": [{"answer_id": 460, "document_id": 184, "question_id": 451, "text": "Add callbacks in the Deployment category of Project Properties. Click the Edit button to the right of Custom JavaScript Actions.", "answer_start": 7122, "answer_category": null}], "is_impossible": false}, {"question": "How to inject required code into an existing web page through the use of an input template?", "id": 452, "answers": [{"answer_id": 461, "document_id": 184, "question_id": 452, "text": "Example 5-2 HTML Input Template\n<html>\n    <head>\n        <title>Host page for JavaFX Application</title>\n        #DT.SCRIPT.CODE#\n        #DT.EMBED.CODE.ONLOAD#\n    </head>\n    <body>\n        <h1>Embed JavaFX application into existing page</h1>\n        <!-- application will be inserted here -->\n        <div id=\"ZZZ\"></div>\n    </body>\n</html>  \n#DT.SCRIPT.CODE# and #DT.EMBED.CODE.ONLOAD# are markers that are substituted with JavaScript and HTML code when the template is processed. ", "answer_start": 8124, "answer_category": null}], "is_impossible": false}], "context": "5.6 Sign the JAR Files\nTo use traditional methods to sign JAR files, see the steps for code signing in the Java Tutorial, and the description of the standard Ant signjar task for information. The traditional method is recommended for Java applications.\nJava packaging tools provide an alternative signing method that helps to reduce the size overhead of signing the JAR file. Using this method, you sign the JAR file as one large object instead of signing every JAR entry individually. This method can save up to 10 percent of the total JAR size.\nTo use the alternative signing method, you need the keystore and signing key. See the Java Tutorial on generating keys for instructions.\nThe following methods are available for signing the JAR files:\n\u2022\tAnt task: See <fx:jar> Usage Examples\n\u2022\tJava Packager tool: See the -signJar command in the javapackager reference for Windows or Solaris, Linux, and OS X.\n\u2022\tNetBeans IDE: Enable signing by selecting the Request unrestricted access check box in the project properties. To sign with a specific certificate, click Edit next to the check box.\n\nNote:\nAll JAR files must be signed or unsigned in the context of a single deployment descriptor file. If you need to mix signed and unsigned JAR files, use an additional Ant task to generate an additional deployment descriptor for each JAR file. These additional deployment descriptors are called extension descriptors. Use <fx:resources> to refer to the extension descriptors when the main descriptor is generated. For an example of how to do this, see Using <fx:resources> for Extension Descriptors.\n\n\n5.7 Run the Deploy Task or Command\nA basic distribution package consists of the following items:\n\u2022\tThe main executable JAR file\n\u2022\t(Optional) A set of auxiliary JAR files, including a JAR file with JavaFX preloader code\n\u2022\tA deployment descriptor, defining how to deploy the application\n\u2022\tEither a basic HTML file with sample code to embed the application into your own web page, or a custom web page that is the result of preprocessing an HTML template\nJava packaging tools can also package the application as a self-contained application bundle. This option is disabled by default. For more information, see Chapter 7, \"Self-Contained Application Packaging.\"\nTo assemble the distributable package, use one of the following methods:\n\u2022\tAnt task: See <fx:deploy> Task Usage Examples.\n\u2022\tJava Packager tool: See the -deploy command in the javapackager reference for Windows or Solaris, Linux, and OS X.\n\u2022\tNetBeans IDE: Creates a distributable package every time the project is built. Packaging options are set in the project's properties.\n5.7.1 Configure the Deployment Descriptor\nThe key part of this task is providing information to fill the deployment descriptor for web deployment. The following information is needed:\n\u2022\tEntry points: the main application class, JavaFX preloader class, and other details\nDefined as attributes of the <fx:application> tag.\n\u2022\tParameters to be passed to the application\nDefined using <fx:param> and <fx:htmlParam> tags under <fx:application>.\n\u2022\tThe preferred application stage size\nIt is crucial to reserve the display area for embedded content.\nWidth and height are defined using width and height attributes in the <fx:deploy> tag for Ant tasks, the javapackager -deploy command in the javapackager tool for Windows or Solaris, Linux, and OS X, or in the Run category of NetBeans project properties.\n\u2022\tA description of the application to be used in dialog boxes that the user sees during application startup\nDefined using the <fx:info> tag.\n\u2022\tPlatform requirements, including the required version of the JRE and JVM settings\nDefined using the <fx:platform> tag. For an example, see <fx:platform> Parameter to Specify JVM Options.\n\u2022\tDesktop integration preferences of the application, such as adding a shortcut to the desktop or a reference to the Start menu.\nDefined using the optional <fx:preferences> tag. See <fx:preferences> Usage Examples.\n\u2022\tPermissions needed to run the application.\nBy default, web applications run in the sandbox. To request elevated permissions, use the <fx:permissions> tag.\n5.7.2 Application Resources\nThe following types of resource files are supported:\n\u2022\tJAR files\n\u2022\tNative JAR files\n\u2022\tJNLP files\n\u2022\tIcons\n\u2022\tLicense files\n\u2022\tData files\nEvery resource has additional metadata associated with it, such as operating system and architecture for which this resource is applicable, plus a priority preference defining the point in the application life cycle at which this resource is needed. Careful use of metadata can have a significant impact on the application startup experience. For a list of supported values, see Table 10-10.\nAll files in the resource set are copied to the build output folder. However, not all of the files are used in all execution modes, as described in the following paragraphs.\nRegardless of execution mode, all regular JAR files from the resource set are added to the application class path.\nNative JAR files and JNLP files are used only for web deployment. Additional JNLP files are typically used to refer to external JNLP extensions, or if the application itself is packaged as a set of components. See Using <fx:resources> for Extension Descriptorsin the JavaFX Ant Task Reference.\nNative JAR files are used to deploy native libraries used by application. Each native JAR file can contain a set of native dynamic libraries and is inherently platform-specific. For more details, see Example 5-11 and Section 5.8.3, \"Packaging Complex Applications.\"\nLicense files are currently applicable to self-contained applications only, and are used to add a click-through license to installable packages. See Section 7.4, \"Installable Packages.\"\nData files do not have special semantics, and applications are free to use them for anything. For example, if your application needs to bundle a movie file, then you can mark it as \"data,\" and it is included in the application package.\nFor further details, see Table 10-10.\n5.7.3 Package Custom JavaScript Actions\nThe Deployment Toolkit provides a set of hooks that you can use to customize the startup behavior when an application is deployed in the browser. Developers must install a callback function to the hook, so the hook is utilized by the Deployment Toolkit.\nChapter 19, \"Deployment in the Browser\" describes in detail what hooks are available and how to use them in the code. However, to ensure that the hooks are correctly installed, they also must be specified at packaging time.\nTo specify callbacks, list them in the <fx:callbacks> tag under <fx:deploy>. Add an <fx:callback> entry for every callback that you want to install, and specify the name of the hook in the name attribute. The content of the <fx:callback> tag is the JavaScript function to be used. You can use a full function definition, or refer to a function defined elsewhere.\nThe following methods are available for packaging custom JavaScript actions:\n\u2022\tAnt task: See <fx:jar> Usage Examples.\n\u2022\tJava Packager tool: See the -deploy command in the javapackager reference for Windows or Solaris, Linux, and OS X.\n\u2022\tNetBeans IDE: Add callbacks in the Deployment category of Project Properties. Click the Edit button to the right of Custom JavaScript Actions.\n5.7.4 Web Page Templates\nBy default, Java packaging tools generate a simple web page with a placeholder for the embedded application. You can manually copy code from this generated page to your web page, but this is error prone and time consuming if you need to do this often.\nJava packaging tools also support injecting required code into an existing web page through the use of an input template. This technique is especially useful when the application is tightly integrated with the web page, for example if the application uses JavaScript to communicate with the web page, or if callbacks are used and their code is kept in the web page itself.\nAn input template is an HTML file containing markers to be replaced with JavaScript or HTML code needed to deploy the application on the web page. Example 5-2 shows an example of an input template for a JavaFX application.\nExample 5-2 HTML Input Template\n<html>\n    <head>\n        <title>Host page for JavaFX Application</title>\n        #DT.SCRIPT.CODE#\n        #DT.EMBED.CODE.ONLOAD#\n    </head>\n    <body>\n        <h1>Embed JavaFX application into existing page</h1>\n        <!-- application will be inserted here -->\n        <div id=\"ZZZ\"></div>\n    </body>\n</html>  \n#DT.SCRIPT.CODE# and #DT.EMBED.CODE.ONLOAD# are markers that are substituted with JavaScript and HTML code when the template is processed. Markers have the form of #MARKERNAME# or #MARKERNAME(id)#, where id is the identifier of an application (specified using the id attribute of the <fx:deploy> tag if you are using Ant), and MARKERNAME is the type of the marker. If id is not specified, then MARKER matches any application. For a list of supported markers, see <fx:template> in the Ant task reference.\nTemplates can be used to deploy multiple applications in the same page. Use the full form of the marker including application ID (an alphanumeric string without spaces), and pass the partially processed template file when packaging each application to insert.\nExample 5-3 shows an example of a template that is used to deploy multiple applications.\nExample 5-3 An Input Template Used to Deploy Multiple Applications\n<html>\n    <head>\n        <title>Page with two application</title>\n        <script src=\"#DT.SCRIPT.URL#\"></script>\n \n        <!-- code to load first app with id 'firstApp'\n             (specified as attribute to fx:application) -->\n        <!-- #DT.EMBED.CODE.ONLOAD(firstApp)# -->\n \n        <!-- code to load first app with id 'secondApp' -->\n        <!-- #DT.EMBED.CODE.ONLOAD(secondApp)# --> \n    </head>\n    <body>\n        <h1>Multiple applications in the same page</h1>\n \n        JavaFX app: <br>\n        <!-- First app. Ant task need to use \"ZZZ_1 as placeholderId -->\n        <div id=\"ZZZ_1\"></div>\n \n        Another app: <br>\n        <!-- Second app. Ant task need to use \"ZZZ_2 as placeholderId -->\n        <div id=\"ZZZ_2\"></div>\n    </body>\n  </html>\nExample 5-3 demonstrate one useful feature of the template processor, which is that the markers can be placed in the HTML comments. If the comment does not contain anything other than marker code, then the comment tags are removed from the content in the resulting HTML. This feature keeps the HTML in the template page well formed.\nThe following methods are available for using templates:\n\u2022\tAnt task: Add a template tag. See <fx:jar> Usage Examples.\n\u2022\tJava Packager tool: See the -deploy command in the javapackager reference for Windows or Solaris, Linux, and OS X.\n\u2022\tNetBeans IDE: Specify the input HTML template file in the Run category of Project Properties.\n\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to prevent downtime during AWS Elastic Beanstalk deployment of a new version of the app?", "id": 565, "answers": [{"answer_id": 568, "document_id": 290, "question_id": 565, "text": "To achieve this goal in Elastic Beanstalk, you'll need to expand your deployment procedure to facilitate multiple Environments (see AWS Elastic Beanstalk Components)\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts.html", "answer_start": 627, "answer_category": null}], "is_impossible": false}], "context": "My understanding of Elastic Beanstalk is that when you deploy a new version of your app, that it deploys it to the Amazon EC2 instances one at a time (if you have more than one). However, even with a minimum of two instances, my application incurs a short amount of downtime when I upload a new .war when it is deploying it, as if it is updating them both simultaneously. Is there a way I can ensure there is no downtime and that one instance is fully updated and accepting requests before the next starts: Here is how the events look. Note this is with zero load on the app, so it will only get worse with production traffic.\nTo achieve this goal in Elastic Beanstalk, you'll need to expand your deployment procedure to facilitate multiple Environments (see AWS Elastic Beanstalk Components)\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts.html\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "ILMerge Best Practices", "id": 321, "answers": [{"answer_id": 330, "document_id": 134, "question_id": 321, "text": "You can't ILMerge any C++ assemblies that have native code. You also can't ILMerge any assemblies that contain XAML for WPF (at least I haven't had any success with that). It complains at runtime that the resources cannot be located.", "answer_start": 311, "answer_category": null}], "is_impossible": false}], "context": "Do you use ILMerge? Do you use ILMerge to merge multiple assemblies to ease deployment of dll's? Have you found problems with deployment/versioning in production after ILMerging assemblies together?I'm looking for some advice in regards to using ILMerge to reduce deployment friction, if that is even possible. You can't ILMerge any C++ assemblies that have native code. You also can't ILMerge any assemblies that contain XAML for WPF (at least I haven't had any success with that). It complains at runtime that the resources cannot be located.", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "python install module apiclient on MACOS?", "id": 2014, "answers": [{"answer_id": 2000, "document_id": 1610, "question_id": 2014, "text": "1 - As suggested by others, install the API client using pip:\n\nsudo pip install --upgrade google-api-python-client\n\n\n2 - Make sure you are calling the library in your code as googleapiclient, and not as apiclient, which is deprecated.\n\n3 - Tell Python to look for packages in the pip folder:\n\nexport PYTHONPATH=/usr/local/lib/python2.7/site-packages", "answer_start": 1983, "answer_category": null}], "is_impossible": false}, {"question": "python install module apiclient", "id": 2015, "answers": [{"answer_id": 2001, "document_id": 1610, "question_id": 2015, "text": "Try this:\n\nsudo pip install --upgrade google-api-python-client\n\n\nOR\n\nMake sure you only have google-api-python-client installed. If you have apiclient installed, it will cause a collision. So, run the following:\n\npip install --force-reinstall google-api-python-client", "answer_start": 784, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nNew to python, and trying to install a module \"apiclient\"\nsince my ide pycharm does not recognize that import:\n\nfrom apiclient.discovery import build\n\n\nwhat I tried:\n\n\npip install apiclient\ndownload manually the package from \n\n\nhttps://developers.google.com/api-client-library/python/start/installation#system-requirements\nthen I extracted it into \n\n/Users/nirregev/anaconda/bin/google-api-python-client-1.5.0\n\n\nand ran this on my mac terminal\npython setup.py install \nbut still pycharm does not recognize this module.\nAccording to pycharm I have the following interpreters installed:\n\n/Library/Frameworks/Python.framework/Versions/3.5/bin/python3.5\n/Users/nirregev/anaconda/bin/python\n/System/Library/Frameworks/Python.framework/Versions/2.7/bin/python2.7\n\n    \n\nTry this:\n\nsudo pip install --upgrade google-api-python-client\n\n\nOR\n\nMake sure you only have google-api-python-client installed. If you have apiclient installed, it will cause a collision. So, run the following:\n\npip install --force-reinstall google-api-python-client\n\n\nAnswer Source\n    \n\nI ran into this problem and had a tough time figuring it out. In the end, this worked for me:\n\npip install google-api-python-client==1.5.3\n\nBefore doing this, I had version 1.6.2 installed. What I think is going on is that later versions of google-api-python-client dropped the apiclient in favor of the googleapiclient alias; which is an issue because some packages (e.g. airflow) still use that apiclient.discovery import.\n\nHope this helps.\n    \n\nIf you have python3 installed somewhere and you are to install apiclient, it may be installing it in your python3 directory. I had the same problem and when I uninstalled python3 my program ran smoothly.\n    \n\nIf you have got both python 2 and python 3 and you're trying to use python 2 for this purpose try the following: sudo pip2 install google-api-python-client==1.5.3 . This worked for me.\n    \n\nI am on Mac, using brew's python, and this worked for me:\n\n1 - As suggested by others, install the API client using pip:\n\nsudo pip install --upgrade google-api-python-client\n\n\n2 - Make sure you are calling the library in your code as googleapiclient, and not as apiclient, which is deprecated.\n\n3 - Tell Python to look for packages in the pip folder:\n\nexport PYTHONPATH=/usr/local/lib/python2.7/site-packages\n\n\nTo make it permanent, add the above line to either your .profile or .bash_profile file in your $HOME.\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to install older version of node.js on Windows?", "id": 692, "answers": [{"answer_id": 696, "document_id": 384, "question_id": 692, "text": "Just uninstall whatever node version you have in your system. Then go to this site https://nodejs.org/download/release/ and choose your desired version like for me its like v7.0.0/ and click on that go get .msi file of that. ", "answer_start": 165, "answer_category": null}], "is_impossible": false}], "context": "I need to install node.js of version 4.0.0 I tried .But I got this message: npm is not recognized as an internal or external command, operable program or batch file.Just uninstall whatever node version you have in your system. Then go to this site https://nodejs.org/download/release/ and choose your desired version like for me its like v7.0.0/ and click on that go get .msi file of that. Finally you will get installer in your system, so install it. Simple solution + good software management. Works as expected. *In terms of time, the nvm answer above is most efficient", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "what accounts to use when installing sql server 2008 developer", "id": 1909, "answers": [{"answer_id": 1896, "document_id": 1480, "question_id": 1909, "text": "So unless you need your Dev SQL Server to use Network Services, you can use Local System account. ", "answer_start": 1129, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI am installing SQL Server 2008 Developer here, and on the Server Configuration step of the installation it asks me about Service Accounts. What do I choose here?\n\nI can see the available ones in the screen shot, although on most of them I can only select two or three of those. When I click the Use the same account for all SQL Server 2008 services button I can choose between NT AUTHORITY\\NETWORK SERVICE and NT AUTHORITY\\SYSTEM.\n\nWhat do I choose here, and why?\n\n\n\n    \n\nFrom Microsoft:\n\n\n  Local System account. The name of this\n  account is NT AUTHORITY\\System. It is\n  a powerful account that has\n  unrestricted access to all local\n  system resources. It is a member of\n  the Windows Administrators group on\n  the local computer, and is therefore a\n  member of the SQL Server sysadmin\n  fixed server role\n  \n  Network Service account. The name of\n  this account is NT\n  AUTHORITY\\NetworkService. It is\n  available in Microsoft Windows XP and\n  Microsoft Windows Server 2003. All\n  services that run under the Network\n  Service account are authenticated to\n  network resources as the local\n  computer.\n\n\nSo unless you need your Dev SQL Server to use Network Services, you can use Local System account. \n\nUpdate:\n\nTo Configure your surface area go Start-&gt;Microsoft SQL Server-&gt;Configuration Tools-&gt;SQL Server Surface Area Configuration.\n\nThats how you setup for incoming connections. Also make sure the SQL Browser server is running. \n\nI would also recommend either SQL 2005 for Dummies or SQL Server 2005 Bible to do some study on some of the basic stuff of SQL Serve. The bible will also go more indepth should you be beyond the dummies type book.\n    \n\nIf you want to play it safe, make a local account like \"SQLServices\" on your machine, with no special rights or access. Then install SQL and choose that account, and the SQL installer will add only those rights that the service account requires. I do it this way so that the service account will have only minimal OS rights.\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "E: Unable to locate package mongodb-org", "id": 877, "answers": [{"answer_id": 872, "document_id": 558, "question_id": 877, "text": "sudo apt-get install -y mongodb ", "answer_start": 4096, "answer_category": null}], "is_impossible": false}], "context": "am trying to download mongodb and I am following the steps on this link.\n\nBut when I get to the step:\n\nsudo apt-get install -y mongodb-org\nI get the following error:\n\nReading package lists... Done\nBuilding dependency tree       \nReading state information... Done\nE: Unable to locate package mongodb-org  //This is the error\nWhy is this occurring and is there a work around?\n\nmongodb\nubuntu\ninstallation\nShare\nImprove this question\nFollow\nedited Feb 24 '20 at 2:12\n\nPenny Liu\n9,74655 gold badges5050 silver badges7575 bronze badges\nasked Mar 9 '15 at 15:35\n\nanonn023432\n2,53244 gold badges2626 silver badges5555 bronze badges\nAdd a comment\n13 Answers\n\n212\n\nI faced same issue but fix it by the changing the package file section command. The whole step that i followed was:\n\nAt first try with this command: sudo apt-get install -y mongodb\n\nThis is the unofficial mongodb package provided by Ubuntu and it is not maintained by MongoDB and conflict with MongoDB\u2019s offically supported packages.\n\nIf the above command not working then you can fix the issue by one of the bellow procedure:\n\n#Step 1:  Import the MongoDB public key\n#In Ubuntu 18.*+, you may get invalid signatures. --recv value may need to be updated to EA312927. \n#See here for more details on the invalid signature issue: [https://stackoverflow.com/questions/34733340/mongodb-gpg-invalid-signatures][1]\n\nsudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 7F0CEB10\n\n#Step 2: Generate a file with the MongoDB repository url\necho 'deb http://downloads-distro.mongodb.org/repo/ubuntu-upstart dist 10gen' | sudo tee /etc/apt/sources.list.d/mongodb.list\n\n#Step 3: Refresh the local database with the packages\nsudo apt-get update\n\n#Step 4: Install the last stable MongoDB version and all the necessary packages on our system\nsudo apt-get install mongodb-org\n\n         #Or\n# The unofficial mongodb package provided by Ubuntu is not maintained by MongoDB and conflict with MongoDB\u2019s offically supported packages. Use the official MongoDB mongodb-org packages, which are kept up-to-date with the most recent major and minor MongoDB releases.\nsudo apt-get install -y mongodb \nHope this will work for you also. You can follow this MongoDB\n\nUpdate The above instruction will install mongodb 2.6 version, if you want to install latest version for Uubuntu 12.04 then just replace above step 2 and follow bellow instruction instead of that:\n\n#Step 2: Generate a file with the MongoDB repository url\necho \"deb http://repo.mongodb.org/apt/ubuntu trusty/mongodb-org/3.0 multiverse\" | sudo tee /etc/apt/sources.list.d/mongodb.list\nIf you are using Ubuntu 14.04 then use bellow step instead of above step 2\n\n#Step 2: Generate a file with the MongoDB repository url\necho \"deb http://repo.mongodb.org/apt/ubuntu trusty/mongodb-org/3.0 multiverse\" | sudo tee /etc/apt/sources.list.d/mongodb-org-3.0.list\nShare\nImprove this answer\nFollow\nedited Nov 20 '18 at 16:07\n\napplecrusher\n4,98833 gold badges3737 silver badges8181 bronze badges\nanswered Mar 10 '15 at 14:28\n\nNaim Rajiv\n3,14411 gold badge1515 silver badges2222 bronze badges\n1\nWorked for me on Ubuntu 14.10 (other answers did not) after I fixed the command in step 2. \u2013 \njulien_c\n Mar 18 '15 at 15:23 \n1\nOoops, no, sorry, still not working (this installs Mongo 2.6, not the lastest stable 3.0). See my own answer. \u2013 \njulien_c\n Mar 18 '15 at 15:34 \n42\nIt sucks that this works and the official site guide does not. \u2013 \nDan\n Apr 11 '15 at 21:14\n1\nMongoDB's official instructions didn't mention that the repository URL was for 2.6. I had to follow your updated step 2 with the correct URL in order to use sudo apt-get install mongodb-org. After that it fixed the problem. \u2013 \nDaniel Eagle\n May 1 '15 at 19:26\n2\nI tried the command for Ubuntu 14.04 and it still not working! \u2013 \nalexventuraio\n Nov 9 '15 at 19:11\nShow 9 more comments\n\n\n62\n\nThe true problem here may be if you have a 32-bit system. MongoDB 3.X was never made to be used on a 32-bit system, so the repostories for 32-bit is empty (hence why it is not found). Installing the default 2.X Ubuntu package might be your best bet with:\n\nsudo apt-get install -y mongodb \n\nAnother workaround, if you nevertheless want to get the latest version of Mongo:\n\nYou can go to https://www.mongodb.org/downloads and use the drop-down to select \"Linux 32-bit legacy\"\n\nBut it comes with severe limitations...\n\nThis 32-bit legacy distribution does not include SSL encryption and is limited to around 2GB of data. In general you should use the 64 bit builds. See here for more information.\n\nShare\nImprove this answer\nFollow\nedited Jun 18 '15 at 4:51\nanswered May 5 '15 at 16:21\n\nPhil B\n4,72966 gold badges3737 silver badges5555 bronze badges\nAdd a comment\n\n51\n\nTry without '-org':\n\nsudo apt-get install -y mongodb\n\nWorked for me!\n\nShare\nImprove this answer\nFollow\nedited Apr 30 '15 at 15:29\nanswered Apr 23 '15 at 23:04\n\nPhil B\n4,72966 gold badges3737 silver badges5555 bronze badges\n8\nThis is probably Ubuntu's default package. Not useful if you want the latest mongo. \u2013 \nbattery\n May 3 '15 at 17:51\n1\nThis may indeed be the Ubuntu default package. Good for a quick start, but if you specifically need the latest package from mongo.org, then extra steps are required. \u2013 \nPhil B\n May 5 '15 at 10:43\n2\nThe true problem here may be if you have a 32-bit system. MongoDB 3.X was never made to be used on a 32-bit system, so the repostories for 32-bit is empty (hence why it is not found). \u2013 \nPhil B\n May 5 '15 at 15:30\n2\nThat is exactly the problem, and this is the real answer :-) Its just something that Mongo's website doesn't clearly say. At one place they say that you should run MongoDB on 64 bit for production, in another it says that they provide packages for both 32 bit and 64 bit for test environments. \u2013 \nbattery\n May 5 '15 at 15:33\nI've added a more detailed answer below that summarizes the problem (based on all the helpful comments, thank you!) and proposes some workarounds. Could you please upvote that answer instead of this one? \u2013 \nPhil B\n May 5 '15 at 16:31 \nAdd a comment\n\n7\n\nI'm running Ubuntu 14.04; and apt still couldn't find package; I tried all the answers above and more. The URL that worked for me is this:\n\necho 'deb http://downloads-distro.mongodb.org/repo/ubuntu-upstart dist 10gen' | sudo tee /etc/apt/sources.list.d/mongodb.list\nSource: http://www.liquidweb.com/kb/how-to-install-mongodb-on-ubuntu-14-04/\n\nShare\nImprove this answer\nFollow\nanswered Apr 22 '15 at 14:23\n\nbmvakili\n7922 bronze badges\nAdd a comment\n\n6\n\nAll steps are correct just change the Step 4 as below\n\nStep 4: Install the last stable MongoDB version and all the necessary packages on our system\nCommand: sudo apt-get install mongodb\n\nIt has worked for me.\n\nShare\nImprove this answer\nFollow\nedited Nov 1 '15 at 9:20\n\nbish\n3,15399 gold badges4545 silver badges6464 bronze badges\nanswered Nov 1 '15 at 8:27\n\nAmit Sharma\n62566 silver badges55 bronze badges\n1\nThis is installs Ubuntu's default unofficial mongo package which is already mentioned in the top answer by Naim. You don't need to follow the previous steps for this. \u2013 \nshaahiin\n Feb 15 '19 at 4:27 \nAdd a comment\n\n4\n\nIf you are currently using the MongoDB 3.3 Repository (as officially currently suggested by MongoDB website) you should take in consideration that the package name used for version 3.3 is:\n\nmongodb-org-unstable\n\nThen the proper installation command for this version will be:\n\nsudo apt-get install -y mongodb-org-unstable\nConsidering this, I will rather suggest to use the current latest stable version (v3.2) until the v3.3 becomes stable, the commands to install it are listed below:\n\nDownload the v3.2 Repository key:\n\nwget -qO - https://www.mongodb.org/static/pgp/server-3.2.asc | sudo apt-key add -\nIf you work with Ubuntu 12.04 or Mint 13 add the following repository:\n\necho \"deb http://repo.mongodb.org/apt/ubuntu precise/mongodb-org/3.2 multiverse\" | sudo tee /etc/apt/sources.list.d/mongodb-org-3.2.list\nIf you work with Ubuntu 14.04 or Mint 17 add the following repository:\n\necho \"deb http://repo.mongodb.org/apt/ubuntu trusty/mongodb-org/3.2 multiverse\" | sudo tee /etc/apt/sources.list.d/mongodb-org-3.2.list\nIf you work with Ubuntu 16.04 or Mint 18 add the following repository:\n\necho \"deb http://repo.mongodb.org/apt/ubuntu xenial/mongodb-org/3.2 multiverse\" | sudo tee /etc/apt/sources.list.d/mongodb-org-3.2.list\nUpdate the package list and install mongo:\n\nsudo apt-get update\nsudo apt-get install -y mongodb-org", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to deploy an Electron app as an executable or installable in Windows?", "id": 1702, "answers": [{"answer_id": 1690, "document_id": 1275, "question_id": 1702, "text": "You can package your program using electron-packager and then build a single setup EXE file using InnoSetup", "answer_start": 111, "answer_category": null}], "is_impossible": false}], "context": "I want to generate a unique .exe file to execute the app or a .msi to install the application. How to do that? You can package your program using electron-packager and then build a single setup EXE file using InnoSetup. 2020 Update You can use electron-builder to create portable .exe file for your electron app. All you need to do is install electron-builder with yarn add electron-builder --dev Then create a package.json file like this(this is just for portable .exe):", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Joomla! 3 installation freezes at creating database table", "id": 821, "answers": [{"answer_id": 816, "document_id": 503, "question_id": 821, "text": "To solve this go to wamp\\www\\Your joomla folder\\installation\\sql\\mysql and open Joomla.sql file find the term\n\"ENGINE=InnoDB\"\nand replace ALL with\n\"ENGINE=MyIsam\"", "answer_start": 428, "answer_category": null}], "is_impossible": false}], "context": "I am trying to install Joomla! 3.2.1 on my system but the installation freezes half way through. I've downloaded and installed the Wamp Server 2.4 and wanted to locally install Joomla! 3.2.1, but the installation freezes and doesn't finish.\n \nIt stops short of finishing the installation during the \"creating database tables\" task: it just stays on this bit seemingly forever.\nthis problem occurring most usually at Joomla 3.x. To solve this go to wamp\\www\\Your joomla folder\\installation\\sql\\mysql and open Joomla.sql file find the term\n\"ENGINE=InnoDB\"\nand replace ALL with\n\"ENGINE=MyIsam\"\nMyIsam is more supported and compatible with Joomla.\nDo also the same with (sample_data.sql) file, if it's quickstart setup.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How can I detect Heroku's environment?", "id": 1718, "answers": [{"answer_id": 1706, "document_id": 1291, "question_id": 1718, "text": "An ENV var seems to the most obvious way of doing this. Either look for an ENV var that you know exists, or set your own:\non_heroku = False\nif 'YOUR_ENV_VAR' in os.environ:\n  on_heroku = True", "answer_start": 355, "answer_category": null}], "is_impossible": false}], "context": "I have a Django webapp, and I'd like to check if it's running on the Heroku stack (for conditional enabling of debugging, etc.) Is there any simple way to do this? An environment variable, perhaps?\nI know I can probably also do it the other way around - that is, have it detect if it's running on a developer machine, but that just doesn't \"sound right\".\nAn ENV var seems to the most obvious way of doing this. Either look for an ENV var that you know exists, or set your own:\non_heroku = False\nif 'YOUR_ENV_VAR' in os.environ:\n  on_heroku = True\nmore at: http://devcenter.heroku.com/articles/config-vars\n\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to merge two configmaps using volume mount in kubernetes", "id": 1314, "answers": [{"answer_id": 1304, "document_id": 883, "question_id": 1314, "text": "You cannot mount two ConfigMaps to the same location.\nBut mentioning subPath and key for every item in each configmaps will let you get items from both configmaps in the same location. You'll have to write mount points for each file manually.", "answer_start": 200, "answer_category": null}], "is_impossible": false}], "context": "I am having two different config maps test-configmap and common-config. I tried to mount them at the same location, but one config map overwrote the other. Then I read about subPath and did not work.\nYou cannot mount two ConfigMaps to the same location.\nBut mentioning subPath and key for every item in each configmaps will let you get items from both configmaps in the same location. You'll have to write mount points for each file manually.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to connect CouchDB?", "id": 200659, "answers": [{"answer_id": 239497, "document_id": 357722, "question_id": 200659, "text": "by using the command-line utility curl", "answer_start": 1454, "answer_category": null}], "is_impossible": false}, {"question": "How to create CouchDB database?", "id": 200661, "answers": [{"answer_id": 239501, "document_id": 357722, "question_id": 200661, "text": "curl -X PUT http://admin:password@127.0.0.1:5984/baseball\n", "answer_start": 3681, "answer_category": null}], "is_impossible": false}, {"question": "How to run a query in CouchDB?", "id": 200662, "answers": [{"answer_id": 239528, "document_id": 357722, "question_id": 200662, "text": " running a Mango Query", "answer_start": 9566, "answer_category": null}], "is_impossible": false}, {"question": "What does Fauxton do?", "id": 200663, "answers": [{"answer_id": 239537, "document_id": 357722, "question_id": 200663, "text": "Fauxton provides full access\nto all of CouchDB\u2019s features and makes it easy to work with some of the more\ncomplex ideas involved.", "answer_start": 5991, "answer_category": null}], "is_impossible": false}, {"question": "How to know the whore CouchDB database?", "id": 200660, "answers": [{"answer_id": 239500, "document_id": 357722, "question_id": 200660, "text": "curl -X GET http://admin:password@127.0.0.1:5984/_all_dbs\n", "answer_start": 2369, "answer_category": null}], "is_impossible": false}], "context": "\n\n\n1.6. Getting Started\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApache CouchDB\u00ae\n\n\n\n\n\n\n\nstable\n\n\n\n\n\n\n\n\n\n\nTable of Contents\nUser Guides\n\n1. Introduction\n1.1. Technical Overview\n1.2. Why CouchDB?\n1.3. Eventual Consistency\n1.4. cURL: Your Command Line Friend\n1.5. Security\n1.6. Getting Started\n1.6.1. All Systems Are Go!\n1.6.2. Welcome to Fauxton\n1.6.3. Your First Database and Document\n1.6.4. Running a Mango Query\n1.6.5. Triggering Replication\n1.6.6. Wrapping Up\n\n\n1.7. The Core API\n\n\n2. Replication\n3. Design Documents\n4. Best Practices\n\nAdministration Guides\n\n1. Installation\n2. Setup\n3. Configuration\n4. Cluster Management\n5. Maintenance\n6. Fauxton\n7. Experimental Features\n\nReference Guides\n\n1. API Reference\n2. JSON Structure Reference\n3. Query Server\n4. Partitioned Databases\n\nOther\n\n1. Release Notes\n2. Security Issues / CVEs\n3. Reporting New Security Problems with Apache CouchDB\n4. License\n5. Contributing to this Documentation\n\nQuick Reference Guides\n\nAPI Quick Reference\nConfiguration Quick Reference\n\nMore Help\n\nCouchDB Homepage\nMailing Lists\nRealtime Chat\nIssue Tracker\nDownload Docs\n\n\n\n\n\n\n\n\nApache CouchDB\u00ae\n\n\n\n\n\nDocs \u00bb\n1. Introduction \u00bb\n1.6. Getting Started\n\nEdit on GitHub\n\n\n\n\n\n\n\n1.6. Getting Started\u00b6\nIn this document, we\u2019ll take a quick tour of CouchDB\u2019s features.\nWe\u2019ll create our first document and experiment with CouchDB views.\n\n1.6.1. All Systems Are Go!\u00b6\nWe\u2019ll have a very quick look at CouchDB\u2019s bare-bones Application Programming\nInterface (API) by using the command-line utility curl. Please note that this\nis not the only way of talking to CouchDB. We will show you plenty more\nthroughout the rest of the documents. What\u2019s interesting about curl is that it\ngives you control over raw HTTP requests, and you can see exactly what is\ngoing on \u201cunderneath the hood\u201d of your database.\nMake sure CouchDB is still running, and then do:\ncurl http://127.0.0.1:5984/\n\n\nThis issues a GET request to your newly installed CouchDB instance.\nThe reply should look something like:\n{\n\"couchdb\": \"Welcome\",\n\"version\": \"3.0.0\",\n\"git_sha\": \"83bdcf693\",\n\"uuid\": \"56f16e7c93ff4a2dc20eb6acc7000b71\",\n\"features\": [\n\"access-ready\",\n\"partitioned\",\n\"pluggable-storage-engines\",\n\"reshard\",\n\"scheduler\"\n],\n\"vendor\": {\n\"name\": \"The Apache Software Foundation\"\n}\n}\n\n\nNot all that spectacular. CouchDB is saying \u201chello\u201d with the running version\nnumber.\nNext, we can get a list of databases:\ncurl -X GET http://admin:password@127.0.0.1:5984/_all_dbs\n\n\nAll we added to the previous request is the _all_dbs string, and our admin user\nname and password (set when installing CouchDB).\nThe response should look like:\n[\"_replicator\",\"_users\"]\n\n\n\nNote\nIn case this returns an empty Array for you, it means you haven\u2019t finished\ninstallation correctly. Please refer to Setup for further\ninformation on this.\nFor the purposes of this example, we\u2019ll not be showing the system databases\npast this point. In your installation, any time you GET /_all_dbs,\nyou should see the system databases in the list, too.\n\nOh, that\u2019s right, we didn\u2019t create any user databases yet!\n\nNote\nThe curl command issues GET requests by default. You can issue POST requests\nusing curl -X POST. To make it easy to work with our terminal history,\nwe usually use the -X option even when issuing GET requests.\nIf we want to send a POST next time, all we have to change is the method.\nHTTP does a bit more under the hood than you can see in the examples here.\nIf you\u2019re interested in every last detail that goes over the wire,\npass in the -v option (e.g., curl -vX GET), which will show you\nthe server curl tries to connect to, the request headers it sends,\nand response headers it receives back. Great for debugging!\n\nLet\u2019s create a database:\ncurl -X PUT http://admin:password@127.0.0.1:5984/baseball\n\n\nCouchDB will reply with:\n{\"ok\":true}\n\n\nRetrieving the list of databases again shows some useful results this time:\ncurl -X GET http://admin:password@127.0.0.1:5984/_all_dbs\n\n\n[\"baseball\"]\n\n\n\nNote\nWe should mention JavaScript Object Notation (JSON) here, the data format\nCouchDB speaks. JSON is a lightweight data interchange format based on\nJavaScript syntax. Because JSON is natively compatible with JavaScript, your\nweb browser is an ideal client for CouchDB.\nBrackets ([]) represent ordered lists, and curly braces ({})\nrepresent key/value dictionaries. Keys must be strings, delimited by quotes\n(\"), and values can be strings, numbers, booleans, lists, or key/value\ndictionaries. For a more detailed description of JSON, see Appendix E, JSON\nPrimer.\n\nLet\u2019s create another database:\ncurl -X PUT http://admin:password@127.0.0.1:5984/baseball\n\n\nCouchDB will reply with:\n{\"error\":\"file_exists\",\"reason\":\"The database could not be created,\nthe file already exists.\"}\n\n\nWe already have a database with that name, so CouchDB will respond with an\nerror. Let\u2019s try again with a different database name:\ncurl -X PUT http://admin:password@127.0.0.1:5984/plankton\n\n\nCouchDB will reply with:\n{\"ok\":true}\n\n\nRetrieving the list of databases yet again shows some useful results:\ncurl -X GET http://admin:password@127.0.0.1:5984/_all_dbs\n\n\nCouchDB will respond with:\n[\"baseball\", \"plankton\"]\n\n\nTo round things off, let\u2019s delete the second database:\ncurl -X DELETE http://admin:password@127.0.0.1:5984/plankton\n\n\nCouchDB will reply with:\n{\"ok\":true}\n\n\nThe list of databases is now the same as it was before:\ncurl -X GET http://admin:password@127.0.0.1:5984/_all_dbs\n\n\nCouchDB will respond with:\n[\"baseball\"]\n\n\nFor brevity, we\u2019ll skip working with documents, as the next section covers a\ndifferent and potentially easier way of working with CouchDB that should\nprovide experience with this. As we work through the example,\nkeep in mind that \u201cunder the hood\u201d everything is being done by the\napplication exactly as you have been doing here manually.\nEverything is done using GET, PUT, POST, and DELETE with a URI.\n\n\n1.6.2. Welcome to Fauxton\u00b6\nAfter having seen CouchDB\u2019s raw API, let\u2019s get our feet wet by playing with\nFauxton, the built-in administration interface. Fauxton provides full access\nto all of CouchDB\u2019s features and makes it easy to work with some of the more\ncomplex ideas involved. With Fauxton we can create and destroy databases; view\nand edit documents; compose and run MapReduce views; and trigger replication\nbetween databases.\nTo load Fauxton in your browser, visit:\nhttp://127.0.0.1:5984/_utils/\n\n\nand log in when prompted with your admin password.\nIn later documents, we\u2019ll focus on using CouchDB from server-side languages\nsuch as Ruby and Python. As such, this document is a great opportunity to\nshowcase an example of natively serving up a dynamic web application using\nnothing more than CouchDB\u2019s integrated web server, something you may wish to do\nwith your own applications.\nThe first thing we should do with a fresh installation of CouchDB is run the\ntest suite to verify that everything is working properly. This assures us\nthat any problems we may run into aren\u2019t due to bothersome issues with our\nsetup. By the same token, failures in the Fauxton test suite are a red flag,\ntelling us to double-check our installation before attempting to use a\npotentially broken database server, saving us the confusion when nothing\nseems to be working quite like we expect!\nTo validate your installation, click on the Verify link on the left-hand\nside, then press the green Verify Installation button. All tests should\npass with a check mark. If any fail, re-check your installation steps.\n\n\n1.6.3. Your First Database and Document\u00b6\nCreating a database in Fauxton is simple. From the overview page,\nclick \u201cCreate Database.\u201d When asked for a name, enter hello-world and click\nthe Create button.\nAfter your database has been created, Fauxton will display a list of all its\ndocuments. This list will start out empty, so let\u2019s\ncreate our first document. Click the plus sign next to \u201cAll Documents\u201d and\nselect the \u201cNew Doc\u201d link. CouchDB will generate a UUID for you.\nFor demoing purposes, having CouchDB assign a UUID is fine. When you write\nyour first programs, we recommend assigning your own UUIDs. If you rely on\nthe server to generate the UUID and you end up making two POST requests\nbecause the first POST request bombed out, you might generate two docs and\nnever find out about the first one because only the second one will be\nreported back. Generating your own UUIDs makes sure that you\u2019ll never end up\nwith duplicate documents.\nFauxton will display the newly created document, with its _id field. To create\na new field, simply use the editor to write valid JSON. Add a new field by\nappending a comma to the _id value, then adding the text:\n\"hello\": \"my new value\"\n\n\nClick the green Create Document button to finalize creating the\ndocument.\nYou can experiment with other JSON values; e.g., [1, 2, \"c\"] or\n{\"foo\": \"bar\"}.\nYou\u2019ll notice that the document\u2019s _rev has been added. We\u2019ll go into more detail\nabout this in later documents, but for now, the important thing to note is\nthat _rev acts like a safety feature when saving a document. As long as you\nand CouchDB agree on the most recent _rev of a document, you can successfully\nsave your changes.\nFor clarity, you may want to display the contents of the document in the all\ndocument view. To enable this, from the upper-right corner of the window,\nselect Options, then check the Include Docs option. Finally, press the Run\nQuery button. The full document should be displayed along with the _id\nand _rev values.\n\n\n1.6.4. Running a Mango Query\u00b6\nNow that we have stored documents successfully, we want to be able to query\nthem. The easiest way to do this in CouchDB is running a Mango Query. There are\nalways two parts to a Mango Query: the index and the selector.\nThe index specifies which fields we want to be able to query on, and the\nselector includes the actual query parameters that define what we are looking\nfor exactly.\nIndexes are stored as rows that are kept sorted by the fields you specify. This\nmakes retrieving data from a range of keys efficient even when there are\nthousands or millions of rows.\nBefore we can run an example query, we\u2019ll need some data to run it on. We\u2019ll\ncreate documents with information about movies. Let\u2019s create documents for\nthree movies. (Allow CouchDB to generate the _id and _rev fields.) Use Fauxton\nto create documents that have a final JSON structure that look like this:\n{\n\"_id\": \"00a271787f89c0ef2e10e88a0c0001f4\",\n\"type\": \"movie\",\n\"title\": \"My Neighbour Totoro\",\n\"year\": 1988,\n\"director\": \"miyazaki\",\n\"rating\": 8.2\n}\n\n\n{\n\"_id\": \"00a271787f89c0ef2e10e88a0c0003f0\",\n\"type\": \"movie\",\n\"title\": \"Kikis Delivery Service\",\n\"year\": 1989,\n\"director\": \"miyazaki\",\n\"rating\": 7.8\n}\n\n\n{\n\"_id\": \"00a271787f89c0ef2e10e88a0c00048b\",\n\"type\": \"movie\",\n\"title\": \"Princess Mononoke\",\n\"year\": 1997,\n\"director\": \"miyazaki\",\n\"rating\": 8.4\n}\n\n\nNow we want to be able to find a movie by its release year, we need to create a\nMango Index. To do this, go to \u201cRun A Query with Mango\u201d in the Database\noverview. Then click on \u201cmanage indexes\u201d, and change the index field on the\nleft to look like this:\n{\n\"index\": {\n\"fields\": [\n\"year\"\n]\n},\n\"name\": \"year-json-index\",\n\"type\": \"json\"\n}\n\n\nThis defines an index on the field year and allows us to send queries for\ndocuments from a specific year.\nNext, click on \u201cedit query\u201d and change the Mango Query to look like this:\n{\n\"selector\": {\n\"year\": {\n\"$eq\": 1988\n}\n}\n}\n\n\nThen click on \u201dRun Query\u201d.\nThe result should be a single result, the movie \u201cMy Neighbour Totoro\u201d which\nhas the year value of 1988. $eq here stands for \u201cequal\u201d.\n\nNote\nNote that if you skip adding the index, the query will still return the\ncorrect results, although you will see a warning about not using a\npre-existing index. Not using an index will work fine on small databases\nand is acceptable for testing out queries in development or training, but\nwe very strongly discourage doing this in any other case, since an index is\nabsolutely vital to good query performance.\n\nYou can also query for all movies during the 1980s, with this selector:\n{\n\"selector\": {\n\"year\": {\n\"$lt\": 1990,\n\"$gte\": 1980\n}\n}\n}\n\n\nThe result are the two movies from 1988 and 1989. $lt here means \u201clower\nthan\u201d, and $gte means \u201cgreater than or equal to\u201d. The latter currently\ndoesn\u2019t have any effect, given that all of our movies are more recent than\n1980, but this makes the query future-proof and allows us to add older\nmovies later.\n\n\n1.6.5. Triggering Replication\u00b6\nFauxton can trigger replication between two local databases,\nbetween a local and remote database, or even between two remote databases.\nWe\u2019ll show you how to replicate data from one local database to another,\nwhich is a simple way of making backups of your databases as we\u2019re working\nthrough the examples.\nFirst we\u2019ll need to create an empty database to be the target of replication.\nReturn to the Databases overview and create a database called\nhello-replication. Now click \u201cReplication\u201d in the sidebar and choose\nhello-world as the source and hello-replication as the target. Click\n\u201cReplicate\u201d to replicate your database.\nTo view the result of your replication, click on the Databases tab again.\nYou should see the hello-replication database has the same number of documents\nas the hello-world database, and it should take up roughly the same size as\nwell.\n\nNote\nFor larger databases, replication can take much longer. It is important to\nleave the browser window open while replication is taking place.\nAs an alternative, you can trigger replication via curl or some other HTTP\nclient that can handle long-running connections. If your client closes the\nconnection before replication finishes, you\u2019ll have to retrigger it.\nLuckily, CouchDB\u2019s replication can take over from where it left off\ninstead of starting from scratch.\n\n\n\n1.6.6. Wrapping Up\u00b6\nNow that you\u2019ve seen most of Fauxton\u2019s features, you\u2019ll be prepared to dive in\nand inspect your data as we build our example application in the next few\ndocuments. Fauxton\u2019s pure JavaScript approach to managing CouchDB shows how\nit\u2019s possible to build a fully featured web application using only CouchDB\u2019s\nHTTP API and integrated web server.\nBut before we get there, we\u2019ll have another look at CouchDB\u2019s HTTP API \u2013 now\nwith a magnifying glass. Let\u2019s curl up on the couch and relax.\n\n\n\n\n\n\nNext\nPrevious\n\n\n\n\n\u00a9 Copyright 2020, Apache Software Foundation. CouchDB\u00ae is a registered trademark of the Apache Software Foundation.\n\n\nRevision 3f39035f.\n\n\n\nBuilt with Sphinx using a theme provided by Read the Docs.\n\n\n\n\n\n\n\n\nRead the Docs\nv: stable\n\n\n\n\nVersions\nmaster\nlatest\nstable\n3.1.1\n2.3.1\n1.6.1\n\n\nDownloads\npdf\nhtml\nepub\n\n\nOn Read the Docs\n\nProject Home\n\n\nBuilds\n\n\n\nFree document hosting provided by Read the Docs.\n\n\n\n\n\n\n\n\n\n\n\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "pip error while installing Python: \"Ignoring ensurepip failure: pip 8.1.1 requires SSL/TLS\"", "id": 1156, "answers": [{"answer_id": 1149, "document_id": 733, "question_id": 1156, "text": "sudo apt-get install libssl1.0", "answer_start": 185, "answer_category": null}], "is_impossible": false}], "context": "It proceeded well until make. When sudo make altinstall ran, it printed:\nIgnoring ensurepip failure: pip 8.1.1 requires SSL/TLS\nWhat went wrong?\nOn Ubuntu 18.04 only this worked for me\nsudo apt-get install libssl1.0\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Capistrano SSH::AuthenticationFailed, not prompting for password", "id": 1635, "answers": [{"answer_id": 1623, "document_id": 1209, "question_id": 1635, "text": "gem uninstall net-ssh -v 2.8.0<", "answer_start": 417, "answer_category": null}], "is_impossible": false}], "context": "I'm not using rsa_keys or anything I want capistrano to prompt for user and password. Suddenly it has decided not to ask for a password, but does ask for user. Then it rolls back and gives me the following error. \nFigured it out! Apparently this issue was with net-ssh gem. I had version 2.8.0 installed recently with some updates to my development environment and was the cause.\nI'm not sure why it was failing, but gem uninstall net-ssh -v 2.8.0< fixed it for me.\nIf anyone actually knows why this was an issue or how I can correct this issue with the newer version of net-ssh I'd be interested to hear it.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Is APC compatible with PHP 5.4 or PHP 5.5?", "id": 659, "answers": [{"answer_id": 664, "document_id": 352, "question_id": 659, "text": "svn co http://svn.php.net/repository/pecl/apc/trunk/ apc-trunk\ncd apc-trunk\nphpize\n./configure\nmake\nmake install", "answer_start": 703, "answer_category": null}], "is_impossible": false}], "context": "It doesn't seem like APC has been updated to coincide with the php 5.4 release (I wish they would have included APC in PHP core like originally planned).\nI can't seem to find any definitive answer to whether current APC works with php 5.4+. I managed to find Ubuntu packages for php 5.4, but php-apc packages won't install.\nThis means the developers do not consider it completely stable. While many people are experiencing no problems at all with the current SVN releases, there is still the odd report of edge cases from people under certain configurations, or under heavy load.\nIt appears that the bug \"may\" have been fixed in the latest revision to the trunk. I've got it working now with PHP 5.4.0.\nsvn co http://svn.php.net/repository/pecl/apc/trunk/ apc-trunk\ncd apc-trunk\nphpize\n./configure\nmake\nmake install\nAs with everything you would want to use in a production environment, make sure you thoroughly test any release (beta or stable) in development or pre-production environments first. This includes load testing!\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "\"unrecognized import path\" with go get", "id": 669, "answers": [{"answer_id": 674, "document_id": 362, "question_id": 669, "text": "I installed Go with brew on OSX 10.11, and found I had to set GOROOT to:\n/usr/local/Cellar/go/1.5.1/libexec", "answer_start": 338, "answer_category": null}], "is_impossible": false}], "context": "I'm trying to install a web.go, but running go get github.com/hoisie/web returns error. How can I install web.go? Using go get (rather than go install) is what is in the README on the github page. My Go version is go version go1.2 linux/amd64. Because GFW forbidden you to access golang.org ! And when i use the proxy , it can work well. I installed Go with brew on OSX 10.11, and found I had to set GOROOT to:\n/usr/local/Cellar/go/1.5.1/libexec\n(Of course replace the version in this path with go version you have)\nBrew uses symlinks, which were fooling the gotool. So follow the links home.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "how to put radio buttons in windows installer?", "id": 964, "answers": [{"answer_id": 959, "document_id": 593, "question_id": 964, "text": "RadioButtons dialog boxes are used to present up to four mutually exclusive choices to a user and return the value of the selected choice during installation.", "answer_start": 68, "answer_category": null}], "is_impossible": false}, {"question": "Different kinds of radio buttons?", "id": 965, "answers": [{"answer_id": 960, "document_id": 593, "question_id": 965, "text": " RadioButtons (2 buttons), RadioButtons (3 buttons), and RadioButtons (4 buttons)", "answer_start": 273, "answer_category": null}], "is_impossible": false}], "context": "RadioButtons User Interface Dialog Box\n11/01/2012\n4 minutes to read\nRadioButtons dialog boxes are used to present up to four mutually exclusive choices to a user and return the value of the selected choice during installation. Three RadioButtons dialog boxes are available: RadioButtons (2 buttons), RadioButtons (3 buttons), and RadioButtons (4 buttons). These three dialog boxes are identical except for the number of option (radio) buttons; each can be added only once for each deployment project.\n\nThe dialog boxes contain two, three, or four option buttons with associated labels, plus additional labels to present information to the user. The text for these labels is specified at design time in the Properties window. Additional properties are provided for each option button to set a default value and to set the default option button.\n\nThe following illustration shows a typical RadioButtons (2 buttons) dialog box, as it would appear during installation.\n\nRadioButtons (2 buttons) dialog box\n\nDeployment UI splash screen\n\nProperties\nThe following properties are available for the RadioButtons dialog box.\n\nPROPERTIES\nProperty\n\nDescription\n\nBannerBitmap\n\nSpecifies a bitmap or JPEG image to be displayed in the banner area. In the above illustration, the default bitmap is shown. For more information, see BannerBitmap Property.\n\nBannerText\n\nSpecifies the text to be displayed in the banner area. In the above illustration, this corresponds to \"This is the banner text.\"\n\nBodyText\n\nSpecifies the text to be displayed above the option buttons. In the above illustration, this corresponds to \"This is the body text.\"\n\nButton1Label\n\nSpecifies the text to be displayed next to the first option button. In the above illustration, this corresponds to \"First option\".\n\nButton1Value\n\nSpecifies a value for the first option button. This is the value that will be returned by the ButtonProperty property if the first option button is selected.\n\nButton2Label\n\nSpecifies the text to be displayed next to the second option button. In the above illustration, this corresponds to \"Second option\".\n\nButton2Value\n\nSpecifies a value for the second option button. This is the value that will be returned by the ButtonProperty property if the second option button is selected.\n\nButton3Label\n\nSpecifies the text to be displayed next to the third option button. Not available for the RadioButtons (2 buttons) dialog box.\n\nButton3Value\n\nSpecifies a value for the third option button. This is the value that will be returned by the ButtonProperty property if the third option button is selected. Not available for the RadioButtons (2 buttons) dialog box.\n\nButton4Label\n\nSpecifies the text to be displayed next to the fourth option button. Not available for the RadioButtons (2 buttons) or RadioButtons (3 buttons) dialog boxes.\n\nButton4Value\n\nSpecifies a value for the fourth option button. This is the value that will be returned by the ButtonProperty property if the fourth option button is selected. Not available for the RadioButtons (2 buttons) or RadioButtons (3 buttons) dialog boxes.\n\nButtonProperty\n\nSpecifies a property name used to retrieve the value of the selected option button. This property can be used in conditions.\n\nDefaultValue\n\nSpecifies which option button will initially be selected when the dialog box is displayed.\n\nControlling the Appearance of the RadioButtons Dialog Box\nSeveral properties control how the RadioButtons dialog box will appear to the user during installation.\n\nThe BannerBitmap property controls the image that appears in the upper right corner of the dialog box. The position and size of the image are fixed.\n\n Tip\n\nFor a consistent user interface, the same image should be used for all dialog boxes in an installer.\n\nThe BannerText property controls the text that is displayed in the banner across the top of the dialog box. The position, size, and font size are fixed. The text will wrap but cannot be more than two lines.\n\nThe BodyText property controls the text that appears just below the banner portion of the dialog box. The position, size, and font size are fixed. The text will wrap but cannot be more than four lines.\n\nThe ButtonNLabel properties control the text in the option button labels. The position and font size are fixed, and the text will not wrap.\n\n Tip\n\nFor dialog boxes that may be localized, allow extra space for the text expansion. A word in one language may translate into a much longer word in another language, causing the text to be cut off.\n\nRetrieving User Choices with RadioButtons Dialog Box Properties\nWhen the installer containing the RadioButtons dialog box is run, the user can choose an option; you can then use that choice to control another phase of installation.\n\nFor example, you might create a RadioButtons (2 buttons) dialog box that gives the user a choice of installing an optional file. In this case, you would set the Button1Label property to \"Install optional file\" and the Button2Label property to \"Do not install optional file\". You would leave the Button1Value and Button2Value properties at their default values of 1 and 2, respectively.\n\nIf you thought that users usually would not want to install the optional file, you could set the DefaultValue property to 2 so that it would be selected by default.\n\nIn the File System Editor, you would set the Condition property for the optional file to the default value of the ButtonProperty property (BUTTON2) plus the Button1Value property (1): BUTTON2=1.\n\nDuring installation, the Condition property for the file will be evaluated. If the user selects the first option button, the ButtonProperty property will return the Button1Value and the file will be installed; otherwise, the condition will evaluate to false and the file will not be installed.\n\nSee Also\nConcepts\nInstallation Dialog Box Restrictions\n\nOther Resources\nUser Interface Management in Deployment\n\nDeployment Dialog Boxes\n\nLaunch Condition Management in Deployment", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "how do i run visual studio as an administrator by default on WIN 10?", "id": 1508, "answers": [{"answer_id": 1497, "document_id": 1086, "question_id": 1508, "text": "Search for Visual Studio on the Start window and select \"Open file location\":\n\n\nSelect \"Troubleshoot compatibility\" :\n\n\nSelect \"troubleshoot program\":", "answer_start": 1513, "answer_category": null}], "is_impossible": false}, {"question": "how do i run visual studio as an administrator by default on WIN 8?", "id": 1509, "answers": [{"answer_id": 1498, "document_id": 1086, "question_id": 1509, "text": "Select \"Troubleshoot program\"\nCheck \"The program requires additional permissions\"\nclick \"Next\", click \"Test the program...\"\nWait for the program to launch\nClick \"Next\"\nSelect \"Yes, save these settings for this program\"\nClick \"Close\"", "answer_start": 1199, "answer_category": null}], "is_impossible": false}, {"question": "how do i run visual studio as an administrator by default on WIN 7?", "id": 1510, "answers": [{"answer_id": 1499, "document_id": 1086, "question_id": 1510, "text": "Right click on the shortcut of the program, then click on Properties.\nClick on the Shortcut tab for a program shortcut, then click on the Advanced button.\nCheck the 'Run as administrator' box, and click on OK.\nClick on OK.\nOpen the program.\nIf prompted by UAC, then click on Yes to apply permission to allow the program to run with full permission as an Administrator.", "answer_start": 490, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI recently discovered that even while logged into my personal laptop as an administrator, Visual Studio does not run in administrator mode and you need to explicitly use Run As Administrator.\n\nIs there a way to make it run as an administrator by default, other than creating a shortcut, etc.?\n    \n\nCopied and pasted from here, the Using Advanced Properties section. This will allow you to always have the program run as an administrator when you open it.\n\nWindows 7:\n\n\nRight click on the shortcut of the program, then click on Properties.\nClick on the Shortcut tab for a program shortcut, then click on the Advanced button.\nCheck the 'Run as administrator' box, and click on OK.\nClick on OK.\nOpen the program.\nIf prompted by UAC, then click on Yes to apply permission to allow the program to run with full permission as an Administrator.\n\n\nNOTE: If you are doing this is while logged in as standard user instead of an administrator, then you will need to provide the administrator's password before the program will run as administrator.\n\nUpdate:   (2015-07-05)\n\nWindows 8, 8.1 and 10\n\nIn Windows 8, you have to right-click devenv.exe and select \"Troubleshoot compatibility\".\n\n\nSelect \"Troubleshoot program\"\nCheck \"The program requires additional permissions\"\nclick \"Next\", click \"Test the program...\"\nWait for the program to launch\nClick \"Next\"\nSelect \"Yes, save these settings for this program\"\nClick \"Close\"\n\n\nUpdate reference original Link\n    \n\nTry the following steps on Windows 10: \n\n\nSearch for Visual Studio on the Start window and select \"Open file location\":\n\n\nSelect \"Troubleshoot compatibility\" :\n\n\nSelect \"troubleshoot program\":\n\n\n\n\nRaise permissions:\n\n\n\nSelect \"Yes, save these settings for this program\"\nSelect \"Close\"\n\n\nOnce that is done, Visual Studio should be running as administrator. \n    \n\nRight click on the application, Props -&gt; Compatibility -&gt; Check the Run the program as administrator\n    \n\nApplying this change will make it so that when you double click on a .sln file Visual Studio will not open.  Also, you will not be able to drag and drop files into Visual Studio.\n\nFollow the numbered instructions for each file in the bullited list. The paths are for a standard 64-bit install so you may have to adjust them for your system.\n\n\nC:\\Program Files (x86)\\Common Files\\microsoft shared\\MSEnv\\VSLauncher.exe\nC:\\Program Files (x86)\\Microsoft Visual Studio 9.0\\Common7\\IDE\\devenv.exe\nC:\\Program Files (x86)\\Microsoft Visual Studio 10.0\\Common7\\IDE\\devenv.exe\nC:\\Program Files (x86)\\Microsoft Visual Studio 11.0\\Common7\\IDE\\devenv.exe\n\n\nRight click on the file and select Properties\nSelect the Compatibility tab\nOptional: Select Change settings for all users\nSelect Run this program as an administrator\nSelect Ok and close the dialog\n\n\n    \n\nWindows 10\n\n\nRight click \"Visual Studio\" and select \"Open file location\n\nRight click \"Visual Studio\" and select \"Properties\"\n\nClick \"Advanced\" and check \"Run as administrator\"\n\n\n    \n\nwindows 8\n\nthere is no advanced tab anymore. So, to do it automatically, you need to follow the next steps :\n\n-right click on the shortcut\n-click on properties\n-under the \"Shortcut\" tab, click on \"Open File Location\"\n-then, right click on devenv.exe\n-Troubleshoot compatibility\n-Troubleshoot program\n-Check \"The program requires additional permissions\"\n-Then next, next next,...\n    \n\nFollow these simple steps:\n\n\nRight Click on \"devenv.exe\" \nClick \"Troubleshoot compatibility\" \nClick    \"Troubleshoot program\" Check \"The program requires additional\npermissions\" \nClick \"Next\" \nClick \"Test the program...\". It should\nlaunch Visual Studio as Administrator \nClick \"Next\" \nClick \"Yes, save\nthese settings for this program\" \nClick \"Close the troubleshooter\"\nNow the Visual Studio will always run as Administrator.\n\n    \n\n\n\n1- either from start menu or when visual studio is open in the task bar, right click on the VS icon\n\n2- in the context menu, right click again on the visual studio icon\n\n3- left click on prorperties \n\n\n\n4- choose advanced\n\n\n\n5- choose Run as Administrator \n\nclick ok all the windows, close the visual studio and reopen again.\n    \n\nOne time fix :\n\nWindows Registry Editor Version 5.00\n[HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\\AppCompatFlags\\Layers]\n\"C:\\\\Program Files (x86)\\\\Microsoft Visual Studio 14.0\\\\Common7\\\\IDE\\\\devenv.exe\"=\"~ RUNASADMIN\"\n\n    \n\nIn Windows 10 do the following steps:\n - Download and install the 'Everything' application that locates files and folders by name instantly.\n - Find the 'devenv.exe' and locate it.\n\n\n\n\nRight-click on 'devenv.exe' and select \"Troubleshoot compatibility\".\nThen select \"Troubleshoot program\".\nThen check \"The program requires additional permissions\".\nThen test the setting and save setting in next page.\n\n\n\n\n\n\n\n\n\n    \n\nRight click on icon --&gt; Properties --&gt; Advanced --&gt; Check checkbox run as Administrator and everytime it will open under Admin Mode (Same for Windows 8)\n    \n\nRight-click the icon, then click Properties. In the properties window, go to the Compatibility tab. There should be a checkbox labeled \"Run this program as an administrator.\" Check that, then click OK. The next time you run the application from that shortcut, it will automatically run as the admin.\n    \n\nFor Windows 8 \n\n\nright click on the shortcut\nclick on properties\nclick on the \"Shortcut\" tab\nclick on Advanced\n\n\nYou will find Run As administrator (Checkbox)\n    \n\nThere are two ways to Run Visual Studio as Administrator:\n\n1. Only 1 time: For this go to startup search bar, search for Visual studio 2017 or what ever version you have, then Right click on VS and Run as Administrator.\n\n2. Permanent or Always:  For this go to startup search bar, search for visual studio, right click to it and go to properties. In the properties click on advanced button and check the Run as Administrator check box and then click on ok. \n    \n\n@Kumar\n\n\"W7 prompts everytime to run this program \"devenv.exe\" , anyway to get rid of that ?\"\n\nYes. You can prevent Windows from prompting you by going to Control Panel/User Accounts/Change User Account Control settings and move the slider down.\n    \n\nIn addition to the above suggestion, to setup admin rights for Visual Studio, if you still get the following intermediate prompt;\nenter image description here\nYou will have also to click on Show more details or go to Control Panel Security and Maintenance and click on Change User Account Control Settings;\nenter image description here\nand choose Never Notify:\nenter image description here\n    \n\nI have always done it by creating a shortcut, which is not really much of a problem. I believe there is no way of doing it otherwise.\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Maven: add a dependency to a jar by relative path", "id": 1875, "answers": [{"answer_id": 1861, "document_id": 1446, "question_id": 1875, "text": "If you really want this (understand, if you can't use a corporate repository), then my advice would be to use a \"file repository\" local to the project and to not use a system scoped dependency. The system scoped should be avoided, such dependencies don't work well in many situation (e.g. in assembly), they cause more troubles than benefits.", "answer_start": 437, "answer_category": null}], "is_impossible": false}], "context": "I have a proprietary jar that I want to add to my pom as a dependency.\nBut I don't want to add it to a repository. The reason is that I want my usual maven commands such as mvn compile, etc, to work out of the box. (Without demanding from the developers a to add it to some repository by themselves).\nI want the jar to be in a 3rdparty lib in source control, and link to it by relative path from the pom.xml file.\nCan this be done? How?\nIf you really want this (understand, if you can't use a corporate repository), then my advice would be to use a \"file repository\" local to the project and to not use a system scoped dependency. The system scoped should be avoided, such dependencies don't work well in many situation (e.g. in assembly), they cause more troubles than benefits.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to install YAWS on Mac OSX?", "id": 937, "answers": [{"answer_id": 932, "document_id": 584, "question_id": 937, "text": "Homebrew installs the stuff you need that Apple (or your Linux system) didn\u2019t.", "answer_start": 24, "answer_category": null}], "is_impossible": false}], "context": "What Does Homebrew Do?\n\nHomebrew installs the stuff you need that Apple (or your Linux system) didn\u2019t.\n\n$ brew install wget\nHomebrew installs packages to their own directory and then symlinks their files into /usr/local.\n\n$ cd /usr/local\n$ find Cellar\nCellar/wget/1.16.1\nCellar/wget/1.16.1/bin/wget\nCellar/wget/1.16.1/share/man/man1/wget.1\n\n$ ls -l bin\nbin/wget -> ../Cellar/wget/1.16.1/bin/wget\nHomebrew won\u2019t install files outside its prefix and you can place a Homebrew installation wherever you like.\n\nTrivially create your own Homebrew packages.\n\n$ brew create https://foo.com/bar-1.0.tgz\nCreated /usr/local/Homebrew/Library/Taps/homebrew/homebrew-core/Formula/bar.rb\nIt\u2019s all Git and Ruby underneath, so hack away with the knowledge that you can easily revert your modifications and merge upstream updates.\n\n$ brew edit wget # opens in $EDITOR!\nHomebrew formulae are simple Ruby scripts:\n\nclass Wget < Formula\n  homepage \"https://www.gnu.org/software/wget/\"\n  url \"https://ftp.gnu.org/gnu/wget/wget-1.15.tar.gz\"\n  sha256 \"52126be8cf1bddd7536886e74c053ad7d0ed2aa89b4b630f76785bac21695fcd\"\n\n  def install\n    system \"./configure\", \"--prefix=#{prefix}\"\n    system \"make\", \"install\"\n  end\nend\nHomebrew complements macOS (or your Linux system). Install your RubyGems with gem and their dependencies with brew.\n\n\u201cTo install, drag this icon\u2026\u201d no more. Homebrew Cask installs macOS apps, fonts and plugins and other non-open source software.\n\n$ brew install --cask firefox\nMaking a cask is as simple as creating a formula.\n\n$ brew create --cask foo\nEditing /usr/local/Homebrew/Library/Taps/homebrew/homebrew-cask/Casks/foo.r", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to associates the application with files in an Ant task when deploying java applications?", "id": 626, "answers": [{"answer_id": 631, "document_id": 330, "question_id": 626, "text": "The <fx:association> element is used in an Ant task to identify the files that can be handled by the application.", "answer_start": 767, "answer_category": null}], "is_impossible": false}, {"question": "How To define default arguments when deploying java applications?", "id": 627, "answers": [{"answer_id": 632, "document_id": 330, "question_id": 627, "text": " use the -argument option with the javapackager deploy command or the <fx:argument> element in an Ant task when the application package is created. ", "answer_start": 291, "answer_category": null}], "is_impossible": false}, {"question": "How To define multiple entry points when deploying java applications?", "id": 628, "answers": [{"answer_id": 633, "document_id": 330, "question_id": 628, "text": " The mainClass attribute for the <fx:application> element identifies the primary entry point. Use the <fx:secondaryLauncher> element with the <fx:deploy> task to define each secondary entry point.", "answer_start": 1434, "answer_category": null}], "is_impossible": false}, {"question": "How To place the application into a specific category in the application menu when deploying java applications?", "id": 629, "answers": [{"answer_id": 634, "document_id": 330, "question_id": 629, "text": "use the category attribute of <fx:info>.", "answer_start": 2301, "answer_category": null}], "is_impossible": false}], "context": "7.3.6 Passing Arguments to a Self-Contained Application\nArguments can be passed to a self-contained application when the application is started from the command line. You can also define a set of arguments to pass to the application if no arguments are provided. To define default arguments, use the -argument option with the javapackager deploy command or the <fx:argument> element in an Ant task when the application package is created. Arguments entered from the command line override the default arguments. If the application is started from the launcher icon, the default arguments are used.\n7.3.7 Associating Files with a Self-Contained Application\nThe installer for a self-contained application can be set up to register file associations for the application. The <fx:association> element is used in an Ant task to identify the files that can be handled by the application. File associations are based on either the file extension or MIME type.\nThe following example associates the application with files that have the MIME type application/x-vnd.MyAppFile.\n<fx:info title=\"Association example\">\n  <fx:association mimetype=\"application/x-vnd.MyAppFile\" description=\"Sample Test Files\">\n  </fx:association>\n</fx:info>\n7.3.8 Supporting Multiple Entry Points\nThe package for self-contained applications can be built to support a suite of products with more than one entry point. Each entry point can have its own shortcut or icon. The mainClass attribute for the <fx:application> element identifies the primary entry point. Use the <fx:secondaryLauncher> element with the <fx:deploy> task to define each secondary entry point.\n\n7.4.4 Linux Packages\nProducing install packages for Linux assumes that the native tools needed to build install packages are installed. For RPM packages, this typically means the RPMBuild package and its dependencies. For DEB packages, dpkg-deb and dependencies are needed.\nNo admin permissions are needed to build the package.\nBy default the resulting package has the following characteristics:\n\u2022\tInstalls the application to the /opt directory\n\u2022\tAdds a shortcut to the application menu\n\u2022\tDoes not have any UI for installation, which is normal behavior for Linux packages\nCustomization tips:\n\u2022\tTo place the application into a specific category in the application menu, use the category attribute of <fx:info>.\nSee Desktop Menu Specification, and your window manager docs for the list of category names.\n\u2022\tThe icon is expected to be a .png file\n\u2022\tAdvanced customization is possible by tuning the build template files using techniques described in Section 7.3.3, \"Customizing the Package Using Drop-In Resources.\".\nSee the DEB/RPM packaging guides for more information on available options.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How do I tell if .NET 3.5 SP1 is installed?", "id": 676, "answers": [{"answer_id": 681, "document_id": 369, "question_id": 676, "text": "You could go to SmallestDotNet using IE from the server. That will tell you the version and also provide a download link if you're out of date.", "answer_start": 186, "answer_category": null}], "is_impossible": false}], "context": "I came to this page while trying to figure out how to detect the framework versions installed on a server without access to remote desktop or registry, so Danny V's answer worked for me.You could go to SmallestDotNet using IE from the server. That will tell you the version and also provide a download link if you're out of date. 136\nLook at HKLM\\SOFTWARE\\Microsoft\\NET Framework Setup\\NDP\\v3.5\\. One of these must be true:\n\u2022\tThe Version value in that key should be 3.5.30729.01\n\u2022\tOr the SP value in the same key should be 1\nIn C# (taken from the first comment), you could do something along these lines:\nconst string name = @\"SOFTWARE\\Microsoft\\NET Framework Setup\\NDP\\v3.5\";\nRegistryKey subKey = Registry.LocalMachine.OpenSubKey(name);\nvar version = subKey.GetValue(\"Version\").ToString();\nvar servicePack = subKey.GetValue(\"SP\").ToString();\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Jenkins - j_acegi_security_check", "id": 728, "answers": [{"answer_id": 731, "document_id": 418, "question_id": 728, "text": "in Manage Jenkins \u2192 Configure Global Security select an option in the Security Realm list", "answer_start": 138, "answer_category": null}], "is_impossible": false}], "context": "pent ages wrestling with this one, make sure a Security Realm is set when you are choosing your Authorization method in Jenkins.\nThat is, in Manage Jenkins \u2192 Configure Global Security select an option in the Security Realm list\nI am trying to setup jenkins, but I cant get the authentication to work. I am running jenkins on Tomcat6 on CentOS 6.2. I enable logging in, and everything goes fine until I try to log in. After giving my credential and pressing login, tomcat gives me a error: HTTP Status 404 - The requested resource () is not available.\"\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "pip installation /usr/local/opt/python/bin/python2.7: bad interpreter: No such file or directory", "id": 657, "answers": [{"answer_id": 662, "document_id": 350, "question_id": 657, "text": "brew link --overwrite python", "answer_start": 238, "answer_category": null}], "is_impossible": false}], "context": "I don't know what's the deal but I am stuck following some stackoverflow solutions which gets nowhere. Can you please help me on this? I had used home-brew to install 2.7 on OS X 10.10 and the new install was missing the sym links. I ran\nbrew link --overwrite python\nas mentioned in How to symlink python in Homebrew? and it solved the problem.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to create a remote Git repository from a local one?", "id": 1543, "answers": [{"answer_id": 1532, "document_id": 1120, "question_id": 1543, "text": "I think you make a bare repository on the remote side, git init --bare, add the remote side as the push/pull tracker for your local repository (git remote add origin URL), and then locally you just say git push origin master. Now any other repository can pull from the remote repository.", "answer_start": 117, "answer_category": null}], "is_impossible": false}], "context": "I have a local Git repository. I would like to make it available on a remote, ssh-enabled, server. How do I do this?\nI think you make a bare repository on the remote side, git init --bare, add the remote side as the push/pull tracker for your local repository (git remote add origin URL), and then locally you just say git push origin master. Now any other repository can pull from the remote repository.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Adding Custom prerequsites to visual studio setup project", "id": 1226, "answers": [{"answer_id": 1219, "document_id": 802, "question_id": 1226, "text": "Basically you just have to create a product manifest and a package manifest, copy them along with your distributable file to : \\Program Files\\Microsoft SDKs\\Windows\\v6.0A\\Bootstrapper\\Packages. Visual studio will automatically pick it up.", "answer_start": 361, "answer_category": null}], "is_impossible": false}], "context": "I have a setup project that I need to install a redistributable that is not available in the default prerequisite list. Is it possible to add this redistributable to the bootstrapper that the setup project creates?\nI figured out how to add Custom Prerequisites to the Visual Studio prerequisites dialog box.\nMSDN as a good article on creating the prerequisite. Basically you just have to create a product manifest and a package manifest, copy them along with your distributable file to : \\Program Files\\Microsoft SDKs\\Windows\\v6.0A\\Bootstrapper\\Packages. Visual studio will automatically pick it up. Take a look at Bootstrapper Manifest Generator tool (BMG) at http://code.msdn.microsoft.com/bmg\nIt is used for creating Bootstrapper packages and automatically adding them to Visual Studio's Prerequisites dialog box.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "error: could not create '/Library/Python/2.7/site-packages/xlrd': Permission denied", "id": 118, "answers": [{"answer_id": 126, "document_id": 79, "question_id": 118, "text": "If you don\u2019t want to install modules to the standard location, or if you don\u2019t\nhave permission to write there, then you need to read about alternate\ninstallations in section Alternate Installation.", "answer_start": 11313, "answer_category": null}], "is_impossible": false}, {"question": " How to build a Python2.0 interpreter with Cygwin", "id": 119, "answers": [{"answer_id": 127, "document_id": 79, "question_id": 119, "text": "To let Distutils compile your extension with Cygwin you have to type:\npython setup.py build --compiler=cygwin\n", "answer_start": 38555, "answer_category": null}], "is_impossible": false}, {"question": " How to build a Python2.0 interpreter with MinGW", "id": 120, "answers": [{"answer_id": 128, "document_id": 79, "question_id": 120, "text": "for Cygwin in no-cygwin mode 3 or for MinGW type:\npython setup.py build --compiler=mingw32", "answer_start": 38671, "answer_category": null}], "is_impossible": false}, {"question": "Python: How to force overwriting of files when using setup.py install (distutil)", "id": 121, "answers": [{"answer_id": 129, "document_id": 79, "question_id": 121, "text": "You could override the default \u201cbuild base\u201d directory and make the\nbuild* commands always forcibly rebuild all files with the\nfollowing:\n[build]\nbuild-base=blib\nforce=1\n\n\nwhich corresponds to the command-line arguments\npython setup.py build --build-base=blib --force", "answer_start": 32193, "answer_category": null}], "is_impossible": false}, {"question": "How to find python distutils options?", "id": 122, "answers": [{"answer_id": 130, "document_id": 79, "question_id": 122, "text": "You can find out the complete list of options for any command using the\n--help option, e.g.:\npython setup.py build --help", "answer_start": 32817, "answer_category": null}], "is_impossible": false}], "context": "\n\nInstalling Python Modules (Legacy version)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis document is for an old version of Python that is no longer supported.\nYou should upgrade and read the\nPython documentation for the current stable release.\n\n\nNavigation\n\n\nindex\n\nmodules |\n\nprevious |\n\nPython \u00bb\n\nen\n2.7.18\nDocumentation \u00bb\n\n\n\n\n\n\n\n\nInstalling Python Modules (Legacy version)\u00b6\n\nAuthor\nGreg Ward\n\n\n\nSee also\n\nInstalling Python ModulesThe up to date module installation documentations\n\n\n\nThis document describes the Python Distribution Utilities (\u201cDistutils\u201d) from the\nend-user\u2019s point-of-view, describing how to extend the capabilities of a\nstandard Python installation by building and installing third-party Python\nmodules and extensions.\n\nNote\nThis guide only covers the basic tools for building and distributing\nextensions that are provided as part of this version of Python. Third party\ntools offer easier to use and more secure alternatives. Refer to the quick\nrecommendations section\nin the Python Packaging User Guide for more information.\n\n\nIntroduction\u00b6\nAlthough Python\u2019s extensive standard library covers many programming needs,\nthere often comes a time when you need to add some new functionality to your\nPython installation in the form of third-party modules.  This might be necessary\nto support your own programming, or to support an application that you want to\nuse and that happens to be written in Python.\nIn the past, there has been little support for adding third-party modules to an\nexisting Python installation.  With the introduction of the Python Distribution\nUtilities (Distutils for short) in Python 2.0, this changed.\nThis document is aimed primarily at the people who need to install third-party\nPython modules: end-users and system administrators who just need to get some\nPython application running, and existing Python programmers who want to add some\nnew goodies to their toolbox.  You don\u2019t need to know Python to read this\ndocument; there will be some brief forays into using Python\u2019s interactive mode\nto explore your installation, but that\u2019s it.  If you\u2019re looking for information\non how to distribute your own Python modules so that others may use them, see\nthe Distributing Python Modules (Legacy version) manual.  Debugging the setup script may also be of\ninterest.\n\nBest case: trivial installation\u00b6\nIn the best case, someone will have prepared a special version of the module\ndistribution you want to install that is targeted specifically at your platform\nand is installed just like any other software on your platform.  For example,\nthe module developer might make an executable installer available for Windows\nusers, an RPM package for users of RPM-based Linux systems (Red Hat, SuSE,\nMandrake, and many others), a Debian package for users of Debian-based Linux\nsystems, and so forth.\nIn that case, you would download the installer appropriate to your platform and\ndo the obvious thing with it: run it if it\u2019s an executable installer, rpm\n--install it if it\u2019s an RPM, etc.  You don\u2019t need to run Python or a setup\nscript, you don\u2019t need to compile anything\u2014you might not even need to read any\ninstructions (although it\u2019s always a good idea to do so anyway).\nOf course, things will not always be that easy.  You might be interested in a\nmodule distribution that doesn\u2019t have an easy-to-use installer for your\nplatform.  In that case, you\u2019ll have to start with the source distribution\nreleased by the module\u2019s author/maintainer.  Installing from a source\ndistribution is not too hard, as long as the modules are packaged in the\nstandard way.  The bulk of this document is about building and installing\nmodules from standard source distributions.\n\n\nThe new standard: Distutils\u00b6\nIf you download a module source distribution, you can tell pretty quickly if it\nwas packaged and distributed in the standard way, i.e. using the Distutils.\nFirst, the distribution\u2019s name and version number will be featured prominently\nin the name of the downloaded archive, e.g. foo-1.0.tar.gz or\nwidget-0.9.7.zip.  Next, the archive will unpack into a similarly-named\ndirectory: foo-1.0 or widget-0.9.7.  Additionally, the\ndistribution will contain a setup script setup.py, and a file named\nREADME.txt or possibly just README, which should explain that\nbuilding and installing the module distribution is a simple matter of running\none command from a terminal:\npython setup.py install\n\n\nFor Windows, this command should be run from a command prompt window\n(Start \u2023 Accessories):\nsetup.py install\n\n\nIf all these things are true, then you already know how to build and install the\nmodules you\u2019ve just downloaded:  Run the command above. Unless you need to\ninstall things in a non-standard way or customize the build process, you don\u2019t\nreally need this manual.  Or rather, the above command is everything you need to\nget out of this manual.\n\n\n\nStandard Build and Install\u00b6\nAs described in section The new standard: Distutils, building and installing a module\ndistribution using the Distutils is usually one simple command to run from a\nterminal:\npython setup.py install\n\n\n\nPlatform variations\u00b6\nYou should always run the setup command from the distribution root directory,\ni.e. the top-level subdirectory that the module source distribution unpacks\ninto.  For example, if you\u2019ve just downloaded a module source distribution\nfoo-1.0.tar.gz onto a Unix system, the normal thing to do is:\ngunzip -c foo-1.0.tar.gz | tar xf -    # unpacks into directory foo-1.0\ncd foo-1.0\npython setup.py install\n\n\nOn Windows, you\u2019d probably download foo-1.0.zip.  If you downloaded the\narchive file to C:\\Temp, then it would unpack into\nC:\\Temp\\foo-1.0; you can use either an archive manipulator with a\ngraphical user interface (such as WinZip) or a command-line tool (such as\nunzip or pkunzip) to unpack the archive.  Then, open a\ncommand prompt window and run:\ncd c:\\Temp\\foo-1.0\npython setup.py install\n\n\n\n\nSplitting the job up\u00b6\nRunning setup.py install builds and installs all modules in one run.  If you\nprefer to work incrementally\u2014especially useful if you want to customize the\nbuild process, or if things are going wrong\u2014you can use the setup script to do\none thing at a time.  This is particularly helpful when the build and install\nwill be done by different users\u2014for example, you might want to build a module\ndistribution and hand it off to a system administrator for installation (or do\nit yourself, with super-user privileges).\nFor example, you can build everything in one step, and then install everything\nin a second step, by invoking the setup script twice:\npython setup.py build\npython setup.py install\n\n\nIf you do this, you will notice that running the install command\nfirst runs the build command, which\u2014in this case\u2014quickly notices\nthat it has nothing to do, since everything in the build directory is\nup-to-date.\nYou may not need this ability to break things down often if all you do is\ninstall modules downloaded off the \u2018net, but it\u2019s very handy for more advanced\ntasks.  If you get into distributing your own Python modules and extensions,\nyou\u2019ll run lots of individual Distutils commands on their own.\n\n\nHow building works\u00b6\nAs implied above, the build command is responsible for putting the\nfiles to install into a build directory.  By default, this is build\nunder the distribution root; if you\u2019re excessively concerned with speed, or want\nto keep the source tree pristine, you can change the build directory with the\n--build-base option. For example:\npython setup.py build --build-base=/path/to/pybuild/foo-1.0\n\n\n(Or you could do this permanently with a directive in your system or personal\nDistutils configuration file; see section Distutils Configuration Files.)  Normally, this\nisn\u2019t necessary.\nThe default layout for the build tree is as follows:\n--- build/ --- lib/\nor\n--- build/ --- lib.<plat>/\ntemp.<plat>/\n\n\nwhere <plat> expands to a brief description of the current OS/hardware\nplatform and Python version.  The first form, with just a lib directory,\nis used for \u201cpure module distributions\u201d\u2014that is, module distributions that\ninclude only pure Python modules.  If a module distribution contains any\nextensions (modules written in C/C++), then the second form, with two <plat>\ndirectories, is used.  In that case, the temp.plat directory holds\ntemporary files generated by the compile/link process that don\u2019t actually get\ninstalled.  In either case, the lib (or lib.plat) directory\ncontains all Python modules (pure Python and extensions) that will be installed.\nIn the future, more directories will be added to handle Python scripts,\ndocumentation, binary executables, and whatever else is needed to handle the job\nof installing Python modules and applications.\n\n\nHow installation works\u00b6\nAfter the build command runs (whether you run it explicitly, or the\ninstall command does it for you), the work of the install\ncommand is relatively simple: all it has to do is copy everything under\nbuild/lib (or build/lib.plat) to your chosen installation\ndirectory.\nIf you don\u2019t choose an installation directory\u2014i.e., if you just run setup.py\ninstall\u2014then the install command installs to the standard\nlocation for third-party Python modules.  This location varies by platform and\nby how you built/installed Python itself.  On Unix (and Mac OS X, which is also\nUnix-based), it also depends on whether the module distribution being installed\nis pure Python or contains extensions (\u201cnon-pure\u201d):\n\n\n\n\n\n\n\n\nPlatform\nStandard installation location\nDefault value\nNotes\n\n\n\nUnix (pure)\nprefix/lib/pythonX.Y/site-packages\n/usr/local/lib/pythonX.Y/site-packages\n(1)\n\nUnix (non-pure)\nexec-prefix/lib/pythonX.Y/site-packages\n/usr/local/lib/pythonX.Y/site-packages\n(1)\n\nWindows\nprefix\\Lib\\site-packages\nC:\\PythonXY\\Lib\\site-packages\n(2)\n\n\n\nNotes:\n\nMost Linux distributions include Python as a standard part of the system, so\nprefix and exec-prefix are usually both /usr on\nLinux.  If you build Python yourself on Linux (or any Unix-like system), the\ndefault prefix and exec-prefix are /usr/local.\nThe default installation directory on Windows was C:\\Program\nFiles\\Python under Python 1.6a1, 1.5.2, and earlier.\n\nprefix and exec-prefix stand for the directories that Python\nis installed to, and where it finds its libraries at run-time.  They are always\nthe same under Windows, and very often the same under Unix and Mac OS X.  You\ncan find out what your Python installation uses for prefix and\nexec-prefix by running Python in interactive mode and typing a few\nsimple commands. Under Unix, just type python at the shell prompt.  Under\nWindows, choose Start \u2023 Programs \u2023 Python X.Y \u2023\nPython (command line).   Once the interpreter is started, you type Python code\nat the prompt.  For example, on my Linux system, I type the three Python\nstatements shown below, and get the output as shown, to find out my\nprefix and exec-prefix:\nPython 2.4 (#26, Aug  7 2004, 17:19:02)\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import sys\n>>> sys.prefix\n'/usr'\n>>> sys.exec_prefix\n'/usr'\n\n\nA few other placeholders are used in this document: X.Y stands for the\nversion of Python, for example 2.7; distname will be replaced by\nthe name of the module distribution being installed.  Dots and capitalization\nare important in the paths; for example, a value that uses python2.7 on UNIX\nwill typically use Python27 on Windows.\nIf you don\u2019t want to install modules to the standard location, or if you don\u2019t\nhave permission to write there, then you need to read about alternate\ninstallations in section Alternate Installation.  If you want to customize your\ninstallation directories more heavily, see section Custom Installation on\ncustom installations.\n\n\n\nAlternate Installation\u00b6\nOften, it is necessary or desirable to install modules to a location other than\nthe standard location for third-party Python modules.  For example, on a Unix\nsystem you might not have permission to write to the standard third-party module\ndirectory.  Or you might wish to try out a module before making it a standard\npart of your local Python installation.  This is especially true when upgrading\na distribution already present: you want to make sure your existing base of\nscripts still works with the new version before actually upgrading.\nThe Distutils install command is designed to make installing module\ndistributions to an alternate location simple and painless.  The basic idea is\nthat you supply a base directory for the installation, and the\ninstall command picks a set of directories (called an installation\nscheme) under this base directory in which to install files.  The details\ndiffer across platforms, so read whichever of the following sections applies to\nyou.\nNote that the various alternate installation schemes are mutually exclusive: you\ncan pass --user, or --home, or --prefix and --exec-prefix, or\n--install-base and --install-platbase, but you can\u2019t mix from these\ngroups.\n\nAlternate installation: the user scheme\u00b6\nThis scheme is designed to be the most convenient solution for users that don\u2019t\nhave write permission to the global site-packages directory or don\u2019t want to\ninstall into it.  It is enabled with a simple option:\npython setup.py install --user\n\n\nFiles will be installed into subdirectories of site.USER_BASE (written\nas userbase hereafter).  This scheme installs pure Python modules and\nextension modules in the same location (also known as site.USER_SITE).\nHere are the values for UNIX, including Mac OS X:\n\n\n\n\n\n\nType of file\nInstallation directory\n\n\n\nmodules\nuserbase/lib/pythonX.Y/site-packages\n\nscripts\nuserbase/bin\n\ndata\nuserbase\n\nC headers\nuserbase/include/pythonX.Y/distname\n\n\n\nAnd here are the values used on Windows:\n\n\n\n\n\n\nType of file\nInstallation directory\n\n\n\nmodules\nuserbase\\PythonXY\\site-packages\n\nscripts\nuserbase\\Scripts\n\ndata\nuserbase\n\nC headers\nuserbase\\PythonXY\\Include{distname}\n\n\n\nThe advantage of using this scheme compared to the other ones described below is\nthat the user site-packages directory is under normal conditions always included\nin sys.path (see site for more information), which means that\nthere is no additional step to perform after running the setup.py script\nto finalize the installation.\nThe build_ext command also has a --user option to add\nuserbase/include to the compiler search path for header files and\nuserbase/lib to the compiler search path for libraries as well as to\nthe runtime search path for shared C libraries (rpath).\n\n\nAlternate installation: the home scheme\u00b6\nThe idea behind the \u201chome scheme\u201d is that you build and maintain a personal\nstash of Python modules.  This scheme\u2019s name is derived from the idea of a\n\u201chome\u201d directory on Unix, since it\u2019s not unusual for a Unix user to make their\nhome directory have a layout similar to /usr/ or /usr/local/.\nThis scheme can be used by anyone, regardless of the operating system they\nare installing for.\nInstalling a new module distribution is as simple as\npython setup.py install --home=<dir>\n\n\nwhere you can supply any directory you like for the --home option.  On\nUnix, lazy typists can just type a tilde (~); the install command\nwill expand this to your home directory:\npython setup.py install --home=~\n\n\nTo make Python find the distributions installed with this scheme, you may have\nto modify Python\u2019s search path or edit\nsitecustomize (see site) to call site.addsitedir() or edit\nsys.path.\nThe --home option defines the installation base directory.  Files are\ninstalled to the following directories under the installation base as follows:\n\n\n\n\n\n\nType of file\nInstallation directory\n\n\n\nmodules\nhome/lib/python\n\nscripts\nhome/bin\n\ndata\nhome\n\nC headers\nhome/include/python/distname\n\n\n\n(Mentally replace slashes with backslashes if you\u2019re on Windows.)\n\nChanged in version 2.4: The --home option used to be supported only on Unix.\n\n\n\nAlternate installation: Unix (the prefix scheme)\u00b6\nThe \u201cprefix scheme\u201d is useful when you wish to use one Python installation to\nperform the build/install (i.e., to run the setup script), but install modules\ninto the third-party module directory of a different Python installation (or\nsomething that looks like a different Python installation).  If this sounds a\ntrifle unusual, it is\u2014that\u2019s why the user and home schemes come before.  However,\nthere are at least two known cases where the prefix scheme will be useful.\nFirst, consider that many Linux distributions put Python in /usr, rather\nthan the more traditional /usr/local.  This is entirely appropriate,\nsince in those cases Python is part of \u201cthe system\u201d rather than a local add-on.\nHowever, if you are installing Python modules from source, you probably want\nthem to go in /usr/local/lib/python2.X rather than\n/usr/lib/python2.X.  This can be done with\n/usr/bin/python setup.py install --prefix=/usr/local\n\n\nAnother possibility is a network filesystem where the name used to write to a\nremote directory is different from the name used to read it: for example, the\nPython interpreter accessed as /usr/local/bin/python might search for\nmodules in /usr/local/lib/python2.X, but those modules would have to\nbe installed to, say, /mnt/@server/export/lib/python2.X.  This could\nbe done with\n/usr/local/bin/python setup.py install --prefix=/mnt/@server/export\n\n\nIn either case, the --prefix option defines the installation base, and\nthe --exec-prefix option defines the platform-specific installation\nbase, which is used for platform-specific files.  (Currently, this just means\nnon-pure module distributions, but could be expanded to C libraries, binary\nexecutables, etc.)  If --exec-prefix is not supplied, it defaults to\n--prefix.  Files are installed as follows:\n\n\n\n\n\n\nType of file\nInstallation directory\n\n\n\nPython modules\nprefix/lib/pythonX.Y/site-packages\n\nextension modules\nexec-prefix/lib/pythonX.Y/site-packages\n\nscripts\nprefix/bin\n\ndata\nprefix\n\nC headers\nprefix/include/pythonX.Y/distname\n\n\n\nThere is no requirement that --prefix or --exec-prefix\nactually point to an alternate Python installation; if the directories listed\nabove do not already exist, they are created at installation time.\nIncidentally, the real reason the prefix scheme is important is simply that a\nstandard Unix installation uses the prefix scheme, but with --prefix\nand --exec-prefix supplied by Python itself as sys.prefix and\nsys.exec_prefix.  Thus, you might think you\u2019ll never use the prefix scheme,\nbut every time you run python setup.py install without any other options,\nyou\u2019re using it.\nNote that installing extensions to an alternate Python installation has no\neffect on how those extensions are built: in particular, the Python header files\n(Python.h and friends) installed with the Python interpreter used to run\nthe setup script will be used in compiling extensions.  It is your\nresponsibility to ensure that the interpreter used to run extensions installed\nin this way is compatible with the interpreter used to build them.  The best way\nto do this is to ensure that the two interpreters are the same version of Python\n(possibly different builds, or possibly copies of the same build).  (Of course,\nif your --prefix and --exec-prefix don\u2019t even point to an\nalternate Python installation, this is immaterial.)\n\n\nAlternate installation: Windows (the prefix scheme)\u00b6\nWindows has no concept of a user\u2019s home directory, and since the standard Python\ninstallation under Windows is simpler than under Unix, the --prefix\noption has traditionally been used to install additional packages in separate\nlocations on Windows.\npython setup.py install --prefix=\"\\Temp\\Python\"\n\n\nto install modules to the \\Temp\\Python directory on the current drive.\nThe installation base is defined by the --prefix option; the\n--exec-prefix option is not supported under Windows, which means that\npure Python modules and extension modules are installed into the same location.\nFiles are installed as follows:\n\n\n\n\n\n\nType of file\nInstallation directory\n\n\n\nmodules\nprefix\\Lib\\site-packages\n\nscripts\nprefix\\Scripts\n\ndata\nprefix\n\nC headers\nprefix\\Include{distname}\n\n\n\n\n\n\nCustom Installation\u00b6\nSometimes, the alternate installation schemes described in section\nAlternate Installation just don\u2019t do what you want.  You might want to tweak just\none or two directories while keeping everything under the same base directory,\nor you might want to completely redefine the installation scheme.  In either\ncase, you\u2019re creating a custom installation scheme.\nTo create a custom installation scheme, you start with one of the alternate\nschemes and override some of the installation directories used for the various\ntypes of files, using these options:\n\n\n\n\n\n\nType of file\nOverride option\n\n\n\nPython modules\n--install-purelib\n\nextension modules\n--install-platlib\n\nall modules\n--install-lib\n\nscripts\n--install-scripts\n\ndata\n--install-data\n\nC headers\n--install-headers\n\n\n\nThese override options can be relative, absolute,\nor explicitly defined in terms of one of the installation base directories.\n(There are two installation base directories, and they are normally the same\u2014\nthey only differ when you use the Unix \u201cprefix scheme\u201d and supply different\n--prefix and --exec-prefix options; using --install-lib will\noverride values computed or given for --install-purelib and\n--install-platlib, and is recommended for schemes that don\u2019t make a\ndifference between Python and extension modules.)\nFor example, say you\u2019re installing a module distribution to your home directory\nunder Unix\u2014but you want scripts to go in ~/scripts rather than\n~/bin. As you might expect, you can override this directory with the\n--install-scripts option; in this case, it makes most sense to supply\na relative path, which will be interpreted relative to the installation base\ndirectory (your home directory, in this case):\npython setup.py install --home=~ --install-scripts=scripts\n\n\nAnother Unix example: suppose your Python installation was built and installed\nwith a prefix of /usr/local/python, so under a standard  installation\nscripts will wind up in /usr/local/python/bin.  If you want them in\n/usr/local/bin instead, you would supply this absolute directory for the\n--install-scripts option:\npython setup.py install --install-scripts=/usr/local/bin\n\n\n(This performs an installation using the \u201cprefix scheme,\u201d where the prefix is\nwhatever your Python interpreter was installed with\u2014 /usr/local/python\nin this case.)\nIf you maintain Python on Windows, you might want third-party modules to live in\na subdirectory of prefix, rather than right in prefix\nitself.  This is almost as easy as customizing the script installation directory\n\u2014you just have to remember that there are two types of modules to worry about,\nPython and extension modules, which can conveniently be both controlled by one\noption:\npython setup.py install --install-lib=Site\n\n\nThe specified installation directory is relative to prefix.  Of\ncourse, you also have to ensure that this directory is in Python\u2019s module\nsearch path, such as by putting a .pth file in a site directory (see\nsite).  See section Modifying Python\u2019s Search Path to find out how to modify\nPython\u2019s search path.\nIf you want to define an entire installation scheme, you just have to supply all\nof the installation directory options.  The recommended way to do this is to\nsupply relative paths; for example, if you want to maintain all Python\nmodule-related files under python in your home directory, and you want a\nseparate directory for each platform that you use your home directory from, you\nmight define the following installation scheme:\npython setup.py install --home=~ \\\n--install-purelib=python/lib \\\n--install-platlib=python/lib.$PLAT \\\n--install-scripts=python/scripts\n--install-data=python/data\n\n\nor, equivalently,\npython setup.py install --home=~/python \\\n--install-purelib=lib \\\n--install-platlib='lib.$PLAT' \\\n--install-scripts=scripts\n--install-data=data\n\n\n$PLAT is not (necessarily) an environment variable\u2014it will be expanded by\nthe Distutils as it parses your command line options, just as it does when\nparsing your configuration file(s).\nObviously, specifying the entire installation scheme every time you install a\nnew module distribution would be very tedious.  Thus, you can put these options\ninto your Distutils config file (see section Distutils Configuration Files):\n[install]\ninstall-base=$HOME\ninstall-purelib=python/lib\ninstall-platlib=python/lib.$PLAT\ninstall-scripts=python/scripts\ninstall-data=python/data\n\n\nor, equivalently,\n[install]\ninstall-base=$HOME/python\ninstall-purelib=lib\ninstall-platlib=lib.$PLAT\ninstall-scripts=scripts\ninstall-data=data\n\n\nNote that these two are not equivalent if you supply a different installation\nbase directory when you run the setup script.  For example,\npython setup.py install --install-base=/tmp\n\n\nwould install pure modules to /tmp/python/lib in the first case, and\nto /tmp/lib in the second case.  (For the second case, you probably\nwant to supply an installation base of /tmp/python.)\nYou probably noticed the use of $HOME and $PLAT in the sample\nconfiguration file input.  These are Distutils configuration variables, which\nbear a strong resemblance to environment variables. In fact, you can use\nenvironment variables in config files on platforms that have such a notion but\nthe Distutils additionally define a few extra variables that may not be in your\nenvironment, such as $PLAT.  (And of course, on systems that don\u2019t have\nenvironment variables, such as Mac OS 9, the configuration variables supplied by\nthe Distutils are the only ones you can use.) See section Distutils Configuration Files\nfor details.\n\nModifying Python\u2019s Search Path\u00b6\nWhen the Python interpreter executes an import statement, it searches\nfor both Python code and extension modules along a search path.  A default value\nfor the path is configured into the Python binary when the interpreter is built.\nYou can determine the path by importing the sys module and printing the\nvalue of sys.path.\n$ python\nPython 2.2 (#11, Oct  3 2002, 13:31:27)\n[GCC 2.96 20000731 (Red Hat Linux 7.3 2.96-112)] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import sys\n>>> sys.path\n['', '/usr/local/lib/python2.3', '/usr/local/lib/python2.3/plat-linux2',\n'/usr/local/lib/python2.3/lib-tk', '/usr/local/lib/python2.3/lib-dynload',\n'/usr/local/lib/python2.3/site-packages']\n>>>\n\n\nThe null string in sys.path represents the current working directory.\nThe expected convention for locally installed packages is to put them in the\n\u2026/site-packages/ directory, but you may want to install Python\nmodules into some arbitrary directory.  For example, your site may have a\nconvention of keeping all software related to the web server under /www.\nAdd-on Python modules might then belong in /www/python, and in order to\nimport them, this directory must be added to sys.path.  There are several\ndifferent ways to add the directory.\nThe most convenient way is to add a path configuration file to a directory\nthat\u2019s already on Python\u2019s path, usually to the .../site-packages/\ndirectory.  Path configuration files have an extension of .pth, and each\nline must contain a single path that will be appended to sys.path.  (Because\nthe new paths are appended to sys.path, modules in the added directories\nwill not override standard modules.  This means you can\u2019t use this mechanism for\ninstalling fixed versions of standard modules.)\nPaths can be absolute or relative, in which case they\u2019re relative to the\ndirectory containing the .pth file.  See the documentation of\nthe site module for more information.\nA slightly less convenient way is to edit the site.py file in Python\u2019s\nstandard library, and modify sys.path.  site.py is automatically\nimported when the Python interpreter is executed, unless the -S switch\nis supplied to suppress this behaviour.  So you could simply edit\nsite.py and add two lines to it:\nimport sys\nsys.path.append('/www/python/')\n\n\nHowever, if you reinstall the same major version of Python (perhaps when\nupgrading from 2.2 to 2.2.2, for example) site.py will be overwritten by\nthe stock version.  You\u2019d have to remember that it was modified and save a copy\nbefore doing the installation.\nThere are two environment variables that can modify sys.path.\nPYTHONHOME sets an alternate value for the prefix of the Python\ninstallation.  For example, if PYTHONHOME is set to /www/python,\nthe search path will be set to ['', '/www/python/lib/pythonX.Y/',\n'/www/python/lib/pythonX.Y/plat-linux2', ...].\nThe PYTHONPATH variable can be set to a list of paths that will be\nadded to the beginning of sys.path.  For example, if PYTHONPATH is\nset to /www/python:/opt/py, the search path will begin with\n['/www/python', '/opt/py'].  (Note that directories must exist in order to\nbe added to sys.path; the site module removes paths that don\u2019t\nexist.)\nFinally, sys.path is just a regular Python list, so any Python application\ncan modify it by adding or removing entries.\n\n\n\nDistutils Configuration Files\u00b6\nAs mentioned above, you can use Distutils configuration files to record personal\nor site preferences for any Distutils options.  That is, any option to any\ncommand can be stored in one of two or three (depending on your platform)\nconfiguration files, which will be consulted before the command-line is parsed.\nThis means that configuration files will override default values, and the\ncommand-line will in turn override configuration files.  Furthermore, if\nmultiple configuration files apply, values from \u201cearlier\u201d files are overridden\nby \u201clater\u201d files.\n\nLocation and names of config files\u00b6\nThe names and locations of the configuration files vary slightly across\nplatforms.  On Unix and Mac OS X, the three configuration files (in the order\nthey are processed) are:\n\n\n\n\n\n\n\nType of file\nLocation and filename\nNotes\n\n\n\nsystem\nprefix/lib/pythonver/distutils/distutils.cfg\n(1)\n\npersonal\n$HOME/.pydistutils.cfg\n(2)\n\nlocal\nsetup.cfg\n(3)\n\n\n\nAnd on Windows, the configuration files are:\n\n\n\n\n\n\n\nType of file\nLocation and filename\nNotes\n\n\n\nsystem\nprefix\\Lib\\distutils\\distutils.cfg\n(4)\n\npersonal\n%HOME%\\pydistutils.cfg\n(5)\n\nlocal\nsetup.cfg\n(3)\n\n\n\nOn all platforms, the \u201cpersonal\u201d file can be temporarily disabled by\npassing the \u2013no-user-cfg option.\nNotes:\n\nStrictly speaking, the system-wide configuration file lives in the directory\nwhere the Distutils are installed; under Python 1.6 and later on Unix, this is\nas shown. For Python 1.5.2, the Distutils will normally be installed to\nprefix/lib/python1.5/site-packages/distutils, so the system\nconfiguration file should be put there under Python 1.5.2.\nOn Unix, if the HOME environment variable is not defined, the user\u2019s\nhome directory will be determined with the getpwuid() function from the\nstandard pwd module. This is done by the os.path.expanduser()\nfunction used by Distutils.\nI.e., in the current directory (usually the location of the setup script).\n(See also note (1).)  Under Python 1.6 and later, Python\u2019s default \u201cinstallation\nprefix\u201d is C:\\Python, so the system configuration file is normally\nC:\\Python\\Lib\\distutils\\distutils.cfg. Under Python 1.5.2, the\ndefault prefix was C:\\Program Files\\Python, and the Distutils were not\npart of the standard library\u2014so the system configuration file would be\nC:\\Program Files\\Python\\distutils\\distutils.cfg in a standard Python\n1.5.2 installation under Windows.\nOn Windows, if the HOME environment variable is not defined,\nUSERPROFILE then HOMEDRIVE and HOMEPATH will\nbe tried. This is done by the os.path.expanduser() function used\nby Distutils.\n\n\n\nSyntax of config files\u00b6\nThe Distutils configuration files all have the same syntax.  The config files\nare grouped into sections.  There is one section for each Distutils command,\nplus a global section for global options that affect every command.  Each\nsection consists of one option per line, specified as option=value.\nFor example, the following is a complete config file that just forces all\ncommands to run quietly by default:\n[global]\nverbose=0\n\n\nIf this is installed as the system config file, it will affect all processing of\nany Python module distribution by any user on the current system.  If it is\ninstalled as your personal config file (on systems that support them), it will\naffect only module distributions processed by you.  And if it is used as the\nsetup.cfg for a particular module distribution, it affects only that\ndistribution.\nYou could override the default \u201cbuild base\u201d directory and make the\nbuild* commands always forcibly rebuild all files with the\nfollowing:\n[build]\nbuild-base=blib\nforce=1\n\n\nwhich corresponds to the command-line arguments\npython setup.py build --build-base=blib --force\n\n\nexcept that including the build command on the command-line means\nthat command will be run.  Including a particular command in config files has no\nsuch implication; it only means that if the command is run, the options in the\nconfig file will apply.  (Or if other commands that derive values from it are\nrun, they will use the values in the config file.)\nYou can find out the complete list of options for any command using the\n--help option, e.g.:\npython setup.py build --help\n\n\nand you can find out the complete list of global options by using\n--help without a command:\npython setup.py --help\n\n\nSee also the \u201cReference\u201d section of the \u201cDistributing Python Modules\u201d manual.\n\n\n\nBuilding Extensions: Tips and Tricks\u00b6\nWhenever possible, the Distutils try to use the configuration information made\navailable by the Python interpreter used to run the setup.py script.\nFor example, the same compiler and linker flags used to compile Python will also\nbe used for compiling extensions.  Usually this will work well, but in\ncomplicated situations this might be inappropriate.  This section discusses how\nto override the usual Distutils behaviour.\n\nTweaking compiler/linker flags\u00b6\nCompiling a Python extension written in C or C++ will sometimes require\nspecifying custom flags for the compiler and linker in order to use a particular\nlibrary or produce a special kind of object code. This is especially true if the\nextension hasn\u2019t been tested on your platform, or if you\u2019re trying to\ncross-compile Python.\nIn the most general case, the extension author might have foreseen that\ncompiling the extensions would be complicated, and provided a Setup file\nfor you to edit.  This will likely only be done if the module distribution\ncontains many separate extension modules, or if they often require elaborate\nsets of compiler flags in order to work.\nA Setup file, if present, is parsed in order to get a list of extensions\nto build.  Each line in a Setup describes a single module.  Lines have\nthe following structure:\nmodule ... [sourcefile ...] [cpparg ...] [library ...]\n\n\nLet\u2019s examine each of the fields in turn.\n\nmodule is the name of the extension module to be built, and should be a\nvalid Python identifier.  You can\u2019t just change this in order to rename a module\n(edits to the source code would also be needed), so this should be left alone.\nsourcefile is anything that\u2019s likely to be a source code file, at least\njudging by the filename.  Filenames ending in .c are assumed to be\nwritten in C, filenames ending in .C, .cc, and .c++ are\nassumed to be C++, and filenames ending in .m or .mm are assumed\nto be in Objective C.\ncpparg is an argument for the C preprocessor,  and is anything starting with\n-I, -D, -U or -C.\nlibrary is anything ending in .a or beginning with -l or\n-L.\n\nIf a particular platform requires a special library on your platform, you can\nadd it by editing the Setup file and running python setup.py build.\nFor example, if the module defined by the line\nfoo foomodule.c\n\n\nmust be linked with the math library libm.a on your platform, simply add\n-lm to the line:\nfoo foomodule.c -lm\n\n\nArbitrary switches intended for the compiler or the linker can be supplied with\nthe -Xcompiler arg and -Xlinker arg options:\nfoo foomodule.c -Xcompiler -o32 -Xlinker -shared -lm\n\n\nThe next option after -Xcompiler and -Xlinker will be\nappended to the proper command line, so in the above example the compiler will\nbe passed the -o32 option, and the linker will be passed\n-shared.  If a compiler option requires an argument, you\u2019ll have to\nsupply multiple -Xcompiler options; for example, to pass -x c++\nthe Setup file would have to contain -Xcompiler -x -Xcompiler c++.\nCompiler flags can also be supplied through setting the CFLAGS\nenvironment variable.  If set, the contents of CFLAGS will be added to\nthe compiler flags specified in the  Setup file.\n\n\nUsing non-Microsoft compilers on Windows\u00b6\n\nBorland/CodeGear C++\u00b6\nThis subsection describes the necessary steps to use Distutils with the Borland\nC++ compiler version 5.5.  First you have to know that Borland\u2019s object file\nformat (OMF) is different from the format used by the Python version you can\ndownload from the Python or ActiveState Web site.  (Python is built with\nMicrosoft Visual C++, which uses COFF as the object file format.) For this\nreason you have to convert Python\u2019s library python25.lib into the\nBorland format.  You can do this as follows:\ncoff2omf python25.lib python25_bcpp.lib\n\n\nThe coff2omf program comes with the Borland compiler.  The file\npython25.lib is in the Libs directory of your Python\ninstallation.  If your extension uses other libraries (zlib, \u2026) you have to\nconvert them too.\nThe converted files have to reside in the same directories as the normal\nlibraries.\nHow does Distutils manage to use these libraries with their changed names?  If\nthe extension needs a library (eg. foo) Distutils checks first if it\nfinds a library with suffix _bcpp (eg. foo_bcpp.lib) and then\nuses this library.  In the case it doesn\u2019t find such a special library it uses\nthe default name (foo.lib.) 1\nTo let Distutils compile your extension with Borland C++ you now have to type:\npython setup.py build --compiler=bcpp\n\n\nIf you want to use the Borland C++ compiler as the default, you could specify\nthis in your personal or system-wide configuration file for Distutils (see\nsection Distutils Configuration Files.)\n\nSee also\n\nC++Builder CompilerInformation about the free C++ compiler from Borland, including links to the\ndownload pages.\n\nCreating Python Extensions Using Borland\u2019s Free CompilerDocument describing how to use Borland\u2019s free command-line C++ compiler to build\nPython.\n\n\n\n\n\nGNU C / Cygwin / MinGW\u00b6\nThis section describes the necessary steps to use Distutils with the GNU C/C++\ncompilers in their Cygwin and MinGW distributions. 2 For a Python interpreter\nthat was built with Cygwin, everything should work without any of these\nfollowing steps.\nNot all extensions can be built with MinGW or Cygwin, but many can.  Extensions\nmost likely to not work are those that use C++ or depend on Microsoft Visual C\nextensions.\nTo let Distutils compile your extension with Cygwin you have to type:\npython setup.py build --compiler=cygwin\n\n\nand for Cygwin in no-cygwin mode 3 or for MinGW type:\npython setup.py build --compiler=mingw32\n\n\nIf you want to use any of these options/compilers as default, you should\nconsider writing it in your personal or system-wide configuration file for\nDistutils (see section Distutils Configuration Files.)\n\nOlder Versions of Python and MinGW\u00b6\nThe following instructions only apply if you\u2019re using a version of Python\ninferior to 2.4.1 with a MinGW inferior to 3.0.0 (with\nbinutils-2.13.90-20030111-1).\nThese compilers require some special libraries.  This task is more complex than\nfor Borland\u2019s C++, because there is no program to convert the library.  First\nyou have to create a list of symbols which the Python DLL exports. (You can find\na good program for this task at\nhttps://sourceforge.net/projects/mingw/files/MinGW/Extension/pexports/).\npexports python25.dll >python25.def\n\n\nThe location of an installed python25.dll will depend on the\ninstallation options and the version and language of Windows.  In a \u201cjust for\nme\u201d installation, it will appear in the root of the installation directory.  In\na shared installation, it will be located in the system directory.\nThen you can create from these information an import library for gcc.\n/cygwin/bin/dlltool --dllname python25.dll --def python25.def --output-lib libpython25.a\n\n\nThe resulting library has to be placed in the same directory as\npython25.lib. (Should be the libs directory under your Python\ninstallation directory.)\nIf your extension uses other libraries (zlib,\u2026) you might  have to convert\nthem too. The converted files have to reside in the same directories as the\nnormal libraries do.\n\nSee also\n\nBuilding Python modules on MS Windows platform with MinGWInformation about building the required libraries for the MinGW environment.\n\n\n\nFootnotes\n\n1\nThis also means you could replace all existing COFF-libraries with OMF-libraries\nof the same name.\n\n2\nCheck https://www.sourceware.org/cygwin/ and http://www.mingw.org/ for more\ninformation\n\n3\nThen you have no POSIX emulation available, but you also don\u2019t need\ncygwin1.dll.\n\n\n\n\n\n\n\n\n\n\n\n\nTable of Contents\n\nInstalling Python Modules (Legacy version)\nIntroduction\nBest case: trivial installation\nThe new standard: Distutils\n\n\nStandard Build and Install\nPlatform variations\nSplitting the job up\nHow building works\nHow installation works\n\n\nAlternate Installation\nAlternate installation: the user scheme\nAlternate installation: the home scheme\nAlternate installation: Unix (the prefix scheme)\nAlternate installation: Windows (the prefix scheme)\n\n\nCustom Installation\nModifying Python\u2019s Search Path\n\n\nDistutils Configuration Files\nLocation and names of config files\nSyntax of config files\n\n\nBuilding Extensions: Tips and Tricks\nTweaking compiler/linker flags\nUsing non-Microsoft compilers on Windows\nBorland/CodeGear C++\nGNU C / Cygwin / MinGW\nOlder Versions of Python and MinGW\n\n\n\n\n\n\n\n\n\nPrevious topic\n10. API Reference\n\nThis Page\n\nShow Source\n\n\n\nQuick search\n\n\n\n\n\n\n\n\n\n\n\n\n\nNavigation\n\n\nindex\n\nmodules |\n\nprevious |\n\nPython \u00bb\n\nen\n2.7.18\nDocumentation \u00bb\n\n\n\n\n\u00a9 Copyright 1990-2020, Python Software Foundation.\n\nThe Python Software Foundation is a non-profit corporation.\nPlease donate.\n\nLast updated on Apr 20, 2020.\nFound a bug?\n\nCreated using Sphinx 2.3.1.\n\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "when to use fabric or ansible?", "id": 585, "answers": [{"answer_id": 591, "document_id": 310, "question_id": 585, "text": "All your logic should live in Ansible and you can use Fabric as a lightweight wrapper around it.\nfab deploy", "answer_start": 246, "answer_category": null}], "is_impossible": false}], "context": "I'd like to have reliable django deployments and I think I'm not following the best practices here. Till now I've been using fabric as a configuration management tool in order to deploy my django sites but I'm not sure that's the best way to go.\nAll your logic should live in Ansible and you can use Fabric as a lightweight wrapper around it.\nfab deploy\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to Add a Desktop Shortcut Option on Finish Page in NSIS installer?", "id": 1750, "answers": [{"answer_id": 1737, "document_id": 1322, "question_id": 1750, "text": "If you are not using readme checkbox on the finish page, you can use it to perform whatever action you want.", "answer_start": 311, "answer_category": null}], "is_impossible": false}], "context": "I'm trying to create an installer using NSIS Modern User Interface for the first time. I would like to know how I can add an option (checkbox) for users to select to have a desktop shortcut created on the Finish Page (the last screen of the installer) in addition to the \"Run XXXX\" option that's already there.\nIf you are not using readme checkbox on the finish page, you can use it to perform whatever action you want.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How install Jekyll on windows?", "id": 113, "answers": [{"answer_id": 121, "document_id": 77, "question_id": 113, "text": "Installing Ruby and JekyllPermalink\nInstallation via RubyInstallerPermalink", "answer_start": 658, "answer_category": null}], "is_impossible": false}], "context": "\n\n\n\n\n\n\n\n\n\n\n\nJekyll on Windows\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJekyll\n\n\n\n\n\n\nHome\nDocs\nResources\nShowcase\nNews\n\n\n\n\n\n\n\nv4.2.1\nGitHub\n\n\n\n\n\nHome\nDocs\nResources\nNews\nGitHub\n\n\n\n\n\n\n\nNavigate the docs\u2026\n\nQuickstart\nInstallation\nRuby 101\nCommunity\nStep by Step Tutorial\n\n\nCommand Line Usage\nConfiguration\nRendering Process\n\n\nPages\nPosts\nFront Matter\nCollections\nData Files\nAssets\nStatic Files\n\n\nDirectory Structure\nLiquid\nVariables\nIncludes\nLayouts\nPermalinks\nThemes\nPagination\n\n\nPlugins\nBlog Migrations\nUpgrading\nDeployment\n\n\n\n\n\n\nImprove this page\n\nJekyll on Windows\nWhile Windows is not an officially-supported platform, it can be used to run Jekyll with the proper tweaks.\nInstalling Ruby and JekyllPermalink\nInstallation via RubyInstallerPermalink\nThe easiest way to install Ruby and Jekyll is by using the RubyInstaller for Windows.\nRubyInstaller is a self-contained Windows-based installer that includes the Ruby language, an execution environment,\nimportant documentation, and more.\nWe only cover RubyInstaller-2.4 and newer here. Older versions need to\ninstall the Devkit manually.\n\nDownload and install a Ruby+Devkit version from RubyInstaller Downloads.\nUse default options for installation.\nRun the ridk install step on the last stage of the installation wizard. This is needed for installing gems with native\nextensions. You can find additional information regarding this in the\nRubyInstaller Documentation\nOpen a new command prompt window from the start menu, so that changes to the PATH environment variable becomes effective.\nInstall Jekyll and Bundler using gem install jekyll bundler\nCheck if Jekyll has been installed properly: jekyll -v\n\nYou may receive an error when checking if Jekyll has not been installed properly. Reboot your system and run jekyll -v again.\nIf the error persists, please open a RubyInstaller issue.\nThat\u2019s it, you\u2019re ready to use Jekyll!\nInstallation via Bash on Windows 10Permalink\nIf you are using Windows 10 version 1607 or later, another option to run Jekyll is by\ninstalling the Windows Subsystem for Linux.\nYou must have Windows Subsystem for Linux enabled.\nMake sure all your packages and repositories are up to date. Open a new Command Prompt or PowerShell window and type bash.\nYour terminal should now be a Bash instance. Next, update your repository lists and packages:\nsudo apt-get update -y && sudo apt-get upgrade -y\n\nNext, install Ruby. To do this, let\u2019s use a repository from BrightBox,\nwhich hosts optimized versions of Ruby for Ubuntu.\nsudo apt-add-repository ppa:brightbox/ruby-ng\nsudo apt-get update\nsudo apt-get install ruby2.5 ruby2.5-dev build-essential dh-autoreconf\n\nNext, update your Ruby gems:\ngem update\n\nInstall Jekyll:\ngem install jekyll bundler\n\nNo sudo here.\nCheck your Jekyll version:\njekyll -v\n\nThat\u2019s it! You\u2019re ready to start using Jekyll.\nYou can make sure time management is working properly by inspecting your _posts folder. You should see a markdown file\nwith the current date in the filename.\n\nNon-superuser account issues\nIf the `jekyll new` command prints the error \"Your user account isn't allowed to install to the system RubyGems\", see\nthe \"Running Jekyll as Non-Superuser\" instructions in\nTroubleshooting.\n\nBash on Ubuntu on Windows is still under development, so you may run into issues.\nEncodingPermalink\nIf you use UTF-8 encoding, Jekyll will break if a file starts with characters representing a BOM. Therefore, remove this sequence of bytes if it appears at the beginning of your file.\nAdditionally, you might need to change the code page of the console window to UTF-8 in case you get a\nLiquid Exception: Incompatible character encoding error during the site generation process. Run the following:\nchcp 65001\n\nTime Zone ManagementPermalink\nSince Windows doesn\u2019t have a native source of zoneinfo data, the Ruby Interpreter doesn\u2019t understand IANA Timezones.\nUsing them had the TZ environment variable default to UTC/GMT 00:00.\nThough Windows users could alternatively define their blog\u2019s timezone by setting the key to use the POSIX format of defining\ntimezones, it wasn\u2019t as user-friendly when it came to having the clock altered to changing DST-rules.\nJekyll now uses a rubygem to internally configure Timezone based on established\nIANA Timezone Database.\nWhile \u2018new\u2019 blogs created with Jekyll v3.4 and greater, will have the following added to their Gemfile by default, existing\nsites will have to update their Gemfile (and installed gems) to enable development on Windows:\n# Windows does not include zoneinfo files, so bundle the tzinfo-data gem\ngem 'tzinfo-data', platforms: [:mingw, :mswin, :x64_mingw, :jruby]\n\n\nTZInfo 2.0 incompatibility\n\nVersion 2.0 of the TZInfo library has introduced a change in how timezone offsets are calculated.\nThis will result in incorrect date and time for your posts when the site is built with Jekyll 3.x on Windows.\n\n\nWe therefore recommend that you lock the Timezone library to version 1.2 and above by listing\ngem 'tzinfo', '~> 1.2' in your Gemfile.\n\n\nAuto RegenerationPermalink\nJekyll uses the listen gem to watch for changes when the --watch switch is specified during a build or serve.\nWhile listen has built-in support for UNIX systems, it may require an extra gem for compatibility with Windows.\nAdd the following to the Gemfile for your site if you have issues with auto-regeneration on Windows alone:\ngem 'wdm', '~> 0.1.1', :install_if => Gem.win_platform?\n\nYou have to use a Ruby+Devkit version of the RubyInstaller and install\nthe MSYS2 build tools to successfully install the wdm gem.\n\n\n\n\nGetting Started\n\nQuickstart\nInstallation\nRuby 101\nCommunity\nStep by Step Tutorial\n\nBuild\n\nCommand Line Usage\nConfiguration\nRendering Process\n\nContent\n\nPages\nPosts\nFront Matter\nCollections\nData Files\nAssets\nStatic Files\n\nSite Structure\n\nDirectory Structure\nLiquid\nVariables\nIncludes\nLayouts\nPermalinks\nThemes\nPagination\n\nGuides\n\nPlugins\nBlog Migrations\nUpgrading\nDeployment\n\n\n\n\n\n\n\n\n\nJekyll is lovingly maintained by the core team of volunteers.\nThe contents of this website are \u00a9\u00a02021 under the terms of the MIT\u00a0License.\n\n\n\nProudly hosted by\n\n\n\n\n\n\n\nJekyll is funded thanks to its\n\nsponsors!\n\n\n\n\n\n\n\n\n\n\n\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Lisp (or other dialect) desktop application for several platforms?", "id": 1310, "answers": [{"answer_id": 1300, "document_id": 879, "question_id": 1310, "text": "This gives you a way of running common lisp programs on linux, windows and mac machines without modification. We use the SDL library and various extensions of it.\nhttp://code.google.com/p/lispbuilder/", "answer_start": 305, "answer_category": null}], "is_impossible": false}], "context": "I would like to make a little game or graphical app, and to be able to show it with a simple installation in a computer with any of these operating systems.\nSomeone has experience with similar situations or could point me to best choices of graphical libraries and compilers, runtime environments, etc...\nThis gives you a way of running common lisp programs on linux, windows and mac machines without modification. We use the SDL library and various extensions of it.\nhttp://code.google.com/p/lispbuilder/\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to run executable at end of Setup Project?", "id": 1770, "answers": [{"answer_id": 1756, "document_id": 1341, "question_id": 1770, "text": "You could modify the tables in the .MSI after it has been built but VS.NET would probably overwrite these changes each time it is built. You may be able to override the last page using a merge module that you include in the installation project.", "answer_start": 600, "answer_category": null}], "is_impossible": false}], "context": "I have a Visual Studio Setup Project that I use to install a fairly simple WinForms application. At the end of the install I have a custom user interface page that shows a single check box which asks the user if they want to run the application. I've seen other installers do this quite often. But I cannot find a way to get the Setup Project to run an executable after the install finishes. An ideas?\nNOTE: You cannot use Custom Actions because these are used as part of the install process, I want to run my installed application once the user presses the 'Close' button at the end of the install.\nYou could modify the tables in the .MSI after it has been built but VS.NET would probably overwrite these changes each time it is built. You may be able to override the last page using a merge module that you include in the installation project.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Determine assembly version during a post-build event", "id": 512, "answers": [{"answer_id": 513, "document_id": 237, "question_id": 512, "text": " If you prefer scripting these methods might also work for you: If you are using the post-build event, you can use the filever.exe tool to grab it out of the already built assembly, Get filever.exe from here: http://support.microsoft.com/kb/913111.", "answer_start": 228, "answer_category": null}], "is_impossible": false}], "context": "Let's say I wanted to create a static text file which ships with each release. I want the file to be updated with the version number of the release (as specified in AssemblyInfo.cs), but I don't want to have to do this manually. If you prefer scripting these methods might also work for you: If you are using the post-build event, you can use the filever.exe tool to grab it out of the already built assembly, Get filever.exe from here: http://support.microsoft.com/kb/913111.", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "cannot install uwsgi on ubuntu 14 04 with python 3 4 paths", "id": 1437, "answers": [{"answer_id": 1426, "document_id": 1009, "question_id": 1437, "text": "Install Python 3:\n\nsudo apt-get install python3\n\n\nInstall Python 3 headers to build uWSGI from source:\n\nsudo apt-get install python3-dev\n\n\nCreate a Python 3 virtualenv in a venv subdir of current dir (prepend the command with sudo if current dir is privileged):\n\nvirtualenv -p python3 venv\n\n\nActivate the venv to be the target for pip (. is a shortcut for source):\n\n. venv/bin/activate\n\n\nFinally, install uWSGI (again, sudo if in a privileged dir):\n\npip install uwsgi\n", "answer_start": 1794, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nThe big picture is that I want Ubuntu server with nginx, uWGI, and Python 3 (virtualenv) to start some project.\n\nI did follow recommendation that can be found on various places. When trying to install uWSGI using the sudo pip install uwsgi, I do observe the following error:\n\nplugins/python/uwsgi_python.h:2:20: fatal error: Python.h: No such file or directory\n\n\nI did find the [J.F.Sebastian's comment][1] from September 2012 about the neccessity to $ sudo apt-get install python2.7-dev -- it was for Python 2.7. So, I did the similar for Python 3.4 (seems successfully). However, I still observer the error.\n\nWhen trying to search for the Python.h, I can find:\n\n$ locate Python.h\n/usr/include/python3.4m/Python.h\n\n\nWhat else should I set to make uWSGI installed? (I am rather new to Ubuntu Linux, even though I did work with Unix far in the middle age ;)\n\nUpdate:\n\nFollowing the jwalker's advice from the comment below, I did pip install uwsgi from within activated virtualenv and without sudo. But then the installer cannot create subdirectories (like build) in the venv directory. I tried to chmod go+w for the venv, but it did not help. I admit I know nothing about virtualenv and pip, and also my Unix knowledge is a bit rusty: \n\n  ...\n  File \"/var/www/hec_project/hec_venv/lib/python3.4/site-packages/pip/req.py\", line 218, in build_location\n    _make_build_dir(build_dir)\n  File \"/var/www/hec_project/hec_venv/lib/python3.4/site-packages/pip/req.py\", line 1527, in _make_build_dir\n    os.makedirs(build_dir)\n  File \"/var/www/hec_project/hec_venv/lib/python3.4/os.py\", line 237, in makedirs\n    mkdir(name, mode)\nPermissionError: [Errno 13] Permission denied: '/var/www/hec_project/hec_venv/build'\n\nStoring debug log for failure in /home/hecadmin/.pip/pip.log\n\n    \n\nInstall Python 3:\n\nsudo apt-get install python3\n\n\nInstall Python 3 headers to build uWSGI from source:\n\nsudo apt-get install python3-dev\n\n\nCreate a Python 3 virtualenv in a venv subdir of current dir (prepend the command with sudo if current dir is privileged):\n\nvirtualenv -p python3 venv\n\n\nActivate the venv to be the target for pip (. is a shortcut for source):\n\n. venv/bin/activate\n\n\nFinally, install uWSGI (again, sudo if in a privileged dir):\n\npip install uwsgi\n\n    \n\nHave you installed the correct python plugin for uwsgi?\n\nhttp://packages.ubuntu.com/precise/uwsgi-plugin-python3\n\nThen in config (your .ini file) put python3 as plugin instead of python like this:\n\n[uwsgi]\nplugins         = python3\n\n# Rest of your configuration...\n\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Gradle: What is the difference between classpath and compile dependencies?", "id": 1840, "answers": [{"answer_id": 1826, "document_id": 1411, "question_id": 1840, "text": "I'm going to guess that you're referencing compile and classpath within the dependencies {} block. If that is so, those are dependency Configurations.", "answer_start": 283, "answer_category": null}], "is_impossible": false}], "context": "When adding dependencies to my project I am never sure what prefix I should give them, e.g. \"classpath\" or \"compile\".\nFor example, should my dependencies below be compile time or classpath?\nAlso, should this be in my applications build.gradle or in the module specific build.gradle?\nI'm going to guess that you're referencing compile and classpath within the dependencies {} block. If that is so, those are dependency Configurations.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "cannot install express globally", "id": 1945, "answers": [{"answer_id": 1932, "document_id": 1525, "question_id": 1945, "text": "sudo npm install -g express-generator", "answer_start": 2079, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI am trying to install express globally but everytime I run the command line 'npm install -g express' I get the following error message:\n\nnpm http GET https://registry.npmjs.org/express\nnpm http 200 https://registry.npmjs.org/express\nnpm ERR! Error: EACCES, mkdir '/usr/local/lib/node_modules/express'\nnpm ERR!  { [Error: EACCES, mkdir '/usr/local/lib/node_modules/express']\nnpm ERR!   errno: 3,\nnpm ERR!   code: 'EACCES',\nnpm ERR!   path: '/usr/local/lib/node_modules/express',\nnpm ERR!   fstream_type: 'Directory',\nnpm ERR!   fstream_path: '/usr/local/lib/node_modules/express',\nnpm ERR!   fstream_class: 'DirWriter',\nnpm ERR!   fstream_stack: \nnpm ERR!    [ '/usr/local/lib/node_modules/npm/node_modules/fstream/lib/dir-writer.js:36:23',\nnpm ERR!      '/usr/local/lib/node_modules/npm/node_modules/mkdirp/index.js:37:53',\nnpm ERR!      'Object.oncomplete (fs.js:107:15)' ] }\nnpm ERR! \nnpm ERR! Please try running this command again as root/Administrator.\n\nnpm ERR! System Darwin 13.0.0\nnpm ERR! command \"node\" \"/usr/local/bin/npm\" \"install\" \"-g\" \"express\"\nnpm ERR! cwd /Folder\nnpm ERR! node -v v0.10.26\nnpm ERR! npm -v 1.4.3\nnpm ERR! path /usr/local/lib/node_modules/express\nnpm ERR! fstream_path /usr/local/lib/node_modules/express\nnpm ERR! fstream_type Directory\nnpm ERR! fstream_class DirWriter\nnpm ERR! code EACCES\nnpm ERR! errno 3\nnpm ERR! stack Error: EACCES, mkdir '/usr/local/lib/node_modules/express'\nnpm ERR! fstream_stack /usr/local/lib/node_modules/npm/node_modules/fstream/lib/dir-writer.js:36:23\nnpm ERR! fstream_stack /usr/local/lib/node_modules/npm/node_modules/mkdirp/index.js:37:53\nnpm ERR! fstream_stack Object.oncomplete (fs.js:107:15)\nnpm ERR! \nnpm ERR! Additional logging details can be found in:\nnpm ERR!     /Users/Folder/npm-debug.log\nnpm ERR! not ok code 0\n\n\nDoes anyone have a clue what the problem is?\n    \n\nYou're trying to install something globally, but you have no access to the global directory you're installing into.\n\nBad but acceptable advice would be to run sudo npm install -g express instead.\n    \n\nUse this instead\n\nsudo npm install -g express-generator\n\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "loading an r package from a custom directory", "id": 1427, "answers": [{"answer_id": 1416, "document_id": 1002, "question_id": 1427, "text": "you dont need to unzip or untar \njust give this command in command prompt and it will unzip into appropriate place\n\nR CMD INSTALL [options] [l-lib] pkgs.tar.gz", "answer_start": 2200, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nIf I download an \"package-name\".tar.gz file from CRAN website, gunzip and untar it into a custom directory, how do I load that package from within R?  I cannot extract the file in the R installation directory.\n    \n\nTry using Hadley Wickham's devtools package, which allows loading packages from a given directory:\n\nlibrary(devtools)\n\n# load package w/o installing\nload_all('/some/package/diR')\n\n# or invoke 'R CMD INSTALL'\ninstall('/some/package/diR')\n\n    \n\nPlease add some extra information on the operating system. If you're on windows, you need Rtools ( http://www.murdoch-sutherland.com/Rtools/ ) to build from source. See that website for more information on how to install everything you need.\n\nEven when you're on Linux, simply extracting the package-file doesn't work. There might be underlying C-code (which is the case for the MSBVAR package), and even R code has to be processed in order to be built into a package that can be loaded directly with the library() function.\n\nPlus, you have to take into account that the package you want to install might have dependencies. For the MSBVAR package, these are the packages coda and bit. When building from source, you need to make sure all dependencies are installed as well, or you can get errors.\n\napart from the R CMD INSTALL you could try from within R :\n\n# from CRAN\ninstall.packages(\"MSBVAR\", type=\"source\")\n# from a local file \ninstall.packages(\"/my/dir/MSBVAR.tar.gz\",repos=NULL, type=\"source\")\n\n\nor why not just do\n\n# from CRAN\ninstall.packages(\"MSBVAR\")\n\n\nThis works perfectly fine.\n    \n\nYou need to install the package to a directory to which you have permission to read and write. First, download the package to an easily accessible directory. If you're on Linux/Mac, try creating a directory called 'rlib' in your home directory.\n\ncd ~; mkdir rlib\nR CMD INSTALL MSBVAR.tar.gz --library=rlib\n\n\nIf you would prefer to install the package from R, do this:\n\n## From CRAN\ninstall.packages(\"MSBVAR\", lib=\"~/rlib\")\n\n    \n\nYou can't call R CMD INSTALL downloadedpackage.gz?\n\nAs I understand it, this should install the package in your user-space if it cannot get write permissions to the R installation folder\n    \n\nyou dont need to unzip or untar \njust give this command in command prompt and it will unzip into appropriate place\n\nR CMD INSTALL [options] [l-lib] pkgs.tar.gz\n\nas explained here\n\nthen you can use it in R by library(the_pkg)\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How do I install a c++ library so I can use it?", "id": 827, "answers": [{"answer_id": 822, "document_id": 509, "question_id": 827, "text": "1.\tPut the header files in a location which your compiler is aware of \n2.\tPut the dll files in a location which your linker is aware of", "answer_start": 716, "answer_category": null}], "is_impossible": false}], "context": "I have this library called BASS which is an audio library which I'm going to use to record with the microphone. I have all the files needed to use it, but I don't know how to install the library. I tried taking the example files and putting them in the same directory as the bass.h file. But I got a bunch of errors saying there are function calls that doesn't exist. The headers will contain the declarations exposed to the developer by the library authors, and your program will #include them in its source code, the dll will contain the compiled code which will be or linked together and used by your program, and they will be found by the linker (or loaded dynamically, but this is another step).\nSo you need to\n1.\tPut the header files in a location which your compiler is aware of \n2.\tPut the dll files in a location which your linker is aware of \n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "MS C++ 2010 and mspdb100.dll", "id": 1880, "answers": [{"answer_id": 1866, "document_id": 1451, "question_id": 1880, "text": "You can add Microsoft Visual Studio 10.0\\Common7\\IDE to your path, and this issue will not exist any more. You will be able to build without running this silly batch file every time.", "answer_start": 568, "answer_category": null}], "is_impossible": false}], "context": "Microsoft's C++ compiler and linker seem to have an odd relationship with mspdb100.dll. When run from the IDE, of course, the compiler and linker work fine. When running either one from the command line, I get an error.\nNo problem, I located the DLL and copied it to the directory. Now the compiler works fine, but the linker dies.\nI could solve the problem by adding \"%VS10%\\Common7\\IDE\" to my PATH, but for various reasons (performance, system purity, OCD, etc), I don't want to do that. Why is this setup so touchy, and is there anything else I can do to solve it?\nYou can add Microsoft Visual Studio 10.0\\Common7\\IDE to your path, and this issue will not exist any more. You will be able to build without running this silly batch file every time.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Homebrew How do I install a specific version of a formula in homebrew? ", "id": 1559, "answers": [{"answer_id": 1548, "document_id": 1136, "question_id": 1559, "text": "1) Check, whether the version is already installed (but not activated)\n2) Check, whether the version is available as a tap\n3) Try some formula from the past\n4) Manually write a formula", "answer_start": 165, "answer_category": null}], "is_impossible": false}], "context": "Homebrew How do I install a specific version of a formula in homebrew? \nFor example, postgresql-8.4.4 instead of the latest 9.0.install specific version of formula?\n1) Check, whether the version is already installed (but not activated)\n2) Check, whether the version is available as a tap\n3) Try some formula from the past\n4) Manually write a formula\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "how to remove installation of mysql on mac os x", "id": 1951, "answers": [{"answer_id": 1938, "document_id": 1533, "question_id": 1951, "text": "sudo pkgutil --forget com.mysql.mysql", "answer_start": 1957, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI need to install older version of mysql server on mac os, but I have a newer version.\n\nI tried to remove this newer installation (5.1), but when start old version install (5.0b) message \"MySQL 5.0.51b-community for Mac OS X can't be installed in this disk. A newer version of this software alrady exists on this disk\".\n\nI can't recognize problem, because I remove all data of previouse install, but installer says no.\n\nMac OS version 10.6.\n    \n\nTry running also\n\nsudo rm -rf /var/db/receipts/com.mysql.*\n\n    \n\nTry this, I had to struggle but this works for me!!!!!!!\n\n\nsudo rm /usr/local/mysql\nsudo rm -rf /usr/local/mysql*\nsudo rm -rf /Library/StartupItems/MySQLCOM\nsudo rm -rf /Library/PreferencePanes/My*\n(Edit /etc/hostconfig) sudo vi /etc/hostconfig (Remove line MYSQLCOM=-YES)\nsudo rm -rf /Library/Receipts/mysql*\nsudo rm -rf /Library/Receipts/MySQL* \nsudo rm -rf /var/db/receipts/com.mysql.*\n\n    \n\nTest finding all files and folders with \"mysql\" in their name, take a look at them and see if they must be deleted as well.\n\nUse the following command to find all the files.\n\nsudo find / | grep -i mysql\n\n\nYou can scroll through the output if you put | less at the end (it won't show anything up until it finds something, just so you wouldn't think the command failed.) :-) You can write it as follows.\n\nsudo find / | grep -i mysql | less\n\n\nTo remove the files/folders, you will have to run the following command (-f means force so you won't be able to restore the files and you won't be asked for a confirmation before they are deleted):\n\nsudo rm -rf /path/to/file/or/folder\n\n\nHope this will be of any help.\n    \n\nI believe you can essentially just delete the /usr/local/mysql-Version/ and unlink the /usr/local/mysql directory. Getting rid of the system pref and the start up item might be harder, but I didn't install those so I can't help there.\n    \n\nYou can use the built-in utility pkgutil to remove the package receipt:\n\nsudo pkgutil --forget com.mysql.mysql\n\n    \n\nOn Snow Leopard I had to additionally kill mysqld and did\n\nrm /private/var/db/receipts/com.mysql.*\n\nsudo rm -rf /Users//Library/StartupItems/MySQLCOM\nsudo rm -rf /Users//Library/PreferencePanes/My*\nsudo rm -rf /Users//Library/Receipts/mysql*\nsudo rm -rf /Users//Library/Receipts/MySQL*\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Install windows service without InstallUtil.exe", "id": 1714, "answers": [{"answer_id": 1702, "document_id": 1287, "question_id": 1714, "text": "You can still use installutil without visual studio, it is included with the .net framework\nOn your server, open a command prompt as administrator then:\nCD C:\\Windows\\Microsoft.NET\\Framework\\v4.0.version (insert your version)\n\ninstallutil \"C:\\Program Files\\YourWindowsService\\YourWindowsService.exe\" (insert your service", "answer_start": 859, "answer_category": null}], "is_impossible": false}], "context": "I'm trying to deploy a windows service but not quite sure how to do it right. I built it as a console app to start with, I've now turned it into a windows service project and just call my class from the OnStart method in the service.\nI now need to install this on a server which doesn't have Visual Studio on it, which if I've understood it correctly means I can't use the InstallUtil.exe and have to create an installer class instead. Is this correct?\nI did have a look at a previous question, Install a .NET windows service without InstallUtil.exe, but I just want to make sure I've understood it correctly.\nIf I create the class that question's accepted answer links to, what is the next step? Upload MyService.exe and MyService.exe.config to the server, double click the exe file and Bob's my uncle?\nThe service will only ever be installed on one server.\nYou can still use installutil without visual studio, it is included with the .net framework\nOn your server, open a command prompt as administrator then:\nCD C:\\Windows\\Microsoft.NET\\Framework\\v4.0.version (insert your version)\n\ninstallutil \"C:\\Program Files\\YourWindowsService\\YourWindowsService.exe\" (insert your service \n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to install Python packages from the tar.gz file without using pip install", "id": 867, "answers": [{"answer_id": 862, "document_id": 547, "question_id": 867, "text": "python -m pip install [options] [-e] <local project path> ...\npython -m pip install [options] <archive url/path> ...", "answer_start": 245, "answer_category": null}], "is_impossible": false}], "context": "Usage\u00b6\n\nUnix/macOS\npython -m pip install [options] <requirement specifier> [package-index-options] ...\npython -m pip install [options] -r <requirements file> [package-index-options] ...\npython -m pip install [options] [-e] <vcs project url> ...\npython -m pip install [options] [-e] <local project path> ...\npython -m pip install [options] <archive url/path> ...\n\nWindows\nDescription\nInstall packages from:\n\nPyPI (and other indexes) using requirement specifiers.\n\nVCS project urls.\n\nLocal project directories.\n\nLocal or remote source archives.\n\npip also supports installing from \u201crequirements files\u201d, which provide an easy way to specify a whole environment to be installed.\n\nOverview\npip install has several stages:\n\nIdentify the base requirements. The user supplied arguments are processed here.\n\nResolve dependencies. What will be installed is determined here.\n\nBuild wheels. All the dependencies that can be are built into wheels.\n\nInstall the packages (and uninstall anything being upgraded/replaced).\n\nNote that pip install prefers to leave the installed version as-is unless --upgrade is specified.\n\nArgument Handling\nWhen looking at the items to be installed, pip checks what type of item each is, in the following order:\n\nProject or archive URL.\n\nLocal directory (which must contain a setup.py, or pip will report an error).\n\nLocal file (a sdist or wheel format archive, following the naming conventions for those formats).\n\nA requirement, as specified in PEP 440.\n\nEach item identified is added to the set of requirements to be satisfied by the install.\n\nWorking Out the Name and Version\nFor each candidate item, pip needs to know the project name and version. For wheels (identified by the .whl file extension) this can be obtained from the filename, as per the Wheel spec. For local directories, or explicitly specified sdist files, the setup.py egg_info command is used to determine the project metadata. For sdists located via an index, the filename is parsed for the name and project version (this is in theory slightly less reliable than using the egg_info command, but avoids downloading and processing unnecessary numbers of files).\n\nAny URL may use the #egg=name syntax (see VCS Support) to explicitly state the project name.\n\nSatisfying Requirements\nOnce pip has the set of requirements to satisfy, it chooses which version of each requirement to install using the simple rule that the latest version that satisfies the given constraints will be installed (but see here for an exception regarding pre-release versions). Where more than one source of the chosen version is available, it is assumed that any source is acceptable (as otherwise the versions would differ).\n\nInstallation Order\nNote\n\nThis section is only about installation order of runtime dependencies, and does not apply to build dependencies (those are specified using PEP 518).\n\nAs of v6.1.0, pip installs dependencies before their dependents, i.e. in \u201ctopological order.\u201d This is the only commitment pip currently makes related to order. While it may be coincidentally true that pip will install things in the order of the install arguments or in the order of the items in a requirements file, this is not a promise.\n\nIn the event of a dependency cycle (aka \u201ccircular dependency\u201d), the current implementation (which might possibly change later) has it such that the first encountered member of the cycle is installed last.\n\nFor instance, if quux depends on foo which depends on bar which depends on baz, which depends on foo:\n\n\nUnix/macOS\n$ python -m pip install quux\n...\nInstalling collected packages baz, bar, foo, quux\n\n$ python -m pip install bar\n...\nInstalling collected packages foo, baz, bar\n\nWindows\nPrior to v6.1.0, pip made no commitments about install order.\n\nThe decision to install topologically is based on the principle that installations should proceed in a way that leaves the environment usable at each step. This has two main practical benefits:\n\nConcurrent use of the environment during the install is more likely to work.\n\nA failed install is less likely to leave a broken environment. Although pip would like to support failure rollbacks eventually, in the mean time, this is an improvement.\n\nAlthough the new install order is not intended to replace (and does not replace) the use of setup_requires to declare build dependencies, it may help certain projects install from sdist (that might previously fail) that fit the following profile:\n\nThey have build dependencies that are also declared as install dependencies using install_requires.\n\npython setup.py egg_info works without their build dependencies being installed.\n\nFor whatever reason, they don\u2019t or won\u2019t declare their build dependencies using setup_requires.\n\nRequirements File Format\nThis section has been moved to Requirements File Format.\n\nRequirement Specifiers\npip supports installing from a package index using a requirement specifier. Generally speaking, a requirement specifier is composed of a project name followed by optional version specifiers. PEP 508 contains a full specification of the format of a requirement. Since version 18.1 pip supports the url_req-form specification.\n\nSome examples:\n\nSomeProject\nSomeProject == 1.3\nSomeProject >=1.2,<2.0\nSomeProject[foo, bar]\nSomeProject~=1.4.2\nSince version 6.0, pip also supports specifiers containing environment markers like so:\n\nSomeProject ==5.4 ; python_version < '3.8'\nSomeProject; sys_platform == 'win32'\nSince version 19.1, pip also supports direct references like so:\n\nSomeProject @ file:///somewhere/...\nEnvironment markers are supported in the command line and in requirements files.\n\nNote\n\nUse quotes around specifiers in the shell when using >, <, or when using environment markers. Don\u2019t use quotes in requirement files. 1\n\nPer-requirement Overrides\nSince version 7.0 pip supports controlling the command line options given to setup.py via requirements files. This disables the use of wheels (cached or otherwise) for that package, as setup.py does not exist for wheels.\n\nThe --global-option and --install-option options are used to pass options to setup.py. For example:\n\nFooProject >= 1.2 --global-option=\"--no-user-cfg\" \\\n                  --install-option=\"--prefix='/usr/local'\" \\\n                  --install-option=\"--no-compile\"\nThe above translates roughly into running FooProject\u2019s setup.py script as:\n\npython setup.py --no-user-cfg install --prefix='/usr/local' --no-compile\nNote that the only way of giving more than one option to setup.py is through multiple --global-option and --install-option options, as shown in the example above. The value of each option is passed as a single argument to the setup.py script. Therefore, a line such as the following is invalid and would result in an installation error.\n\n# Invalid. Please use '--install-option' twice as shown above.\nFooProject >= 1.2 --install-option=\"--prefix=/usr/local --no-compile\"\nPre-release Versions\nStarting with v1.4, pip will only install stable versions as specified by pre-releases by default. If a version cannot be parsed as a compliant PEP 440 version then it is assumed to be a pre-release.\n\nIf a Requirement specifier includes a pre-release or development version (e.g. >=0.0.dev0) then pip will allow pre-release and development versions for that requirement. This does not include the != flag.\n\nThe pip install command also supports a --pre flag that enables installation of pre-releases and development releases.\n\nVCS Support\nThis is now covered in VCS Support.\n\nFinding Packages\npip searches for packages on PyPI using the HTTP simple interface, which is documented here and there.\n\npip offers a number of package index options for modifying how packages are found.\n\npip looks for packages in a number of places: on PyPI (if not disabled via --no-index), in the local filesystem, and in any additional repositories specified via --find-links or --index-url. There is no ordering in the locations that are searched. Rather they are all checked, and the \u201cbest\u201d match for the requirements (in terms of version number - see PEP 440 for details) is selected.\n\nSee the pip install Examples.\n\nSSL Certificate Verification\nStarting with v1.3, pip provides SSL certificate verification over HTTP, to prevent man-in-the-middle attacks against PyPI downloads. This does not use the system certificate store but instead uses a bundled CA certificate store. The default bundled CA certificate store certificate store may be overridden by using --cert option or by using PIP_CERT, REQUESTS_CA_BUNDLE, or CURL_CA_BUNDLE environment variables.\n\nCaching\nThis is now covered in Caching.\n\nWHEEL CACHE\nThis is now covered in Caching.\n\nHash-Checking Mode\nSince version 8.0, pip can check downloaded package archives against local hashes to protect against remote tampering. To verify a package against one or more hashes, add them to the end of the line:\n\nFooProject == 1.2 --hash=sha256:2cf24dba5fb0a30e26e83b2ac5b9e29e1b161e5c1fa7425e73043362938b9824 \\\n                  --hash=sha256:486ea46224d1bb4fb680f34f7c9ad96a8f24ec88be73ea8e5a6c65260e9cb8a7\n(The ability to use multiple hashes is important when a package has both binary and source distributions or when it offers binary distributions for a variety of platforms.)\n\nThe recommended hash algorithm at the moment is sha256, but stronger ones are allowed, including all those supported by hashlib. However, weaker ones such as md5, sha1, and sha224 are excluded to avoid giving a false sense of security.\n\nHash verification is an all-or-nothing proposition. Specifying a --hash against any requirement not only checks that hash but also activates a global hash-checking mode, which imposes several other security restrictions:\n\nHashes are required for all requirements. This is because a partially-hashed requirements file is of little use and thus likely an error: a malicious actor could slip bad code into the installation via one of the unhashed requirements. Note that hashes embedded in URL-style requirements via the #md5=... syntax suffice to satisfy this rule (regardless of hash strength, for legacy reasons), though you should use a stronger hash like sha256 whenever possible.\n\nHashes are required for all dependencies. An error results if there is a dependency that is not spelled out and hashed in the requirements file.\n\nRequirements that take the form of project names (rather than URLs or local filesystem paths) must be pinned to a specific version using ==. This prevents a surprising hash mismatch upon the release of a new version that matches the requirement specifier.\n\n--egg is disallowed, because it delegates installation of dependencies to setuptools, giving up pip\u2019s ability to enforce any of the above.\n\nHash-checking mode can be forced on with the --require-hashes command-line option:\n\n\nUnix/macOS\n$ python -m pip install --require-hashes -r requirements.txt\n...\nHashes are required in --require-hashes mode (implicitly on when a hash is\nspecified for any package). These requirements were missing hashes,\nleaving them open to tampering. These are the hashes the downloaded\narchives actually had. You can add lines like these to your requirements\nfiles to prevent tampering.\n   pyelasticsearch==1.0 --hash=sha256:44ddfb1225054d7d6b1d02e9338e7d4809be94edbe9929a2ec0807d38df993fa\n   more-itertools==2.2 --hash=sha256:93e62e05c7ad3da1a233def6731e8285156701e3419a5fe279017c429ec67ce0\n\nWindows\nThis can be useful in deploy scripts, to ensure that the author of the requirements file provided hashes. It is also a convenient way to bootstrap your list of hashes, since it shows the hashes of the packages it fetched. It fetches only the preferred archive for each package, so you may still need to add hashes for alternatives archives using pip hash: for instance if there is both a binary and a source distribution.\n\nThe wheel cache is disabled in hash-checking mode to prevent spurious hash mismatch errors. These would otherwise occur while installing sdists that had already been automatically built into cached wheels: those wheels would be selected for installation, but their hashes would not match the sdist ones from the requirements file. A further complication is that locally built wheels are nondeterministic: contemporary modification times make their way into the archive, making hashes unpredictable across machines and cache flushes. Compilation of C code adds further nondeterminism, as many compilers include random-seeded values in their output. However, wheels fetched from index servers are the same every time. They land in pip\u2019s HTTP cache, not its wheel cache, and are used normally in hash-checking mode. The only downside of having the wheel cache disabled is thus extra build time for sdists, and this can be solved by making sure pre-built wheels are available from the index server.\n\nHash-checking mode also works with pip download and pip wheel. See Repeatable Installs for a comparison of hash-checking mode with other repeatability strategies.\n\nWarning\n\nBeware of the setup_requires keyword arg in setup.py. The (rare) packages that use it will cause those dependencies to be downloaded by setuptools directly, skipping pip\u2019s hash-checking. If you need to use such a package, see Controlling setup_requires.\n\nWarning\n\nBe careful not to nullify all your security work when you install your actual project by using setuptools directly: for example, by calling python setup.py install, python setup.py develop, or easy_install. Setuptools will happily go out and download, unchecked, anything you missed in your requirements file\u2014and it\u2019s easy to miss things as your project evolves. To be safe, install your project using pip and --no-deps.\n\nInstead of python setup.py develop, use\u2026\n\n\nUnix/macOS\npython -m pip install --no-deps -e .\n\nWindows\nInstead of python setup.py install, use\u2026\n\n\nUnix/macOS\npython -m pip install --no-deps .\n\nWindows\nHASHES FROM PYPI\nPyPI provides an MD5 hash in the fragment portion of each package download URL, like #md5=123..., which pip checks as a protection against download corruption. Other hash algorithms that have guaranteed support from hashlib are also supported here: sha1, sha224, sha384, sha256, and sha512. Since this hash originates remotely, it is not a useful guard against tampering and thus does not satisfy the --require-hashes demand that every package have a local hash.\n\nLocal project installs\npip supports installing local project in both regular mode and editable mode. You can install local projects by specifying the project path to pip:\n\n\nUnix/macOS\npython -m pip install path/to/SomeProject\n\nWindows\nNote\n\nDepending on the build backend used by the project, this may generate secondary build artifacts in the project directory, such as the .egg-info and build directories in the case of the setuptools backend.\n\nPip has a legacy behaviour that copies the entire project directory to a temporary location and installs from there. This approach was the cause of several performance and correctness issues, so it is now disabled by default, and it is planned that pip 22.1 will remove it.\n\nTo opt in to the legacy behavior, specify the --use-deprecated=out-of-tree-build option in pip\u2019s command line.\n\n\u201cEDITABLE\u201d INSTALLS\n\u201cEditable\u201d installs are fundamentally \u201csetuptools develop mode\u201d installs.\n\nYou can install local projects or VCS projects in \u201ceditable\u201d mode:\n\n\nUnix/macOS\npython -m pip install -e path/to/SomeProject\npython -m pip install -e git+http://repo/my_project.git#egg=SomeProject\n\nWindows\n(See the VCS Support section above for more information on VCS-related syntax.)\n\nFor local projects, the \u201cSomeProject.egg-info\u201d directory is created relative to the project path. This is one advantage over just using setup.py develop, which creates the \u201cegg-info\u201d directly relative the current working directory.\n\nBuild System Interface\nThis is now covered in Build System Interface.", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How can I tell if SP1 has been installed on VS2008?", "id": 1745, "answers": [{"answer_id": 1732, "document_id": 1317, "question_id": 1745, "text": "In Help->About, you can view the installed products. You should see something similar to\nMicrosoft Visual Studio Team System 2008 Team Suite - ENU Service Pack 1 (KB945140) KB945140", "answer_start": 154, "answer_category": null}], "is_impossible": false}], "context": "How can I tell if SP1 has been installed on VS2008? e.g. If I'm working on a co-worker's machine - how can I tell if he/she has installed SP1 for VS2008?\nIn Help->About, you can view the installed products. You should see something similar to\nMicrosoft Visual Studio Team System 2008 Team Suite - ENU Service Pack 1 (KB945140) KB945140\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Too many different Python versions on my system and causing problems", "id": 114, "answers": [{"answer_id": 122, "document_id": 78, "question_id": 114, "text": "I think the principle would be that be aware of that different ways and tools install things independently to different locations, so use them mindfully. ", "answer_start": 1102, "answer_category": null}], "is_impossible": false}], "context": "Why did it get messed up?\n\nThere're a couples of different way to install Python, as the update of OP says, and they locate files in different locations. For example, macports puts things into /opt/local/, while homebrew puts things into /usr/local/. Also, Mac OS X brings a few python versions with itself. So, if you install python many times via different ways, you will get many python versions existing independently on your system.\n\nWhat problem does it cause?\n\nI don't know exactly. I guess the problem is that if you have many versions of python, then which one to use and where to find packages will be determined by the path order in your system PATH and the PYTHONPATH respectively. So you may lose control of where to install python modules. Consider that if you run sudo python setup.py install to install a module (it finds python by the root's PATH) and then try to import the module by python -c \"import it\" (this time it finds python by your PATH), maybe something will go wrong. This is my guess, I didn't validate it. But in my own case, something did go wrong.\n\nHow to avoid this?\n\nI think the principle would be that be aware of that different ways and tools install things independently to different locations, so use them mindfully. \n\n\nUnless you intend to, don't install the same thing twice via different\nways. (If you intend to do it for python, you might want to check out virtualenv)\nKeep an eye on the path order in your PATH and consider if it's\ncorrect. \nWhen installing modules, be clear which python (or pip) is\nrunning and where the module is installed.\n\n\nSo, how did I solve my own case?\n\nSince it had been messing up already and seemed to be very hard to cure, so finally I solved this question by a full OS re-installation, and started to follow the DOs-and-DONTs above. For the installation of the scientific environment with python (numpy/scipy/matplotlib, which had shown problems to make me ask this question), I found this tutorial was extremely helpful. So, problem solved finally.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to list dependencies of a JAR", "id": 1864, "answers": [{"answer_id": 1850, "document_id": 1435, "question_id": 1864, "text": "You can do the following (if its a maven project):\nmvn dependency:tree\nIt shows transitive dependencies as well, making it very useful to debug dependency conflicts.", "answer_start": 371, "answer_category": null}], "is_impossible": false}], "context": "is there a tool that can list third party \"packages\" that contain (third party) classes referenced in the JAR ? Let say that it would recognize what is \"home\" package from JAR file definition and it would print out a list of fully qualified names of third party c\nthe purpose is that I need to find maven dependencies for that JAR file and deploy it as a maven artifact.\nYou can do the following (if its a maven project):\nmvn dependency:tree\nIt shows transitive dependencies as well, making it very useful to debug dependency conflicts.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Loading Maven dependencies from GitHub", "id": 1872, "answers": [{"answer_id": 1858, "document_id": 1443, "question_id": 1872, "text": "Now you can import a Java library from a GitHub repo using JitPack. In your pom.xml:\nAdd repository:\n<repository>\n    <id>jitpack.io</id>\n    <url>https://jitpack.io</url>\n</repository>\nAdd dependency\n<dependency>\n    <groupId>com.github.User</groupId>\n    <artifactId>Repo name</artifactId>\n    <version>Release tag</version>\n</dependency>", "answer_start": 190, "answer_category": null}], "is_impossible": false}], "context": "How do I add a Java library from its GitHub repo (the library uses Maven as a build system) as a dependency to my Maven project? Can I do that without downloading and compiling the library?\nNow you can import a Java library from a GitHub repo using JitPack. In your pom.xml:\nAdd repository:\n<repository>\n    <id>jitpack.io</id>\n    <url>https://jitpack.io</url>\n</repository>\nAdd dependency\n<dependency>\n    <groupId>com.github.User</groupId>\n    <artifactId>Repo name</artifactId>\n    <version>Release tag</version>\n</dependency>\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "how to run java  if I do not set the PATH variable?", "id": 385, "answers": [{"answer_id": 392, "document_id": 170, "question_id": 385, "text": " you need to specify the full path to the executable file every time you run it, such as:\n\nC:\\> \"C:\\Program Files\\Java\\jdk1.8.0\\bin\\javac\" MyClass.java", "answer_start": 451, "answer_category": null}], "is_impossible": false}, {"question": "how to  set the PATH variable permanently  for windows?", "id": 386, "answers": [{"answer_id": 393, "document_id": 170, "question_id": 386, "text": "Click Start, then Control Panel, then System.\n\nClick Advanced, then Environment Variables.\n\nAdd the location of the bin folder of the JDK installation to the PATH variable in System Variables. The following is a typical value for the PATH variable:\n\nC:\\WINDOWS\\system32;C:\\WINDOWS;C:\\Program Files\\Java\\jdk1.8.0\\bin", "answer_start": 937, "answer_category": null}], "is_impossible": false}], "context": "This page describes how to install and uninstall JDK 8 for Windows.\n\nThe page has these topics:\n\n\"System Requirements\"\n\n\"Installation Instructions Notation\"\n\n\"Installation Instructions\"\n\n\"Uninstalling the JDK\"\n\n\"Installed Directory Tree\"\n\n\"Installation Troubleshooting\"\n\nSee \"JDK 8 and JRE 8 Installation Start Here\" for general information about installing JDK 8 and JRE 8.\n\nUpdating the PATH Environment Variable\nIf you do not set the PATH variable, you need to specify the full path to the executable file every time you run it, such as:\n\nC:\\> \"C:\\Program Files\\Java\\jdk1.8.0\\bin\\javac\" MyClass.java\nIt is useful to set the PATH variable permanently so it will persist after rebooting.\n\nTo set the PATH variable permanently, add the full path of the jdk1.8.0\\bin directory to the PATH variable. Typically, this full path looks something like C:\\Program Files\\Java\\jdk1.8.0\\bin. Set the PATH variable as follows on Microsoft Windows:\n\nClick Start, then Control Panel, then System.\n\nClick Advanced, then Environment Variables.\n\nAdd the location of the bin folder of the JDK installation to the PATH variable in System Variables. The following is a typical value for the PATH variable:\n\nC:\\WINDOWS\\system32;C:\\WINDOWS;C:\\Program Files\\Java\\jdk1.8.0\\bin\nNote:\n\nThe PATH environment variable is a series of directories separated by semicolons (;) and is not case-sensitive. Microsoft Windows looks for programs in the PATH directories in order, from left to right.\n\nYou should only have one bin directory for a JDK in the path at a time. Those following the first instance are ignored.\n\nIf you are not sure where to add the JDK path, append it.\n\nThe new path takes effect in each new command window you open after setting the PATH variable.\n\nStarting to Use the JDK\nUse the Java item in the Windows Start menu to get access to essential Java information and functions, including help, API documentation, the Java Control Panel, checking for updates, and Java Mission Control.\n\nIf you are new to developing and running programs in the Java programming language, see http://docs.oracle.com/javase/tutorial for some guidance. Note especially the tutorial trails under the heading Trails Covering the Basics.\n\nYou can also download the JDK documentation from http://www.oracle.com/technetwork/java/javase/downloads/index-jsp-138363.html page.", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Deploying a minimal flask app in docker - server connection issues", "id": 1664, "answers": [{"answer_id": 1651, "document_id": 1237, "question_id": 1664, "text": "If you change:\nif __name__ == '__main__':\n    app.run()\nto\nif __name__ == '__main__':\n    app.run(host='0.0.0.0')", "answer_start": 154, "answer_category": null}], "is_impossible": false}], "context": "The problem is you are only binding to the localhost interface, you should be binding to 0.0.0.0 if you want the container to be accessible from outside. If you change:\nif __name__ == '__main__':\n    app.run()\nto\nif __name__ == '__main__':\n    app.run(host='0.0.0.0')\nIt should work.\nNote that this will bind to all interfaces on the host, which may in some circumstances be a security risk - see https://stackoverflow.com/a/58138250/4332 for more information on binding to a specific interface.\nI have an app whose only dependency is flask, which runs fine outside docker and binds to the default port 5000. Here is the full source:\nfrom flask import Flask\n \napp = Flask(__name__)\napp.debug = True\n \n@app.route('/')\ndef main():\n    return 'hi'\n \nif __name__ == '__main__':\n    app.run()\nThe problem is that when I deploy this in docker, the server is running but is unreachable from outside the container.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "installing mysql noinstall zip archive", "id": 1362, "answers": [{"answer_id": 1351, "document_id": 930, "question_id": 1362, "text": " Extract the archive to the desired install directory\n  Create an option file  \n  Choose a MySQL server type  \n  Start the MySQL server\n  Secure the default user accounts", "answer_start": 441, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI've been trying to use my mysql noinstall zip archive and the steps that I found in some of the websites are not descriptive. Can someone help?\n    \n\nIf you follow the instruction here you will be alright. From that page:\n\n\n  Users who are installing from the noinstall package can use the instructions in this section to manually install MySQL. The process for installing MySQL from a Zip archive is as follows:\n  \n  \n  Extract the archive to the desired install directory\n  Create an option file  \n  Choose a MySQL server type  \n  Start the MySQL server\n  Secure the default user accounts\n  \n\n\nTo start the sever run \n\nmysqld --console\n\n\nand after that run \n\nmysql -u root\n\n\nto start doing whatever you want.\n\nNotice that MySql will function exactly the same as if you would have installed or configured it to run as a service. Your schema's are stored on disk. If you start it after you've stopped it your previous data will still be there. So it doesn't turn magically into an in-memory database.\n    \n\nIf you want to install the MySQL database as a service you can use the command below.\n\nmysqld.exe --install\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How do I push a new local branch to a remote Git repository and track it too?", "id": 1851, "answers": [{"answer_id": 1837, "document_id": 1422, "question_id": 1851, "text": "In Git 1.7.0 and later, you can checkout a new branch:\ngit checkout -b <branch>", "answer_start": 460, "answer_category": null}], "is_impossible": false}], "context": "I want to be able to do the following:\nCreate a local branch based on some other (remote or local) branch (via git branch or git checkout -b)\nPush the local branch to the remote repository (publish), but make it trackable so git pull and git push will work immediately.\nHow do I do that?\nI know about --set-upstream in Git 1.7, but that is a post-creation action. I want to find a way to make a similar change when pushing the branch to the remote repository.\nIn Git 1.7.0 and later, you can checkout a new branch:\ngit checkout -b <branch>\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Uninstall ReSharper 4.5", "id": 649, "answers": [{"answer_id": 654, "document_id": 342, "question_id": 649, "text": "Click on the Resharper 4.5 installer (exe file) and instead of \"Install\", select \"Remove\". It works just like how the Visual Studio installer works, same exe file is used to install/uninstall the software.\n\u2022\tRemove the Resharper files from \"JetBrains\" folder in AppData.", "answer_start": 324, "answer_category": null}], "is_impossible": false}], "context": "I have ReSharper 4.5 in Visual Studio 2008. Now I want to install ReSharper 5, but I can't do it before I uninstall ReSharper 4.5.\nHow can I uninstall ReSharper 4.5?\nAfter uninstalling Resharper from Control Panel, you might still see it in your visual studio.\nSo in order to remove it completely, follow these 2 steps :-\n\u2022\tClick on the Resharper 4.5 installer (exe file) and instead of \"Install\", select \"Remove\". It works just like how the Visual Studio installer works, same exe file is used to install/uninstall the software.\n\u2022\tRemove the Resharper files from \"JetBrains\" folder in AppData.\nAfter doing this, visual studio will ask you to modify some settings. Restore it to default and then you're good to go.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to deploy separated frontend and backend?", "id": 1087, "answers": [{"answer_id": 1079, "document_id": 664, "question_id": 1087, "text": "You can deploy\nyour frontend on a static hosting service and a CDN\nAWS S3 + AWS CloudFront\nGoogle Cloud Storage + Google Cloud CDN\nyour backend on a cloud computing service\nAWS Elastic beanstalk or AWS EC2", "answer_start": 181, "answer_category": null}], "is_impossible": false}], "context": "I am developing a new project with react/express as the frontend and loopback as the backend api. I have separated both of them in my development environment with different ports.\n\nYou can deploy\nyour frontend on a static hosting service and a CDN\nAWS S3 + AWS CloudFront\nGoogle Cloud Storage + Google Cloud CDN\nyour backend on a cloud computing service\nAWS Elastic beanstalk or AWS EC2\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "differences between osxs pkg and pkg mpkg installers", "id": 1367, "answers": [{"answer_id": 1356, "document_id": 936, "question_id": 1367, "text": "Please see the this blog post:\n\nhttp://followtheheard.blogspot.com/2008/09/changing-bitmap-banner-in-visual-studio.html\n\nYou can follow Stewbob's way (which is the right way), but it will require you to change the image on every page of every installer you create. The above link will let you change the image in the template that Visual Studio uses to create all of your installer files going forward. That way, you can leave your pages pointing to the Default image, because you changed the Default image to be YOUR image.", "answer_start": 752, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nA default setup project in visual studio 2008 gives you dialogs with a logo-type image in the upper right of a monitor. All I want to do is change this image to new logo. Don't want to deal with custom dialogs. Any way to just change the image?\n    \n\nIn your setup project, go into the User Interface Dialog editor and click on one of the dialogs.  In the properties window for that dialog, you can change the BannerBitmap property to any image that you want. \n\nIt's a long thin bitmap that extends across the entire top of the dialog.  The one that I use has about a 7.5:1 width to height ratio.\nSpecifically, it's a 497x69 pixel bitmap (.bmp).  I don't remember how I came up with that exact size, but it has worked for me.\n    \n\nPlease see the this blog post:\n\nhttp://followtheheard.blogspot.com/2008/09/changing-bitmap-banner-in-visual-studio.html\n\nYou can follow Stewbob's way (which is the right way), but it will require you to change the image on every page of every installer you create. The above link will let you change the image in the template that Visual Studio uses to create all of your installer files going forward. That way, you can leave your pages pointing to the Default image, because you changed the Default image to be YOUR image.\n\nThis solution worked great for me. Set it and forget it.\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to update Ruby to 1.9.x on Mac?", "id": 651, "answers": [{"answer_id": 656, "document_id": 344, "question_id": 651, "text": "(assuming that you have tapped homebrew/versions, which can be done by running brew tap homebrew/versions)\nbrew install ruby193\nWorked out of the box for me on OS X 10.8.4. Or if you want 2.0, you just brew install ruby\nMore generally, brew search ruby shows you the different repos available, and if you want to get really specific you can use brew versions ruby and checkout a specific version instead.", "answer_start": 299, "answer_category": null}], "is_impossible": false}], "context": "I have created a new user account on my mac and I am trying to update to the current version of ruby on it (1.9.2) from the snow leopard default of 1.8.7. Can somebody point me to tutorial or explain the best method to update Ruby on my mac from 1.8 to 1.9.2? Thanks. With brew this is a one-liner:\n(assuming that you have tapped homebrew/versions, which can be done by running brew tap homebrew/versions)\nbrew install ruby193\nWorked out of the box for me on OS X 10.8.4. Or if you want 2.0, you just brew install ruby\nMore generally, brew search ruby shows you the different repos available, and if you want to get really specific you can use brew versions ruby and checkout a specific version instead. And then, everything was working perfectly ! The ruby from my system was updated ! Hope it will help for the next adventurers !\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Error installing a crate via cargo: specified package has no binaries", "id": 824, "answers": [{"answer_id": 819, "document_id": 506, "question_id": 824, "text": "add it to your Cargo.toml.\nRead the Rust getting started guide and the Cargo getting started guide for further information. In short:\ncargo new my_project\ncd my_project\necho 'curl = \"0.3.0\"' >> Cargo.toml", "answer_start": 396, "answer_category": null}], "is_impossible": false}], "context": "What does this mean? Do I have to build it from source first? What's the point of Cargo if it does not install it in the first place? I'm trying to install a Rust crate on my system (Arch Linux) using Cargo. I can search for crates and find what I need, cargo install is used to install binary packages that happen to be distributed through crates.io.\nIf you want to use a crate as a dependency, add it to your Cargo.toml.\nRead the Rust getting started guide and the Cargo getting started guide for further information. In short:\ncargo new my_project\ncd my_project\necho 'curl = \"0.3.0\"' >> Cargo.toml\nAn important thing to note is that every Cargo project manages and compiles a separate set of dependencies (some background info). Thus it doesn't make sense to install a compiled library. The source code for each version of a library will be cached locally, avoiding downloading it multiple times.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "bundle install returns \"Could not locate Gemfile\"", "id": 642, "answers": [{"answer_id": 647, "document_id": 335, "question_id": 642, "text": "You may also indicate the path to the gemfile in the same command e.g.BUNDLE_GEMFILE=\"MyProject/Gemfile.ios\" bundle install.", "answer_start": 392, "answer_category": null}], "is_impossible": false}], "context": "I'm new to Rails and am currently working through a guide. The guide states:Use a text editor to update the Gemfile needed by Bundler with the contents of Listing .We then install and include the gems using the bundle install command:$ bundle install --without production .If Bundler complains about no such file to load -- readline (LoadError) try adding gem \u2019rb-readline\u2019 to your Gemfile.) You may also indicate the path to the gemfile in the same command e.g.BUNDLE_GEMFILE=\"MyProject/Gemfile.ios\" bundle install.I followed the steps even adding on gem 'rb-readline' to the Gemfile, but apparently the file can't be found and when I go to my text editor I do see the Gemfile itself. I noticed that they made me put gem 'rails', 3.2.3 and my version of Rails is 3.2.1 so I tried changing it to 3.2.1 but that didn't work either.Any thoughts or advice would be much appreciated.", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Angular 7 app has to clear cache after new deployment", "id": 457, "answers": [{"answer_id": 466, "document_id": 190, "question_id": 457, "text": "You should try this command\uff1anode --max_old_space_size=8192 node_modules/@angular/cli/bin/ng build --prod --aot --output-hashing=all.", "answer_start": 184, "answer_category": null}], "is_impossible": false}], "context": "I have a angular 7 application. my problem that while I'm doing any changes and uploading those on tomcat9 server, those changes does not reflect till i don't hard refresh the browser.You should try this command\uff1anode --max_old_space_size=8192 node_modules/@angular/cli/bin/ng build --prod --aot --output-hashing=all.", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to get started deploying PHP applications from a subversion repository?", "id": 508, "answers": [{"answer_id": 509, "document_id": 233, "question_id": 508, "text": "For PHP, you might want to look into Xinc or phpUnderControl", "answer_start": 368, "answer_category": null}], "is_impossible": false}], "context": "I've heard the phrase \"deploying applications\" which sounds much better/easier/more reliable than uploading individual changed files to a server, but I don't know where to begin.\nAutomatic deploy + run of tests to a staging server is known as continuous integration. The idea is that if you check in something that breaks the tests, you would get notified right away. For PHP, you might want to look into Xinc or phpUnderControl\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "using a perl script on windows without installing activeperl?", "id": 960, "answers": [{"answer_id": 955, "document_id": 591, "question_id": 960, "text": "Strawberry Perl is a perl environment for MS Windows containing all you need to run and develop perl applications", "answer_start": 167, "answer_category": null}], "is_impossible": false}], "context": "The Perl for MS Windows, free of charge!\nPerl is a programming language suitable for writing simple scripts as well as complex applications \u2014 see https://www.perl.org.Strawberry Perl is a perl environment for MS Windows containing all you need to run and develop perl applications. It is designed to be as close as possible to perl environment on UNIX systems.It includes perl binaries, compiler (gcc) + related tools, all the external libraries (crypto, math, graphics, xml\u2026), all the bundled database clients and all you expect from Strawberry Perl.\nRecommended version:\n\nstrawberry-perl-5.32.1.1-64bit.msi\nstrawberry-perl-5.32.1.1-32bit.msi\nMore downloads (all releases):\n\nZIP, Portable, special editions\nYou can find here release notes and other details.\nSupport us by becoming an EPO member.", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "visual studio 2017 rc setup operation failed", "id": 1969, "answers": [{"answer_id": 1955, "document_id": 1554, "question_id": 1969, "text": "So, create a Dev account (free) then go to downloads from the dev homepage and select the version you want to download and check x86. ", "answer_start": 2171, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI first tried to install VS 2017 RC and a problem occurred during installation. I tried to repair the installation but while it was repairing - my computer blue screened (probably unrelated to installation I was running a lot of programs). Now, whenever I try to install VS 2017 - it instantly gives me this error log and says 'Setup operation failed'. What do I do to fix this?\n\nError log:\n\n\n  [29cc:0004][2016-12-22T07:39:53] Error 0x80004003: \n     at Microsoft.VisualStudio.Setup.Cache.InstanceRepository.GetInstance()\n     at Microsoft.VisualStudio.Setup.Cache.CacheRepository.d__27.MoveNext()\n     at System.Linq.Enumerable.FirstOrDefault[TSource](IEnumerable1 source, Func2 predicate)\n     at Microsoft.VisualStudio.Setup.Engine.VerifyInstallationPath(IServiceProvider services, String installationPath, IInstance instance, IQuery query)\n     at Microsoft.VisualStudio.Setup.Engine.Install(Product product, String destination, CancellationToken token)Object reference not set to an instance of an object.\n\n\nI tried posting on VS developer community but it is instantly marked as duplicated and linked to other duplicates which link to other duplicates which have no solution.\n    \n\nRefer to the error message, you can try to remove the existing VS 2017 RC that install failed before as below:\n\n\nGo to Control Panel\u2014Programs and Features, if there has \u2018Microsoft Visual Studio 2017\u2019, right click it and choose \u2018Uninstall\u2019\nTry to delete the following installed folders: (you can back up those folders before any modification, in case we need to roll back)\n\n\n\u2022   Go to the folder: %ProgramData%\\Microsoft\\VisualStudio and delete the \u2018Packages\u2019 folder and \u2018Setup\u2019 folder if there has. \n\n\u2022   Go to the Visual Studio installation folder (normally under %ProgramFiles(x86)%\\Microsoft Visual Studio) and delete the folder \u20182017\u2019\n\nAfter that, make sure windows update is up-to-date, clean up the %temp% folder and temporarily disable any antivirus software, reboot the computer and re-run the VS 2017 RC installer as administrator again.\n    \n\nA common problem is that when you go to the downloads page it automatically downloads the x64 version of VS. So, create a Dev account (free) then go to downloads from the dev homepage and select the version you want to download and check x86. Hope this helps!\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Heroku deployment error H10 (App crashed)", "id": 1660, "answers": [{"answer_id": 1648, "document_id": 1234, "question_id": 1660, "text": "$ heroku run rails console", "answer_start": 591, "answer_category": null}], "is_impossible": false}], "context": "I ran into the same error above, app was crashing on heroku (running fine in dev) but error logs on heroku were not revealing any clues. I read other answers on this page and broke out in a sweat after seeing \"rebuilding the app.\" I figured maybe I could get in the heroku console and look around. I did and even the console crashed, but this time it told me why. It was some obscure variable I forgot to delete during a troubleshooting session hours earlier. I am not saying you will run into the same problem, but I found more info when I tried to go through the console. Hope this helps.\n$ heroku run rails console\nI have a RoR app working on my local computer, but when I send it to heroku, it crashes. The error log gives an error H10 & says.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to manually deploy a web service on Tomcat 6?", "id": 582, "answers": [{"answer_id": 588, "document_id": 307, "question_id": 582, "text": "You should try this excellent article:http://java.sun.com/developer/technicalArticles/J2SE/jax_ws_2/ ", "answer_start": 233, "answer_category": null}], "is_impossible": false}], "context": "I'd like to know the steps to manually deploy it on Tomcat, in order to learn how it's done and because I don't like depending on an IDE.\nI mean, I'd like to know how everything could be done from the command line and a text editor.\nYou should try this excellent article:http://java.sun.com/developer/technicalArticles/J2SE/jax_ws_2/ .\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "how to download Linux x64 RPM Java binary file?", "id": 174, "answers": [{"answer_id": 181, "document_id": 103, "question_id": 174, "text": "Go to http://java.com and click on the Download button.\nDownload and check the download file size to ensure that you have downloaded the full, uncorrupted software bundle. Before you download the file, notice its byte size provided on the download page on the web site. Once the download has completed, compare that file size to the size of the downloaded file to make sure they are equal.", "answer_start": 736, "answer_category": null}], "is_impossible": false}, {"question": "how to uninstall earlier installations of the Java packages for Linux x64 RPM ?", "id": 175, "answers": [{"answer_id": 182, "document_id": 103, "question_id": 175, "text": "rpm -e package_name", "answer_start": 1858, "answer_category": null}], "is_impossible": false}, {"question": "how to install the java  package for Linux x64 RPM? ", "id": 176, "answers": [{"answer_id": 183, "document_id": 103, "question_id": 176, "text": "rpm -ivh jre-8u73-linux-x64.rpm", "answer_start": 1900, "answer_category": null}], "is_impossible": false}, {"question": "how to update the java  package for Linux x64 RPM? ", "id": 177, "answers": [{"answer_id": 184, "document_id": 103, "question_id": 177, "text": "rpm -Uvh jre-8u73-linux-x64.rpm", "answer_start": 1955, "answer_category": null}], "is_impossible": false}], "context": "Linux x64 RPM Java installation instructions\nThis article applies to:\nPlatform(s): Oracle Enterprise Linux, Oracle Linux, Red Hat Linux, SLES, SUSE Linux\nJava version(s): 7.0, 8.0\nLinux System Requirements\nSee supported System Configurations for information about supported platforms, operating systems, desktop managers, and browsers.\n\nNote: For downloading Java other flavors of Linux see Java for Ubuntu Java for Fedora\n\nFollow these steps to download and install Java for Linux.\nDownload\nInstall\nDownload\nThis procedure installs the Java Runtime Environment (JRE) for 64-bit RPM-based Linux platforms, such as Red Hat and SuSE, using an RPM binary file (.rpm) in the system location. You must be root to perform this installation.\n\nGo to http://java.com and click on the Download button.\nDownload and check the download file size to ensure that you have downloaded the full, uncorrupted software bundle. Before you download the file, notice its byte size provided on the download page on the web site. Once the download has completed, compare that file size to the size of the downloaded file to make sure they are equal.\n\nInstall\nThe instructions below are for installing version Java 8 Update 73 (8u73). If you are installing another version, make sure you change the version number appropriately when you type the commands at the terminal. Example: For Java 8u79 replace 8u73 with 8u79. Note that, as in the preceding example, the version number is sometimes preceded with the letter u and sometimes it is preceded with an underbar, for example, jre1.8.0_73.\n\nBecome root by running su and entering the super-user password.\nChange to the directory in which you want to install. Type:\ncd directory_path_name\nFor example, to install the software in the /usr/java/ directory, Type:\ncd /usr/java\n\nUninstall any earlier installations of the Java packages.\nrpm -e package_name\n\nInstall the package.\nrpm -ivh jre-8u73-linux-x64.rpm\n\nTo upgrade a package:\nrpm -Uvh jre-8u73-linux-x64.rpm\n\nDelete the .rpm file if you want to save disk space.\nExit the root shell. No need to reboot.", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Git: auto pull from repository?", "id": 339, "answers": [{"answer_id": 346, "document_id": 151, "question_id": 339, "text": "Git has \"hooks\", actions that can be executed after other actions. What you seem to be looking for is \"post-receive hook\". In the github admin, you can set up a post-receive url that will be hit (with a payload containing data about what was just pushed) everytime somebody pushes to your repo.", "answer_start": 465, "answer_category": null}], "is_impossible": false}], "context": "Is there any way to set up git such that it listens for updates from a remote repo and will pull whenever something changes? The use case is I want to deploy a web app using git (so I get version control of the deployed application) but want to put the \"central\" git repo on Github rather than on the web server (Github's interface is just soooo nice).\nHas anyone gotten this working? How does Heroku do it? My Google-fu is failing to give me any relevant results. Git has \"hooks\", actions that can be executed after other actions. What you seem to be looking for is \"post-receive hook\". In the github admin, you can set up a post-receive url that will be hit (with a payload containing data about what was just pushed) everytime somebody pushes to your repo.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "how to use the final project exe config file when creating a setup project", "id": 1910, "answers": [{"answer_id": 1897, "document_id": 1482, "question_id": 1910, "text": "ng.\n\n  &lt;Target Name=\"BeforeBuild\"&gt;\n    &lt;ItemGroup&gt;\n      &lt;ConfigSourceFiles Include=\"Web.$(Configuration).config\" /&gt;\n      &lt;ConfigDestinationFiles Include=\"Web.config\" /&gt;\n    &lt;/ItemGroup&gt;\n    &lt;Copy SourceFiles=\"@(ConfigSourceFiles)\" DestinationFiles=\"@(", "answer_start": 1675, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI have created a console application (blah.exe) with specific app.config's for dev and prod. These are named dev_app.config and prod_app.config. I have hooked up an AfterBuild target in my csproj file* which copies the correct config file to the bin directory as blah.exe.config.  \n\nI have also created a setup project for this console app but I have run into a slight issue. It seems that the setup project uses the actual app.config from the project directory as opposed to the final blah.exe.config (located in bin directory).\n\n|--Bin\n|  |--Debug\n|     |--Blah.exe.config &lt;--- I want the setup project to use this file\n|--app.config &lt;--- Setup project uses this file at the moment\n|--dev_app.config\n|--prod_app.config\n\n\nHow can I force the setup project to use the final config file generated in the bin folder and not the actual app.config file?\n\nAdditional Information:\n\nMy current solution involves adding another AfterBuild command which overwrites the actual app.config file. I don't like approach since it forces me to have an additional file that I don't need. Also, having this file has caused me some grief already since I made changes to the app.config file which got overwritten when building. The question is about how to get the setup project use the final config file in the bin folder and NOT how to manage the config or ways to create a config file.  \n\n* Adapted from Deploy an app.config based on build configuration\n    \n\nI have been using that exact same scenario but I use the BeforeBuild instead of AfterBuild, and it has always been fine.  I have been doing this on both web and windows projects.  Below is the code I am using.\n\n  &lt;Target Name=\"BeforeBuild\"&gt;\n    &lt;ItemGroup&gt;\n      &lt;ConfigSourceFiles Include=\"Web.$(Configuration).config\" /&gt;\n      &lt;ConfigDestinationFiles Include=\"Web.config\" /&gt;\n    &lt;/ItemGroup&gt;\n    &lt;Copy SourceFiles=\"@(ConfigSourceFiles)\" DestinationFiles=\"@(ConfigDestinationFiles)\" /&gt;\n  &lt;/Target&gt;\n\n\nHope this helps.\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "What is execution mode \u201cRun as a standalone program\u201d  when deploying java applications?", "id": 435, "answers": [{"answer_id": 444, "document_id": 181, "question_id": 435, "text": "The application package is available on a local drive. Users launch it using a Java launcher, such as java -jar MyApp.jar, or by double-clicking the application JAR file.", "answer_start": 988, "answer_category": null}], "is_impossible": false}, {"question": "What is execution mode \u201cLaunched from a remote server using Java Web Start\n\u201d  when deploying java applications?\n", "id": 436, "answers": [{"answer_id": 445, "document_id": 181, "question_id": 436, "text": "Users click a link in a web page to start the application from a remote web server. Once downloaded, a Java Web Start application can also be started from a desktop shortcut.", "answer_start": 1212, "answer_category": null}], "is_impossible": false}, {"question": "What is execution mode \u201cEmbedded in a browse\u201d  when deploying java applications?", "id": 437, "answers": [{"answer_id": 446, "document_id": 181, "question_id": 437, "text": "Java content is embedded in the web page and hosted on a remote web server.", "answer_start": 1411, "answer_category": null}], "is_impossible": false}, {"question": "What is execution mode \u201cLaunched as a self-contained application\u201d  when deploying java applications?", "id": 438, "answers": [{"answer_id": 447, "document_id": 181, "question_id": 438, "text": "Application is installed on the local drive and runs as a standalone program using a private copy of the JRE. The application can be launched in the same way as other native applications for that operating system, for example, using a desktop shortcut or menu entry.", "answer_start": 1530, "answer_category": null}], "is_impossible": false}, {"question": "Why users\u2019 JRE auto-update functionality are not sufficient after deploying java applications? ", "id": 439, "answers": [{"answer_id": 448, "document_id": 181, "question_id": 439, "text": "The user does not have admin permissions to install the JRE.\n\nThe user requires an older version of the JRE for other applications.\n\nYou want to set the exact version of the JRE to be used by your application.\n\nYour distribution channel disallows dependencies on external frameworks.", "answer_start": 3472, "answer_category": null}], "is_impossible": false}], "context": "Skip to Content\nHome PageJava SoftwareJava SE DownloadsJava SE 8 DocumentationSearch\nJava Platform, Standard Edition Deployment Guide\nContents    Previous    Next\n3 Application Execution Modes\nThis topic explains the different application execution modes that are available for deploying your Java and JavaFX applications.\n\nOne of the main features of the Java application model is that you can write one application and easily deploy it several different ways. The user can experience the same application running on the desktop, in a browser, or starting from a link in a browser.\n\nHowever, differences exist among the execution modes and need to be considered while developing the application.\n\nThis topic contains the following sections:\n\nExecution Modes\n\nUnderstanding Feature Differences\n\n3.1 Execution Modes\nTable 3-1 describes the execution modes in which Java applications can be deployed.\n\nTable 3-1 Java Execution Modes\n\nExecution Mode\tDescription\nRun as a standalone program\n\nThe application package is available on a local drive. Users launch it using a Java launcher, such as java -jar MyApp.jar, or by double-clicking the application JAR file.\n\nLaunched from a remote server using Java Web Start\n\nUsers click a link in a web page to start the application from a remote web server. Once downloaded, a Java Web Start application can also be started from a desktop shortcut.\n\nEmbedded in a browser\n\nJava content is embedded in the web page and hosted on a remote web server.\n\nLaunched as a self-contained application\n\nApplication is installed on the local drive and runs as a standalone program using a private copy of the JRE. The application can be launched in the same way as other native applications for that operating system, for example, using a desktop shortcut or menu entry.\n\n\nEach execution environment has its drawbacks. For example, for remote applications the loading phase can be very long because the application has to be loaded from the network. This is less of an issue for applications that run on a local drive.\n\n3.2 Understanding Feature Differences\nFigure 3-1 lists some of the features that behave differently in the different environments. The following sections describe the figure in more detail.\n\nFigure 3-1 Features of Deployment Types\n\nDescription of Figure 3-1 follows\nDescription of \"Figure 3-1 Features of Deployment Types\"\n\n3.2.1 Preloader Support for JavaFX Applications\nThe preloader is a small JavaFX application that receives notifications about application loading and initialization progress. The preloader is used with all execution modes, but depending on the execution mode, the preloader implementation receives a different set of events and optimal behavior may be different.\n\nFor example, in self-contained application or standalone execution mode or when launched from a shortcut, the preloader does not get any loading progress events, because there is nothing to load. See Chapter 13, \"Preloaders for JavaFX Applications\" for information about preloader implementation and differences in behavior.\n\n3.2.8 Managing Platform Dependencies\nTo run Java content, a recent version of the JRE is required. Unless the application is self-contained, the JRE must be installed on the user's system.\n\nIf users do not have the required version of the JRE, they are directed to install it. However, situations exist in which system installations of the JRE and the auto-update functionality are not sufficient, for example:\n\nThe user does not have admin permissions to install the JRE.\n\nThe user requires an older version of the JRE for other applications.\n\nYou want to set the exact version of the JRE to be used by your application.\n\nYour distribution channel disallows dependencies on external frameworks.\n\nDeploying your application as a self-contained application resolves these issues. The JRE is included in the application package, and users do not need to install it separately. The self-contained application package can be as simple as a .zip file distribution, or it can be wrapped into an installable package using technology that is native to the target operating system. See the topic Section 5, \"Packaging Basics\" for more details about self-contained application packages.\n\nContents    Previous    Next\nCopyright \u00a9 1993, 2021, Oracle and/or its affiliates. All rights reserved. | Cookie \u559c\u597d\u8bbe\u7f6e | Ad Choices.Contact Us", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Self updating application install with WIX?", "id": 1793, "answers": [{"answer_id": 1779, "document_id": 1365, "question_id": 1793, "text": "Yeah, ClickThrough really is what you're looking for here. There are bits and pieces in it that work. You might be able to dissect the code and use it yourself without all the extra \"UI + build integration\". Most of the bugs are in the higher levels. At the root, the RSS update and executable bootstrapper work fine.", "answer_start": 529, "answer_category": null}], "is_impossible": false}], "context": "I am writing an application that needs to be installed on a large number of desktops and also needs to update itself. We are looking at WIX for creating the installation. I have used ClickOnce and it is not a good solution for this install. WIX seems to fit, but there is no good process for auto update that I have found.\nI have looked at ClickThrough, but it doesn't seem ready for prime time yet. Does anyone have another good solution to use with WIX (or maybe another install program) to auto update an application install?\nYeah, ClickThrough really is what you're looking for here. There are bits and pieces in it that work. You might be able to dissect the code and use it yourself without all the extra \"UI + build integration\". Most of the bugs are in the higher levels. At the root, the RSS update and executable bootstrapper work fine.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "installing windowbuilder on eclipse 4 2", "id": 1394, "answers": [{"answer_id": 1383, "document_id": 966, "question_id": 1394, "text": "Goto - Menu &gt; Help &gt; Install New Software...\nSelect - Work With: Juno - http://download.eclipse.org/releases/juno\nThe WindowBuilder items are under \"General Purpose Tools\" (or use the f", "answer_start": 1041, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI'm using Eclipse Juno 4.2, downloaded from here.\n\nOn previous installs, I've been using 3.7, and I've been using WindowBuilder, which I find very useful. I noticed it wasn't included this time, so I used this update site provided on this page (the zip file download gives a \"file unavailable\" error).\n\nHowever, when I run the install, it rapidly climbs to 28%, then freezes. After half an hour, I get a very long error, whose message starts with this text:\n\n\n  An error occurred while collecting items to be installed session\n  \n  context was:(profile=epp.package.java,\n  phase=org.eclipse.equinox.internal.p2.engine.phases.Collect, operand=,\n  action=).\n  \n  Multiple problems occurred while downloading.\n  Unable to read repository at http://download.eclipse.org/windowbuilder/WB/integration/4.2/plugins/org.eclipse.wb.core_1.5.0.r42x201205291332.jar.pack.gz.\n\n\n(full text)\n\nDoes anyone know how I can go about installing it?\n    \n\nTry the juno repository at http://download.eclipse.org/releases/juno/.\n\nIn Eclipse:\n\n\nGoto - Menu &gt; Help &gt; Install New Software...\nSelect - Work With: Juno - http://download.eclipse.org/releases/juno\nThe WindowBuilder items are under \"General Purpose Tools\" (or use the filter)\n\n    \n\nInstalling WindowBuilder from this link http://download.eclipse.org/windowbuilder/WB/release/R201209281200/4.2/ - just add a new updatesite.\n\nOther versions: \nEclipse 4.3 (Kepler) - http://download.eclipse.org/windowbuilder/WB/integration/4.3/\nEclipse 3.8 (Juno) - http://download.eclipse.org/windowbuilder/WB/release/R201302221200/3.8/\n    \n\nFor Juno, I downloaded the archive update site from: http://eclipse.org/downloads/download.php?file=/windowbuilder/WB/release/R201306261200/WB_v1.6.0_UpdateSite_for_Eclipse4.2.zip\n\nThen I added the site as an archive to eclipse and installed the components just fine.  For some reason the update URL did not work for me after setting up my proxy.  Hope this helps!\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Which different did for statement in python than in C?", "id": 424, "answers": [{"answer_id": 432, "document_id": 179, "question_id": 424, "text": "The for statement in Python differs a bit from what you may be used to in C or Pascal. Rather than always iterating over an arithmetic progression of numbers (like in Pascal), or giving the user the ability to define both the iteration step and halting condition (as C), Python\u2019s for statement iterates over the items of any sequence (a list or a string), in the order that they appear in the sequence. ", "answer_start": 20, "answer_category": null}], "is_impossible": false}, {"question": "What is match Statements\uff1f", "id": 425, "answers": [{"answer_id": 433, "document_id": 179, "question_id": 425, "text": "A match statement takes an expression and compares its value to successive patterns given as one or more case blocks. ", "answer_start": 693, "answer_category": null}], "is_impossible": false}, {"question": "How to create a python function\uff1f", "id": 426, "answers": [{"answer_id": 434, "document_id": 179, "question_id": 426, "text": "We can create a function that writes the Fibonacci series to an arbitrary boundary:\n\n>>>\n>>> def fib(n):    # write Fibonacci series up to n\n...     \"\"\"Print a Fibonacci series up to n.\"\"\"\n...     a, b = 0, 1\n...     while a < n:\n...         print(a, end=' ')\n...         a, b = b, a+b\n...     print()", "answer_start": 1048, "answer_category": null}], "is_impossible": false}, {"question": "What is function Annotations\uff1f\n", "id": 427, "answers": [{"answer_id": 435, "document_id": 179, "question_id": 427, "text": "Function annotations are completely optional metadata information about the types used by user-defined functions (see PEP 3107 and PEP 484 for more information).", "answer_start": 3560, "answer_category": null}], "is_impossible": false}, {"question": "What happened in the function call\uff1f", "id": 428, "answers": [{"answer_id": 437, "document_id": 179, "question_id": 428, "text": "The actual parameters (arguments) to a function call are introduced in the local symbol table of the called function when it is called; thus, arguments are passed using call by value (where the value is always an object reference, not the value of the object). 1 When a function calls another function, or calls itself recursively, a new local symbol table is created for that call.\n", "answer_start": 2851, "answer_category": null}], "is_impossible": false}], "context": "4.2. for Statements\nThe for statement in Python differs a bit from what you may be used to in C or Pascal. Rather than always iterating over an arithmetic progression of numbers (like in Pascal), or giving the user the ability to define both the iteration step and halting condition (as C), Python\u2019s for statement iterates over the items of any sequence (a list or a string), in the order that they appear in the sequence. For example (no pun intended):\nCode that modifies a collection while iterating over that same collection can be tricky to get right. Instead, it is usually more straight-forward to loop over a copy of the collection or to create a new collection:\n\n4.6. match Statements\nA match statement takes an expression and compares its value to successive patterns given as one or more case blocks. This is superficially similar to a switch statement in C, Java or JavaScript (and many other languages), but it can also extract components (sequence elements or object attributes) from the value into variables.\n\n4.7. Defining Functions\nWe can create a function that writes the Fibonacci series to an arbitrary boundary:\n\n>>>\n>>> def fib(n):    # write Fibonacci series up to n\n...     \"\"\"Print a Fibonacci series up to n.\"\"\"\n...     a, b = 0, 1\n...     while a < n:\n...         print(a, end=' ')\n...         a, b = b, a+b\n...     print()\n...\n>>> # Now call the function we just defined:\n... fib(2000)\n0 1 1 2 3 5 8 13 21 34 55 89 144 233 377 610 987 1597\nThe keyword def introduces a function definition. It must be followed by the function name and the parenthesized list of formal parameters. The statements that form the body of the function start at the next line, and must be indented.\n\nThe first statement of the function body can optionally be a string literal; this string literal is the function\u2019s documentation string, or docstring. (More about docstrings can be found in the section Documentation Strings.) There are tools which use docstrings to automatically produce online or printed documentation, or to let the user interactively browse through code; it\u2019s good practice to include docstrings in code that you write, so make a habit of it.\n\nThe execution of a function introduces a new symbol table used for the local variables of the function. More precisely, all variable assignments in a function store the value in the local symbol table; whereas variable references first look in the local symbol table, then in the local symbol tables of enclosing functions, then in the global symbol table, and finally in the table of built-in names. Thus, global variables and variables of enclosing functions cannot be directly assigned a value within a function (unless, for global variables, named in a global statement, or, for variables of enclosing functions, named in a nonlocal statement), although they may be referenced.\n\nThe actual parameters (arguments) to a function call are introduced in the local symbol table of the called function when it is called; thus, arguments are passed using call by value (where the value is always an object reference, not the value of the object). 1 When a function calls another function, or calls itself recursively, a new local symbol table is created for that call.\n\nA function definition associates the function name with the function object in the current symbol table. The interpreter recognizes the object pointed to by that name as a user-defined function. Other names can also point to that same function object and can also be used to access the function:\n\n4.8.8. Function Annotations\nFunction annotations are completely optional metadata information about the types used by user-defined functions (see PEP 3107 and PEP 484 for more information).\n\nAnnotations are stored in the __annotations__ attribute of the function as a dictionary and have no effect on any other part of the function. Parameter annotations are defined by a colon after the parameter name, followed by an expression evaluating to the value of the annotation. Return annotations are defined by a literal ->, followed by an expression, between the parameter list and the colon denoting the end of the def statement. The following example has a required argument, an optional argument, and the return value annotated:\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to remove old binary files in the installation dir using cmake?", "id": 25, "answers": [{"answer_id": 26, "document_id": 43, "question_id": 25, "text": "You can remove files using the file command in cmake. Here is an example to remove old binaries, just define EXECUTABLE_OUTPUT_PATH.", "answer_start": 625, "answer_category": null}], "is_impossible": false}, {"question": "How to build  makefiles under the MSYS shell?", "id": 26, "answers": [{"answer_id": 29, "document_id": 43, "question_id": 26, "text": "use the MSYS Makefiles generator", "answer_start": 378, "answer_category": null}], "is_impossible": false}, {"question": "What is the build tool of Cmake under windows?", "id": 27, "answers": [{"answer_id": 28, "document_id": 43, "question_id": 27, "text": "mingw32-make", "answer_start": 190, "answer_category": null}], "is_impossible": false}], "context": "\nGenerates makefiles for use with mingw32-make under a Windows command prompt.Use this generator under a Windows command prompt with MinGW (Minimalist GNU for Windows) in the PATH and using mingw32-make as the build tool.  The generated makefiles use cmd.exe as the shell to launch build rules.  They are not compatible with MSYS or a unix shell. To build under the MSYS shell, use the MSYS Makefiles generator.\n\nI am using cmake to build and install my C++ application.The target property RUNTIME_OUTPUT_DIRECTORY supersedes this variable for a target if it is set. Executable targets are otherwise placed in this directory.You can remove files using the file command in cmake. Here is an example to remove old binaries, just define EXECUTABLE_OUTPUT_PATH.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Installing node.js on Debian 6.0", "id": 1198, "answers": [{"answer_id": 1191, "document_id": 774, "question_id": 1198, "text": "1.\tsudo apt-get update && sudo apt-get install git-core curl build-essential openssl libssl-dev\n2.\tgit clone https://github.com/nodejs/node.git\n3.\tcd node\n4.\tgit checkout v5.0.0 (or the version you want to build)\n5.\t./configure\n6.\tmake\n7.\tsudo make install\n8.\tnode -v\n9.\tnpm -v", "answer_start": 535, "answer_category": null}], "is_impossible": false}], "context": "Now I would like to install Node.js, but none of the tutorials on the web seem to work. I have tried all of the results google gives me when I enter \"install node.js on linux\" I mostly failed because I had no idea how to install the dependencies (I'm new to linux btw)\nHow to install node.js on a Debian system that was just setup (so no programs already installed on it and such)?\n101\nIf anyone else needs step-by-step instructions, I used the following steps from Install Node.js on Debian Squeeze and it installed with no problems:\n1.\tsudo apt-get update && sudo apt-get install git-core curl build-essential openssl libssl-dev\n2.\tgit clone https://github.com/nodejs/node.git\n3.\tcd node\n4.\tgit checkout v5.0.0 (or the version you want to build)\n5.\t./configure\n6.\tmake\n7.\tsudo make install\n8.\tnode -v\n9.\tnpm -v\nI didn't have to install any additional sources into apt to get it to build.\n\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "how to update ruby to 1 9 x on mac?", "id": 873, "answers": [{"answer_id": 868, "document_id": 554, "question_id": 873, "text": "I'll make a strong suggestion for rvm.\n\nIt's a great way to manage multiple Rubies and gems sets without colliding with the system version.", "answer_start": 2644, "answer_category": null}], "is_impossible": false}], "context": "As The Tin Man suggests (above) RVM (Ruby Version Manager) is the Standard for upgrading your Ruby installation on OSX: https://rvm.io\n\nTo get started, open a Terminal Window and issue the following command:\n\n\\curl -L https://get.rvm.io | bash -s stable --ruby\n( you will need to trust the RVM Dev Team that the command is not malicious - if you're a paranoid penguin like me, you can always go read the source: https://github.com/wayneeseguin/rvm ) When it's complete you need to restart the terminal to get the rvm command working.\n\nrvm list known\n( shows you the latest available versions of Ruby )\n\nrvm install ruby-2.3.1\nFor a specific version, followed by\n\nrvm use ruby-2.3.1\nor if you just want the latest (current) version:\n\nrvm install current && rvm use current\n( installs the current stable release - at time of writing ruby-2.3.1 - please update this wiki when new versions released )\n\nNote on Compiling Ruby: In my case I also had to install Homebrew http://mxcl.github.com/homebrew/ to get the gems I needed (RSpec) which in turn forces you to install Xcode (if you haven't already) https://itunes.apple.com/us/app/xcode/id497799835 AND/OR install the GCC package from: https://github.com/kennethreitz/osx-gcc-installer to avoid errors running \"make\".\n\nEdit: As of Mavericks you can choose to install only the Xcode command line tools instead of the whole Xcode package, which comes with gcc and lots of other things you might need for building packages. It can be installed by running xcode-select --install and following the on-screen prompt.\n\nExamples: https://rvm.io/workflow/examples/\nScreencast: http://screencasts.org/episodes/how-to-use-rvm\nNote on erros: if you get the error \"RVM is not a function\" while trying this command, visit: How do I change my Ruby version using RVM? for the solution.\n\nShare\nImprove this answer\nFollow\nedited May 23 '17 at 11:47\ncommunity wiki\n8 revs, 5 users 81%\nnelsonic\nSetting all of this up on Mountain Lion takes a lot of patience. If you run into an error \"Error running 'env GEM_PATH=/Users/...\" it's not your paths, and you will see in the log that it's that openssl (which is probably installed) is busted in RVM and they provide a page on working through it rvm.io/packages/openssl \u2013 \nDylan Valade\n Mar 22 '13 at 20:20\n4\nI truly hate the mess that is Ruby. It's a shambles. However, this answer got me on the right path and many thanks indeed for that! \u2013 \nIan Lewis\n Feb 23 '14 at 23:43\n1\nWandering between posts which lead me to other problems, I found this one very helpful as it explains how to use RVM, not only use it. \u2013 \nHector Ordonez\n Feb 25 '15 at 10:12\nAdd a comment\n\n\n66\n\nI'll make a strong suggestion for rvm.\n\nIt's a great way to manage multiple Rubies and gems sets without colliding with the system version.\n\nI'll add that now (4/2/2013), I use rbenv a lot, because my needs are simple. RVM is great, but it's got a lot of capability I never need, so I have it on some machines and rbenv on my desktop and laptop. It's worth checking out both and seeing which works best for your needs.\n\nShare\nImprove this answer\nFollow\nedited Apr 2 '13 at 18:17\nanswered Sep 13 '10 at 4:19\n\nthe Tin Man\n153k3939 gold badges203203 silver badges282282 bronze badges\nAgreed. And even if multiple versions aren't an issue (ha!) rvm is great for finding out which native libraries are missing. Pretty much indispensable. \u2013 \nIsaac Rabinovitch\n Apr 2 '13 at 18:04\nAdd a comment\n\n19\n\nWith brew this is a one-liner:\n\n(assuming that you have tapped homebrew/versions, which can be done by running brew tap homebrew/versions)\n\nbrew install ruby193\nWorked out of the box for me on OS X 10.8.4. Or if you want 2.0, you just brew install ruby\n\nMore generally, brew search ruby shows you the different repos available, and if you want to get really specific you can use brew versions ruby and checkout a specific version instead.\n\nShare\nImprove this answer\nFollow\nedited Aug 7 '14 at 4:50\n\nLuiz Berti\n1,19111 gold badge1313 silver badges2323 bronze badges\nanswered Aug 18 '13 at 5:01\n\nNils\n5,33644 gold badges3232 silver badges3636 bronze badges\n1\nGreat one-liner for installing a specific version of ruby! switching between ruby versions with brew is not quite as easy ... stackoverflow.com/questions/8730676 \u2013 \nnelsonic\n Nov 25 '13 at 11:38\nAdd a comment\n\n\n4\n\nI know it's an older post, but i wanna add some extra informations about that. Firstly, i think that rvm does great BUT it wasn't updating ruby from my system (MAC OS Yosemite).\n\nWhat rvmwas doing : installing to another location and setting up the path there to my environment variable ... And i was kinda bored, because i had two ruby now on my system.\n\nSo to fix that, i uninstalled the rvm, then used the Homebrew package manager available here and installed ruby throw terminal command by doing brew install ruby.\n\nAnd then, everything was working perfectly ! The ruby from my system was updated ! Hope it will help for the next adventurers !\n\nShare\nImprove this answer\nFollow\nanswered Mar 19 '15 at 16:31\n\nDamiii\n1,26733 gold badges2222 silver badges4444 bronze badges\n1\nI was struggling to decide whether should I use rvm or brew. this helps. thanks \u2013 \nunifreak\n Jun 29 '17 at 13:49\nDoesn't brew also do the same thing? \u2013 \nanon\n Jul 4 '17 at 0:53\nAdd a comment\n\n3\n\nI'll disagree with The Tin Man here. I regard rbenv as preferable to RVM. rbenv doesn't interfere drastically with your shell the way RVM does, and it lets you add separate Ruby installations in ordinary folders that you can examine directly. It allows you to compile Ruby yourself. Good outline of the differences here: https://github.com/sstephenson/rbenv/wiki/Why-rbenv%3F\n\nI provide instructions for compiling Ruby 1.9 for rbenv here. Further, more detailed information here. I have used this technique with easy success on Snow Leopard, Lion, and Mountain Lion.\n\nShare\nImprove this answer\nFollow\nedited May 23 '17 at 10:31\n\nCommunityBot\n111 silver badge\nanswered Jan 6 '13 at 16:14\n\nmatt\n467k7777 gold badges782782 silver badges10141014 bronze badges\n1\nAt the time this question was originally asked, rbenv wasn't an option. Times change, as does code. RVM is still a good choice, but it is heavyweight in comparison to rbenv. I use both on different machines, because they're both useful in different ways. \u2013 \nthe Tin Man\n Apr 2 '13 at 18:13\nAdd a comment\n\n2\n\nDan Benjamin's Hivelogic article Installing Ruby, RubyGems, and Rails on Snow Leopard is the recommended place to go although the article is for 1.8, so here's a Ruby 1.9-specific install on Snow Leopard. Watch out for the 64-bit thing... either go all 64-bit 'fat' (as is - for example - Apache on OS X, which can cause problems with 32-bit libraries) or check any gems you're likely to use to make sure they're okay for 64-bit.\n\nShare\nImprove this answer\nFollow\nedited Sep 12 '10 at 21:34\nanswered Sep 12 '10 at 21:00\n\nDave Everitt\n15.4k66 gold badges6363 silver badges8989 bronze badges\nAdd a comment\n\n2\n\nThis command actually works\n\n\\curl -L https://get.rvm.io | bash -s stable --ruby\n\nShare\nImprove this answer\nFollow\nedited May 15 '14 at 10:13\n\nNirMH\n4,31722 gold badges3939 silver badges6262 bronze badges\nanswered May 15 '14 at 9:42\n\nAnthony\n4155 bronze badges\nAdd a comment\n\n0\n\nAs previously mentioned, the bundler version may be too high for your version of rails.\n\nI ran into the same problem using Rails 3.0.1 which requires Bundler v1.0.0 - v1.0.22\n\nCheck your bundler version using: gem list bundler\n\nIf your bundler version is not within the appropriate range, I found this solution to work: rvm @global do gem uninstall bundler\n\nNote: rvm is required for this solution... another case for why you should be using rvm in the first place.\n\nShare\nImprove this answer\nFollow\nanswered Jan 13 '13 at 20:43\n\nwhiteshooz\n6111 silver badge77 bronze badges\nAdd a comment\n\n0\n\nThere are several other version managers to consider, see for a few examples and one that's not listed there that I'll be giving a try soon is ch-ruby. I tried rbenv but had too many problems with it. RVM is my mainstay, though it sometimes has the odd problem (hence my wish to try ch-ruby when I get a chance). I wouldn't touch the system Ruby, as other things may rely on it.\n\nI should add I've also compiled my own Ruby several times, and using the Hivelogic article (as Dave Everitt has suggested) is a good idea if you take that route.", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "ExecJS::ProgramError: Unexpected token punc \u00ab(\u00bb, expected punc \u00ab:\u00bb when running rake assets:precompile on production", "id": 1696, "answers": [{"answer_id": 1684, "document_id": 1269, "question_id": 1696, "text": "Run rails console and:\nJS_PATH = \"app/assets/javascripts/**/*.js\"; \nDir[JS_PATH].each do |file_name|\n  puts \"\\n#{file_name}\"\n  puts Uglifier.compile(File.read(file_name), harmony: true)\nend", "answer_start": 318, "answer_category": null}], "is_impossible": false}], "context": "The file in question is valid, it works on localhost. I also tried running rake assests:precompile on localhost, it all passes. Finally, I tried to remove the content from the file, git push and redeploy - still got the same error. Only completely removing the file and re-deploying helps.\nWould appreciate any ideas.\nRun rails console and:\nJS_PATH = \"app/assets/javascripts/**/*.js\"; \nDir[JS_PATH].each do |file_name|\n  puts \"\\n#{file_name}\"\n  puts Uglifier.compile(File.read(file_name), harmony: true)\nend\nIt will show you the file and the line where the Uglifier is making the problem.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to automate migration (schema and data) for PHP/MySQL application", "id": 562, "answers": [{"answer_id": 565, "document_id": 287, "question_id": 562, "text": "What you want to do is create a 'db_schema_versions' table:\nCREATE TABLE db_schema_versions (\n  `table` varchar(255) NOT NULL PRIMARY KEY, \n  `version` INT NOT NULL\n)\nAfter your database can track what version # it is on - it can do SQL upgrades automatically.", "answer_start": 371, "answer_category": null}], "is_impossible": false}], "context": "I have an application in PHP/MySQL. I am searching for an automated way upgrading database behind the application. I don't need to have the compatibility with older versions once it is upgraded.\nI have read jeff's and K. Scott Allen's articles on this.\nI am still not sure how to implement this for a PHP/MySQL application.\nIs there any simple and good process for this?\nWhat you want to do is create a 'db_schema_versions' table:\nCREATE TABLE db_schema_versions (\n  `table` varchar(255) NOT NULL PRIMARY KEY, \n  `version` INT NOT NULL\n)\nAfter your database can track what version # it is on - it can do SQL upgrades automatically.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Deploying assembly package with maven-release-plugin\t", "id": 547, "answers": [{"answer_id": 549, "document_id": 272, "question_id": 547, "text": "Now I have a project which contains an assembly that puts together all needed components and then packages them into a .tar.gz package with the desired directory structure.", "answer_start": 69, "answer_category": null}], "is_impossible": false}], "context": "We use Hudson and the maven-release-plugin to do the release builds. Now I have a project which contains an assembly that puts together all needed components and then packages them into a .tar.gz package with the desired directory structure.", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to develop containerized applications?", "id": 76, "answers": [{"answer_id": 80, "document_id": 68, "question_id": 76, "text": "eate and test individual containers for each component of your application by first creating Docker images.\n\n\nAssemble your containers and supporting infrastructure into a complete application.\n\n\nTest, share, and deploy your complete containerized application.\n\n\n", "answer_start": 512, "answer_category": null}], "is_impossible": false}], "context": "Build and run your image\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSearch\n\n\n\n\n\n\n\nToggle navigation\n\n\n\n\n\n\n\nHomeGuidesProduct manualsReferenceSamples\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBuild and run your imageEstimated reading time: 8 minutes\nOrientation and setup\nBuild and run your image\nShare images on Docker Hub\n\nPrerequisites\ud83d\udd17\nWork through the orientation and setup in Part 1.\nIntroduction\ud83d\udd17\nNow that you\u2019ve set up your development environment, you can begin to develop containerized applications. In general, the development workflow looks like this:\n\n\nCreate and test individual containers for each component of your application by first creating Docker images.\n\n\nAssemble your containers and supporting infrastructure into a complete application.\n\n\nTest, share, and deploy your complete containerized application.\n\n\nIn this stage of the tutorial, let\u2019s focus on step 1 of this workflow: creating the images that your containers will be based on. Remember, a Docker image captures the private filesystem that your containerized processes will run in; you need to create an image that contains just what your application needs to run.\nSet up\ud83d\udd17\nLet us download the node-bulletin-board example project. This is a simple bulletin board application written in Node.js.\n\nGit\nWindows (without Git)\nMac or Linux (without Git)\n\n\n\nGit\ud83d\udd17\nIf you are using Git, you can clone the example project from GitHub:\n\n\ngit clone https://github.com/dockersamples/node-bulletin-board\ncd node-bulletin-board/bulletin-board-app\n\n\n\n\n\nWindows (without Git)\ud83d\udd17\nIf you are using a Windows machine and prefer to download the example project without installing Git, run the following commands in PowerShell:\n\n\ncurl.exe -LO https://github.com/dockersamples/node-bulletin-board/archive/master.zip\ntar.exe xf master.zip\ncd node-bulletin-board-master\\bulletin-board-app\n\n\n\n\n\nMac or Linux (without Git)\ud83d\udd17\nIf you are using a Mac or a Linux machine and prefer to download the example project without installing Git, run the following commands in a terminal:\n\n\ncurl -LO https://github.com/dockersamples/node-bulletin-board/archive/master.zip\nunzip master.zip\ncd node-bulletin-board-master/bulletin-board-app\n\n\n\n\n\n\nDefine a container with Dockerfile\ud83d\udd17\nAfter downloading the project, take a look at the file called Dockerfile in the bulletin board application. Dockerfiles describe how to assemble a private filesystem for a container, and can also contain some metadata describing how to run a container based on this image.\nFor more information about the Dockerfile used in the bulletin board application, see Sample Dockerfile.\nBuild and test your image\ud83d\udd17\nNow that you have some source code and a Dockerfile, it\u2019s time to build your first image, and make sure the containers launched from it work as expected.\nMake sure you\u2019re in the directory node-bulletin-board/bulletin-board-app in a terminal or PowerShell using the cd command. Run the following command to build your bulletin board image:\ndocker build --tag bulletinboard:1.0 .\n\nYou\u2019ll see Docker step through each instruction in your Dockerfile, building up your image as it goes. If successful, the build process should end with a message Successfully tagged bulletinboard:1.0.\n\nWindows users:\nThis example uses Linux containers. Make sure your environment is running Linux containers by right-clicking on the Docker logo in your system tray, and clicking Switch to Linux containers. Don\u2019t worry - all the commands in this tutorial work the exact same way for Windows containers.\nYou may receive a message titled \u2018SECURITY WARNING\u2019 after running the image, noting the read, write, and execute permissions being set for files added to your image. We aren\u2019t handling any sensitive information in this example, so feel free to disregard the warning in this example.\n\nRun your image as a container\ud83d\udd17\n\n\nRun the following command to start a container based on your new image:\ndocker run --publish 8000:8080 --detach --name bb bulletinboard:1.0\n\nThere are a couple of common flags here:\n\n--publish asks Docker to forward traffic incoming on the host\u2019s port 8000 to the container\u2019s port 8080. Containers have their own private set of ports, so if you want to reach one from the network, you have to forward traffic to it in this way. Otherwise, firewall rules will prevent all network traffic from reaching your container, as a default security posture.\n--detach asks Docker to run this container in the background.\n--name specifies a name with which you can refer to your container in subsequent commands, in this case bb.\n\n\n\nVisit your application in a browser at localhost:8000. You should see your bulletin board application up and running. At this step, you would normally do everything you could to ensure your container works the way you expected; now would be the time to run unit tests, for example.\n\n\nOnce you\u2019re satisfied that your bulletin board container works correctly, you can delete it:\ndocker rm --force bb\n\nThe --force option stops a running container, so it can be removed. If you stop the container running with docker stop bb first, then you do not need to use --force to remove it.\n\n\nConclusion\ud83d\udd17\nAt this point, you\u2019ve successfully built an image, performed a simple containerization of an application, and confirmed that your app runs successfully in its container. The next step will be to share your images on Docker Hub, so they can be easily downloaded and run on any destination machine.\nOn to Part 3 >>\nDeploying to the cloud\ud83d\udd17\nTo run your containers in the cloud with either Azure or AWS, check out our docs on getting started with cloud deployments.\n\nDeploying with Docker and AWS\nDeploying with Docker and Azure\n\nSample Dockerfile\ud83d\udd17\nWriting a Dockerfile is the first step to containerizing an application. You can think of these Dockerfile commands as a step-by-step recipe on how to build up your image. The Dockerfile in the bulletin board app looks like this:\n# Use the official image as a parent image.\nFROM node:current-slim\n\n# Set the working directory.\nWORKDIR /usr/src/app\n\n# Copy the file from your host to your current location.\nCOPY package.json .\n\n# Run the command inside your image filesystem.\nRUN npm install\n\n# Add metadata to the image to describe which port the container is listening on at runtime.\nEXPOSE 8080\n\n# Run the specified command within the container.\nCMD [ \"npm\", \"start\" ]\n\n# Copy the rest of your app's source code from your host to your image filesystem.\nCOPY . .\n\nThe dockerfile defined in this example takes the following steps:\n\nStart FROM the pre-existing node:current-slim image. This is an official image, built by the node.js vendors and validated by Docker to be a high-quality image containing the Node.js Long Term Support (LTS) interpreter and basic dependencies.\nUse WORKDIR to specify that all subsequent actions should be taken from the directory /usr/src/app in your image filesystem (never the host\u2019s filesystem).\nCOPY the file package.json from your host to the present location (.) in your image (so in this case, to /usr/src/app/package.json)\nRUN the command npm install inside your image filesystem (which will read package.json to determine your app\u2019s node dependencies, and install them)\nCOPY in the rest of your app\u2019s source code from your host to your image filesystem.\n\nYou can see that these are much the same steps you might have taken to set up and install your app on your host. However, capturing these as a Dockerfile allows you to do the same thing inside a portable, isolated Docker image.\nThe steps above built up the filesystem of our image, but there are other lines in your Dockerfile.\nThe CMD directive is the first example of specifying some metadata in your image that describes how to run a container based on this image. In this case, it\u2019s saying that the containerized process that this image is meant to support is npm start.\nThe EXPOSE 8080 informs Docker that the container is listening on port 8080 at runtime.\nWhat you see above is a good way to organize a simple Dockerfile; always start with a FROM command, follow it with the steps to build up your private filesystem, and conclude with any metadata specifications. There are many more Dockerfile directives than just the few you see above. For a complete list, see the Dockerfile reference.\nCLI references\ud83d\udd17\nFurther documentation for all CLI commands used in this article are available here:\n\ndocker image\ndocker container\nDockerfile reference\n\ncontainers, images, dockerfiles, node, code, coding, build, push, runRate this page:\u00a08762\u00a0681\u00a0i\n\n\n\n\n\nDocker overviewGet DockerGet startedPart 1: Orientation and setupPart 2: Build and run your imagePart 3: Share images on Docker HubDevelop with DockerOverviewNode.jsBuild imagesRun containersDevelopBest practicesBuild imagesDockerfile best practicesBuild images with BuildKitUse multi-stage buildsManage imagesCreate your own base image (advanced)Set up CI/CDCI/CD Best practicesConfigure GitHub ActionsDeploy your app to the cloudDocker and ACIDocker and ECSRun your app in productionOrchestrationOverviewDeploy to KubernetesDeploy to SwarmConfigure all objectsApply custom metadata to objectsPrune unused objectsFormat command and log outputConfigure the daemonConfigure and run DockerControl Docker with systemdCollect metrics with PrometheusConfigure containersStart containers automaticallyKeep containers alive during daemon downtimeRun multiple services in a containerContainer runtime metricsRuntime options with Memory, CPUs, and GPUsLoggingView a container's logsConfigure logging driversUse docker logs with a logging driverUse a logging driver pluginCustomize log driver outputLogging driver detailsLocal file logging driverLogentries logging driverJSON File logging driverGraylog Extended Format (GELF) logging driverSyslog logging driverAmazon CloudWatch logs logging driverETW logging driverFluentd logging driverGoogle Cloud logging driverJournald logging driverSplunk logging driverSecurityDocker securityDocker security non-eventsProtect the Docker daemon socketUsing certificates for repository client verificationUse trusted imagesOverviewAutomationDelegationsDeploy NotaryManage content trust keysPlay in a content trust sandboxAntivirus softwareAppArmor security profilesSeccomp security profilesIsolate containers with a user namespaceRootless modeScale your appSwarm mode overviewSwarm mode key conceptsGet started with swarm modeSet up for the tutorialCreate a swarmAdd nodes to the swarmDeploy a serviceInspect the serviceScale the serviceDelete the serviceApply rolling updatesDrain a nodeUse swarm mode routing meshHow swarm mode worksHow nodes workHow services workManage swarm security with PKISwarm task statesRun Docker in swarm modeJoin nodes to a swarmManage nodes in a swarmDeploy services to a swarmStore service configuration dataManage sensitive data with Docker secretsLock your swarmSwarm administration guideRaft consensus in swarm modeExtend DockerManaged plugin systemAccess authorization pluginExtending Docker with pluginsDocker network driver pluginsVolume pluginsPlugin configurationPluginsConfigure networkingNetworking overviewUse bridge networksUse overlay networksUse host networkingUse Macvlan networksDisable networking for a containerNetworking tutorialsBridge network tutorialHost networking tutorialOverlay networking tutorialMacvlan network tutorialConfigure the daemon and containersConfigure the daemon for IPv6Docker and iptablesContainer networkingConfigure Docker to use a proxy serverLegacy networking content(Legacy) Container linksManage application dataStorage overviewVolumesBind mountstmpfs mountsTroubleshoot volume problemsStore data within containersAbout storage driversSelect a storage driverUse the AUFS storage driverUse the Btrfs storage driverUse the Device mapper storage driverUse the OverlayFS storage driverUse the ZFS storage driverUse the VFS storage driverEducational resourcesOpen source at DockerContribute to documentationOther ways to contributeDocumentation archive\n\n\n\n\n\n\n\nEdit this page Request docs changes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn this page:\n\nPrerequisites\nIntroduction\nSet up\n\nGit\nWindows (without Git)\nMac or Linux (without Git)\n\n\nDefine a container with Dockerfile\nBuild and test your image\nRun your image as a container\nConclusion\nDeploying to the cloud\nSample Dockerfile\nCLI references\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhy Docker?\nWhat is a Container?\n\n\n\n\nProducts\nDocker Desktop\nDocker Hub\nFeatures\nContainer Runtime\nDeveloper Tools\nKubernetes\n\n\n\n\nDevelopers\nUse Cases\nPlay with Docker\nCommunity\nOpen Source\nDocker Captains\n\n\n\n\nCompany\nAbout Us\nBlog\nCustomers\nPartners\nNewsroom\nCareers\nContact Us\n\n\n\n\n\n\nStatus\nSecurity\nLegal\nContact\n\n\n\n\n\n\n\nCopyright \u00a9 2013-2020 Docker Inc. All rights reserved.\n\n\n\nTwitter\nYoutube\nGitHub\nLinkedin\nFacebook\nSlideshare\nReddit\n\n\n\n\n\n\n\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to use mpi on Mac OS X", "id": 1771, "answers": [{"answer_id": 1757, "document_id": 1342, "question_id": 1771, "text": "If you have anaconda installed, this is the fastest option for me:\nconda install mpi4py openmpi", "answer_start": 263, "answer_category": null}], "is_impossible": false}], "context": "I have been searching for a way to use mpi on my mac but everything is very advanced.\nI have successfully installed open-mpi using\nbrew install open-mpi\nI have .c files ready for compiling and running. When I type:\nmpicc -o <file> <file.c>\nWhat am I doing wrong?\nIf you have anaconda installed, this is the fastest option for me:\nconda install mpi4py openmpi\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Is it necessary to deploy the XML file in a class library?", "id": 1066, "answers": [{"answer_id": 1059, "document_id": 644, "question_id": 1066, "text": "You only need it if you want Visual Studio to show documentation in IntelliSense.\nIf you're deploying a consumer-facing app (as opposed to a developer-facing reusable library), you do not need it.", "answer_start": 311, "answer_category": null}], "is_impossible": false}], "context": "I have developed a lot of class library projects in VS 2012 to be used in Windows Forms and Web forms applications.\nThe question is simple. Do I need to deploy the DLL file itself together with the XML file that is created?\nThe XML file contains the doc comments for the public types & members in the assembly.\nYou only need it if you want Visual Studio to show documentation in IntelliSense.\nIf you're deploying a consumer-facing app (as opposed to a developer-facing reusable library), you do not need it.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Certificate Verify Failed in \"gem install foundation\"", "id": 1733, "answers": [{"answer_id": 1720, "document_id": 1305, "question_id": 1733, "text": "If you're using RVM (highly recommended) you can run\nrvm osx-ssl-certs update all", "answer_start": 414, "answer_category": null}], "is_impossible": false}], "context": "Trying to install Foundation after installing nodejs, ruby, and git plus bower.\nI keep getting the following error and I dont know how to fix it:\nCould not find a valid gem 'foundation' (>= 0), here is why: Unable to download data from https://rubygems.org/ - SSL_connect returned=1 errno=0 state=SSLv3 read server certificate B: certificate verify failed (https://api.rubygems.org/latest_specs.4.8.gz)\nAny ideas?\nIf you're using RVM (highly recommended) you can run\nrvm osx-ssl-certs update all\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Does IntelliJ IDEA work on a 64-bit Windows system?", "id": 1143, "answers": [{"answer_id": 1136, "document_id": 720, "question_id": 1143, "text": "In C:\\Program Files (x86)\\JetBrains\\${Intellij Idea version}\\bin, there is an executable called idea64.exe. This is for 64-bit systems. You can simply change your IntelliJ shortcut path to it.", "answer_start": 321, "answer_category": null}], "is_impossible": false}], "context": "I have downloaded and installed IDEA 13.1. However, even having a 64-bit system, Windows has automatically installed the program in the folder Program Files (x86), which is dedicated for 32-bit programs.\nIs there another download for the 64-bit version or maybe a special setting to make it suitable for a 64-bit system? In C:\\Program Files (x86)\\JetBrains\\${Intellij Idea version}\\bin, there is an executable called idea64.exe. This is for 64-bit systems. You can simply change your IntelliJ shortcut path to it.\nYou also need to add a new environment variable IDEA_JDK_64 pointing to your 64-bit JDK so IntelliJ can use a 64-bit JDK.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How can I use setuptools to generate a console_scripts entry point which calls `python -m mypackage`?", "id": 587, "answers": [{"answer_id": 593, "document_id": 312, "question_id": 587, "text": "I think this is the wrong way to look at the problem. You don't want your script to call python -m mypackage, but you want the script to have the same entry point as python -m mypackage", "answer_start": 204, "answer_category": null}], "is_impossible": false}], "context": "I am trying to be a good Pythonista and following PEP 338 for my package I plan on deploying.\nHow can I use entry_points to generate a binary that calls python -m mypackage (and passes *args, **kwargs) ?\nI think this is the wrong way to look at the problem. You don't want your script to call python -m mypackage, but you want the script to have the same entry point as python -m mypackage\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "WFMLRSVCApp.ear file not found", "id": 1202, "answers": [{"answer_id": 1195, "document_id": 778, "question_id": 1202, "text": "copy what is inside components folder ( i think four files ) in win64_11gR2_database_2of2.zip and paste it inside the win64_11gR2_database_1of2.zip components folder and re run the setup .", "answer_start": 291, "answer_category": null}], "is_impossible": false}], "context": "I have downloaded win64_11gR2_database_1of2.zip and win64_11gR2_database_2of2.zip.\nI have put them in the same folder and started the setup file from the 1of2 folder. I go until certain steps and finally during installation i get WFMLRSVCApp.ear file not found popup.\nWhat you have to do is copy what is inside components folder ( i think four files ) in win64_11gR2_database_2of2.zip and paste it inside the win64_11gR2_database_1of2.zip components folder and re run the setup . it works\nI googled and i see some solutions where 1of2 and 2of2 file should be unzipped to same folder and then start setup. I did the same couple of times but still im getting the same error. Any anyone guess what is missing?\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Django - Deployment for a newbie", "id": 1278, "answers": [{"answer_id": 1270, "document_id": 849, "question_id": 1278, "text": "You can see this blog: http://rogueleaderr.com/post/65157477648/the-idiomatic-guide-to-deploying-django-in-production", "answer_start": 533, "answer_category": null}], "is_impossible": false}], "context": "I have finished my first Django/Python Project and now I need to put the things working in a real production webserver.\nI have read some papers over the Internet and I'm inclined to choose Ngix, Gunicorn and Git but the documentation that I found is not very complete and I have many doubs if this is the best option.\nWhat do you think about this subject? I need a simple way to put my Django Project on-line but the website is still very buggy, I will need to change the code many times in the production server in the times ahead.\nYou can see this blog: http://rogueleaderr.com/post/65157477648/the-idiomatic-guide-to-deploying-django-in-production\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "how to install mongodb on windows?", "id": 899, "answers": [{"answer_id": 894, "document_id": 566, "question_id": 899, "text": "I. Download the zip file http://www.mongodb.org/downloads\nII. Extract it and copy the files into your desired location.\nIII. Start the DB engine.\nIV. Test the installation and use it.", "answer_start": 2260, "answer_category": null}], "is_impossible": false}, {"question": "How to create database directory in MongoDB.", "id": 900, "answers": [{"answer_id": 895, "document_id": 566, "question_id": 900, "text": "From the Command Interpreter, create the data directories:\n\ncd C:\\\nmd \"\\data\\db\"", "answer_start": 6383, "answer_category": null}], "is_impossible": false}], "context": "Install MongoDB Community Edition on Windows\nNOTE\nMongoDB Atlas\nMongoDB Atlas is a hosted MongoDB service option in the cloud which requires no installation overhead and offers a free tier to get started.\n\nOverview\nUse this tutorial to install MongoDB 5.0 Community Edition on Windows using the default installation wizard.\n\nMongoDB Version\nThis tutorial installs MongoDB 5.0 Community Edition. To install a different version of MongoDB Community, use the version drop-down menu in the upper-left corner of this page to select the documentation for that version.\n\nInstallation Method\nThis tutorial installs MongoDB on Windows using the default MSI installation wizard. To install MongoDB using the msiexec.exe command-line tool instead, see Install MongoDB using msiexec.exe. The msiexec.exe tool is useful for system administrators who wish to deploy MongoDB in an unattended fashion using automation.\n\nConsiderations\nMongoDB Shell, mongosh\nThe MongoDB Shell (mongosh) is not installed with MongoDB Server. You need to follow the mongosh installation instructions to download and install mongosh separately.\n\nPlatform Support\nMongoDB 5.0 Community Edition supports the following 64-bit versions of Windows on x86_64 architecture:\n\nWindows Server 2019\nWindows 10 / Windows Server 2016\nMongoDB only supports the 64-bit versions of these platforms.\n\nSee Supported Platforms for more information.\n\nVirtualization\nOracle offers experimental support for VirtualBox on Windows hosts where Hyper-V is running. However, Microsoft does not support VirtualBox on Hyper-V.\n\nDisable Hyper-V if you want to install MongoDB on Windows using VirtualBox.\n\nProduction Notes\nBefore deploying MongoDB in a production environment, consider the Production Notes document which offers performance considerations and configuration recommendations for production MongoDB deployments.\n\nInstall MongoDB Community Edition\nProcedure\nFollow these steps to install MongoDB Community Edition using the MongoDB Installer wizard. The installation process installs both the MongoDB binaries as well as the default configuration file <install directory>\\bin\\mongod.cfg.\n\n1\nDownload the installer.\nDownload the MongoDB Community .msi installer from the following link:\n\n\u27a4 MongoDB Download Center\nI. Download the zip file http://www.mongodb.org/downloads\nII. Extract it and copy the files into your desired location.\nIII. Start the DB engine.\nIV. Test the installation and use it.\nIn the Version dropdown, select the version of MongoDB to download.\nIn the Platform dropdown, select Windows.\nIn the Package dropdown, select msi.\nClick Download.\n2\nRun the MongoDB installer.\nFor example, from the Windows Explorer/File Explorer:\n\nGo to the directory where you downloaded the MongoDB installer (.msi file). By default, this is your Downloads directory.\nDouble-click the .msi file.\n3\nFollow the MongoDB Community Edition installation wizard.\nThe wizard steps you through the installation of MongoDB and MongoDB Compass.\n\nChoose Setup Type\nYou can choose either the Complete (recommended for most users) or Custom setup type. The Complete setup option installs MongoDB and the MongoDB tools to the default location. The Custom setup option allows you to specify which executables are installed and where.\nService Configuration\nStarting in MongoDB 4.0, you can set up MongoDB as a Windows service during the install or just install the binaries.\n\nInstall MongoDB Compass\nOptional. To have the wizard install MongoDB Compass, select Install MongoDB Compass (Default).\nWhen ready, click Install.\nInstall mongosh\nThe .msi installer does not include mongosh. Follow the mongosh installation instructions to download and install the shell separately.\n\nIf You Installed MongoDB as a Windows Service\nThe MongoDB service starts upon successful installation. Configure the MongoDB instance with the configuration file <install directory>\\bin\\mongod.cfg.\n\nIf You Did Not Install MongoDB as a Windows Service\nIf you only installed the executables and did not install MongoDB as a Windows service, you must manually start the MongoDB instance.\n\nSee Run MongoDB Community Edition from the Command Interpreter for instructions to start a MongoDB instance.\n\nRun MongoDB Community Edition as a Windows Service\nStarting in version 4.0, you can install and configure MongoDB as a Windows Service during installation. The MongoDB service starts upon successful installation. Configure the MongoDB instance with the configuration file <install directory>\\bin\\mongod.cfg.\n\nIf you have not already done so, follow the mongosh installation instructions to download and install the MongoDB Shell (mongosh).\n\nBe sure to add the path to your mongosh.exe binary to your PATH environment variable during installation.\n\nOpen a new Command Interpreter and enter mongosh.exe to connect to MongoDB.\n\nFor more information on connecting to a mongod using mongosh.exe, such as connecting to a MongoDB instance running on a different host and/or port, see Connect to a Deployment.\n\nFor information on CRUD (Create, Read, Update, Delete) operations, see:\n\nInsert Documents\nQuery Documents\nUpdate Documents\nDelete Documents\nStart MongoDB Community Edition as a Windows Service\nTo start/restart the MongoDB service, use the Services console:\n\nFrom the Services console, locate the MongoDB service.\nRight-click on the MongoDB service and click Start.\nStop MongoDB Community Edition as a Windows Service\nTo stop/pause the MongoDB service, use the Services console:\n\nFrom the Services console, locate the MongoDB service.\nRight-click on the MongoDB service and click Stop (or Pause).\nRemove MongoDB Community Edition as a Windows Service\nTo remove the MongoDB service, first use the Services console to stop the service. Then open a Windows command prompt/interpreter (cmd.exe) as an Administrator, and run the following command:\n\nsc.exe delete MongoDB\nRun MongoDB Community Edition from the Command Interpreter\nYou can run MongoDB Community Edition from the Windows command prompt/interpreter (cmd.exe) instead of as a service.\n\nOpen a Windows command prompt/interpreter (cmd.exe) as an Administrator.\n\nIMPORTANT\nYou must open the command interpreter as an Administrator.\n\n1\nCreate database directory.\nCreate the data directory where MongoDB stores data. MongoDB's default data directory path is the absolute path \\data\\db on the drive from which you start MongoDB.\n\nFrom the Command Interpreter, create the data directories:\n\ncd C:\\\nmd \"\\data\\db\"\n2\nStart your MongoDB database.\nTo start MongoDB, run exe.\n\n\"C:\\Program Files\\MongoDB\\Server\\5.0\\bin\\mongod.exe\" --dbpath=\"c:\\data\\db\"\nThe --dbpath option points to your database directory.\n\nIf the MongoDB database server is running correctly, the Command Interpreter displays:\n\n[initandlisten] waiting for connections\nIMPORTANT\nDepending on the Windows Defender Firewall settings on your Windows host, Windows may display a Security Alert dialog box about blocking \"some features\" of C:\\Program Files\\MongoDB\\Server\\5.0\\bin\\mongod.exe from communicating on networks. To remedy this issue:\n\nClick Private Networks, such as my home or work network.\nClick Allow access.\nTo learn more about security and MongoDB, see the Security Documentation.\n\n3\nConnect to MongoDB.\nIf you have not already done so, follow the mongosh installation instructions to download and install the MongoDB Shell (mongosh).\n\nBe sure to add the path to your mongosh.exe binary to your PATH environment variable during installation.\n\nOpen a new Command Interpreter and enter mongosh.exe to connect to MongoDB.\n\nFor more information on connecting to mongod using mongosh.exe, such as connecting to a MongoDB instance running on a different host and/or port, see Connect to a Deployment.\n\nFor information on CRUD (Create, Read, Update, Delete) operations, see:\n\nInsert Documents\nQuery Documents\nUpdate Documents\nDelete Documents\nAdditional Considerations\nLocalhost Binding by Default\nBy default, MongoDB launches with bindIp set to 127.0.0.1, which binds to the localhost network interface. This means that the mongod.exe can only accept connections from clients that are running on the same machine. Remote clients will not be able to connect to the mongod.exe, and the mongod.exe will not be able to initialize a replica set unless this value is set to a valid network interface.\n\nThis value can be configured either:\n\nin the MongoDB configuration file with bindIp, or\nvia the command-line argument --bind_ip", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "how do i add my program to the users start menu with vs2010 setup project", "id": 1904, "answers": [{"answer_id": 1891, "document_id": 1475, "question_id": 1904, "text": "    \n\nRight click on the setup project and click View -&gt; File System.  There is a foler there for User's Programs Menu.  You can add a shortcut to your project's executab", "answer_start": 1478, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nHow do I add my program to the users start menu with VS2010 Setup Project?\n\nI don't see an obvious place to do this.\n\nWhat would be even better than this is a link to a reference that holds my hand through setting up projects so I can see how it is supposed to be done.\n\nThanks!\n    \n\n\nGo to Application Folder, look on the right hand side it should say, Primary output from X (make sure you don't have other widgets covering up this real estate.)\nRight click that =&gt; make shortcut\nGo to User's Program's Menu ==&gt; create the folder that you want your shortcut to reside in, probably your company name\nDrag the shortcut from step 2 to the folder you just created.\n\n    \n\nYou can add a short cut in to the executable in the File System view.\n\n\n\nMore steps here\n\n\n    \n\nIn the Setup Project File System, right-click the User's Programs Menu, and create a folder for your application.  Go to the Application Folder, right-click project output, then select Create Shortcut to xxx.  Drag the shortcut to the application folder.\n    \n\nAfter wasting an hour, I got it done - VS 2010 (this article helped):\n\nIn your Deployment Project go to File System Editor\nClick on \"Application Folder\" (I assume you have \"Primary Output\" from your projects there)\nRight click on \"Primary Output\" of the project you want to target with your shortcut - select \"Create Shortcut to Primary Output ...\"\nRename shortcut\nDrag shortcut to Desktop / Programs Menu folder\nThat's it!\n    \n\nRight click on the setup project and click View -&gt; File System.  There is a foler there for User's Programs Menu.  You can add a shortcut to your project's executable there.\n\nHere is a link that walks you through the process.\n    \n\nCreating the shortcut at the top level of the start menu is not really standard behavior. The shortcut should be created in the folder [Manufacturer][ProductName] in the user's \"Programs\" menu. However, it seems you can't use the installer properties in the Visual Studio deployment proje\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to get Fabric to automatically (instead of user-interactively) interact with shell commands? Combine with pexpect?", "id": 1078, "answers": [{"answer_id": 1070, "document_id": 655, "question_id": 1078, "text": "You may need this: https://www.hugedomains.com/domain_profile.cfm?d=ilogue&e=com.", "answer_start": 564, "answer_category": null}], "is_impossible": false}], "context": "Seeking means to get Fabric to automatically (instead of user-interactively) interact with shell commands (and not just requests for passwords, but also requested user input when no \"stdin/interactive override\" like apt-get install -y is available).\nThis question along with these Fabric docs suggest that Fabric can only \"push the interactivity\" back to the human user that's running the Fabric program. Seeking to instead fully automate without any human presence. Don't yet have a \"real,\" current problem to solve, just preparing for possible, future obstacle.\nYou may need this: https://www.hugedomains.com/domain_profile.cfm?d=ilogue&e=com.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Develop Tampermonkey scripts in a real IDE with automatic deployment to OpenUserJs repo", "id": 1073, "answers": [{"answer_id": 1066, "document_id": 651, "question_id": 1073, "text": "We'll configure just a couple of things so that you can code in your editor and see the changes reflected in the browser without a nuisance.\n1.Go to Chrome => Extensions and find the TamperMonkey 'card'. Click details. On the page that opens, allow it access to file URLs:\nenter image description here\n2.Save your script file wherever you want in your filesystem. Save the entire thing, including the ==UserScript== header. This works in all desktop OS's, but since I'm using macOS, my path will be: /Users/me/Scripts/SameWindowHref.user.js\n3.Now, go to the TM extension's dashboard, open the script in question in its editor and delete everything except the entire ==UserScript== header\n4.Add to the header a @require property pointing to the script's absolute path.", "answer_start": 918, "answer_category": null}], "is_impossible": false}], "context": "I recently started development on a Tampermonkey script, which is hosted on OpenUserJs. It seems that I'm going to invest a bit more time in the future on this script by keep it up to date and extend his features when time is there. The first lines I wrote on the Tampermonkey editor which is integrated in chrome (edit button of a script).\nBut I don't like it, the most thing I'm missing is some kind of autocomplete/intellisense. Visual Studio is much better here, so I switched to VS. The problem: After any changes, I have to copy the hole code and paste it in the Tampermonkey editor (Google Chrome). Thats annoying and not very flexible, since I can't really split my code in multiple js files when the script grows.\nSo is there a way to automate this? My imagination would be: I save the js file in VS (ctrl + s), then the script is loaded in my local development instance of google chrome for testing purpose.\nWe'll configure just a couple of things so that you can code in your editor and see the changes reflected in the browser without a nuisance.\n1.Go to Chrome => Extensions and find the TamperMonkey 'card'. Click details. On the page that opens, allow it access to file URLs:\nenter image description here\n2.Save your script file wherever you want in your filesystem. Save the entire thing, including the ==UserScript== header. This works in all desktop OS's, but since I'm using macOS, my path will be: /Users/me/Scripts/SameWindowHref.user.js\n3.Now, go to the TM extension's dashboard, open the script in question in its editor and delete everything except the entire ==UserScript== header\n4.Add to the header a @require property pointing to the script's absolute path.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Installing bsddb package \u2013 python", "id": 1779, "answers": [{"answer_id": 1765, "document_id": 1350, "question_id": 1779, "text": "bsddb is deprecated since 2.6. Maybe you should use the bsddb3 module.", "answer_start": 430, "answer_category": null}], "is_impossible": false}], "context": "I am totally new to python and I have this message when I try to import bsdddb\nSo I followed this and this, so I downloaded this package bsddb3-4.5.0.tar.gz. What am I suppose to do with it, I tried to run python install setup.py int the bsddb3-4.5.0 in the right directory (I am using an osx). Then I get\nCan't find a local BerkeleyDB installation.\n(suggestion: try the --berkeley-db=/path/to/bsddb option)\nSome one could help ?\nbsddb is deprecated since 2.6. Maybe you should use the bsddb3 module.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Running java without installing jre?", "id": 1096, "answers": [{"answer_id": 1088, "document_id": 673, "question_id": 1096, "text": "You can use Launch4j for this. Well documented and easy to use. While the resulting program still needs a JRE to run, you don't have to install the JRE on the target system. You can just copy it with your application and tell Launch4j were to find it or just wrap it up with everything else.", "answer_start": 263, "answer_category": null}], "is_impossible": false}], "context": "As asked and answered here, python has a useful way of deployment without installers. Can Java do the same thing?\nIs there any way to run Java's jar file without installing jre?\nIs there a tool something like java2exe (win32), java2bin (linux) or java2app (mac)?\nYou can use Launch4j for this. Well documented and easy to use. While the resulting program still needs a JRE to run, you don't have to install the JRE on the target system. You can just copy it with your application and tell Launch4j were to find it or just wrap it up with everything else.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "visual studio and sql server correct installation sequence", "id": 1495, "answers": [{"answer_id": 1484, "document_id": 1074, "question_id": 1495, "text": "Install Visual Studio Professional 2008.\nInstall SQL 2008 Developer\nApply SQL SP1\nApply VS SP1", "answer_start": 4998, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI am rebuilding my development machine. This issue is not new to me, but I don't remember the solution.\nI started with SQL 2008 Developer, then VS 2008 Pro, then the SQL SP1, then VS SP1. The result is that I cannot open SSIS projects (see the error below). What is the correct order so that I can avoid the installation of SQL Server Express and still have all the features working?\n\n---------------------------\nMicrosoft Visual Studio\n---------------------------\nPackage Load Failure\n\nPackage 'DataWarehouse VSIntegration layer' has failed to load properly ( GUID =\n{4A0C6509-BF90-43DA-ABEE-0ABA3A8527F1} ). Please contact package vendor for \nassistance. Application restart is recommended, due to possible environment \ncorruption. Would you like to disable loading this package in the future? You \nmay use 'devenv /resetskippkgs' to re-enable package loading.\n---------------------------\nYes   No   \n---------------------------\n\n    \n\nYou should install SQL 2008 Developer first, this will rule out the need for VS installing SQL which comes with it. Or you could do like others suggested and choose custom VS installation. \n\nMy favorite way is this:\n\n\nSQL 2008 Developer\nVisual Studio Professional 2008\nRun Windows Updates\nInstall Resharper :-)\nInstall RedGate SQL ToolBelt\n\n\nBut it seems that to get some features to work the proper order is:\n\n\nVisual Studio Professional 2008 (with SQL Unchecked)\nSQL 2008 Developer\nRun Windows Updates\nInstall Resharper/Redgate Tools\n\n\nResharper and Redgate in my opinion are far the best tools for developing in C#/SQL.\n    \n\nIt turns out I was missing 2 important pieces of information in my question that make this installation such a pain:\n\n\nI was installing the 64 bit version of SQL Server\nI was installing to a non-standard location (i.e. the D: drive, because C: is an SSD with not all \nthat much space)\n\n\nThis was not successful. Here is what I ended up doing in my first (second, and third) attempt:\n\n\nInstall VS 2008 Professional on D:\nInstall VS 2008 SP 1\nInstall SQL Server 2008 Developer Edition (instance and shared components on D:)\nInstall SQL Server SP 1\n\n\nApparently VS 2008 Pro does not require a database. I am not sure why I remembered that it does. While the Visual Studio installation was eventless, I encountered the following errors during the SQL Server installation:\n\n\n  The INSTANCESHAREDWOWDIR command line value was not specified.\n\n\nI was able to resolve that be starting setup from the command line (thanks to this MSDN forum post):\n\nsetup.exe /INSTALLSHAREDWOWDIR=\"D:\\Program Files(x86)\\Microsoft SQL \nServer\" /INSTALLSHAREDDIR=\"D:\\Program Files\\Microsoft SQL Server\"\n/ACTION=install\n\n\nThen towards the end of the installation, I got this error:\n\n\n  Upgrade Failed due to the following Error.The error code is :-2147467259.Message:Unspecified error \n\n\nThere is no solution, but a workaround, described in this post on connect.microsoft.com. It consists of copying a VS config file around before and after the installation, and re-running the installer just for BIDS (which is the one component that failed).\n\nThe SQL Server SP1 installation ran without issue. VS Studio, when trying to load a solution with an SSIS project, still threw the error that I posted in my original question.\n\nI had ignored this error that popped up towards the end of the BIDS installation:\n\n\n  Microsoft Visual Studio Tools for Applications 2.0 \n  Cannot find one or more components. Please reinstall the application.\n\n\nHowever, there was no error in the install log and it completed \"successfully\", so I thought it would be ok to ignore the error.\n\nIt seems that my configuration is not possible (but I know with certainty that I had the 2005 versions of VS and SQL Server on a D: drive). \n\nI uninstalled everything once again, manually deleted whatever folders were left, and reinstalled on the C: drive, including the SQL Server shared components. I put the instance folder on the D:. \n\n\nInstall VS 2008 Professional on C:\nInstall VS 2008 SP 1\nInstall SQL Server 2008 Developer Edition (instance on D:, shared components on C:)\nInstall SQL Server SP 1\n\n\nThis time everything installed and I can open my SSIS project.\n    \n\nIf you are trying to install SQL-2008 and you also have visual studio 2008 installed with service pack one (SP1) and get the error\n\n\n  A previous release of Microsoft Visual Studio 2008 is installed  on\n  this computer. Upgrade Microsoft Visual Studio 2008 to the SP1  before\n  installing SQL Server 2008\u201d\n\n\nThen welcome to the Microsoft beta testing program even if you purchased the faulty software.\n\nWith XP the most common solution seem to be renaming the registry key\n\nHKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\DevDiv\\VS\\Servicing\\9.0\n\n\nand replacing 9.0 with 9.0Old. However this does not solve the problem if you are using windows 7 and the only solution I managed to find that worked was to uninstall VS2008 and all the components and to then install SQL-2008 and finally reinstall VS2008.\n    \n\n\nInstall Visual Studio Professional 2008.\nInstall SQL 2008 Developer\nApply SQL SP1\nApply VS SP1\n\n\nThen all should be good. \n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Where is a reliable registry key to find install location of Excel 2007?", "id": 936, "answers": [{"answer_id": 931, "document_id": 583, "question_id": 936, "text": "[HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Office\\X.0\\Common\\InstallRoot]", "answer_start": 15, "answer_category": null}], "is_impossible": false}], "context": "7\n\nHow about:\n\n[HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Office\\X.0\\Common\\InstallRoot]\nwhich contains a key named 'Path' with the installation directory of that version of Office. This is consistent for Excel 8.0 through 12.0. If you want to look for a specific product, use Excel, Word, Access, etc., in place of Common.\n\nJon ------- Jon Peltier, Microsoft Excel MVP\nFrom: http://www.developersdex.com/vb/message.asp?p=2677&r=6199020\n\nShare\nImprove this answer\nFollow\nanswered Oct 28 '08 at 19:44\n\nFionnuala\n89k77 gold badges104104 silver badges142142 bronze badges\nThis is not in all my systems that have Excel 2007. \u2013 \nJason\n Oct 28 '08 at 19:46\nAdd a comment\n\n7\n\nI'm using the following key:\n\n[HKLM\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\App Paths\\excel.exe]\nIf the folder name contains Office12, you've got 2007. I'm using this method to decide during installation to install Office 2000-2003 addins or Office 2007 addins, as well as the folder to install them to.\n\nThis is extremely reliable, and also works well with localized versions of Windows. So far we've tested on French, German, Spanish and Italian with much better success than the other methods we were previously using. XLSTART should be the same in all languages, but be wary when developing word addins as \"STARTUP\" is localized in some cases.\n\nShare\nImprove this answer\nFollow\nanswered Nov 14 '08 at 1:50\n\nsaschabeaumont\n21.6k44 gold badges6161 silver badges8585 bronze badges\nI have Excel 2007 and I do not have this key. :( I have up to \\App Paths, but no excel.exe listing. \u2013 \nJason\n Nov 14 '08 at 2:21\nAdd a comment\n\n1\n\nHere is another direction you can go. I have not tested this.\n\nhttp://support.microsoft.com/kb/240794\n\nShare\nImprove this answer\nFollow\nanswered Oct 28 '08 at 19:48\n\nTravis Collins\n3,91433 gold badges2929 silver badges4343 bronze badges\nAdd a comment\n\n0\n\nHKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Office\\12.0\\Excel\\InstallRoot\\\n\nShare\nImprove this answer\nFollow\nanswered Oct 28 '08 at 19:44\n\nTravis Collins\n3,91433 gold badges2929 silver badges4343 bronze badges\nThis is not on all my systems. \u2013 \nJason\n Oct 28 '08 at 19:46\nAdd a comment\n\n0\n\nI have found this key to be consistent across all my Office 2007 installations.\n\n[HKLM\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Installer\\UserData\\S-1-5-18\\Components\\9B905EB838DBFEE4991CF8E66F518BBF]\nIf you are reading this, and you have Excel 2007, can you Vote this up (or leave a comment) if it is reliable for you too? (Vote it down, or post a comment if it is wrong?)\n\nNOTE: This is not consistent across my machines.\n\n[HKLM\\SOFTWARE\\Microsoft\\Office\\X.0\\Common\\InstallRoot]\nShare\nImprove this answer\nFollow", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "how to successfully install pyproj and geopandas?", "id": 1192, "answers": [{"answer_id": 1185, "document_id": 768, "question_id": 1192, "text": "I download (from http://www.lfd.uci.edu/~gohlke/pythonlibs/) and install the .whl files for GDAL, Fiona, pyproj, Shapely and Rtree using the command\npip install filename.whl\nfor each .whl file.\nAs a final step I run\npip install https://github.com/geopandas/geopandas/archive/master.zip", "answer_start": 380, "answer_category": null}], "is_impossible": false}], "context": "I have tried to install geopandas via I python by running !pip install geopandas, but this fails with \"python setup.py egg_info\" failed with error code 1 and then Path to long directory. I read online that pyproj is required for geopandas and also tried to install it however no luck, similar error. Would anyone be able to point me in the right direction? Thank you. On Windows, I download (from http://www.lfd.uci.edu/~gohlke/pythonlibs/) and install the .whl files for GDAL, Fiona, pyproj, Shapely and Rtree using the command\npip install filename.whl\nfor each .whl file.\nAs a final step I run\npip install https://github.com/geopandas/geopandas/archive/master.zip\nbecause I want the dev version of geopandas. The conda w/ ioos method looks easier - but I haven't tried it yet.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "which file shall I download for installing server JRE 8 release on the Oracle Solaris platform?", "id": 219, "answers": [{"answer_id": 227, "document_id": 117, "question_id": 219, "text": "Download File(s)\tArchitecture\tWho Can Install\nserver-jre-8uversion-solaris-sparcv9.tar.gz\t64-bit SPARC\tanyone\nserver-jre-8uversion-solaris-x64.tar.gz\t64-bit x64, EM64T\tanyone", "answer_start": 1077, "answer_category": null}], "is_impossible": false}, {"question": " how to install Server JRE 8  on the Oracle Solaris Operating System", "id": 220, "answers": [{"answer_id": 228, "document_id": 117, "question_id": 220, "text": "Download the file.\n\nBefore a file can be downloaded, you must accept the license agreement. The archive binary can be installed by anyone (not only root users), in any location that you can write to.\n\nThe .tar.gz archive file (also called a tarball) is a file that can be simultaneously uncompressed and extracted in one step.\n\nChange directory to the location where you want the Server JRE to be installed, then move the .tar.gz archive binary to the current directory.\n\nUnpack the tarball and install the Server JRE.\n\nOn SPARC processors:\n\n% gzip -dc server-jre-8uversion-solaris-sparcv9.tar.gz | tar xf -\nOn x64/EM64T processors:\n\n% gzip -dc server-jre-8uversion-solaris-x64.tar.gz | tar xf -\nThe Server JRE is installed in a directory called jdk1.8.0_version in the current directory.\n", "answer_start": 2045, "answer_category": null}], "is_impossible": false}], "context": "Skip to Content\nHome PageJava SoftwareJava SE DownloadsJava SE 8 DocumentationSearch\nJava Platform, Standard Edition Installation Guide\nContents    Previous    Next\n5 Server JRE 8 Installation on the Oracle Solaris Operating System\nThis page describes Oracle Solaris system requirements for the server JRE and gives installation instructions.\n\nThis page has these topics:\n\n\"System Requirements\"\n\n\"Server JRE Installation Instructions\"\n\nSee \"JDK 8 and JRE 8 Installation Start Here\" for general information about installing.\n\nFor information on enhancements to JDK 8 that relate to the installer, see \"Installer Enhancements in JDK 8\".\n\nSystem Requirements\nThis version of the Server JRE is supported on the Oracle Solaris 10 Update 9 or later OS, Oracle Solaris 11 Express OS, and Oracle Solaris 11 OS. For supported processors and browsers, see http://www.oracle.com/technetwork/java/javase/certconfig-2095354.html.\n\nServer JRE Installation Instructions\nThe following table lists the options available for downloading the Server JRE 8 release on the Oracle Solaris platform.\n\nDownload File(s)\tArchitecture\tWho Can Install\nserver-jre-8uversion-solaris-sparcv9.tar.gz\t64-bit SPARC\tanyone\nserver-jre-8uversion-solaris-x64.tar.gz\t64-bit x64, EM64T\tanyone\nInstallation Instruction Notation\nFor instructions containing the notation version, substitute the appropriate Server JRE update version number. For example, if you are installing Server JRE 8 update release 21, the following string representing the name of the bundle:\n\nserver-jre-8uversion-solaris-sparcv9.tar.gz\nbecomes:\n\nserver-jre-8u21-solaris-sparcv9.tar.gz\nNote that, as in the preceding example, the version number is sometimes preceded with the letter u, for example, 8u21, and sometimes it is preceded with an underbar, for example, jre1.8.0_21.\n\nInstallation\nYou can install a JRE archive binary in any location that you can write to. It will not displace the system version of the Java platform provided by the Oracle Solaris OS.\n\nInstall the server JRE by following these steps:\n\nDownload the file.\n\nBefore a file can be downloaded, you must accept the license agreement. The archive binary can be installed by anyone (not only root users), in any location that you can write to.\n\nThe .tar.gz archive file (also called a tarball) is a file that can be simultaneously uncompressed and extracted in one step.\n\nChange directory to the location where you want the Server JRE to be installed, then move the .tar.gz archive binary to the current directory.\n\nUnpack the tarball and install the Server JRE.\n\nOn SPARC processors:\n\n% gzip -dc server-jre-8uversion-solaris-sparcv9.tar.gz | tar xf -\nOn x64/EM64T processors:\n\n% gzip -dc server-jre-8uversion-solaris-x64.tar.gz | tar xf -\nThe Server JRE is installed in a directory called jdk1.8.0_version in the current directory.\n\nDelete the .tar.gz file if you want to save disk space.\n\nContents    Previous    Next\nCopyright \u00a9 1993, 2021, Oracle and/or its affiliates. All rights reserved.Contact Us", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Is there a standard way to deploy Laravel?", "id": 1060, "answers": [{"answer_id": 1056, "document_id": 641, "question_id": 1060, "text": "There are a lot of deployment tool, like Capistrano. I recommend you to take a look at Deployer: it's has simple api, bundled with recipes for popular frameworks and apps, and can do 100% parallel task execution. Also it requires only for PHP.", "answer_start": 558, "answer_category": null}], "is_impossible": false}], "context": "I'm starting to use Laravel 4 seriously in my projects. I understand that this framework offers many advantages when developing RESTful applications. But I understand that there is no consensus about how do deployment / publishing and app using Laravel. I am still using FTP to transfer files to my Production host. But my question is, Is there any standard way to do the same but from Laravel? I am faithful believing that with a little ingenuity one can create something like php artisan publish [Production server name and SSH credentials] as parameters.\nThere are a lot of deployment tool, like Capistrano. I recommend you to take a look at Deployer: it's has simple api, bundled with recipes for popular frameworks and apps, and can do 100% parallel task execution. Also it requires only for PHP.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to get tf.exe (TFS command line client)?", "id": 661, "answers": [{"answer_id": 666, "document_id": 354, "question_id": 661, "text": "In Visual Studio 2017 & 2019, it can be found here :\n-Replace {YEAR} by the appropriate year (\"2017\", \"2019\").\n-Replace {EDITION} by the appropriate edition name (\"Enterprise\", \"Professional\", or \"Community\")\nC:\\Program Files (x86)\\Microsoft Visual Studio\\{YEAR}\\{EDITION}\\Common7\\IDE\\CommonExtension", "answer_start": 407, "answer_category": null}], "is_impossible": false}], "context": "What's the minimum amount of software I need to install to get the 'tf.exe' program? 59\nI'm in a virtual machine, and am trying to keep my VHD as small as possible, so I find Team Explorer is a really heavyweight solution (300+ MB install). As an alternative, I've had some luck copying a minimal set of EXEs/DLLs from a Team Explorer installation to a clean machine (.NET 4.0 is still required, of course).In Visual Studio 2017 & 2019, it can be found here :\n-Replace {YEAR} by the appropriate year (\"2017\", \"2019\").\n-Replace {EDITION} by the appropriate edition name (\"Enterprise\", \"Professional\", or \"Community\")\nC:\\Program Files (x86)\\Microsoft Visual Studio\\{YEAR}\\{EDITION}\\Common7\\IDE\\CommonExtension\n\nI've only tried a handful of operations so far, but this set of files (about 8.5 MB) has been enough to get basic source-control functionality via tf.exe:\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "is it safe to do rmdir r instdir in nsis?", "id": 1364, "answers": [{"answer_id": 1353, "document_id": 932, "question_id": 1364, "text": "RmDir /r will delete the whole directory tree if it can, so it is \"unsafe\". See http://nsis.sourceforge.net/Uninstall_only_installed_files for a way to only delete the files you install", "answer_start": 812, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI noticed this warning in the documentation for NSIS's RmDir method:\n\n\n  Warning: using RMDir /r $INSTDIR in\n  the uninstaller is not safe. Though it\n  is unlikely, the user might select to\n  install to the Program Files folder\n  and so this command will wipe out the\n  entire Program Files folder, including\n  other programs that has nothing to do\n  with the uninstaller.\n\n\nThis scares me, since up until now I had not considered this possibility and I had that exact line in my script.  But when I tested if this would happen by installing my program to a pre-existing location containing pre-existing files and then running my uninstaller with RmDir /r /REBOOTOK $INSTDIR in it, the existing files were left unharmed.  \n\nIs this an outdated warning?  I'm using NSIS v. 2.46.\n\nThanks\n    \n\nRmDir /r will delete the whole directory tree if it can, so it is \"unsafe\". See http://nsis.sourceforge.net/Uninstall_only_installed_files for a way to only delete the files you install\n    \n\nRMDir on a directory without /r (recursive) flag will remove the directory if it is empty. At some point they have added this feature, not sure when.\n\nNSIS Scripting Reference - RMDir\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "install(TARGETS ...) and add_subdirectory", "id": 1776, "answers": [{"answer_id": 1762, "document_id": 1347, "question_id": 1776, "text": "You could try using file install rather than install targets. The downside is that you will have to handle shared and static builds.\ninstall(FILES gtest-1.5.0/gtest_main.so DESTINATION lib)", "answer_start": 352, "answer_category": null}], "is_impossible": false}], "context": "Is it possible to use install(TARGETS ...) with targets that are defined in directories added with add_subdirectory?\nMy use case is, that I want to build e.gg an rpm for gtest. the gtest project happens to have a CMakeLists.txt without any install statements. I want to build the package without adding those statements to the CMakeLists.txt of gtest.\nYou could try using file install rather than install targets. The downside is that you will have to handle shared and static builds.\ninstall(FILES gtest-1.5.0/gtest_main.so DESTINATION lib)\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "repository element was not specified in the POM inside distributionManagement element or in -DaltDep loymentRepository=id::layout::url parameter", "id": 1695, "answers": [{"answer_id": 1683, "document_id": 1268, "question_id": 1695, "text": "You should include the repository where you want to deploy in the distribution management section of the pom.xml", "answer_start": 1328, "answer_category": null}], "is_impossible": false}], "context": "I'm having a problem while deploying and here is the error message I get:\n[INFO]\n[INFO] --- maven-deploy-plugin:2.7:deploy (default-deploy) @ core ---\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD FAILURE\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time: 3.296 s\n[INFO] Finished at: 2014-11-26T17:05:00+02:00\n[INFO] Final Memory: 13M/244M\n[INFO] ------------------------------------------------------------------------\n[ERROR] Failed to execute goal org.apache.maven.plugins:maven-deploy-plugin:2.7:\ndeploy (default-deploy) on project core: Deployment failed: repository element w\nas not specified in the POM inside distributionManagement element or in -DaltDep\nloymentRepository=id::layout::url parameter -> [Help 1]\n[ERROR]\n[ERROR] To see the full stack trace of the errors, re-run Maven with the -e swit\nch.\n[ERROR] Re-run Maven using the -X switch to enable full debug logging.\n[ERROR]\n[ERROR] For more information about the errors and possible solutions, please rea\nd the following articles:\n[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionE\nxception\nI checked some resources on the internet and none of them worked for my case. I think it's related to my pom.xml, so here are its related parts. You should include the repository where you want to deploy in the distribution management section of the pom.xml.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Why Do Applications Need Installing?", "id": 1811, "answers": [{"answer_id": 1796, "document_id": 1382, "question_id": 1811, "text": "There are several reasons:\n1.They hook into the system. When you install a browser, it will be started if you open a file which starts with \"http://\" or ends with \".html\"\n2.Many apps ask for a license key and/or online authentication to avoid piracy.\n3.Many apps come with DLLs. To make sure they work, they overwrite all DLLs in the Window's system directory (and possibly break all other apps but who cares ;", "answer_start": 188, "answer_category": null}], "is_impossible": false}], "context": "I'm confused. Right now I write some small apps using C++ and Java, but none of them need to be installed. Why do \"big apps\" like browsers, media players, games etc. need to be installed?\nThere are several reasons:\n1.They hook into the system. When you install a browser, it will be started if you open a file which starts with \"http://\" or ends with \".html\"\n2.Many apps ask for a license key and/or online authentication to avoid piracy.\n3.Many apps come with DLLs. To make sure they work, they overwrite all DLLs in the Window's system directory (and possibly break all other apps but who cares ;\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Jenkins: Multiple Git repositories for one project", "id": 544, "answers": [{"answer_id": 546, "document_id": 269, "question_id": 544, "text": "Multiple SCMs Plugin is now deprecated so users should migrate to Pipeline plugin.", "answer_start": 149, "answer_category": null}], "is_impossible": false}], "context": "I want to build a project using two Git repositories. One of them contains of the source code, while the other has the build and deployment scripts.\nMultiple SCMs Plugin is now deprecated so users should migrate to Pipeline plugin.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Installing glib in non-standard prefix fails", "id": 1212, "answers": [{"answer_id": 1205, "document_id": 788, "question_id": 1212, "text": "In your example you should be able to do:\n./configure --prefix=/root/build && make clean && make && make install", "answer_start": 555, "answer_category": null}], "is_impossible": false}], "context": "I'm trying to install glib in a non-standard prefix but I get the following when running make install:\n/bin/sh ../libtool --mode=install /usr/bin/install -c libgthread-2.0.la '/root/build/lib'\nlibtool: install: error: cannot install `libgthread-2.0.la' to a directory not ending in /usr/local/lib\nAny reason why I have to install gthread only in a prefix ending with /usr/local/lib?\nI also just stumbled over that problem when compiling MonetDB on my Linux machine. Here is the solution/workaround that worked for me: Always make clean after ./configure.\nIn your example you should be able to do:\n./configure --prefix=/root/build && make clean && make && make install\nI found the solution in a discussion on an apache httpd bug where Joe Orton shares his knowledge:\nA \"make clean\" is usually necessary after re-running \"configure\"\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Setting up PHPUnit on OSX", "id": 733, "answers": [{"answer_id": 734, "document_id": 421, "question_id": 733, "text": "curl https://phar.phpunit.de/phpunit.phar -L -o phpunit.phar\n\nchmod +x phpunit.phar\n\nmv phpunit.phar /usr/local/bin/phpunit", "answer_start": 480, "answer_category": null}], "is_impossible": false}], "context": "Though I'm sure others have eventually managed to figure this out, I've been following the various documentation out there and have been having a heck of a rough time of it.\nhttp://www.phpunit.de/manual/current/en/installation.html\nMakes it sound pretty easy. However depending on your setup, you might be going down a rabbit hole.\nPEAR for example must be of a version higher than 1.8.1. I had 1.8.0 at the time, so I went to find out how to update PEAR\nTo install via terminal:\ncurl https://phar.phpunit.de/phpunit.phar -L -o phpunit.phar\n\nchmod +x phpunit.phar\n\nmv phpunit.phar /usr/local/bin/phpunit\nthis should be the correct answer, one thing though, not sure if Im the only one but I had to add \"-L\" to the curl command to make it follow redirect\n\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Error : DEP6200 : Bootstrapping 'Device' failed. Device cannot be found", "id": 1284, "answers": [{"answer_id": 1276, "document_id": 855, "question_id": 1284, "text": "You should install on phone:\n1.Turn off Developer mode on your phone.\n2.Deploy app (UWP or SL) by VS2015 -> Error.\n3.Turn on Developer mode on your phone.\n4.sDeploy app (UWP or SL) by VS2015 -> Working.", "answer_start": 612, "answer_category": null}], "is_impossible": false}], "context": "I've read every StackOverflow that I could find on this issue, but I still can't work it out.\nIt's worth mentioning that I went from Windows 7 to Windows 10. It's also worth mentioning that my computer was missing the IpOverUsbSvc.exe (not just the registry post, but the actual service was missing). Couldn't find any way to get this back so, so my colleague sent over the .exe and the DLLs and I placed it in it's location. And the service is now running and no longer gives any errors. My device is recognized by the PC, it's running Windows 10 Mobile (but I've also tried with a device running Windows 8.1).\nYou should install on phone:\n1.Turn off Developer mode on your phone.\n2.Deploy app (UWP or SL) by VS2015 -> Error.\n3.Turn on Developer mode on your phone.\n4.sDeploy app (UWP or SL) by VS2015 -> Working.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How do you migrate an IIS 7 site to another server?", "id": 1671, "answers": [{"answer_id": 1658, "document_id": 1244, "question_id": 1671, "text": "1.\tIn IIS manager, click the Server node\n2.\tGo to Shared Configuration under \"Management\"\n3.\tClick \u201cExport Configuration\u201d. (You can use a password if you are sending them across the internet, if you are just gonna move them via a USB key then don't sweat it.)\n4.\tMove these files to your new server\n5.\tadministration.config\n6.\tapplicationHost.config\n7.\tconfigEncKey.key \n8.\tOn the new server, go back to the \u201cShared Configuration\u201d section and check \u201cEnable shared configuration.\u201d Enter the location in physical path to these files and apply them.\n9.\tIt should prompt for the encryption password(if you set it) and reset IIS.", "answer_start": 50, "answer_category": null}], "is_impossible": false}], "context": "I'd say export your server config in IIS manager:\n1.\tIn IIS manager, click the Server node\n2.\tGo to Shared Configuration under \"Management\"\n3.\tClick \u201cExport Configuration\u201d. (You can use a password if you are sending them across the internet, if you are just gonna move them via a USB key then don't sweat it.)\n4.\tMove these files to your new server\n5.\tadministration.config\n6.\tapplicationHost.config\n7.\tconfigEncKey.key \n8.\tOn the new server, go back to the \u201cShared Configuration\u201d section and check \u201cEnable shared configuration.\u201d Enter the location in physical path to these files and apply them.\n9.\tIt should prompt for the encryption password(if you set it) and reset IIS.\nBAM! Go have a beer!\nProbably should use the Import Server or Site Package instead of just copying the files over, although I haven't tested this either. But I do know there are file paths in the applicationHost.config that won't necessarily be present on the new server, which would cause it to break. Also, you should probably mention that there can't be ANY sites currently in IIS, so this process doesn't clobber currently running configuration.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "With android studio no jvm found, JAVA_HOME has been set", "id": 832, "answers": [{"answer_id": 827, "document_id": 514, "question_id": 832, "text": "Set your JAVA_HOME variable to the following:\nC:\\Program Files\\Java\\jdk1.7.0_45\nIf that does not work, check that the JDK version is 1.7.0_45. If not, change the JAVA_HOME variable to (with JAVAVERSION as the Java version number:\nC:\\Program Files\\Java\\jdkJAVAVERSION", "answer_start": 487, "answer_category": null}], "is_impossible": false}], "context": "I have a JAVA_HOME variable set to:\nC:\\Program Files (x86)\\Java\\jdk1.7.0_45\\ And the path to the jdk is: C:\\Program Files (x86)\\Java\\jdk1.7.0_45\nHowever when I try to start android studio I get the error.\nIt says that it should be a 64-bit JDK. I have a feeling that you installed (at a previous time) a 32-bit version of Java. The path for all 32-bit applications in Windows 7 and Vista is:\nC:\\Program Files (x86)\\\nYou were setting the JAVA_HOME variable to the 32-bit version of Java. Set your JAVA_HOME variable to the following:\nC:\\Program Files\\Java\\jdk1.7.0_45\nIf that does not work, check that the JDK version is 1.7.0_45. If not, change the JAVA_HOME variable to (with JAVAVERSION as the Java version number:\nC:\\Program Files\\Java\\jdkJAVAVERSION\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "apk installation from web page", "id": 1172, "answers": [{"answer_id": 1165, "document_id": 749, "question_id": 1172, "text": "Just link to the apk file in the HTML. It couldn't be any simpler.\n<a href=\"path to my .apk file\">link</a>", "answer_start": 135, "answer_category": null}], "is_impossible": false}], "context": "I'm looking for a sample web page (html code) with a link that will install an apk file directly on my phone by clicking on the link. \nJust link to the apk file in the HTML. It couldn't be any simpler.\n<a href=\"path to my .apk file\">link</a>\nYou will have to have \"install apps from unknown sources\" enabled on your phone.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "react-native iOS app not showing static assets (images) after deploying", "id": 568, "answers": [{"answer_id": 571, "document_id": 293, "question_id": 568, "text": "Try:\n./react-native bundle --minify --entry-file index.ios.js --platform ios --dev false --bundle-output ./release/main.jsbundle --assets-dest ./release\nThe bundler will create an assets folder in there anyway.", "answer_start": 256, "answer_category": null}], "is_impossible": false}], "context": "I have all my static images in a folder called \"images\" in the root of my project. However, after I run the following command to bundle my app, the app works but no image is shown.\nThe asset destination and the main.jsbundle have to be in the same folder. Try:\n./react-native bundle --minify --entry-file index.ios.js --platform ios --dev false --bundle-output ./release/main.jsbundle --assets-dest ./release\nThe bundler will create an assets folder in there anyway.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "What are some good SSH Servers for windows?", "id": 327, "answers": [{"answer_id": 335, "document_id": 139, "question_id": 327, "text": "From install to administration it does it all through a GUI so you won't be putting together a sshd_config file. ", "answer_start": 288, "answer_category": null}], "is_impossible": false}], "context": "Trying to setup an SSH server on Windows Server 2003. What are some good ones? Preferably open source. I plan on using WinSCP as a client so a server which supports the advanced features implemented by that client would be great. I've been using Bitvise SSH Server and it's really great. From install to administration it does it all through a GUI so you won't be putting together a sshd_config file. Plus if you use their client, Tunnelier, you get some bonus features (like mapping shares, port forwarding setup up server side, etc.) If you don't use their client it will still work with the Open Source SSH clients.", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Get list of installed android applications", "id": 1741, "answers": [{"answer_id": 1728, "document_id": 1313, "question_id": 1741, "text": "The following will get you a list of ALL the installed applications. This will include a lot of system apps that you probably aren't interested in.\nPackageManager pm = getPackageManager();\nList<ApplicationInfo> apps = pm.getInstalledApplications(0);", "answer_start": 553, "answer_category": null}], "is_impossible": false}], "context": "Hi I want to get a list of all of the installed applications on the users device I have been googling for the longest time but can't find what i want this link was the closest though and works fine except me being new don't understand how to use the method getPackages(); and create a list with it\nhttp://www.androidsnippets.com/get-installed-applications-with-name-package-name-version-and-icon\nAny help on how to create the actual list would be a major help i have all that code already in just can't get the list to actually show thanks for any help\nThe following will get you a list of ALL the installed applications. This will include a lot of system apps that you probably aren't interested in.\nPackageManager pm = getPackageManager();\nList<ApplicationInfo> apps = pm.getInstalledApplications(0);\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Set installation prefix automatically to custom path if not explicitly specified on the command line", "id": 1193, "answers": [{"answer_id": 1186, "document_id": 769, "question_id": 1193, "text": " You can override it in the following way:\nif (CMAKE_INSTALL_PREFIX_INITIALIZED_TO_DEFAULT)\n    set (CMAKE_INSTALL_PREFIX \"${CMAKE_BINARY_DIR}/installed\"\n           CACHE PATH \"default install path\" FORCE)\nendif()", "answer_start": 466, "answer_category": null}], "is_impossible": false}], "context": "My questions are:\n(a) Are there any issues with the above, beyond the annoyance of the extra flag needing to be passed to CMake to get CMAKE_INSTALL_PREFIX to have an effect?\n(b) Is there a better, cleaner, more robust, more idiomatic and/or less annoying way to achieve the above?\nThanks.\nCMake sets the boolean variable CMAKE_INSTALL_PREFIX_INITIALIZED_TO_DEFAULT if CMAKE_INSTALL_PREFIX has not been explicitly specified and is initialized to its default setting. You can override it in the following way:\nif (CMAKE_INSTALL_PREFIX_INITIALIZED_TO_DEFAULT)\n    set (CMAKE_INSTALL_PREFIX \"${CMAKE_BINARY_DIR}/installed\"\n           CACHE PATH \"default install path\" FORCE)\nendif()\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to debug into my nuget package deployed from TeamCity?", "id": 318, "answers": [{"answer_id": 327, "document_id": 131, "question_id": 318, "text": "You can use Traditional method: 1.Put the pdb in the NuGet package alongside the dll.2.Add the source code to the Debug Source Files for the solution that references the package.", "answer_start": 212, "answer_category": null}], "is_impossible": false}], "context": "When I check 'Include Symbols and Source' in the Nuget Pack build step, TeamCity creates a .Symbol.nupkg in addition to the .nupkg file in the network folder. The .Symbol.nupkg contains the src and the .pdb file.You can use Traditional method: 1.Put the pdb in the NuGet package alongside the dll.2.Add the source code to the Debug Source Files for the solution that references the package.This means you'll be able to step through code and view exceptions, but you might have to find a file on disk and open it before you can set a breakpoint. Obviously you need to be careful that the source is at the right revision.", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How do I install a package that has been archived from CRAN?", "id": 1176, "answers": [{"answer_id": 1169, "document_id": 753, "question_id": 1176, "text": "If using Rstudio, select \"install from Package Archive File(.zip;.tar.gz)\" in \"Install Packages\" window", "answer_start": 246, "answer_category": null}], "is_impossible": false}], "context": "However, one of my coworkers did the exact same thing on the exact same version of R (3.1.0) and it worked. In addition, I've managed to install other packages successfully.\nAny idea why this does not work? Any help would be greatly appreciated.\nIf using Rstudio, select \"install from Package Archive File(.zip;.tar.gz)\" in \"Install Packages\" window. So now I know the version number of the most recent version. The way forward is to download the tarball, install all package dependencies and then install the package from the local downloaded file\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Android app starts updating with cordova-webintent and force stops", "id": 667, "answers": [{"answer_id": 672, "document_id": 360, "question_id": 667, "text": "Thread related issue, depending on cellphone or tablet's type of processors (how many threads can run simultaneously) keep in mind that cordoba-webintent is an async based call.\n\u2022\tBoth same versions (cordoba-webintent and cordoba) might missing common plugins. (Plugins was meant to be there but not incuded!)", "answer_start": 732, "answer_category": null}], "is_impossible": false}], "context": "This problem appeared some time ago and I can't understand why this happened, because the version of cordova-webintent and cordova are the same. Could you provide some more info on this like Android version, cordova version and is it device specific? Also by any chance you application goes to background during updation? Please throws some light on this to dig deeper.After the app force stopped, I tapped on the application icon, but there is a message that \"App isn't installed\". The app continues installing in the background after some seconds. I tapped again on the application icon and the updated application opened.\nWhy is the app force stopping while installing? 1I have two potential reasons dealing with your problem:\n\u2022\tThread related issue, depending on cellphone or tablet's type of processors (how many threads can run simultaneously) keep in mind that cordoba-webintent is an async based call.\n\u2022\tBoth same versions (cordoba-webintent and cordoba) might missing common plugins. (Plugins was meant to be there but not incuded!)\nI hope to pin point a different approach to your case!\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "What does \"No binary rubies available\" mean?", "id": 1807, "answers": [{"answer_id": 1792, "document_id": 1378, "question_id": 1807, "text": "When installing new Rubies, RVM first tries to use a pre-compiled version it downloads from https://rvm_io.global.ssl.fastly.net/binaries/. This significantly speeds up the installation of Rubies.", "answer_start": 223, "answer_category": null}], "is_impossible": false}], "context": "Whenever I use rvm install x.x.x, I get this warning even in successful installation.\nI tried to read rvm help mount, but it was beyond the scope of my knowledge.\nCan someone explain this warning in simple English? Thanks!\nWhen installing new Rubies, RVM first tries to use a pre-compiled version it downloads from https://rvm_io.global.ssl.fastly.net/binaries/. This significantly speeds up the installation of Rubies.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Global Node modules not installing correctly. Command not found", "id": 865, "answers": [{"answer_id": 860, "document_id": 545, "question_id": 865, "text": "npm config set prefix /usr/local", "answer_start": 2179, "answer_category": null}], "is_impossible": false}], "context": "91\n\n\n24\nI am having a problem installing global node modules and everything I find online says the solve is just adding -g. Which is not the problem. I believe it's a linking issue or wrong directory issue.\n\nHere is what I do:\n\n$ npm install -g express\nnpm http GET https://registry.npmjs.org/express\nnpm http 304 https://registry.npmjs.org/express\nnpm http GET https://registry.npmjs.org/range-parser/0.0.4\nnpm http GET https://registry.npmjs.org/mkdirp/0.3.3\n...downloads correctly\n\n$ express myapp\nbash: express: command not found\nHowever when I run the direct link location to express it works:\n\n   $ /usr/local/share/npm/bin/express myapp\n\n   create : myapp\n   create : myapp/package.json\n   create : myapp/app.js\n... Builds app correctly\nWhere the module is:\n\n$ which node\n/usr/local/bin/node\n$ node -pe process.execPath\n/usr/local/Cellar/node/0.8.20/bin/node\n$ npm link express\n/Users/bentonrr/Development/Personal/node_modules/express -> /usr/local/share/npm/lib/node_modules/express\nIn my .bash_profile I have:\n\nexport PATH=/usr/local/bin:$PATH\nexport NODE_PATH=/usr/local/lib/node_modules:/usr/local/lib/node\nDo I need to change my Node environment to download to correct folder? Is something not linking correctly? I am lost..\n\nThanks!\n\nOther Specs:\n\n$ node --version\nv0.8.20\n$ npm --version\n1.2.11\n$ brew --version\n0.9.4\nOSX Version 10.8.2\nnode.js\nterminal\ninstallation\nnpm\nShare\nImprove this question\nFollow\nedited Aug 22 '16 at 18:01\nasked Feb 24 '13 at 17:26\n\nim_benton\n2,28833 gold badges1818 silver badges3030 bronze badges\n1\nIt's because /usr/local/share/npm/bin/ is not in your $PATH of your shell. That's why the shell can't find express. \u2013 \nJP Richardson\n Feb 24 '13 at 19:59\n2\nI added: export PATH=/usr/local/share/npm/bin:$PATH to my .bash_profile and it worked. Thanks! Add an answer so I can accept it and upvote. \u2013 \nim_benton\n Feb 24 '13 at 21:42\nIn my case, also having NPM installed via Cellar on OSX, my bin path, which I set in .bash_profile, is: export PATH=$PATH:/usr/local/Cellar/node/10.5.0/bin \u2013 \nShane K\n Jun 20 '19 at 16:45\nAdd a comment\n9 Answers\n\n149\n\nThis may mean your node install prefix isn't what you expect.\n\nYou can set it like so:\n\nnpm config set prefix /usr/local\n\nthen try running npm install -g again, and it should work out. Worked for me on a mac, and the solution comes from this site:\n\nhttp://webbb.be/blog/command-not-found-node-npm/\n\nEDIT: Note that I just came across this again on a new Mac I'm setting up, and had to do the process detailed here on stackoverflow as well.\n\nShare\nImprove this answer\nFollow\nedited May 23 '17 at 11:47\n\nCommunityBot\n111 silver badge\nanswered Jul 29 '14 at 14:55\n\nBrad Parks\n57.1k5959 gold badges229229 silver badges300300 bronze badges\nI am using nodenv and there is most likely a better way to solve this issue for my case. However, your solution worked as a quick and dirty fix! I then switched the prefix back to the nodenv prefix and I was able to run the package from the /usr/local path. Thanks! \u2013 \nGarrett Tacoronte\n Jan 19 '17 at 6:31\n3\nI have spent months meaning to fix this and this is the only answer I've seen that fixed it for me! Thank you SO so much. \u2013 \nazz0r\n Jun 15 '17 at 8:48\nIt still doesn't work for me. What can I do? I want to install globally typescript on win7. \u2013 \nArtimal\n Dec 13 '17 at 15:38\n1\nI tried a lot of solutions but only this one did it for me. For some reason my prefix was set to /Users/(username)/.npm-global even though I did a fresh install of everything. I'm also on a Mac \u2013 \nCastilho\n Jan 3 '19 at 16:07 \nThis will break you if you have installed Node via Cellar. \u2013 \nShane K\n Jun 20 '19 at 16:31\nShow 1 more comment\n\n\n79\n\nAdd $(npm get prefix)/bin to your PATH (e.g., in .bashrc), like so:\n\necho \"export PATH=$PATH:$(npm get prefix)/bin\" >> ~/.bashrc\n\nFor more info, see npm help npm:\n\nglobal mode: npm installs packages into the install prefix at prefix/lib/node_modules and bins are installed in prefix/bin.\n\nYou can find the install prefix with npm get prefix or npm config list | grep prefix.\n\nShare\nImprove this answer\nFollow\nedited Dec 4 '20 at 23:06\n\nataraxis\n91088 silver badges2222 bronze badges\nanswered Mar 25 '13 at 19:59\n\nTim Smith\n96377 silver badges66 bronze badges\nwhat about for non sudo using nvm, shouldn't the path be $HOME/.npm to the .bash_profile or .bashrc? I use nvm, so want to make sure im not cross tying things on this. \u2013 \nblamb\n May 3 '17 at 5:16 \nFor anyone that has installed Node and npm via Homebrew on OSX, the location of the bin folder that you need to add to the PATH will be /usr/local/Cellar/node/VERSION_NUMBER/bin/. \u2013 \nJon Betts\n Nov 10 '20 at 0:06\nIn my case, I extended Tim's answer a bit, so that the actual path is dynamic in the bash profile script: echo \"export PATH=\\$PATH:\\$(npm get prefix)/bin\" >> ~/.bash_profile I used .bash_profile instead, however you can also use .bashrc as well, in most cases. \u2013 \nMichael Behrens\n May 27 at 3:27\nAdd a comment\n\n14\n\nMy npm couldn't find global packages as well. I did what Brad Parks suggested:\n\nnpm config set prefix /usr/local\nThen I got a EACCES permissions error (DON'T USE sudo npm install -g <package>) and fixed it through the official npm docs: https://docs.npmjs.com/resolving-eacces-permissions-errors-when-installing-packages-globally\n\nOn the command line, in your home directory, create a directory for global installations:\n mkdir ~/.npm-global\nConfigure npm to use the new directory path:\n npm config set prefix '~/.npm-global'\nIn your preferred text editor, open or create a ~/.profile file and add this line:\n export PATH=~/.npm-global/bin:$PATH\nOn the command line, update your system variables:\n source ~/.profile\nThen install a package globally and test it! For example:\nnpm install -g awsmobile-cli\nawsmobile configure\n\nShare\nImprove this answer\nFollow\nedited Oct 4 '19 at 19:51\n\nTim S. Van Haren\n8,64522 gold badges2828 silver badges3434 bronze badges\nanswered Feb 9 '19 at 16:30\n\nVicente\n1,90699 silver badges1515 bronze badges\n1\nhi everyone, I have question. After step 2, I don't find bin/ in ~/.npm-global. So , It don't work, right? \u2013 \nAquariusPotter\n Dec 3 '19 at 9:14 \nIt should still work. Don't forget that export PATH=~/.npm-global/bin:$PATH line is in the ~/.profile file. You get any error? \u2013 \nVicente\n Dec 3 '19 at 9:23\nAdd a comment\n\n9\n\nIn my case, The NODE_PATH environment variable was empty. Check whether it is empty-\n\necho $NODE_PATH\nif the NODE_PATH is empty. Then change ~/.bash_profile and add NODE_PATH\n\nnano ~/.bash_profile\nexport NODE_PATH=`npm root -g`\nsource ~/.bash_profile\nNow install npm modules again and check whether that is being installed on the path npm root -g", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Can not deserialize instance of java.lang.String out of START_OBJECT token", "id": 1673, "answers": [{"answer_id": 1660, "document_id": 1246, "question_id": 1673, "text": "Defining nested json object as JsonNode should work ,for example :\n{\"id\":2,\"socket\":\"0c317829-69bf-43d6-b598-7c0c550635bb\",\"type\":\"getDashboard\",\"data\":{\"workstationUuid\":\"ddec1caa-a97f-4922-833f-632da07ffc11\"},\"reply\":true}\n\n@JsonProperty(\"data\")\n    private JsonNode data;", "answer_start": 189, "answer_category": null}], "is_impossible": false}], "context": "I'm running into an issue where my deployable jar hits an exception that doesn't happen when I run this locally in IntelliJ.\nIf you do not want to define a separate class for nested json , Defining nested json object as JsonNode should work ,for example :\n{\"id\":2,\"socket\":\"0c317829-69bf-43d6-b598-7c0c550635bb\",\"type\":\"getDashboard\",\"data\":{\"workstationUuid\":\"ddec1caa-a97f-4922-833f-632da07ffc11\"},\"reply\":true}\n\n@JsonProperty(\"data\")\n    private JsonNode data;\nUpdate: To prevent this question from being incredibly long, I've included my pom.xml here: http://pastebin.com/1ZUtKCfE. I believe this is a dependency issue since the error only occurs on my deployable jar and not on my local PC.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "What is the difference between electron and electron-prebuilt?", "id": 1808, "answers": [{"answer_id": 1793, "document_id": 1379, "question_id": 1808, "text": "you should forget electron-prebuilt ever existed and always install the electron package instead.", "answer_start": 107, "answer_category": null}], "is_impossible": false}], "context": "Saw many times that package name, but did't get when do I need to install it instead of common electron...\nyou should forget electron-prebuilt ever existed and always install the electron package instead.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Which option decides the max number for open database?", "id": 200664, "answers": [{"answer_id": 239554, "document_id": 357784, "question_id": 200664, "text": "couchdb/max_dbs_open", "answer_start": 3475, "answer_category": null}], "is_impossible": false}, {"question": "How to increase the maximum connections CouchDB?", "id": 200665, "answers": [{"answer_id": 239560, "document_id": 357784, "question_id": 200665, "text": "Adding the following directive to (prefix)/etc/vm.args (or\nequivalent) will increase this limit", "answer_start": 4035, "answer_category": null}], "is_impossible": false}, {"question": "What is the minimum file descriptor on MAC OS?", "id": 200666, "answers": [{"answer_id": 239567, "document_id": 357784, "question_id": 200666, "text": "1024", "answer_start": 4352, "answer_category": null}], "is_impossible": false}, {"question": "What is the minimum file descriptor on Windows?", "id": 200667, "answers": [{"answer_id": 239608, "document_id": 357784, "question_id": 200667, "text": " 8192", "answer_start": 4255, "answer_category": null}], "is_impossible": false}, {"question": "What is the max connections in CouchDB?", "id": 200668, "answers": [{"answer_id": 239614, "document_id": 357784, "question_id": 200668, "text": "2048", "answer_start": 8316, "answer_category": null}], "is_impossible": false}], "context": "\n\n\n5.2. Performance\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApache CouchDB\u00ae\n\n\n\n\n\n\n\nstable\n\n\n\n\n\n\n\n\n\n\nTable of Contents\nUser Guides\n\n1. Introduction\n2. Replication\n3. Design Documents\n4. Best Practices\n\nAdministration Guides\n\n1. Installation\n2. Setup\n3. Configuration\n4. Cluster Management\n5. Maintenance\n5.1. Compaction\n5.2. Performance\n5.2.1. Disk I/O\n5.2.1.1. File Size\n5.2.1.2. Disk and File System Performance\n\n\n5.2.2. System Resource Limits\n5.2.2.1. CouchDB Configuration Options\n5.2.2.2. Erlang\n5.2.2.3. Maximum open file descriptors (ulimit)\n\n\n5.2.3. Network\n5.2.3.1. Connection limit\n\n\n5.2.4. CouchDB\n5.2.4.1. DELETE operation\n5.2.4.2. Document\u2019s ID\n\n\n5.2.5. Views\n5.2.5.1. Views Generation\n5.2.5.2. Built-In Reduce Functions\n\n\n\n\n5.3. Backing up CouchDB\n\n\n6. Fauxton\n7. Experimental Features\n\nReference Guides\n\n1. API Reference\n2. JSON Structure Reference\n3. Query Server\n4. Partitioned Databases\n\nOther\n\n1. Release Notes\n2. Security Issues / CVEs\n3. Reporting New Security Problems with Apache CouchDB\n4. License\n5. Contributing to this Documentation\n\nQuick Reference Guides\n\nAPI Quick Reference\nConfiguration Quick Reference\n\nMore Help\n\nCouchDB Homepage\nMailing Lists\nRealtime Chat\nIssue Tracker\nDownload Docs\n\n\n\n\n\n\n\n\nApache CouchDB\u00ae\n\n\n\n\n\nDocs \u00bb\n5. Maintenance \u00bb\n5.2. Performance\n\nEdit on GitHub\n\n\n\n\n\n\n\n5.2. Performance\u00b6\nWith up to tens of thousands of documents you will generally find CouchDB to\nperform well no matter how you write your code. Once you start getting into\nthe millions of documents you need to be a lot more careful.\n\n5.2.1. Disk I/O\u00b6\n\n5.2.1.1. File Size\u00b6\nThe smaller your file size, the less I/O operations there will be,\nthe more of the file can be cached by CouchDB and the operating system,\nthe quicker it is to replicate, backup etc. Consequently you should carefully\nexamine the data you are storing. For example it would be silly to use keys\nthat are hundreds of characters long, but your program would be hard to\nmaintain if you only used single character keys. Carefully consider data\nthat is duplicated by putting it in views.\n\n\n5.2.1.2. Disk and File System Performance\u00b6\nUsing faster disks, striped RAID arrays and modern file systems can all speed\nup your CouchDB deployment. However, there is one option that can increase\nthe responsiveness of your CouchDB server when disk performance is a\nbottleneck. From the Erlang documentation for the file module:\n\nOn operating systems with thread support, it is possible to let file\noperations be performed in threads of their own, allowing other Erlang\nprocesses to continue executing in parallel with the file operations.\nSee the command line flag +A in erl(1).\nSetting this argument to a number greater than zero can keep your CouchDB\ninstallation responsive even during periods of heavy disk utilization. The\neasiest way to set this option is through the ERL_FLAGS environment\nvariable. For example, to give Erlang four threads with which to perform I/O\noperations add the following to (prefix)/etc/defaults/couchdb\n(or equivalent):\nexport ERL_FLAGS=\"+A 4\"\n\n\n\n\n\n5.2.2. System Resource Limits\u00b6\nOne of the problems that administrators run into as their deployments become\nlarge are resource limits imposed by the system and by the application\nconfiguration. Raising these limits can allow your deployment to grow beyond\nwhat the default configuration will support.\n\n5.2.2.1. CouchDB Configuration Options\u00b6\n\n5.2.2.1.1. max_dbs_open\u00b6\nIn your configuration (local.ini or similar) familiarize\nyourself with the couchdb/max_dbs_open:\n[couchdb]\nmax_dbs_open = 100\n\n\nThis option places an upper bound on the number of databases that can be\nopen at one time. CouchDB reference counts database accesses internally and\nwill close idle databases when it must. Sometimes it is necessary to keep\nmore than the default open at once, such as in deployments where many databases\nwill be continuously replicating.\n\n\n\n5.2.2.2. Erlang\u00b6\nEven if you\u2019ve increased the maximum connections CouchDB will allow,\nthe Erlang runtime system will not allow more than 65536 connections by\ndefault. Adding the following directive to (prefix)/etc/vm.args (or\nequivalent) will increase this limit (in this case to 102400):\n+Q 102400\n\n\nNote that on Windows, Erlang will not actually increase the file descriptor\nlimit past 8192 (i.e. the system header\u2013defined value of FD_SETSIZE). On\nmacOS, the limit may be as low as 1024. See this tip for a possible\nworkaround and this thread for a deeper explanation.\n\n\n5.2.2.3. Maximum open file descriptors (ulimit)\u00b6\nIn general, modern UNIX-like systems can handle very large numbers of file\nhandles per process (e.g. 100000) without problem. Don\u2019t be afraid to increase\nthis limit on your system.\nThe method of increasing these limits varies, depending on your init system and\nparticular OS release. The default value for many OSes is 1024 or 4096. On a\nsystem with many databases or many views, CouchDB can very rapidly hit this\nlimit.\nFor systemd-based Linuxes (such as CentOS/RHEL 7, Ubuntu 16.04+, Debian 8\nor newer), assuming you are launching CouchDB from systemd, you must\noverride the upper limit via editing the override file. The best practice\nfor this is via the systemctl edit couchdb command. Add these lines to\nthe file in the editor:\n[Service]\nLimitNOFILE=65536\n\n\n\u2026or whatever value you like. To increase this value higher than 65536, you\nmust also add the Erlang +Q parameter to your etc/vm.args file by\nadding the line:\n+Q 102400\n\n\nThe old ERL_MAX_PORTS environment variable is ignored by the version of\nErlang supplied with CouchDB.\nIf your system is set up to use the Pluggable Authentication Modules (PAM),\nand you are not launching CouchDB from systemd, increasing this limit\nis straightforward. For example, creating a file named\n/etc/security/limits.d/100-couchdb.conf with the following contents will\nensure that CouchDB can open up to 65536 file descriptors at once:\n#<domain>    <type>    <item>    <value>\ncouchdb      hard      nofile    65536\ncouchdb      soft      nofile    65536\n\n\nIf you are using our Debian/Ubuntu sysvinit script (/etc/init.d/couchdb),\nyou also need to raise the limits for the root user:\n#<domain>    <type>    <item>    <value>\nroot         hard      nofile    65536\nroot         soft      nofile    65536\n\n\nYou may also have to edit the /etc/pam.d/common-session and\n/etc/pam.d/common-session-noninteractive files to add the line:\nsession required pam_limits.so\n\n\nif it is not already present.\nIf your system does not use PAM, a ulimit command is usually available for\nuse in a custom script to launch CouchDB with increased resource limits.\nTypical syntax would be something like ulimit -n 65536.\n\n\n\n5.2.3. Network\u00b6\nThere is latency overhead making and receiving each request/response.\nIn general you should do your requests in batches. Most APIs have some\nmechanism to do batches, usually by supplying lists of documents or keys in\nthe request body. Be careful what size you pick for the batches. The larger\nbatch requires more time your client has to spend encoding the items into JSON\nand more time is spent decoding that number of responses. Do some benchmarking\nwith your own configuration and typical data to find the sweet spot.\nIt is likely to be between one and ten thousand documents.\nIf you have a fast I/O system then you can also use concurrency - have\nmultiple requests/responses at the same time. This mitigates the latency\ninvolved in assembling JSON, doing the networking and decoding JSON.\nAs of CouchDB 1.1.0, users often report lower write performance of documents\ncompared to older releases. The main reason is that this release ships with\nthe more recent version of the HTTP server library MochiWeb, which by default\nsets the TCP socket option SO_NODELAY to false. This means that small data\nsent to the TCP socket, like the reply to a document write request (or reading\na very small document), will not be sent immediately to the network - TCP will\nbuffer it for a while hoping that it will be asked to send more data through\nthe same socket and then send all the data at once for increased performance.\nThis TCP buffering behaviour can be disabled via\nhttpd/socket_options:\n[httpd]\nsocket_options = [{nodelay, true}]\n\n\n\nSee also\nBulk load and store API.\n\n\n5.2.3.1. Connection limit\u00b6\nMochiWeb handles CouchDB requests.\nThe default maximum number of connections is 2048. To change this limit, use the\nserver_options configuration variable. max indicates maximum number of\nconnections.\n[chttpd]\nserver_options = [{backlog, 128}, {acceptor_pool_size, 16}, {max, 4096}]\n\n\n\n\n\n5.2.4. CouchDB\u00b6\n\n5.2.4.1. DELETE operation\u00b6\nWhen you DELETE a document the database will create a new\nrevision which contains the _id and _rev fields as well as\nthe _deleted flag. This revision will remain even after a database\ncompaction so that the deletion can be replicated. Deleted documents, like\nnon-deleted documents, can affect view build times, PUT and\nDELETE request times, and the size of the database since they\nincrease the size of the B+Tree. You can see the number of deleted documents\nin database information. If your use case creates lots of\ndeleted documents (for example, if you are storing short-term data like log\nentries, message queues, etc), you might want to periodically switch to a new\ndatabase and delete the old one (once the entries in it have all expired).\n\n\n5.2.4.2. Document\u2019s ID\u00b6\nThe db file size is derived from your document and view sizes but also on a\nmultiple of your _id sizes. Not only is the _id present in the document,\nbut it and parts of it are duplicated in the binary tree structure CouchDB uses\nto navigate the file to find the document in the first place. As a real world\nexample for one user switching from 16 byte ids to 4 byte ids made a database\ngo from 21GB to 4GB with 10 million documents (the raw JSON text when from\n2.5GB to 2GB).\nInserting with sequential (and at least sorted) ids is faster than random ids.\nConsequently you should consider generating ids yourself, allocating them\nsequentially and using an encoding scheme that consumes fewer bytes.\nFor example, something that takes 16 hex digits to represent can be done in\n4 base 62 digits (10 numerals, 26 lower case, 26 upper case).\n\n\n\n5.2.5. Views\u00b6\n\n5.2.5.1. Views Generation\u00b6\nViews with the JavaScript query server are extremely slow to generate when\nthere are a non-trivial number of documents to process. The generation process\nwon\u2019t even saturate a single CPU let alone your I/O. The cause is the latency\ninvolved in the CouchDB server and separate couchjs query server, dramatically\nindicating how important it is to take latency out of your implementation.\nYou can let view access be \u201cstale\u201d but it isn\u2019t practical to determine when\nthat will occur giving you a quick response and when views will be updated\nwhich will take a long time. (A 10 million document database took about 10\nminutes to load into CouchDB but about 4 hours to do view generation).\nIn a cluster, \u201cstale\u201d requests are serviced by a fixed set of shards in order\nto present users with consistent results between requests. This comes with an\navailability trade-off - the fixed set of shards might not be the most\nresponsive / available within the cluster. If you don\u2019t need this kind of\nconsistency (e.g. your indexes are relatively static), you can tell CouchDB to\nuse any available replica by specifying stable=false&update=false instead of\nstale=ok, or stable=false&update=lazy instead of stale=update_after.\nView information isn\u2019t replicated - it is rebuilt on each database so you\ncan\u2019t do the view generation on a separate sever.\n\n\n5.2.5.2. Built-In Reduce Functions\u00b6\nIf you\u2019re using a very simple view function that only performs a sum or count\nreduction, you can call native Erlang implementations of them by simply\nwriting _sum or _count in place of your function declaration.\nThis will speed up things dramatically, as it cuts down on IO between CouchDB\nand the JavaScript query server. For example, as\nmentioned on the mailing list, the time for outputting an (already indexed\nand cached) view with about 78,000 items went down from 60 seconds to 4 seconds.\nBefore:\n{\n\"_id\": \"_design/foo\",\n\"views\": {\n\"bar\": {\n\"map\": \"function (doc) { emit(doc.author, 1); }\",\n\"reduce\": \"function (keys, values, rereduce) { return sum(values); }\"\n}\n}\n}\n\n\nAfter:\n{\n\"_id\": \"_design/foo\",\n\"views\": {\n\"bar\": {\n\"map\": \"function (doc) { emit(doc.author, 1); }\",\n\"reduce\": \"_sum\"\n}\n}\n}\n\n\n\nSee also\nBuilt-in Reduce Functions\n\n\n\n\n\n\n\n\nNext\nPrevious\n\n\n\n\n\u00a9 Copyright 2020, Apache Software Foundation. CouchDB\u00ae is a registered trademark of the Apache Software Foundation.\n\n\nRevision 3f39035f.\n\n\n\nBuilt with Sphinx using a theme provided by Read the Docs.\n\n\n\n\n\n\n\n\nRead the Docs\nv: stable\n\n\n\n\nVersions\nmaster\nlatest\nstable\n3.1.1\n2.3.1\n1.6.1\n\n\nDownloads\npdf\nhtml\nepub\n\n\nOn Read the Docs\n\nProject Home\n\n\nBuilds\n\n\n\nFree document hosting provided by Read the Docs.\n\n\n\n\n\n\n\n\n\n\n\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Install MSI with msiexec into specific directory", "id": 822, "answers": [{"answer_id": 817, "document_id": 504, "question_id": 822, "text": "Use TARGETDIR instead of INSTALLDIR. Note that the quote marks for TARGETDIR property are only around the path in the case of spaces.\nmsiexec /i \"msi path\" TARGETDIR=\"C:\\myfolder\" /qb. ", "answer_start": 127, "answer_category": null}], "is_impossible": false}], "context": "Using \"INSTALLDIR\" is not working properly because the MSI is installed into the default path and not into the specified path. Use TARGETDIR instead of INSTALLDIR. Note that the quote marks for TARGETDIR property are only around the path in the case of spaces.\nmsiexec /i \"msi path\" TARGETDIR=\"C:\\myfolder\" /qb. This worked for me as well. What property name you use depends largely on what tool was used to build the MSI file. My MSI file was built using WiX 3.11.1 and it appears that INSTALLFOLDER is the property name that's used by WiX, while TARGETDIR is used by others\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Kubernetes deployment read-only filesystem error", "id": 1308, "answers": [{"answer_id": 1299, "document_id": 878, "question_id": 1308, "text": "The source of your problems for two reasons: first, you have smashed the airflow user's home directory by volume mounting your secret onto the image directly into a place where the image expects a directory owned by airflow.", "answer_start": 209, "answer_category": null}], "is_impossible": false}], "context": "I am facing an error while deploying Airflow on Kubernetes (precisely this version of Airflow https://github.com/puckel/docker-airflow/blob/1.8.1/Dockerfile) regarding writing permissions onto the filesystem.\nThe source of your problems for two reasons: first, you have smashed the airflow user's home directory by volume mounting your secret onto the image directly into a place where the image expects a directory owned by airflow.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Can I deploy my ReactJS app on a regular host?", "id": 1270, "answers": [{"answer_id": 1262, "document_id": 841, "question_id": 1270, "text": "You can set .css in principle in one file (or separate folder with several files) and include at the top of your index.html", "answer_start": 420, "answer_category": null}], "is_impossible": false}], "context": "I'm new to React, and I've seen many guides where people teach you how to deploy your react app on services like digital ocean, heroku, GitHub Pages, aws. But I'm wondering if I can deploy my React app (create-react-app) which consists of only front end in a host service like 000webhost or Ipage? Because a person wants me to design a website, and he says that he already has a domain name and a host service in lpage.\nYou can set .css in principle in one file (or separate folder with several files) and include at the top of your index.html\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Ant error when trying to build file, can't find tools.jar?", "id": 1720, "answers": [{"answer_id": 1708, "document_id": 1293, "question_id": 1720, "text": "You need JDK for that.\nSet JAVA_HOME to point to the JDK.", "answer_start": 372, "answer_category": null}], "is_impossible": false}], "context": "When I run ant it says:\nUnable to locate tools.jar. Expected to find it in C:\\Program Files\\Java\\jre6\\lib\\tools.jar\nBuildfile: build.xml does not exist!\nBuild failed\nWhat package can I use to download the file required > C:\\Program Files\\Java\\jre6\\lib\\tools.jar\nI just downloaded this one:\njre-6u19-windows-i586-s.exe\nbut unfortunately it appears that it was not on it...\nYou need JDK for that.\nSet JAVA_HOME to point to the JDK.\nThis is a really useful answer. The error seemed to occur out of the blue for me, I checked the classpath and it was pointing to JDK just as it is normal. The problem must have been related to an update that Java prompted me to install a day earlier, which apparently modified JRE such that tools.jar was removed from the lib path. What I did to was copy the tools.jar from JDK/lib to JRE/lib and the error disappeared\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Removing .svn folders from project for deployment", "id": 1333, "answers": [{"answer_id": 1323, "document_id": 902, "question_id": 1333, "text": "You can use:\nsvn export <url-to-repo> <dest-path>\nIt gets just the source, nothing else. Look in svn export (in Version Control with Subversion) for more information.", "answer_start": 211, "answer_category": null}], "is_impossible": false}], "context": "I'm using subversion (TortoiseSVN) and I want to remove the .svn folders from my project for deployment, is there an automated way of doing this using subversion or do I have to create a custom script for this?\nYou can use:\nsvn export <url-to-repo> <dest-path>\nIt gets just the source, nothing else. Look in svn export (in Version Control with Subversion) for more information.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Is there any way to ignore INSTALL_FAILED_VERSION_DOWNGRADE on application install with the Android Debug Bridge?", "id": 889, "answers": [{"answer_id": 884, "document_id": 562, "question_id": 889, "text": "So use: adb install -r -d <link to apk>", "answer_start": 1310, "answer_category": null}], "is_impossible": false}], "context": "131\n\n\n29\nIt seems like the most recent Android 4.2 has introduced this error condition on installation when one attempts to install an APK with a lower version. In prior versions of Android, one would be able to install older APK's simply via adb install -r <link to APK>. For debugging purposes, I frequently need to re-test older APK's; and the -r flag would replace the older build in older Android versions. Is there a work-around here to ignore [INSTALL_FAILED_VERSION_DOWNGRADE]?\n\nandroid\ninstallation\nadb\nShare\nImprove this question\nFollow\nedited Sep 3 '14 at 8:50\n\nJonik\n75.9k6767 gold badges251251 silver badges360360 bronze badges\nasked Nov 15 '12 at 15:17\n\nAaronMT\n1,43522 gold badges1010 silver badges66 bronze badges\n1\nYou mean, besides uninstalling the newer version? \u2013 \nCommonsWare\n Nov 15 '12 at 17:21\n1\nCorrect; I assumed the -r flag would handle this. \u2013 \nAaronMT\n Nov 15 '12 at 17:30\n5\nThis question also valid for cases where you have (somewhat incorrectly) misnumbered builds. E.g. maybe your v1.0 from a dev box has a higher versionCode than v2.0 from a build server. \u2013 \nparkerfath\n Mar 18 '14 at 0:59 \nAdd a comment\n11 Answers\n\n308\n\nIt appears the latest version of adb tools has an \"allow downgrade flag\" that isn't shown in the adb help, but it is shown in the \"pm\" help on the device. So use: adb install -r -d <link to apk>\n\nShare\nImprove this answer\nFollow\nanswered Dec 7 '12 at 23:25\n\nsupereee\n3,21511 gold badge1212 silver badges99 bronze badges\n5\nTried with adb install -rd <apkfile> which didn't work. Your versino (with options separated) works fine. Thanks! \u2013 \nMarSoft\n Nov 22 '15 at 22:05\n1\nAnd it seems to be documented now! Just not that you have to use -r -d, -rd will not work \u2013 \nplaisthos\n Jan 8 '16 at 12:44\n19\nSince Android 7 (Nougat), adb install -d no longer works unless the package is marked as debuggable. android.googlesource.com/platform/frameworks/base/+/921dd75 \u2013 \nDarpan\n Jun 18 '18 at 10:28 \n3\nFor me it worked when I did adb install -t -r -d app.apk \u2013 \narekolek\n Oct 13 '20 at 6:53\n1\n@Darpan So for signed APKs, it's impossible to install older versions on top of new ones, right? \u2013 \nandroid developer\n Mar 1 at 9:03\nShow 1 more comment\n\n21\n\nYou can try and use adb uninstall -k <package> and then installing the older apk. From the adb usage guide:\n\n  adb uninstall [-k] <package> - remove this app package from the device\n                                 ('-k' means keep the data and cache directories)\nI've tried it myself with my apk, and it seems to work for most of the data (some data like RawContacts was not saved)\n\nShare\nImprove this answer\nFollow\nanswered Dec 3 '12 at 12:33\n\nTalihawk\n1,23299 silver badges1818 bronze badges\nIt appears that this command does not actually execute the uninstall, instead simply yielding a warning. The command to actually excecute this appears to be adb shell pm uninstall -k <package>. \u2013 \nPaul Lammertsma\n Dec 23 '13 at 13:40\nThis is also useful for downgrading, if adb -d still complains as it did for me. \u2013 \nPointer Null\n Apr 9 '15 at 21:45\nAdd a comment\n\n16\n\nDid you enabled Multiple account on your device (and push your apk via ADB)? If so you have to remove the apk in every account. After complete uninstall, your push will be OK.\n\nShare\nImprove this answer\nFollow\nanswered Nov 21 '12 at 15:07\n\nOlivierTurpin\n28011 silver badge44 bronze badges\nRestoring backup via Titanium Backup did not work. Fresh install did not work. Installing via APK and/or adb install did not work. This cost me 1,5h and your simple tip did the trick. Thanks! \u2013 \nbentolor\n Sep 17 '17 at 9:02\nThis helped in the following situation: I uninstalled an app and restored an older version with Titanium Backup. After an automatic app update I tried to downgrade again with TB > hung up on restore; tried installing the APK manually > didn't work with the error \"App not installed\". So my take away is that TB falesly seems to install apps globally for all users. Samsung S9, Android 9. \u2013 \nsir_brickalot\n Sep 6 '20 at 12:54", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to invoke python?", "id": 412, "answers": [{"answer_id": 420, "document_id": 174, "question_id": 412, "text": "The Python interpreter is usually installed as /usr/local/bin/python3.10 on those machines where it is available; putting /usr/local/bin in your Unix shell\u2019s search path makes it possible to start it by typing the command:\n\npython3.10\nto the shell.", "answer_start": 30, "answer_category": null}], "is_impossible": false}, {"question": "How to use python 3.10 on Windows?\n", "id": 413, "answers": [{"answer_id": 421, "document_id": 174, "question_id": 413, "text": "On Windows machines where you have installed Python from the Microsoft Store, the python3.10 command will be available. ", "answer_start": 518, "answer_category": null}], "is_impossible": false}, {"question": "What are the interpreter\u2019s line-editing features?", "id": 415, "answers": [{"answer_id": 423, "document_id": 174, "question_id": 415, "text": "\nThe interpreter\u2019s line-editing features include interactive editing, history substitution and code completion on systems that support the GNU Readline library.", "answer_start": 1031, "answer_category": null}], "is_impossible": false}, {"question": "How to use python modules as scripts?", "id": 416, "answers": [{"answer_id": 424, "document_id": 174, "question_id": 416, "text": "Some Python modules are also useful as scripts. These can be invoked using python -m module [arg] ..., which executes the source file for module as if you had spelled out its full name on the command line.", "answer_start": 2240, "answer_category": null}], "is_impossible": false}, {"question": "How to run scripts in interactive mode?", "id": 418, "answers": [{"answer_id": 426, "document_id": 174, "question_id": 418, "text": "This can be done by passing -i before the script.", "answer_start": 2566, "answer_category": null}], "is_impossible": false}], "context": "2.1. Invoking the Interpreter\nThe Python interpreter is usually installed as /usr/local/bin/python3.10 on those machines where it is available; putting /usr/local/bin in your Unix shell\u2019s search path makes it possible to start it by typing the command:\n\npython3.10\nto the shell. 1 Since the choice of the directory where the interpreter lives is an installation option, other places are possible; check with your local Python guru or system administrator. (E.g., /usr/local/python is a popular alternative location.)\n\nOn Windows machines where you have installed Python from the Microsoft Store, the python3.10 command will be available. If you have the py.exe launcher installed, you can use the py command. See Excursus: Setting environment variables for other ways to launch Python.\n\nTyping an end-of-file character (Control-D on Unix, Control-Z on Windows) at the primary prompt causes the interpreter to exit with a zero exit status. If that doesn\u2019t work, you can exit the interpreter by typing the following command: quit().\n\nThe interpreter\u2019s line-editing features include interactive editing, history substitution and code completion on systems that support the GNU Readline library. Perhaps the quickest check to see whether command line editing is supported is typing Control-P to the first Python prompt you get. If it beeps, you have command line editing; see Appendix Interactive Input Editing and History Substitution for an introduction to the keys. If nothing appears to happen, or if ^P is echoed, command line editing isn\u2019t available; you\u2019ll only be able to use backspace to remove characters from the current line.\n\nThe interpreter operates somewhat like the Unix shell: when called with standard input connected to a tty device, it reads and executes commands interactively; when called with a file name argument or with a file as standard input, it reads and executes a script from that file.\n\nA second way of starting the interpreter is python -c command [arg] ..., which executes the statement(s) in command, analogous to the shell\u2019s -c option. Since Python statements often contain spaces or other characters that are special to the shell, it is usually advised to quote command in its entirety with single quotes.\n\nSome Python modules are also useful as scripts. These can be invoked using python -m module [arg] ..., which executes the source file for module as if you had spelled out its full name on the command line.\n\nWhen a script file is used, it is sometimes useful to be able to run the script and enter interactive mode afterwards. This can be done by passing -i before the script.\n\nAll command line options are described in Command line and environment.\n\n2.1.1. Argument Passing\nWhen known to the interpreter, the script name and additional arguments thereafter are turned into a list of strings and assigned to the argv variable in the sys module. You can access this list by executing import sys. The length of the list is at least one; when no script and no arguments are given, sys.argv[0] is an empty string. When the script name is given as '-' (meaning standard input), sys.argv[0] is set to '-'. When -c command is used, sys.argv[0] is set to '-c'. When -m module is used, sys.argv[0] is set to the full name of the located module. Options found after -c command or -m module are not consumed by the Python interpreter\u2019s option processing but left in sys.argv for the command or module to handle.\n\n2.1.2. Interactive Mode\nWhen commands are read from a tty, the interpreter is said to be in interactive mode. In this mode it prompts for the next command with the primary prompt, usually three greater-than signs (>>>); for continuation lines it prompts with the secondary prompt, by default three dots (...). The interpreter prints a welcome message stating its version number and a copyright notice before printing the first prompt:", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "react-native iOS app not showing static assets (images) after deploying", "id": 569, "answers": [{"answer_id": 572, "document_id": 294, "question_id": 569, "text": "Please check XCode console log first. You will find that the app couldn't find out those specific image assets on path \"{project name}.app/assets/resources/..\"", "answer_start": 181, "answer_category": null}], "is_impossible": false}], "context": "I have all my static images in a folder called \"images\" in the root of my project. However, after I run the following command to bundle my app, the app works but no image is shown. Please check XCode console log first. You will find that the app couldn't find out those specific image assets on path \"{project name}.app/assets/resources/..\"", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "how can i install qt 5 2 1 from the command line in cygwin", "id": 1516, "answers": [{"answer_id": 1505, "document_id": 1092, "question_id": 1516, "text": ".\\qt-opensource-windows-x86-msvc2013_64-5.5.1.exe --script .\\qt-installer-noninteractive.qs", "answer_start": 4474, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\n$ wget --quiet http://download.qt-project.org/official_releases/qt/5.2/5.2.1/qt-opensource-windows-x86-msvc2012_64_opengl-5.2.1.exe\n$\n\n\nAs seen above, I first downloaded the Qt package for Visual Studio in a Cygwin Bash shell.\n\nA sidenote: The Qt library packaged within Cygwin is not useful for me because I need to use the Visual Studio C++ compiler.\n\nFirst I set the correct permissions on the file\n\n$ chmod 755 qt-opensource-windows-x86-msvc2012_64_opengl-5.2.1.exe\n\n\nIf I start it like this\n\n$ ./qt-opensource-windows-x86-msvc2012_64_opengl-5.2.1.exe\n\n\na graphical window (GUI) is shown but that is not what I want as I would later like to have the installation procedure written into a Bash script that I could run in Cygwin.\n\nIf I add the option --help, like this\n\n$ ./qt-opensource-windows-x86-msvc2012_64_opengl-5.2.1.exe --help\n\n\na new terminal window is opened with the following text \n\nUsage: SDKMaintenanceTool [OPTIONS]\n\nUser:\n  --help                                      Show commandline usage                  \n  --version                                   Show current version                    \n  --checkupdates                              Check for updates and return an XML file describing\n                                              the available updates                   \n  --updater                                   Start in updater mode.                  \n  --manage-packages                           Start in packagemanager mode.  \n  --proxy                                     Set system proxy on Win and Mac.        \n                                              This option has no effect on Linux.     \n  --verbose                                   Show debug output on the console        \n  --create-offline-repository                 Offline installer only: Create a local repository inside the\n                                              installation directory based on the offline\n                                              installer's content.                    \n\nDeveloper:\n  --runoperation [OPERATION] [arguments...]   Perform an operation with a list of arguments\n  --undooperation [OPERATION] [arguments...]  Undo an operation with a list of arguments\n  --script [scriptName]                       Execute a script                        \n  --no-force-installations                    Enable deselection of forced components \n  --addRepository [URI]                       Add a local or remote repo to the list of user defined repos.\n  --addTempRepository [URI]                   Add a local or remote repo to the list of temporary available\n                                              repos.                                  \n  --setTempRepository [URI]                   Set a local or remote repo as tmp repo, it is the only one\n                                              used during fetch.                      \n                                              Note: URI must be prefixed with the protocol, i.e. file:///\n                                              http:// or ftp://. It can consist of multiple\n                                              addresses separated by comma only.      \n  --show-virtual-components                   Show virtual components in package manager and updater\n  --binarydatafile [binary_data_file]         Use the binary data of another installer or maintenance tool.\n  --update-installerbase [new_installerbase]  Patch a full installer with a new installer base\n  --dump-binary-data -i [PATH] -o [PATH]      Dumps the binary content into specified output path (offline\n                                              installer only).                        \n                                              Input path pointing to binary data file, if omitted\n                                              the current application is used as input.\n\n\nI don't know how to continue from here. Do you know how I could install the Qt 5.2.1 library (for Visual Studio) from the Bash shell in Cygwin?\n\nUpdate: The advantage of writing the build script for a Cygwin environment is that commands like git, wget, and scp are available. This Stackoverflow answer describes how to invoke the MSVC compiler from a Cygwin bash script. Note, that the Qt application I'm building is not going to have any dependency on Cygwin.\n    \n\nI didn't test with Cygwin but I successfully installed Qt5.5 using a script. To do so, you must use the --script line of the normal installer.\n\n.\\qt-opensource-windows-x86-msvc2013_64-5.5.1.exe --script .\\qt-installer-noninteractive.qs\n\n\nHere's an example of qt-installer-noninteractive.qs file I used in the command above\n\nfunction Controller() {\n  installer.autoRejectMessageBoxes();\n  installer.installationFinished.connect(function() {\n    gui.clickButton(buttons.NextButton);\n  })\n}\n\nController.prototype.WelcomePageCallback = function() {\n  gui.clickButton(buttons.NextButton);\n}\n\nController.prototype.CredentialsPageCallback = function() {\n  gui.clickButton(buttons.NextButton);\n}\n\nController.prototype.IntroductionPageCallback = function() {\n  gui.clickButton(buttons.NextButton);\n}\n\nController.prototype.TargetDirectoryPageCallback = function() {\n  gui.currentPageWidget().TargetDirectoryLineEdit.setText(\"C:/Qt/Qt5.5.1\");\n  gui.clickButton(buttons.NextButton);\n}\n\nController.prototype.ComponentSelectionPageCallback = function() {\n  var widget = gui.currentPageWidget();\n\n  widget.deselectAll();\n  widget.selectComponent(\"qt.55.win64_msvc2013_64\");\n  // widget.selectComponent(\"qt.55.qt3d\");\n  // widget.selectComponent(\"qt.55.qtcanvas3d\");\n  // widget.selectComponent(\"qt.55.qtquick1\");\n  // widget.selectComponent(\"qt.55.qtscript\");\n  // widget.selectComponent(\"qt.55.qtwebengine\");\n  // widget.selectComponent(\"qt.55.qtquickcontrols\");\n  // widget.selectComponent(\"qt.55.qtlocation\");\n\n  gui.clickButton(buttons.NextButton);\n}\n\nController.prototype.LicenseAgreementPageCallback = function() {\n  gui.currentPageWidget().AcceptLicenseRadioButton.setChecked(true);\n  gui.clickButton(buttons.NextButton);\n}\n\nController.prototype.StartMenuDirectoryPageCallback = function() {\n  gui.clickButton(buttons.NextButton);\n}\n\nController.prototype.ReadyForInstallationPageCallback = function()\n{\n  gui.clickButton(buttons.NextButton);\n}\n\nController.prototype.FinishedPageCallback = function() {\n  var checkBoxForm = gui.currentPageWidget().LaunchQtCreatorCheckBoxForm\n  if (checkBoxForm &amp;&amp; checkBoxForm.launchQtCreatorCheckBox) {\n    checkBoxForm.launchQtCreatorCheckBox.checked = false;\n  }\n  gui.clickButton(buttons.FinishButton);\n}\n\n\nThe tricky part was to found the id of the components! I was able to found the right id qt.55.win64_msvc2013_64 by adding the flag --verbose and installing it normally with the UI and stopping at the last page; all the ids that you selected for installation are there.\n\nThere is slightly more information in this answer if you need more details.\n\n\n\nEDIT (29-11-2017): For installer 3.0.2-online, the \"Next\" button in the \"Welcome\" page is disabled for 1 second so you must add a delay\n\ngui.clickButton(buttons.NextButton, 3000);\n\n\nEDIT (10-11-2019): See Joshua Wade's answer for more traps and pitfalls, like the \"User Data Collection\" form and \"Archive\" and \"Latest releases\" checkboxes.\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "What tools do you use for Automated Builds / Automated Deployments? Why?", "id": 1244, "answers": [{"answer_id": 1237, "document_id": 816, "question_id": 1244, "text": "You can use Hudson for automated builds. ", "answer_start": 342, "answer_category": null}], "is_impossible": false}], "context": "As it currently stands, this question is not a good fit for our Q&A format. We expect answers to be supported by facts, references, or expertise, but this question will likely solicit debate, arguments, polling, or extended discussion. If you feel that this question can be improved and possibly reopened, visit the help center for guidance.\nYou can use Hudson for automated builds. I chose it because it was the easiest to setup and demo. A system that's too complex and isn't slick-looking won't impress management enough to get them on-board for automated builds. Especially in a project that has a lot of inertia.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How do I package a python application to make it pip-installable?", "id": 1849, "answers": [{"answer_id": 1835, "document_id": 1420, "question_id": 1849, "text": "you can specify required packages in your setup.py which are automatically fetched when installing your app.", "answer_start": 641, "answer_category": null}], "is_impossible": false}], "context": "I'm writing a django application in my spare time for a footy-tipping competition we're running at work. I figured I'd use this time wisely, and get up to speed on virtualenv, pip, packaging, django 1.3, and how to write an easily redistributable application. So far, so good.\nI'm up to the packaging part. A lot of the django apps on GitHub for instance are mostly bundled (roughly) the same way. I'll use django-uni-forms as an example.\nAn assumption I'm making is that the MANIFEST.in and setup.py are the only required pieces that pip needs to do its job. Is that correct? What other components are necessary if my assumption is wrong?\t\nyou can specify required packages in your setup.py which are automatically fetched when installing your app.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to install a Emacs plugin (many times it's a .el file) on Windows platform?", "id": 789, "answers": [{"answer_id": 785, "document_id": 472, "question_id": 789, "text": "add the following in your .emacs file:\n(add-to-list 'load-path \"~/.emacs.d/\")\n(load \"myplugin.el\")\nAlso, in many cases you would need the following instead of the second line:\n(require 'myplugin)", "answer_start": 229, "answer_category": null}], "is_impossible": false}], "context": "I'm new to Emacs. I found many emacs plugins are released as an .el file. I'm not sure how to install them. Can I just put them in my emacs installation directory? After placing it, say myplugin.el to your ~/.emacs.d/ directory, add the following in your .emacs file:\n(add-to-list 'load-path \"~/.emacs.d/\")\n(load \"myplugin.el\")\nAlso, in many cases you would need the following instead of the second line:\n(require 'myplugin). Many times, an emacs plugin will consist of a directory of elisp files that need to be accessible from the load path. This will ensure that all elisp files that are located either in either the ~/.emacs.d/site-lisp directory or in a subdirectory under that directory are accessible.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How do I run Visual Studio as an administrator by default?", "id": 1610, "answers": [{"answer_id": 1597, "document_id": 1184, "question_id": 1610, "text": "In Windows 8, you have to right-click devenv.exe and select \"Troubleshoot compatibility\".\nSelect \"Troubleshoot program\"\nCheck \"The program requires additional permissions\" click \"Next\", click \"Test the program...\"\nWait for the program to launch\nClick \"Next\"\nSelect \"Yes, save these settings for this program\"\nClick \"Close\"", "answer_start": 292, "answer_category": null}], "is_impossible": false}], "context": "I recently discovered that even while logged into my personal laptop as an administrator, Visual Studio does not run in administrator mode and you need to explicitly use Run As Administrator.\nIs there a way to make it run as an administrator by default, other than creating a shortcut, etc.?\nIn Windows 8, you have to right-click devenv.exe and select \"Troubleshoot compatibility\".\nSelect \"Troubleshoot program\"\nCheck \"The program requires additional permissions\" click \"Next\", click \"Test the program...\"\nWait for the program to launch\nClick \"Next\"\nSelect \"Yes, save these settings for this program\"\nClick \"Close\"\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Moq - Is it possible to specify in a Setup the Verify criteria (e.g. Times called)?", "id": 1139, "answers": [{"answer_id": 1132, "document_id": 716, "question_id": 1139, "text": "So in order to avoid duplication I generally resort to the following:\nExpression<Action<MockedType>> expression = mockedTypeInstance => mockedTypeInstance.MockedMethod(It.Is<TFirstArgument>(firstArgument => <some complex statement>)/*, ...*/);\n_mock.Setup(expression);\n\n/* run the test*/\n\n_mock.Verify(expression, Times.Once);", "answer_start": 631, "answer_category": null}], "is_impossible": false}], "context": "I just don't like having to call Setup and Verify. Well, since this is a good idea for AAA, to rephrase, I don't like having to repeat the expression for the Setup and Verify. At the moment I store the expression in a variable and pass it to each method, but doesn't feel so clean. I have this problem all the time. I use strict mocks, and I want to specify strictly (i.e. I used It.Is<>() instead of It.IsAny()) as well as verify strictly (i.e. specifying Times). You cannot use verifiable for this sadly, because Moq is missing a Verifiable(Times) overload.\nThe full expression of the call, including It.Is<>() is generally big. So in order to avoid duplication I generally resort to the following:\nExpression<Action<MockedType>> expression = mockedTypeInstance => mockedTypeInstance.MockedMethod(It.Is<TFirstArgument>(firstArgument => <some complex statement>)/*, ...*/);\n_mock.Setup(expression);\n\n/* run the test*/\n\n_mock.Verify(expression, Times.Once);\nNot extremely readable, but I don't think there is another way to both use strict setup and strict verification.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "IIS7 deployment - duplicate 'system.web.extensions/scripting/scriptResourceHandler' section", "id": 1651, "answers": [{"answer_id": 1639, "document_id": 1225, "question_id": 1651, "text": " you will need to cleanup the web.config that includes all the section Definitions that point to .net 3.5.", "answer_start": 422, "answer_category": null}], "is_impossible": false}], "context": "On attempting to deploy a .net 3.5 website on the default app pool in IIS7 having the framework section set to 4.0, I get the following error.\nThere is a duplicate 'system.web.extensions/scripting/scriptResourceHandler' section defined.\nCommenting off the offending lines didn't help either. Any pointers on what I need to do or look at?\nif your plan is to deploy to an IIS that has an Application Pool running in .net 4.0 you will need to cleanup the web.config that includes all the section Definitions that point to .net 3.5. The reason this fails is because these section definitions are already included in the root web.config in .NET 4.0 (see %windir%\\microsoft.net\\framework\\v4.0.30319\\config\\machine.config) that include all the system.web.extensions declared already\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to realise a deployment branch in Git", "id": 1111, "answers": [{"answer_id": 1103, "document_id": 688, "question_id": 1111, "text": "You should:\nmanage those config/doc files a separate git sub-projects (note: the use of submodules has been discussed here)\nor record partial merge (using \"ours\" strategy for files we don't want to merge), then --amend it.", "answer_start": 246, "answer_category": null}], "is_impossible": false}], "context": "I'm using git for a PHP project, I think it's really handy. There is one thing that would be great if I get it to work.\nI have created a branch, meant for deployment. It has some differences, like different configuration files and documentation.\nYou should:\nmanage those config/doc files a separate git sub-projects (note: the use of submodules has been discussed here)\nor record partial merge (using \"ours\" strategy for files we don't want to merge), then --amend it.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Run ant exec task from different folder", "id": 1335, "answers": [{"answer_id": 1325, "document_id": 904, "question_id": 1335, "text": "You can try:\nAttribute    Description\ndir          the directory in which the command should be executed.\n<exec executable=\"${grails}\" dir=\"${my.project.dir}\">", "answer_start": 188, "answer_category": null}], "is_impossible": false}], "context": "I want to run my \"exec grails\" task into my grails project. I set grail path in exec task like\n <exec executable=\"${grails}\"\nHow can I say , that exec should start from my project folder?\nYou can try:\nAttribute    Description\ndir          the directory in which the command should be executed.\n<exec executable=\"${grails}\" dir=\"${my.project.dir}\">\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Can IntelliJ display a searchable maven dependency like Eclipse?", "id": 1821, "answers": [{"answer_id": 1806, "document_id": 1392, "question_id": 1821, "text": "You can try Maven Helper plugin: once you install it, every POM file gets a tab for \"Dependency Analyzer\" which also includes a search bar.\nIf you need something more, report an issue. But it should be easy to implement it yourself and send a pull request.", "answer_start": 621, "answer_category": null}], "is_impossible": false}], "context": "I'm starting to use IntelliJ IDEA 13 Ultimate Edition and was wonder if it had a searchable tabular view of maven dependencies for a project like Eclipse does. For example, in Eclipse I can check my project's maven dependencies by going to its pom and then clicking on the \"Dependencies Hierarchy\" tab. From there, I can search for the existence of specific dependencies and have the ability to view the POMs of said dependencies.\nAll I've found so far in IntelliJ is the diagram view of dependencies, which can get really cluttered when you have a lot of dependencies and doesn't really allow me to view POMs or search.\nYou can try Maven Helper plugin: once you install it, every POM file gets a tab for \"Dependency Analyzer\" which also includes a search bar.\nIf you need something more, report an issue. But it should be easy to implement it yourself and send a pull request.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Wordpress keeps redirecting to install-php after migration", "id": 778, "answers": [{"answer_id": 775, "document_id": 462, "question_id": 778, "text": "Eventually I realized that the Wordpress MySQL-user on my production environment had not been assigned sufficient privileges", "answer_start": 402, "answer_category": null}], "is_impossible": false}], "context": "And then when I try to open my site on the new location it just directs me to wp-admin/install.php Now just to make the scenario clearer: The destination folder(on live server) is a sub directori in a public_html folder which already has another wordpress install inside it(I'm saying this just in case it should matter). I experienced a similar issue. None of the suggestions above helped me, though.\nEventually I realized that the Wordpress MySQL-user on my production environment had not been assigned sufficient privileges.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How do I find the fully qualified name of an assembly?", "id": 1638, "answers": [{"answer_id": 1626, "document_id": 1212, "question_id": 1638, "text": "If you can load the assembly into a .NET application, you can do:\ntypeof(SomeTypeInTheAssembly).Assembly.FullName\nIf you cannot then you can use ildasm.exe and it will be in there somewhere:\nildasm.exe MyAssembly.dll /text", "answer_start": 161, "answer_category": null}], "is_impossible": false}], "context": "How do I find out the fully qualified name of my assembly such as:\nMyNamespace.MyAssembly, version=1.0.3300.0, \nCulture=neutral, PublicKeyToken=b77a5c561934e089\nIf you can load the assembly into a .NET application, you can do:\ntypeof(SomeTypeInTheAssembly).Assembly.FullName\nIf you cannot then you can use ildasm.exe and it will be in there somewhere:\nildasm.exe MyAssembly.dll /text\nI've managed to get my PublicKeyToken using the sn.exe in the SDK, but I'ld like to easily get the full qualified name.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "cannot download, $GOPATH not set", "id": 1537, "answers": [{"answer_id": 1526, "document_id": 1114, "question_id": 1537, "text": "You may still find this useful if you want to understand the GOPATH layout, customize it, etc.]\nThe official Go site discusses GOPATH and how to lay out a workspace directory.", "answer_start": 239, "answer_category": null}], "is_impossible": false}], "context": "I want to install json2csv using go get github.com/jehiah/json2csv but I receive this error:\npackage github.com/jehiah/json2csv: cannot download, $GOPATH not set. For more details see: go help go path\nAny help on how to fix this on MacOS?\nYou may still find this useful if you want to understand the GOPATH layout, customize it, etc.]\nThe official Go site discusses GOPATH and how to lay out a workspace directory.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Why the snapshot name always has date in its jar file name ? How to remove it", "id": 1339, "answers": [{"answer_id": 1329, "document_id": 908, "question_id": 1339, "text": "It can be removed, this site use the -DuniqueVersion=false parameter :\nmvn deploy:deploy-file -Durl=file:///C:/m2-repo \\\n                   -DrepositoryId=some.id \\\n                   -Dfile=your-artifact-1.0.jar \\\n                   -DpomFile=your-pom.xml \\\n                   -DuniqueVersion=false", "answer_start": 172, "answer_category": null}], "is_impossible": false}], "context": "When I deploy jar, maven always add date in its file name, this make the file name repository different from the file in my local. How can I remove the date in file name ?\nIt can be removed, this site use the -DuniqueVersion=false parameter :\nmvn deploy:deploy-file -Durl=file:///C:/m2-repo \\\n                   -DrepositoryId=some.id \\\n                   -Dfile=your-artifact-1.0.jar \\\n                   -DpomFile=your-pom.xml \\\n                   -DuniqueVersion=false\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Different dependencies for different build profiles", "id": 1829, "answers": [{"answer_id": 1815, "document_id": 1400, "question_id": 1829, "text": "Your groupId, artifactId should be tokenized in your profiles as properties and you can move your dependencies to the generic section.", "answer_start": 254, "answer_category": null}], "is_impossible": false}], "context": "Is it possible to have a different set of dependencies in a maven pom.xml file for different profiles?\nI'd like to pick up a different dependency jar file in one profile that has the same class names and different implementations of the same interfaces.\nYour groupId, artifactId should be tokenized in your profiles as properties and you can move your dependencies to the generic section.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "grant system permissions to an app in the android emulator", "id": 1984, "answers": [{"answer_id": 1970, "document_id": 1569, "question_id": 1984, "text": "ed to:\n\n\ndownload the Android source and build an emulator firmware image.\nsign your application with the keys in the Android source tree at /build/target/product/security/.\nadd android:sharedUserId=\"android.uid.system\" to your application's manifest.\nrun your application on an emulator using the image built i", "answer_start": 1700, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI am building an app that will be bundled on an android device as a system app.  The manufacturer is a ways out on delivering the device to us, so in the meantime I'd like to grant my app system level permissions in the emulator so I can work on an auto update feature that will do silent installs of APKs without any interactions from the user.  From what I've read, its my understanding that the only way to be able to do silent installs on android is if your app is signed with the same cert as the OS.  So how can I simulate this in the emulator?\n    \n\nIf you want a signatureOrSystem permission, you just need to be placed on the system image; you don't need to be signed with any special cert.  You can do this as a one-off (until you exit the emulator) like this:\n\n&gt; adb root\n&gt; adb remount\n&gt; adb push /path/to/My.apk /system/app/My.apk\n\n\nOnce you have done that, you can use the normal process to install further updates on the data partition (\"adb install -r /path/to/My.apk\" which is what the developer tools do when you run from Eclipse).  When installing this way, the app retains any signatureOrSystem permissions it had requested from the original version on the system image, but can not gain any new such permissions.\n\nIf you need pure signature permissions, you need to sign your app with the same cert as whatever is declaring those permissions (typically the core framework, but the media system is a separate cert etc).  If you are requesting signature permissions you don't need to be installed on the system image, you can just install it as a normal app and it can still get the permissions because of the signing.\n    \n\nAs far as I can tell, you need to:\n\n\ndownload the Android source and build an emulator firmware image.\nsign your application with the keys in the Android source tree at /build/target/product/security/.\nadd android:sharedUserId=\"android.uid.system\" to your application's manifest.\nrun your application on an emulator using the image built in step 1.\n\n\nThe reason for having to build your own firmware image is so that you can get at the keys. Now, it might be possible that the keys for the standard emulator image are available somewhere, which will save you the long and exceedingly tedious process of building your own Android, but I'm afraid I have no idea where these might be.\n\nDisclaimer: never tried this myself.\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to include other files to the output directory in C# upon build?", "id": 712, "answers": [{"answer_id": 715, "document_id": 402, "question_id": 712, "text": "You can add files to your project and select their properties: \"Build Action\" as \"Content\" and \"Copy to output directory\" as \"Copy Always\" or Copy if Newer (the latter is preferable because otherwise the project rebuilds fully every time you build it)", "answer_start": 529, "answer_category": null}], "is_impossible": false}], "context": "I have some library files needed for my application to work.\nMy application has a setup and deployment included.\nI already know that in order for a library file to be added to the output directory of the application when installing, I just have to reference those libraries inside the .NET IDE before building... the only problem is that these libraries can't be referenced... So I need to be able to copy these libraries to the installation directory of my application... At the moment, I am copying these libraries manually... You can add files to your project and select their properties: \"Build Action\" as \"Content\" and \"Copy to output directory\" as \"Copy Always\" or Copy if Newer (the latter is preferable because otherwise the project rebuilds fully every time you build it). Note that Setup and Deployment projects are NOT supported in Visual Studio 2012.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "JBoss: WAR file in EAR can't find JAR library on classpath", "id": 1353, "answers": [{"answer_id": 1343, "document_id": 922, "question_id": 1353, "text": "you need to go back to using explicitly declared JAR files:\n<module>\n    <java>lib/core.jar</java>\n</module>\nAs for your properties files, you need to add the directory that they're in as a java module, so for your example:\n<module>\n    <java>lib/classes</java>\n</module>", "answer_start": 597, "answer_category": null}], "is_impossible": false}], "context": "I am having a problem deploying an ear with bundled wars, jars, and configuration files (.properties files) on JBoss 4.3-eap.\nThese are just a few of the sites I have looked at. The problems might stem from the setup of the project as well, as this is a large established project that is (to an extent) being migrated from a weblogic deployment to Jboss. So if there is anything that SHOULD work, but doesn't, it might be an issue with some of the code/project configuration. Unfortunately, I am not at the point yet where I can tell if its a JBoss related problem, or a problem with the project.\nyou need to go back to using explicitly declared JAR files:\n<module>\n    <java>lib/core.jar</java>\n</module>\nAs for your properties files, you need to add the directory that they're in as a java module, so for your example:\n<module>\n    <java>lib/classes</java>\n</module>\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "installing php and apache on amazon ec2 instances", "id": 1952, "answers": [{"answer_id": 1939, "document_id": 1534, "question_id": 1952, "text": "you read the error message ? Read again:\n\nError: httpd24-tools conflicts with httpd-tools-2.2.25-1.0.amzn1.x86_64\nError: php54-common conflict", "answer_start": 1522, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI am trying to install PHP on amazon ec2 instances but when I run the following command:\nsudo yum -y install httpd php54-fpm php54-common \n\nit gives me the following errors:\n\n     Loaded plugins: priorities, security, update-motd, upgrade-helper\n    Setting up Install Process\n    Resolving Dependencies\n   --&gt; Running transaction check\n   ---&gt; Package httpd.x86_64 0:2.2.25-1.0.amzn1 will be installed\n   --&gt; Processing Dependency: httpd-tools = 2.2.25-1.0.amzn1 for package: httpd-2.2.25-     1.0.amzn1.x86_64\n   --&gt; Processing Dependency: apr-util-ldap for package: httpd-2.2.25-1.0.amzn1.x86_64\n   ---&gt; Package php54-common.x86_64 0:5.4.17-2.41.amzn1 will be installed\n   ---&gt; Package php54-fpm.x86_64 0:5.4.17-2.41.amzn1 will be installed\n    --&gt; Running transaction check\n  enter code here ---&gt; Package apr-util-ldap.x86_64 0:1.4.1-4.14.amzn1 will be installed\n   ---&gt; Package httpd-tools.x86_64 0:2.2.25-1.0.amzn1 will be installed\n--&gt; Processing Conflict: httpd24-tools-2.4.6-2.47.amzn1.x86_64 conflicts httpd-tools &lt; 2.4.6\n--&gt; Processing Conflict: php54-common-5.4.17-2.41.amzn1.x86_64 conflicts php-common &lt; 5.4.17-2.41.amzn1\n--&gt; Finished Dependency Resolution\nError: httpd24-tools conflicts with httpd-tools-2.2.25-1.0.amzn1.x86_64\nError: php54-common conflicts with php-common-5.3.27-1.0.amzn1.x86_64\n You could try using --skip-broken to work around the problem\n You could try running: rpm -Va --nofiles --nodigest\n\n\nThanks in advance.\n    \n\nHave you read the error message ? Read again:\n\nError: httpd24-tools conflicts with httpd-tools-2.2.25-1.0.amzn1.x86_64\nError: php54-common conflicts with php-common-5.3.27-1.0.amzn1.x86_64\n\n\nYou are trying to install HTTPD 2.4 when you appear to have HTTPD 2.2 installed and same thing with php, namely, you have PHP 5.3 installed and you are trying to install 5.4. A simple way to confirm this is to type in the following into bash:\n\nphp -v\nhttpd -V\n\n\nIf you want to install newer versions then remove the older versions. \n\nyum remove httpd-tools-2.2.25-1.0.amzn1.x86_64 php-common-5.3.27-1.0.amzn1.x86_64\n\n    \n\nMore specifically, httpd == version 2.2. php54 is trying to install httpd24.\n\nDon't specify httpd. Let your version of PHP determine which version of Apache to install.\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "how do you install a maven2 plugin", "id": 1924, "answers": [{"answer_id": 1911, "document_id": 1497, "question_id": 1924, "text": "You don't install it, Maven will do that for you. But you need to tell Maven from where it can download the plugin if the plugin is not available in the public repository. So, declare the plugin repository:", "answer_start": 594, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI found this plugin for Google App Engine development that seems to be what I need.\n\nBut I have no idea how to install it.\n\nI downloaded the JAR file from this page but I don't know where to put it:\n\nhttp://code.google.com/p/maven-gae-plugin/\n\nCould anyone point me in the right direction? I've tried search for installation instructions but nothing is coming up. It seems like some kind of insider secret. Sorry - I'm new to Maven so I apologize if this should be obvious.\n\nThis is the pom I'm using:\n\nhttp://code.google.com/p/thoughtsite/source/browse/trunk/pom.xml\n    \n\nYou don't install it, Maven will do that for you. But you need to tell Maven from where it can download the plugin if the plugin is not available in the public repository. So, declare the plugin repository:\n\n&lt;project&gt;\n    [...]\n    &lt;repositories&gt;\n        [...]\n        &lt;repository&gt;\n            &lt;id&gt;maven-gae-plugin-repo&lt;/id&gt;\n            &lt;name&gt;maven-gae-plugin repository&lt;/name&gt;\n            &lt;url&gt;http://maven-gae-plugin.googlecode.com/svn/repository&lt;/url&gt;\n        &lt;/repository&gt;\n    &lt;/repositories&gt;\n\n    &lt;pluginRepositories&gt;\n        [...]\n        &lt;pluginRepository&gt;\n            &lt;id&gt;maven-gae-plugin-repo&lt;/id&gt;\n            &lt;name&gt;maven-gae-plugin repository&lt;/name&gt;\n            &lt;url&gt;http://maven-gae-plugin.googlecode.com/svn/repository&lt;/url&gt;\n        &lt;/pluginRepository&gt;\n    &lt;/pluginRepositories&gt;\n    [...]\n&lt;/project&gt;\n\n\nAnd use the plugin:\n\n&lt;project&gt;\n    [...]\n    &lt;build&gt;\n        &lt;plugins&gt;\n            [...]\n            &lt;plugin&gt;\n                &lt;groupId&gt;net.kindleit&lt;/groupId&gt;\n                &lt;artifactId&gt;maven-gae-plugin&lt;/artifactId&gt;\n                &lt;version&gt;[plugin version]&lt;/version&gt;\n            &lt;/plugin&gt;\n        &lt;/plugins&gt;\n    &lt;/build&gt;\n    [...]\n&lt;/project&gt;\n\n\nAnd let Maven do its job. This is actually documented in the Usage page.\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "What is the difference between producing a self-contained application package and basic application package when deploying java applications?", "id": 616, "answers": [{"answer_id": 622, "document_id": 328, "question_id": 616, "text": "Self-contained application packages must be explicitly requested by passing additional arguments to the <fx:deploy> Ant task or javapackager tool.\n\u2022\tOperating system and tool requirements must be met to be able to build a package in a specific format.\n\u2022\tSelf-contained application packages can only be built using JDK 7 Update 6 or later.", "answer_start": 953, "answer_category": null}], "is_impossible": false}, {"question": "What is the benefit of self-contained application package when deploying java applications?", "id": 617, "answers": [{"answer_id": 623, "document_id": 328, "question_id": 617, "text": "Users install the application with an installer that is familiar to them and launch it in the usual way.\n\u2022\tYou control the version of the JRE used by the application.\n\u2022\tApplications can be deployed on fresh systems with no requirement for the JRE to be installed.\n\u2022\tDeployment occurs with no need for admin permissions when using ZIP or user-level installers.\n\u2022\tFile associations can be registered for the application.\n\u2022\tSupport for secondary launchers enables a suite of applications to be bundled in a single self-contained application package.", "answer_start": 1792, "answer_category": null}], "is_impossible": false}, {"question": "What is the drawback of self-contained application package when deploying java applications?", "id": 619, "answers": [{"answer_id": 624, "document_id": 328, "question_id": 619, "text": "\"Download and run\" user experience\n\u2022\tLarger download size\n\u2022\tPackage per target platform\n\u2022\tApplication updates are the responsibility of developer", "answer_start": 2407, "answer_category": null}], "is_impossible": false}], "context": "This topic describes how to generate the package for a self-contained application. A self-contained application contains your Java or JavaFX application and the JRE needed to run the application.\nThis topic includes the following sections:\n\u2022\tIntroduction\n\u2022\tBenefits and Drawbacks of Self-Contained Application Packages\n\u2022\tBasics\n\u2022\tInstallable Packages\n\u2022\tWorking Through a Deployment Scenario\n7.1 Introduction\nThe Java packaging tools provide built-in support for several formats of self-contained application packages. The basic package is a single folder on your hard drive that includes all application resources and the JRE. The package can be redistributed as is, or you can build an installable package (for example, EXE or DMG format.)\nFrom the standpoint of process, producing a self-contained application package is similar to producing a basic application package as discussed in Chapter 5, \"Packaging Basics,\" with the following differences:\n\u2022\tSelf-contained application packages must be explicitly requested by passing additional arguments to the <fx:deploy> Ant task or javapackager tool.\n\u2022\tOperating system and tool requirements must be met to be able to build a package in a specific format.\n\u2022\tSelf-contained application packages can only be built using JDK 7 Update 6 or later.\nWhile it is easy to create a basic self-contained application package, tailoring it to achieve the best user experience for a particular distribution method usually requires some effort and a deeper understanding of the topic.\n7.2 Benefits and Drawbacks of Self-Contained Application Packages\nDeciding whether the use of self-contained application packages is the best way to deploy your application depends on your requirements.\nSelf-contained application packages provide the following benefits:\n\u2022\tUsers install the application with an installer that is familiar to them and launch it in the usual way.\n\u2022\tYou control the version of the JRE used by the application.\n\u2022\tApplications can be deployed on fresh systems with no requirement for the JRE to be installed.\n\u2022\tDeployment occurs with no need for admin permissions when using ZIP or user-level installers.\n\u2022\tFile associations can be registered for the application.\n\u2022\tSupport for secondary launchers enables a suite of applications to be bundled in a single self-contained application package.\nSelf-contained application packages have the following drawbacks:\n\u2022\t\"Download and run\" user experience\n\u2022\tLarger download size\n\u2022\tPackage per target platform\n\u2022\tApplication updates are the responsibility of developer\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How should I detect unnecessary #include files in a large C++ project?", "id": 1857, "answers": [{"answer_id": 1843, "document_id": 1428, "question_id": 1857, "text": "While it won't reveal unneeded include files, Visual studio has a setting /showIncludes (right click on a .cpp file, Properties->C/C++->Advanced) that will output a tree of all included files at compile time. This can help in identifying files that shouldn't need to be included.", "answer_start": 384, "answer_category": null}], "is_impossible": false}], "context": "I am working on a large C++ project in Visual Studio 2008, and there are a lot of files with unnecessary #include directives. Sometimes the #includes are just artifacts and everything will compile fine with them removed, and in other cases classes could be forward declared and the #include could be moved to the .cpp file. Are there any good tools for detecting both of these cases?\nWhile it won't reveal unneeded include files, Visual studio has a setting /showIncludes (right click on a .cpp file, Properties->C/C++->Advanced) that will output a tree of all included files at compile time. This can help in identifying files that shouldn't need to be included.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to install MySQLdb package? (ImportError: No module named setuptools)", "id": 1160, "answers": [{"answer_id": 1153, "document_id": 737, "question_id": 1160, "text": "simply using sudo apt-get install python-mysqldb", "answer_start": 176, "answer_category": null}], "is_impossible": false}], "context": "Does anybody knows how to solve this problem? And I have no system-administrator-rights. Do I still have a chance to install MySQLdb?\nThank you. After trying many suggestions, simply using sudo apt-get install python-mysqldb worked for me.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to show maintenance page during deployment?", "id": 1085, "answers": [{"answer_id": 1077, "document_id": 662, "question_id": 1085, "text": "You should try put App_Offline.htm to the root directory. https://weblogs.asp.net/scottgu/426755", "answer_start": 745, "answer_category": null}], "is_impossible": false}], "context": "I want to plan a schedule maintenance down time on one of my production asp.net website hosted on IIS windows server 2003.\nI think this is the preferred behavior:\nAll request to http://www.x.com including www.x.com/asb/asd/ will be redirected to a notification page (site is currently down. come back later)\nThe maintenance will take around an hour. how do I ensure for having this redirection to maintenance page to have the least impact to SEO/google ranking\nPreferrably I want to be able to quietly test the production site before it goes back 'live'\nPreferrably I dont want to rely on pointing DNS elsewhere.\nTo make it simple please pretend that I don't have any other hardware in front of the web servers (i.e load balancer, firewall etc)\nYou should try put App_Offline.htm to the root directory. https://weblogs.asp.net/scottgu/426755\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "VSO(TFS) - get current date time as variable", "id": 593, "answers": [{"answer_id": 599, "document_id": 318, "question_id": 593, "text": "You can define a variable with any value, and then modify the variable as current date.", "answer_start": 85, "answer_category": null}], "is_impossible": false}], "context": "How can I get a current date-time and pass it as a variable to some Deployment task?\nYou can define a variable with any value, and then modify the variable as current date.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Python: How to force overwriting of files when using setup.py install (distutil)", "id": 1151, "answers": [{"answer_id": 1144, "document_id": 728, "question_id": 1151, "text": "Run the following command:\npython setup.py install --force", "answer_start": 265, "answer_category": null}], "is_impossible": false}], "context": "I run into problems when I want to install an older branch of my code over a new one: setup.py install won't overwrite older files. A work around is touching (touch <filename>) all files so they are forced to be newer than those installed, but this is pretty ugly. Run the following command:\npython setup.py install --force\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "What does VSIX installer require?", "id": 1023, "answers": [{"answer_id": 1018, "document_id": 617, "question_id": 1023, "text": "VSIX deployment requires that you have the Visual Studio SDK installed", "answer_start": 1283, "answer_category": null}], "is_impossible": false}, {"question": "What is VSIX format?", "id": 1024, "answers": [{"answer_id": 1019, "document_id": 617, "question_id": 1024, "text": " The VSIX format uses strictly file-based deployment and does not support writing to the Global Assembly Cache (GAC), or to the system registry", "answer_start": 1055, "answer_category": null}], "is_impossible": false}, {"question": "What is the advantages of VSIX?", "id": 1025, "answers": [{"answer_id": 1020, "document_id": 617, "question_id": 1025, "text": "VSIX is the preferred deployment method for the extension types that it supports", "answer_start": 1200, "answer_category": null}], "is_impossible": false}, {"question": "What are the features supported by the AssemblyFoldersEx registration?", "id": 1026, "answers": [{"answer_id": 1021, "document_id": 617, "question_id": 1026, "text": "Bulk installation for all controls in a directory without having to register controls individually.\n\nSupport for assigning icons and grouping controls in the Toolbox.\n\nIncluding assemblies in the Visual Studio Add References dialog box and optionally including controls in the Toolbox Choose Items dialog box.\n\nSimplified registration in a single location in the base framework registry key. This avoids the complexity of checking for different Visual Studio SKUs and locating each root key.\n\nSupport for a single control assembly with multiple design-time assemblies that target particular Visual Studio versions or Expression Blend versions.", "answer_start": 2603, "answer_category": null}], "is_impossible": false}], "context": "Visual Studio Extension Deployment\n01/30/2013\n4 minutes to read\nVisual Studio supports three formats for deploying extensions: Visual Studio Extension (VSIX), Windows Installer (MSI), and Visual Studio Content Installer (VSI). You can also register custom controls with Visual Studio by using Platform Registration. This topic compares the different deployments strategies, and lists their capabilities.\n\nSelecting a Deployment Method\nSelect your deployment format depending on the type of extension to deploy and the capabilities of the deployment method.\n\nVSIX\nWhen you upload extensions in the VSIX package format to the Visual Studio Gallery Web site, users can find and install them by checking for online extensions in Extension Manager. For more information about Extension Manager, see Installing and Managing Visual Studio Tools and Extensions.\n\nYou can use the VSIX format to package project and item templates, Visual Studio Integration Packages, Managed Extensibility Framework (MEF) components, toolbox controls, assemblies, and custom types. The VSIX format uses strictly file-based deployment and does not support writing to the Global Assembly Cache (GAC), or to the system registry. VSIX is the preferred deployment method for the extension types that it supports.\n\nVSIX deployment requires that you have the Visual Studio SDK installed. For more information, see VSIX Deployment in the Visual Studio SDK documentation.\n\nMSI\nWhen you create a Setup project in Visual Studio, add it to the solution that contains your extension, and build the project, you get an .msi file. The MSI format supports most application and extension types, and can perform installation operations such as writing the GAC and the system registry. Extension Manager will show and install MSI-based extensions, but cannot enable or disable them. For more information about MSI deployment, see Visual Studio Installer Deployment.\n\nVSI\nThe Visual Studio Content Installer does not support uploading to Visual Studio Gallery, uninstall, or writing to the GAC or the system registry. However, you can use it to deploy macros, add-ins, and code snippets, which the VSIX format does not support. For more information, see How to: Package Components to Use the Visual Studio Content Installer.\n\nPlatform Registration\nIn Visual Studio 2010, registration with a managed platform in the AssemblyFoldersEx registry key includes the functionality of the Toolbox Controls Installer mechanism provided in Visual Studio 2008 and earlier. The following list shows the features supported by the AssemblyFoldersEx registration.\n\nBulk installation for all controls in a directory without having to register controls individually.\n\nSupport for assigning icons and grouping controls in the Toolbox.\n\nIncluding assemblies in the Visual Studio Add References dialog box and optionally including controls in the Toolbox Choose Items dialog box.\n\nSimplified registration in a single location in the base framework registry key. This avoids the complexity of checking for different Visual Studio SKUs and locating each root key.\n\nSupport for a single control assembly with multiple design-time assemblies that target particular Visual Studio versions or Expression Blend versions.\n\nFor information and samples using this functionality, see Deploying a Custom Control and Design-time Assemblies.\n\nPlatform Registration in MSI\nTo add your assemblies to the Add References dialog box and to add your controls to the Toolbox Choose Items dialog box, you must register your assemblies with the managed platform\u2019s AssemblyFoldersEx registry key. To install your controls in the Toolbox using your icons and tab location, you must also specify values in the Toolbox registry key. For more information, see Deploying a Custom Control and Design-time Assemblies.\n\nSupported Extension Types\nThe following table shows which packaging formats support which extension types.\n\nSUPPORTED EXTENSION TYPES\nExtension type\n\nVSIX\n\nMSI\n\nVSI\n\nPlatform Registration\n\nProject Template\n\nYes\n\nNo\n\nYes\n\nNo\n\nItem Template\n\nYes\n\nNo\n\nYes\n\nNo\n\nAssembly\n\nYes\n\nYes\n\nNo\n\nYes\n\nMEF Component\n\nYes\n\nYes\n\nNo\n\nNo\n\nVSPackage\n\nYes\n\nYes\n\nNo\n\nNo\n\nToolbox Control\n\nYes\n\nYes\n\nYes\n\nYes\n\nMacro\n\nNo\n\nYes\n\nYes\n\nNo\n\nAdd-in\n\nNo\n\nYes\n\nYes\n\nNo\n\nCode Snippet\n\nNo\n\nNo\n\nYes\n\nNo\n\nCustom Extension Type\n\nYes\n\nYes\n\nNo\n\nNo\n\nSupported Capabilities\nThe following table shows which packaging formats support which installation options.\n\nSUPPORTED CAPABILITIES\nCapability\n\nVSIX\n\nMSI\n\nVSI\n\nPlatform Registration\n\nPlatform Registration in MSI\n\nEnable and Disable installed extension\n\nYes\n\nNo\n\nNo\n\nNo\n\nNo\n\nCheck for updates\n\nYes\n\nNo\n\nNo\n\nNo\n\nNo\n\nWrite to system registry\n\nNo\n\nYes\n\nNo\n\nYes\n\nYes\n\nRegister with GAC\n\nNo\n\nYes\n\nNo\n\nYes\n\nYes\n\nConfiguration during install\n\nNo\n\nYes\n\nNo\n\nYes\n\nYes\n\nUpload to Visual Studio Gallery\n\nYes\n\nYes\n\nNo\n\nNo\n\nNo\n\nDouble-click install\n\nYes\n\nYes\n\nYes\n\nNo\n\nYes\n\nPresence in the Toolbox Choose Items dialog box\n\nNo\n\nYes\n\nNo\n\nYes\n\nYes\n\nPresence in the Add References dialog box\n\nNo\n\nYes\n\nNo\n\nYes\n\nYes\n\nInstall before (or after) Visual Studio\n\nNo\n\nYes\n\nNo\n\nYes\n\nYes\n\nTarget additional designers, such as Expression Blend\n\nNo\n\nYes\n\nNo\n\nYes\n\nYes\n\nPublication\nYou can distribute any Visual Studio extension by uploading it to a Web site or network share. If the deployment package is in the VSIX format, or is an MSI, you can make it available to Extension Manager for all Visual Studio users by uploading it to the Visual Studio Gallery Web site, which makes the package visible from Extension Manager. You can add extensions of other types to the gallery as reference links.\n\nRelated Topics\nRELATED TOPICS\nTitle\n\nDescription\n\nHow to: Package Components to Use the Visual Studio Content Installer\n\nDescribes how to package macros, add-ins and code snippets into .vsi files.\n\nVSIX Deployment.\n\nDescribes the XML schema on which to base a .vsix manifest file for a Visual Studio extension. This topic is part of the Visual Studio SDK documentation.\n\nInstalling and Managing Visual Studio Tools and Extensions\n\nShows how to use Extension Manager to add, remove, and enable Visual Studio extensions.\n\nDeploying a Custom Control and Design-time Assemblies\n\nShows how to register assemblies with a platform, add Toolbox presence, and add design-time support.", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Install .ipa to iPad with or without iTunes", "id": 1582, "answers": [{"answer_id": 1571, "document_id": 1159, "question_id": 1582, "text": "No need to bother with iTunesConnect for sharing your adhoc builds. Just upload your ipa file to diawi and after successful uploading you will get a link open the link in safari and you will be asked to install app. Tap on install and enjoy", "answer_start": 234, "answer_category": null}], "is_impossible": false}], "context": "I have the .ipa from PhoneGap build and I need to test it. I got provisioning profile from Developer account.\nSo my question is: can I directly put my .ipa to iPad to install for testing, or do I have to follow some rules to install?\nNo need to bother with iTunesConnect for sharing your adhoc builds. Just upload your ipa file to diawi and after successful uploading you will get a link open the link in safari and you will be asked to install app. Tap on install and enjoy\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to install MongoDB on Windows?", "id": 205, "answers": [{"answer_id": 213, "document_id": 114, "question_id": 205, "text": "ise Edition using the\nWindows Install", "answer_start": 2914, "answer_category": null}], "is_impossible": false}, {"question": "How to remove MongoDB Enterprise Edition on Windows?", "id": 206, "answers": [{"answer_id": 214, "document_id": 114, "question_id": 206, "text": "rvice. Then open a Windows command prompt/interpreter\n(cmd.exe) as an Administrator, and\nrun the following command:sc.exe de", "answer_start": 9095, "answer_category": null}], "is_impossible": false}, {"question": "Where is  data path for MongoDB on Windows?", "id": 207, "answers": [{"answer_id": 215, "document_id": 114, "question_id": 207, "text": "ory path is the absolute path\n\\data\\db on the drive from which you st", "answer_start": 5594, "answer_category": null}], "is_impossible": false}, {"question": "How to create the MongoDB data directories on Windows?", "id": 208, "answers": [{"answer_id": 216, "document_id": 114, "question_id": 208, "text": "directories:cd C:\\", "answer_start": 5721, "answer_category": null}], "is_impossible": false}, {"question": "How to start MongoDB on Windows?", "id": 209, "answers": [{"answer_id": 217, "document_id": 114, "question_id": 209, "text": "rt MongoDB, run exe.\"C:\\Program Files\\MongoDB\\Server\\5.0\\bin\\mongod.exe\" --dbpath=", "answer_start": 5788, "answer_category": null}], "is_impossible": false}], "context": "Install MongoDB Enterprise Edition on Windows&lt;iframe src=\"https://www.googletagmanager.com/ns.html?id=GTM-GDFN\" height=\"0\" width=\"0\" style=\"display: none; visibility: hidden\" aria-hidden=\"true\"&gt;&lt;/iframe&gt;Close \u00d7MongoDB ManualVersion 5.0IntroductionInstallationInstall MongoDB Community EditionInstall MongoDB EnterpriseInstall on LinuxInstall on macOSInstall on WindowsInstall using msiexec.exeInstall with DockerUpgrade MongoDB Community to MongoDB EnterpriseVerify Integrity of MongoDB PackagesMongoDB Shell (mongosh)MongoDB CRUD OperationsAggregationData ModelsTransactionsIndexesSecurityChange StreamsReplicationShardingAdministrationStorageFrequently Asked QuestionsReferenceRelease NotesTechnical SupportNavigationInstall MongoDB > Install MongoDB EnterpriseInstall MongoDB Enterprise Edition on Windows\u00b6On this pageOverviewConsiderationsInstall MongoDB Enterprise EditionStart MongoDB Enterprise Edition from the Command InterpreterStart MongoDB Enterprise Edition as a Windows ServiceStop MongoDB Enterprise Edition as a Windows ServiceRemove MongoDB Enterprise Edition as a Windows ServiceAdditional ConsiderationsNoteMongoDB AtlasMongoDB Atlas\nis a hosted MongoDB service option in the cloud which requires no\ninstallation overhead and offers a free tier to get started.Overview\u00b6Use this tutorial to install MongoDB 5.0 Enterprise Edition on\nWindows using the default installation wizard.MongoDB Enterprise Edition\nis available on select platforms and contains support for several\nfeatures related to security and monitoring.MongoDB Version\u00b6This tutorial installs MongoDB 5.0 Enterprise\nEdition. To install a different version of MongoDB Enterprise,\nuse the version drop-down menu in the upper-left corner of this page to\nselect the documentation for that version.Installation Method\u00b6This tutorial installs MongoDB on Windows using the default MSI\ninstallation wizard. To install MongoDB using the msiexec.exe\ncommand-line tool instead, see Install MongoDB using msiexec.exe. The\nmsiexec.exe tool is useful for system administrators who wish to\ndeploy MongoDB in an unattended fashion using automation.Considerations\u00b6MongoDB Shell, mongosh\u00b6The MongoDB Shell (mongosh) is not installed with\nMongoDB Server. You need to follow the mongosh\ninstallation instructions to download and install\nmongosh separately.Platform Support\u00b6MongoDB 5.0 Enterprise Edition supports the following\n64-bit versions of Windows on\nx86_64 architecture:Windows Server 2019Windows 10 / Windows Server 2016MongoDB only supports the 64-bit versions of these platforms.See Supported Platforms for more information.Production Notes\u00b6Before deploying MongoDB in a production environment, consider the\nProduction Notes document which offers\nperformance considerations and configuration recommendations for\nproduction MongoDB deployments.Install MongoDB Enterprise Edition\u00b6Procedure\u00b6Follow these steps to install MongoDB Enterprise Edition using the\nWindows Installation wizard. The installation process installs both the\nMongoDB binaries as well as the default configuration file <install\ndirectory>\\bin\\mongod.cfg.1Download the installer.\u00b6Download the MongoDB Enterprise .msi installer from the following\nlink:\u27a4 MongoDB Download CenterIn the Version dropdown, select the version of\nMongoDB to download.In the Platform dropdown, select Windows.In the Package dropdown, select msi.Click Download.2Run the MongoDB installer.\u00b6For example, from the Windows Explorer/File Explorer:Go to the directory where you downloaded the MongoDB installer (.msi file).\nBy default, this is your Downloads directory.Double-click the .msi file.3Follow the MongoDB Enterprise Edition installation wizard.\u00b6The wizard steps you through the installation of MongoDB and MongoDB\nCompass.Choose Setup TypeYou can choose either the Complete (recommended for\nmost users) or Custom setup type. The\nComplete setup option installs MongoDB and the\nMongoDB tools to the default location. The Custom\nsetup option allows you to specify which executables are\ninstalled and where.Service ConfigurationStarting in MongoDB 4.0, you can set up MongoDB as a Windows service\nduring the install or just install the binaries.Install MongoDB CompassOptional. To have the wizard install MongoDB Compass, select\nInstall MongoDB Compass (Default).When ready, click Install.If You Installed MongoDB as a Windows Service\u00b6The MongoDB service starts upon successful installation. Configure the\nMongoDB instance with the configuration file\n<install directory>\\bin\\mongod.cfg.If you have not already done so, follow the\nmongosh installation instructions to download and\ninstall the MongoDB Shell (mongosh).Be sure to add the path to your mongosh.exe binary to your\nPATH environment variable during installation.Open a new Command Interpreter and enter mongosh.exe\nto connect to MongoDB.For more information on connecting to mongod using\nmongosh.exe, such as connecting to a MongoDB instance\nrunning on a different host and/or port, see\nConnect to a Deployment.For information on CRUD (Create, Read, Update, Delete) operations,\nsee:Insert DocumentsQuery DocumentsUpdate DocumentsDelete DocumentsIf You Did Not Install MongoDB as a Windows Service\u00b6If you only installed the executables and did not install MongoDB as a\nWindows service, you must manually start the MongoDB instance.See Start MongoDB Enterprise Edition from the Command Interpreter for instructions to start a\nMongoDB instance.Start MongoDB Enterprise Edition from the Command Interpreter\u00b61Create database directory.\u00b6Create the data directory where MongoDB stores data.\nMongoDB's default data directory path is the absolute path\n\\data\\db on the drive from which you start MongoDB.From the Command Interpreter, create the data directories:cd C:\\md \"\\data\\db\"2Start your MongoDB database.\u00b6To start MongoDB, run exe.\"C:\\Program Files\\MongoDB\\Server\\5.0\\bin\\mongod.exe\" --dbpath=\"c:\\data\\db\"The --dbpath option points to your\ndatabase directory.If the MongoDB database server is running correctly, the\nCommand Interpreter displays:[initandlisten] waiting for connectionsImportantDepending on the\nWindows Defender Firewall\nsettings on your Windows host, Windows may display a\nSecurity Alert dialog box about blocking\n\"some features\" of C:\\Program Files\\MongoDB\\Server\\5.0\\bin\\mongod.exe\nfrom communicating on networks. To remedy this issue:Click Private Networks, such as my home or work\nnetwork.Click Allow access.To learn more about security and MongoDB, see the\nSecurity Documentation.3Connect to MongoDB.\u00b6If you have not already done so, follow the\nmongosh installation instructions to download\nand install the MongoDB Shell (mongosh).Be sure to add the path to your mongosh.exe binary to your\nPATH environment variable during installation.Open a new Command Interpreter and enter mongosh.exe\nto connect to MongoDB.For more information on connecting to mongod using\nmongosh.exe, such as connecting to a MongoDB instance\nrunning on a different host and/or port, see\nConnect to a Deployment.For information on CRUD (Create, Read, Update, Delete) operations,\nsee:Insert DocumentsQuery DocumentsUpdate DocumentsDelete DocumentsStart MongoDB Enterprise Edition as a Windows Service\u00b6Starting in version 4.0, you can install and configure MongoDB as a\nWindows Service during the install, the MongoDB service\nstarts upon successful installation.To start/restart the MongoDB service, use the Services console:From the Services console, locate the MongoDB service.Right-click on the MongoDB service and click Start.You can also manually manage the service from the command line. To\nstart the MongoDB service from the command line, open a Windows\ncommand prompt/interpreter (cmd.exe) as an Administrator, and\nrun the following command:1Start the MongoDB service.\u00b6Close all other command prompts, then invoke the following command:net start MongoDB2Verify that MongoDB has started successfully.\u00b6Check your MongoDB log file for the following line:[initandlisten] waiting for connections on port 27017You may see non-critical warnings in the process\noutput. As long as you see this message in the MongoDB log, you can\nsafely ignore these warnings during your initial evaluation of\nMongoDB.3Connect to the MongoDB server.\u00b6If you have not already done so, follow the\nmongosh installation instructions to download\nand install the MongoDB Shell (mongosh).Be sure to add the path to your mongosh.exe binary to your\nPATH environment variable during installation.Open a new Command Interpreter and enter mongosh.exe\nto connect to MongoDB.Stop MongoDB Enterprise Edition as a Windows Service\u00b6To stop/pause the MongoDB service, use the Services console:From the Services console, locate the MongoDB service.Right-click on the MongoDB service and click Stop (or Pause).You can also manage the service from the command line. To stop the\nMongoDB service from the command line, open a Windows command\nprompt/interpreter (cmd.exe) as an Administrator, and\nrun the following command:net stop MongoDBRemove MongoDB Enterprise Edition as a Windows Service\u00b6To remove the MongoDB service, first use the Services console to stop\nthe service. Then open a Windows command prompt/interpreter\n(cmd.exe) as an Administrator, and\nrun the following command:sc.exe delete MongoDBAdditional Considerations\u00b6Localhost Binding by Default\u00b6By default, MongoDB launches with bindIp set to\n127.0.0.1, which binds to the localhost network interface. This\nmeans that the mongod.exe can only accept connections from\nclients that are running on the same machine. Remote clients will not be\nable to connect to the mongod.exe, and the mongod.exe will\nnot be able to initialize a replica set unless this value is set\nto a valid network interface.This value can be configured either:in the MongoDB configuration file with bindIp, orvia the command-line argument --bind_ipWarningBefore binding to a non-localhost (e.g. publicly accessible)\nIP address, ensure you have secured your cluster from unauthorized\naccess. For a complete list of security recommendations, see\nSecurity Checklist. At minimum, consider\nenabling authentication and\nhardening network infrastructure.For more information on configuring bindIp, see\nIP Binding.Point Releases and .msi\u00b6If you installed MongoDB with the Windows installer (.msi), that\n.msi automatically upgrades within its release series (e.g. 4.2.1 to 4.2.2).Upgrading a full release series (e.g. 4.0 to 4.2) requires a new\ninstallation.Add MongoDB binaries to the System PATH\u00b6All command-line examples in this tutorial are provided with absolute\npaths to the MongoDB binaries. You can add C:\\Program\nFiles\\MongoDB\\Server\\5.0\\bin to your System PATH and then\nomit the full path to the MongoDB binaries.\u00a9 MongoDB, Inc 2008-present. MongoDB, Mongo, and the leaf logo are registered trademarks of MongoDB, Inc.Give Feedback\u2190 \u00a0Install MongoDB Enterprise on macOSInstall MongoDB Enterprise on Windows using msiexec.exe\u00a0\u2192On this pageOverviewConsiderationsInstall MongoDB Enterprise EditionStart MongoDB Enterprise Edition from the Command InterpreterStart MongoDB Enterprise Edition as a Windows ServiceStop MongoDB Enterprise Edition as a Windows ServiceRemove MongoDB Enterprise Edition as a Windows ServiceAdditional Considerations\n\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How uninstall Docker Engine on Debian?", "id": 63, "answers": [{"answer_id": 66, "document_id": 65, "question_id": 63, "text": "er versions of Docker were called docker, docker.io, or docker-engine.\nIf these are installed, uninstall them:\n$ sudo apt-get remove docker docker-engine docker.io containerd runc\n\nIt\u2019s OK if apt-get reports that none of these packages are installed.\nThe contents of /var/lib/docker/, including images, containers, volumes, and\nnetworks, are preserved. The Docker Engine package is now called docker-ce.\nIn", "answer_start": 615, "answer_category": null}], "is_impossible": false}, {"question": "How install Docker Engine on Debian?", "id": 64, "answers": [{"answer_id": 67, "document_id": 65, "question_id": 64, "text": "e the apt package index, and install the latest version of Docker\nEngine and containerd, or go to the next step to install a specific version:\n$ sudo apt-get update\n$ sudo apt-get install docker-ce docker-ce-cli containerd.io\n\n\nGot", "answer_start": 3935, "answer_category": null}], "is_impossible": false}, {"question": "What if I get multiple Docker repositories?", "id": 67, "answers": [{"answer_id": 70, "document_id": 65, "question_id": 67, "text": "u have multiple Docker repositories enabled, installing\nor updating without specifying a version in the apt-get install or\napt-get update command always installs the highest possible version,\nwhich may not be appropriate for your stability needs.\n\n\n\nT", "answer_start": 4202, "answer_category": null}], "is_impossible": false}, {"question": "What if I fail to install Docker Engine throughDocker\u2019s repository?", "id": 68, "answers": [{"answer_id": 71, "document_id": 65, "question_id": 68, "text": " cannot use Docker\u2019s repository to install Docker Engine, you can download the\n.deb file for your release and install it manually. You need to download\na new file each time you want to upgrade Docker.\n\n\nGo ", "answer_start": 6043, "answer_category": null}], "is_impossible": false}, {"question": "How to delete all images, containers, and volumes?", "id": 70, "answers": [{"answer_id": 74, "document_id": 65, "question_id": 70, "text": "containers, volumes, or customized configuration files on your host\nare not automatically removed. To delete all images, containers, and\nvolumes:\n$ sudo rm -rf /var/lib/docker\n$ sudo rm -rf /var/lib/containerd\n\n\n\nYou ", "answer_start": 10743, "answer_category": null}], "is_impossible": false}, {"question": "How to use Docker as a non-root user?", "id": 69, "answers": [{"answer_id": 73, "document_id": 65, "question_id": 69, "text": "would like to use Docker as a non-root user, you should now consider\nadding your user to the \u201cdocker\u201d group with something like:\nsudo usermod -aG docker your-user\n\nRemem", "answer_start": 9354, "answer_category": null}], "is_impossible": false}], "context": "Install Docker Engine on Debian\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSearch\n\n\n\n\n\n\n\nToggle navigation\n\n\n\n\n\n\n\nHomeGuidesProduct manualsReferenceSamples\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInstall Docker Engine on DebianEstimated reading time: 11 minutesTo get started with Docker Engine on Debian, make sure you\nmeet the prerequisites, then\ninstall Docker.\nPrerequisites\ud83d\udd17\nOS requirements\ud83d\udd17\nTo install Docker Engine, you need the 64-bit version of one of these Debian or\nRaspbian versions:\n\nDebian Buster 10 (stable)\nDebian Stretch 9 / Raspbian Stretch\n\nDocker Engine is supported on x86_64 (or amd64), armhf, and arm64 architectures.\nUninstall old versions\ud83d\udd17\nOlder versions of Docker were called docker, docker.io, or docker-engine.\nIf these are installed, uninstall them:\n$ sudo apt-get remove docker docker-engine docker.io containerd runc\n\nIt\u2019s OK if apt-get reports that none of these packages are installed.\nThe contents of /var/lib/docker/, including images, containers, volumes, and\nnetworks, are preserved. The Docker Engine package is now called docker-ce.\nInstallation methods\ud83d\udd17\nYou can install Docker Engine in different ways, depending on your needs:\n\n\nMost users\nset up Docker\u2019s repositories and install\nfrom them, for ease of installation and upgrade tasks. This is the\nrecommended approach, except for Raspbian.\n\n\nSome users download the DEB package and\ninstall it manually and manage\nupgrades completely manually. This is useful in situations such as installing\nDocker on air-gapped systems with no access to the internet.\n\n\nIn testing and development environments, some users choose to use automated\nconvenience scripts to install Docker.\nThis is currently the only approach for Raspbian.\n\n\nInstall using the repository\ud83d\udd17\nBefore you install Docker Engine for the first time on a new host machine, you need\nto set up the Docker repository. Afterward, you can install and update Docker\nfrom the repository.\n\nRaspbian users cannot use this method!\nFor Raspbian, installing using the repository is not yet supported. You must\ninstead use the convenience script.\n\nSet up the repository\n\n\nUpdate the apt package index and install packages to allow apt to use a\nrepository over HTTPS:\n$ sudo apt-get update\n\n$ sudo apt-get install \\\napt-transport-https \\\nca-certificates \\\ncurl \\\ngnupg-agent \\\nsoftware-properties-common\n\n\n\nAdd Docker\u2019s official GPG key:\n$ curl -fsSL https://download.docker.com/linux/debian/gpg | sudo apt-key add -\n\nVerify that you now have the key with the fingerprint\n9DC8 5822 9FC7 DD38 854A  E2D8 8D81 803C 0EBF CD88, by searching for the\nlast 8 characters of the fingerprint.\n$ sudo apt-key fingerprint 0EBFCD88\n\npub   4096R/0EBFCD88 2017-02-22\nKey fingerprint = 9DC8 5822 9FC7 DD38 854A  E2D8 8D81 803C 0EBF CD88\nuid                  Docker Release (CE deb) <docker@docker.com>\nsub   4096R/F273FCD8 2017-02-22\n\n\n\nUse the following command to set up the stable repository. To add the\nnightly or test repository, add the word nightly or test (or both)\nafter the word stable in the commands below. Learn about nightly and test channels.\n\nNote: The lsb_release -cs sub-command below returns the name of your\nDebian distribution, such as helium. Sometimes, in a distribution\nlike BunsenLabs Linux, you might need to change $(lsb_release -cs)\nto your parent Debian distribution. For example, if you are using\nBunsenLabs Linux Helium, you could use stretch. Docker does not offer any guarantees on untested\nand unsupported Debian distributions.\n\n\nx86_64 / amd64\narmhf\narm64\n\n\n\n$ sudo add-apt-repository \\\n\"deb [arch=amd64] https://download.docker.com/linux/debian \\\n$(lsb_release -cs) \\\nstable\"\n\n\n\n$ sudo add-apt-repository \\\n\"deb [arch=armhf] https://download.docker.com/linux/debian \\\n$(lsb_release -cs) \\\nstable\"\n\n\n\n$ sudo add-apt-repository \\\n\"deb [arch=arm64] https://download.docker.com/linux/debian \\\n$(lsb_release -cs) \\\nstable\"\n\n\n\n\n\n\nInstall Docker Engine\n\nNote: This procedure works for Debian on x86_64 / amd64, Debian ARM,\nor Raspbian.\n\n\n\nUpdate the apt package index, and install the latest version of Docker\nEngine and containerd, or go to the next step to install a specific version:\n$ sudo apt-get update\n$ sudo apt-get install docker-ce docker-ce-cli containerd.io\n\n\nGot multiple Docker repositories?\nIf you have multiple Docker repositories enabled, installing\nor updating without specifying a version in the apt-get install or\napt-get update command always installs the highest possible version,\nwhich may not be appropriate for your stability needs.\n\n\n\nTo install a specific version of Docker Engine, list the available versions\nin the repo, then select and install:\na. List the versions available in your repo:\n$ apt-cache madison docker-ce\n\ndocker-ce | 5:18.09.1~3-0~debian-stretch | https://download.docker.com/linux/debian stretch/stable amd64 Packages\ndocker-ce | 5:18.09.0~3-0~debian-stretch | https://download.docker.com/linux/debian stretch/stable amd64 Packages\ndocker-ce | 18.06.1~ce~3-0~debian        | https://download.docker.com/linux/debian stretch/stable amd64 Packages\ndocker-ce | 18.06.0~ce~3-0~debian        | https://download.docker.com/linux/debian stretch/stable amd64 Packages\n...\n\nb. Install a specific version using the version string from the second column,\nfor example, 5:18.09.1~3-0~debian-stretch .\n$ sudo apt-get install docker-ce=<VERSION_STRING> docker-ce-cli=<VERSION_STRING> containerd.io\n\n\n\nVerify that Docker Engine is installed correctly by running the hello-world\nimage.\n$ sudo docker run hello-world\n\nThis command downloads a test image and runs it in a container. When the\ncontainer runs, it prints an informational message and exits.\n\n\nDocker Engine is installed and running. The docker group is created but no users\nare added to it. You need to use sudo to run Docker commands.\nContinue to Linux postinstall to allow non-privileged\nusers to run Docker commands and for other optional configuration steps.\nUpgrade Docker Engine\nTo upgrade Docker Engine, first run sudo apt-get update, then follow the\ninstallation instructions, choosing the new\nversion you want to install.\nInstall from a package\ud83d\udd17\nIf you cannot use Docker\u2019s repository to install Docker Engine, you can download the\n.deb file for your release and install it manually. You need to download\na new file each time you want to upgrade Docker.\n\n\nGo to https://download.docker.com/linux/debian/dists/,\nchoose your Debian version, then browse to pool/stable/, choose amd64,\narmhf, or arm64 and download the .deb file for the Docker version\nyou want to install.\n\nNote: To install a nightly or test (pre-release) package,\nchange the word stable in the above URL to nightly or test.\nLearn about nightly and test channels.\n\n\n\nInstall Docker Engine, changing the path below to the path where you downloaded\nthe Docker package.\n$ sudo dpkg -i /path/to/package.deb\n\nThe Docker daemon starts automatically.\n\n\nVerify that Docker Engine is installed correctly by running the hello-world\nimage.\n$ sudo docker run hello-world\n\nThis command downloads a test image and runs it in a container. When the\ncontainer runs, it prints an informational message and exits.\n\n\nDocker Engine is installed and running. The docker group is created but no users\nare added to it. You need to use sudo to run Docker commands.\nContinue to Post-installation steps for Linux to allow\nnon-privileged users to run Docker commands and for other optional configuration\nsteps.\nUpgrade Docker Engine\nTo upgrade Docker Engine, download the newer package file and repeat the\ninstallation procedure, pointing to the new file.\n\nInstall using the convenience script\ud83d\udd17\nDocker provides convenience scripts at get.docker.com\nand test.docker.com for installing edge and\ntesting versions of Docker Engine - Community into development environments quickly and\nnon-interactively. The source code for the scripts is in the\ndocker-install repository.\nUsing these scripts is not recommended for production\nenvironments, and you should understand the potential risks before you use\nthem:\n\nThe scripts require root or sudo privileges to run. Therefore,\nyou should carefully examine and audit the scripts before running them.\nThe scripts attempt to detect your Linux distribution and version and\nconfigure your package management system for you. In addition, the scripts do\nnot allow you to customize any installation parameters. This may lead to an\nunsupported configuration, either from Docker\u2019s point of view or from your own\norganization\u2019s guidelines and standards.\nThe scripts install all dependencies and recommendations of the package\nmanager without asking for confirmation. This may install a large number of\npackages, depending on the current configuration of your host machine.\nThe script does not provide options to specify which version of Docker to install,\nand installs the latest version that is released in the \u201cedge\u201d channel.\nDo not use the convenience script if Docker has already been installed on the\nhost machine using another mechanism.\n\nThis example uses the script at get.docker.com to\ninstall the latest release of Docker Engine - Community on Linux. To install the latest\ntesting version, use test.docker.com instead. In\neach of the commands below, replace each occurrence of get with test.\n\nWarning:\nAlways examine scripts downloaded from the internet before\nrunning them locally.\n\n$ curl -fsSL https://get.docker.com -o get-docker.sh\n$ sudo sh get-docker.sh\n\n<output truncated>\n\nIf you would like to use Docker as a non-root user, you should now consider\nadding your user to the \u201cdocker\u201d group with something like:\nsudo usermod -aG docker your-user\n\nRemember to log out and back in for this to take effect!\n\nWarning:\nAdding a user to the \u201cdocker\u201d group grants them the ability to run containers\nwhich can be used to obtain root privileges on the Docker host. Refer to\nDocker Daemon Attack Surface\nfor more information.\n\nDocker Engine - Community is installed. It starts automatically on DEB-based distributions. On\nRPM-based distributions, you need to start it manually using the appropriate\nsystemctl or service command. As the message indicates, non-root users can\u2019t\nrun Docker commands by default.\n\nNote:\nTo install Docker without root privileges, see\nRun the Docker daemon as a non-root user (Rootless mode).\nRootless mode is currently available as an experimental feature.\n\nUpgrade Docker after using the convenience script\nIf you installed Docker using the convenience script, you should upgrade Docker\nusing your package manager directly. There is no advantage to re-running the\nconvenience script, and it can cause issues if it attempts to re-add\nrepositories which have already been added to the host machine.\nUninstall Docker Engine\ud83d\udd17\n\n\nUninstall the Docker Engine, CLI, and Containerd packages:\n$ sudo apt-get purge docker-ce docker-ce-cli containerd.io\n\n\n\nImages, containers, volumes, or customized configuration files on your host\nare not automatically removed. To delete all images, containers, and\nvolumes:\n$ sudo rm -rf /var/lib/docker\n$ sudo rm -rf /var/lib/containerd\n\n\n\nYou must delete any edited configuration files manually.\nNext steps\ud83d\udd17\n\nContinue to Post-installation steps for Linux.\nReview the topics in Develop with Docker to learn how to build new applications using Docker.\n\nrequirements, apt, installation, debian, install, uninstall, upgrade, updateRate this page:\u00a086\u00a038\u00a0i\n\n\n\n\n\nDocker EngineOverviewInstallInstallation per distroInstall on CentOSInstall on DebianInstall on FedoraInstall on UbuntuInstall binariesOptional post-installation stepsRelease notesPrevious versionsEngine 18.09 release notesEngine 18.06 release notesEngine 18.05 release notesEngine 18.04 release notesEngine 18.03 release notesEngine 18.02 release notesEngine 18.01 release notesEngine 17.12 release notesEngine 17.11 release notesEngine 17.10 release notesEngine 17.09 release notesEngine 17.07 release notesEngine 17.06 release notesEngine 17.05 release notesEngine 17.04 release notesEngine 17.03 release notesEngine 1.13 and earlierDeprecated featuresDocker AppDocker BuildxDocker ContextDocker ScanDocker ComposeOverview of Docker ComposeInstall ComposeRelease notesGetting startedEnvironment variables in ComposeEnvironment fileExtend services in ComposeNetworking in ComposeUsing Compose in productionControl startup orderSample apps with ComposeDocker Compose release notesDocker DesktopOverviewMacInstall Docker Desktop for MacUser manualDeploy on KubernetesLeveraging Multi-CPU architecture supportNetworkingDisk utilizationLogs and troubleshootingFAQsStable release notesEdge release notesWindowsInstall Docker Desktop for WindowsInstall Docker Desktop on Windows HomeUser manualDeploy on KubernetesNetworkingLogs and troubleshootingFAQsStable release notesEdge release notesDocker Desktop WSL 2 backendDashboardOpen source licensingDocker HubQuickstartDocker ID accountsSecurity and authenticationManage access tokensTwo-factor authenticationEnable two-factor authenticationDisable two-factor authenticationRecover your Docker Hub accountGenerate a new recovery codeTeams and organizationsRepositoriesVulnerability scanningOfficial imagesAutomated buildsSet up automated buildsTesting in automated buildsAdvanced automated buildsLink to GitHub and BitBucketWebhooksSlack integrationDownload rate limitAdministrationConvert an account into an organizationDeactivate an account or an organizationBillingOverviewUpgrade your planDowngrade your planAdd seatsRemove seatsPublisher & certified contentOverviewSubmit a product for Docker HubUser FAQsPublisher FAQsCertify images & pluginsCertify logging pluginsTrust chainBring Your Own License (BYOL)Release notesOpen-source projectsNotaryGet started with NotaryUse the Notary clientUnderstand the service architectureRun a Notary serviceNotary changelogConfiguration filesServer configurationSigner configurationClient configurationCommon Server and signer configurationsDocker RegistryRegistry overviewUnderstand the RegistryDeploy a registry serverConfigure a registryWork with notificationsRecipesRecipes overviewAuthenticating proxy with apacheAuthenticating proxy with nginxMirroring Docker HubRunning on macOSGarbage collectionTesting an insecure registryDeprecated featuresCompatibilityGetting helpRelease notesSuperseded products and toolsDocker MachineMachine overviewInstall MachineRelease notesGet started with a local VMProvision hosts in the cloudLearn by exampleProvision Digital Ocean DropletsProvision AWS EC2 instancesMachine concepts and helpMachine (docker-machine) CLIMachine CLI overviewMachine command-line completionactiveconfigcreateenvhelpinspectipkilllsmountprovisionregenerate-certsrestartrmscpsshstartstatusstopupgradeurlMachine driversDrivers overviewDriver options and operating system defaultsAmazon Web ServicesDigital OceanExoscaleGenericGoogle Compute EngineIBM SoftlayerMicrosoft AzureMicrosoft Hyper-VOpenStackOracle VirtualBoxRackspaceVMware FusionVMware vCloud AirVMware vSphereMigrate from Boot2Docker to MachineDocker Toolbox (deprecated)KitematicKitematic user guide: intro & overviewSet up an Nginx web serverSet up a Minecraft ServerCreating a local RethinkDB database for developmentFrequently asked questionsKnown issuesTroubleshooting\n\n\n\n\n\n\n\nEdit this page Request docs changes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn this page:\n\nPrerequisites\n\nOS requirements\nUninstall old versions\n\n\nInstallation methods\n\nInstall using the repository\n\nSet up the repository\nInstall Docker Engine\nUpgrade Docker Engine\n\n\nInstall from a package\n\nUpgrade Docker Engine\n\n\nInstall using the convenience script\n\nUpgrade Docker after using the convenience script\n\n\n\n\nUninstall Docker Engine\nNext steps\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhy Docker?\nWhat is a Container?\n\n\n\n\nProducts\nDocker Desktop\nDocker Hub\nFeatures\nContainer Runtime\nDeveloper Tools\nKubernetes\n\n\n\n\nDevelopers\nUse Cases\nPlay with Docker\nCommunity\nOpen Source\nDocker Captains\n\n\n\n\nCompany\nAbout Us\nBlog\nCustomers\nPartners\nNewsroom\nCareers\nContact Us\n\n\n\n\n\n\nStatus\nSecurity\nLegal\nContact\n\n\n\n\n\n\n\nCopyright \u00a9 2013-2020 Docker Inc. All rights reserved.\n\n\n\nTwitter\nYoutube\nGitHub\nLinkedin\nFacebook\nSlideshare\nReddit\n\n\n\n\n\n\n\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "What is a docker image", "id": 73, "answers": [{"answer_id": 77, "document_id": 67, "question_id": 73, "text": "n image includes everything you need to run an application - the code or binary, runtime, dependencies, and any other file system objects required.\nT", "answer_start": 471, "answer_category": null}], "is_impossible": false}, {"question": "What is a Dockerfile\uff1f", "id": 74, "answers": [{"answer_id": 78, "document_id": 67, "question_id": 74, "text": "kerfile is a text document that contains all the commands a user could call on the command line to assemble an image. When we tell Docker to build our image by executing the docker build command, Docker reads these instructions and executes them one by one and creates a Docker image as a result.\nLet\u2019", "answer_start": 3032, "answer_category": null}], "is_impossible": false}, {"question": "How to build a docker image?", "id": 75, "answers": [{"answer_id": 79, "document_id": 67, "question_id": 75, "text": "this, we use the docker build command. The docker build command builds Docker images from a Dockerfile and a \u201ccontext\u201d. A build\u2019s context is the set of files located in the specified PATH or URL. The Docker build process can access any of the files located in the context.\nThe build command optionally takes a --tag flag. The tag is used to set the name of the image and an optional tag in the format \u2018name:tag\u2019. We\u2019ll leave off the optional \u201ctag\u201d for now to help simplify things. If you do not pass a tag, Docker will use \u201clatest\u201d as its default tag. You\u2019ll see this in the last line of the build output.\nLet\u2019s build our first Docker image.\n$ docker build --tag node-docker .\n\nSendi", "answer_start": 7172, "answer_category": null}], "is_impossible": false}], "context": "Build your Node image\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSearch\n\n\n\n\n\n\n\nToggle navigation\n\n\n\n\n\n\n\nHomeGuidesProduct manualsReferenceSamples\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBuild your Node imageEstimated reading time: 11 minutes\nBuild images\nRun your image as a container\nUse containers for development\n\nPrerequisites\ud83d\udd17\nWork through the orientation and setup in Get started Part 1.\nOverview\ud83d\udd17\nNow that we have a good overview of containers and the Docker platform, let\u2019s take a look at building our first image. An image includes everything you need to run an application - the code or binary, runtime, dependencies, and any other file system objects required.\nTo complete this tutorial, you need the following:\n\nNode.js version 12.18 or later. Download Node.js\nDocker running locally: Follow the instructions to download and install Docker.\nAn IDE or a text editor to edit files. We recommend using Visual Studio Code.\n\nSample application\ud83d\udd17\nLet\u2019s create a simple Node.js application that we can use as our example. Create a directory on your local machine named node-docker and follow the steps below to create a simple REST API.\n$ cd [path to your node-docker directory]\n$ npm init -y\n$ npm install ronin-server ronin-mocks\n$ touch server.js\n\nNow, let\u2019s add some code to handle our REST requests. We\u2019ll use a mock server so we can focus on Dockerizing the application.\nOpen this working directory in your IDE and add the following code into the server.js file.\nconst ronin     = require( 'ronin-server' )\nconst mocks     = require( 'ronin-mocks' )\n\nconst server = ronin.server()\n\nserver.use( '/', mocks.server( server.Router(), false, true ) )\nserver.start()\n\nThe mocking server is called Ronin.js and will listen on port 8000 by default. You can make POST requests to the root (/) endpoint and any JSON structure you send to the server will be saved in memory. You can also send GET requests to the same endpoint and receive an array of JSON objects that you have previously POSTed.\nTest application\ud83d\udd17\nLet\u2019s start our application and make sure it\u2019s running properly. Open your terminal and navigate to your working directory you created.\n$ node server.js\n\nTo test that the application is working properly, we\u2019ll first POST some JSON to the API and then make a GET request to see that the data has been saved. Open a new terminal and run the following curl commands:\n$ curl --request POST \\\n--url http://localhost:8000/test \\\n--header 'content-type: application/json' \\\n--data '{\n\"msg\": \"testing\"\n}'\n{\"code\":\"success\",\"payload\":[{\"msg\":\"testing\",\"id\":\"31f23305-f5d0-4b4f-a16f-6f4c8ec93cf1\",\"createDate\":\"2020-08-28T21:53:07.157Z\"}]}\n\n$ curl http://localhost:8000/test\n{\"code\":\"success\",\"meta\":{\"total\":1,\"count\":1},\"payload\":[{\"msg\":\"testing\",\"id\":\"31f23305-f5d0-4b4f-a16f-6f4c8ec93cf1\",\"createDate\":\"2020-08-28T21:53:07.157Z\"}]}\n\nSwitch back to the terminal where our server is running. You should now see the following requests in the server logs.\n2020-XX-31T16:35:08:4260  INFO: POST /test\n2020-XX-31T16:35:21:3560  INFO: GET /test\n\nCreate a Dockerfile for Node.js\ud83d\udd17\nA Dockerfile is a text document that contains all the commands a user could call on the command line to assemble an image. When we tell Docker to build our image by executing the docker build command, Docker reads these instructions and executes them one by one and creates a Docker image as a result.\nLet\u2019s walk through the process of creating a Dockerfile for our application. In the root of your working directory, create a file named Dockerfile and open this file in your text editor.\n\nNote\nThe name of the Dockerfile is not important but the default filename for many commands is simply Dockerfile. So, we\u2019ll use that as our filename throughout this series.\n\nThe first thing we need to do is to add a line in our Dockerfile that tells Docker what base image we would like to use for our application.\nFROM node:12.18.1\n\nDocker images can be inherited from other images. Therefore, instead of creating our own base image, we\u2019ll use the official Node.js image that already has all the tools and packages that we need to run a Node.js application. You can think of this in the same way you would think about class inheritance in object oriented programming. For example, if we were able to create Docker images in JavaScript, we might write something like the following.\nclass MyImage extends NodeBaseImage {}\nThis would create a class called MyImage that inherited functionality from the base class NodeBaseImage.\nIn the same way, when we use the FROM command, we tell Docker to include in our image all the functionality from the node:12.18.1 image.\n\nNote\nIf you want to learn more about creating your own base images, see Creating base images.\n\nThe NODE_ENV environment variable specifies the environment in which an application is running (usually, development or production). One of the simplest things you can do to improve performance is to set NODE_ENV to production.\nENV NODE_ENV=production\n\nTo make things easier when running the rest of our commands, let\u2019s create a working directory. This instructs Docker to use this path as the default location for all subsequent commands. This way we do not have to type out full file paths but can use relative paths based on the working directory.\nWORKDIR /app\n\nUsually the very first thing you do once you\u2019ve downloaded a project written in Node.js is to install npm packages. This will ensure that your application has all its dependencies installed into the node_modules directory where the Node runtime will be able to find them.\nBefore we can run npm install, we need to get our package.json and package-lock.json files into our images. We use the COPY command to do this. The  COPY command takes two parameters. The first parameter tells Docker what file(s) you would like to copy into the image. The second parameter tells Docker where you want that file(s) to be copied to. We\u2019ll copy the package.json and package-lock.json file into our working directory /app.\nCOPY [\"package.json\", \"package-lock.json*\", \"./\"]\n\nOnce we have our package.json files inside the image, we can use the RUN command to execute the command npm install. This works exactly the same as if we were running npm install locally on our machine, but this time these Node modules will be installed into the node_modules directory inside our image.\nRUN npm install --production\n\nAt this point, we have an image that is based on node version 12.18.1 and we have installed our dependencies. The next thing we need to do is to add our source code into the image. We\u2019ll use the COPY command just like we did with our package.json files above.\nCOPY . .\n\nThe COPY command takes all the files located in the current directory and copies them into the image. Now, all we have to do is to tell Docker what command we want to run when our image is run inside of a container. We do this with the CMD command.\nCMD [ \"node\", \"server.js\" ]\n\nHere\u2019s the complete Dockerfile.\nFROM node:12.18.1\nENV NODE_ENV=production\n\nWORKDIR /app\n\nCOPY [\"package.json\", \"package-lock.json*\", \"./\"]\n\nRUN npm install --production\n\nCOPY . .\n\nCMD [ \"node\", \"server.js\" ]\n\nBuild image\ud83d\udd17\nNow that we\u2019ve created our Dockerfile, let\u2019s build our image. To do this, we use the docker build command. The docker build command builds Docker images from a Dockerfile and a \u201ccontext\u201d. A build\u2019s context is the set of files located in the specified PATH or URL. The Docker build process can access any of the files located in the context.\nThe build command optionally takes a --tag flag. The tag is used to set the name of the image and an optional tag in the format \u2018name:tag\u2019. We\u2019ll leave off the optional \u201ctag\u201d for now to help simplify things. If you do not pass a tag, Docker will use \u201clatest\u201d as its default tag. You\u2019ll see this in the last line of the build output.\nLet\u2019s build our first Docker image.\n$ docker build --tag node-docker .\n\nSending build context to Docker daemon  82.94kB\nStep 1/7 : FROM node:12.18.1\n---> f5be1883c8e0\nStep 2/7 : WORKDIR /code\n...\nSuccessfully built e03018e56163\nSuccessfully tagged node-docker:latest\nViewing Local Images\nTo see a list of images we have on our local machine, we have two options. One is to use the CLI and the other is to use Docker Desktop. Since we are currently working in the terminal let\u2019s take a look at listing images with the CLI.\n\nTo list images, simply run the images command.\n$ docker images\nREPOSITORY          TAG                 IMAGE ID            CREATED              SIZE\nnode-docker         latest              3809733582bc        About a minute ago   945MB\nnode                12.18.1             f5be1883c8e0        2 months ago         918MB\n\nYou should see at least two images listed. One for the base image node:12.18.1 and the other for our image we just build node-docker:latest.\nTag images\ud83d\udd17\nAn image name is made up of slash-separated name components. Name components may contain lowercase letters, digits and separators. A separator is defined as a period, one or two underscores, or one or more dashes. A name component may not start or end with a separator.\nAn image is made up of a manifest and a list of layers. In simple terms, a \u201ctag\u201d points to a combination of these artifacts. You can have multiple tags for an image. Let\u2019s create a second tag for the image we built and take a look at its layers.\nTo create a new tag for the image we built above, run the following command.\n$ docker tag node-docker:latest node-docker:v1.0.0\n\nThe Docker tag command creates a new tag for an image. It does not create a new image. The tag points to the same image and is just another way to reference the image.\nNow run the docker images command to see a list of our local images.\n$ docker images\nREPOSITORY          TAG                 IMAGE ID            CREATED             SIZE\nnode-docker         latest              3809733582bc        24 minutes ago      945MB\nnode-docker         v1.0.0              3809733582bc        24 minutes ago      945MB\nnode                12.18.1             f5be1883c8e0        2 months ago        918MB\n\nYou can see that we have two images that start with node-docker. We know they are the same image because if you look at the IMAGE ID column, you can see that the values are the same for the two images.\nLet\u2019s remove the tag that we just created. To do this, we\u2019ll use the rmi command. The rmi command stands for \u201cremove image\u201d.\n$ docker rmi node-docker:v1.0.0\nUntagged: node-docker:v1.0.0\n\nNotice that the response from Docker tells us that the image has not been removed but only \u201cuntagged\u201d. Verify this by running the images command.\n$ docker images\nREPOSITORY          TAG                 IMAGE ID            CREATED             SIZE\nnode-docker         latest              3809733582bc        32 minutes ago      945MB\nnode                12.18.1             f5be1883c8e0        2 months ago        918MB\n\nOur image that was tagged with :v1.0.0 has been removed but we still have the node-docker:latest tag available on our machine.\nConclusion\ud83d\udd17\nIn this module, we took a look at setting up our example Node application that we will use for the rest of the tutorial. We also created a Dockerfile that we used to build our Docker image. Then, we took a look at tagging our images and removing images.\nIn the next module, we\u2019ll take a look at running containers.\ncontainers, images, node.js, node, dockerfiles, node, coding, build, push, runRate this page:\u00a038\u00a02\u00a0i\n\n\n\n\n\nDocker overviewGet DockerGet startedPart 1: Orientation and setupPart 2: Build and run your imagePart 3: Share images on Docker HubDevelop with DockerOverviewNode.jsBuild imagesRun containersDevelopBest practicesBuild imagesDockerfile best practicesBuild images with BuildKitUse multi-stage buildsManage imagesCreate your own base image (advanced)Set up CI/CDCI/CD Best practicesConfigure GitHub ActionsDeploy your app to the cloudDocker and ACIDocker and ECSRun your app in productionOrchestrationOverviewDeploy to KubernetesDeploy to SwarmConfigure all objectsApply custom metadata to objectsPrune unused objectsFormat command and log outputConfigure the daemonConfigure and run DockerControl Docker with systemdCollect metrics with PrometheusConfigure containersStart containers automaticallyKeep containers alive during daemon downtimeRun multiple services in a containerContainer runtime metricsRuntime options with Memory, CPUs, and GPUsLoggingView a container's logsConfigure logging driversUse docker logs with a logging driverUse a logging driver pluginCustomize log driver outputLogging driver detailsLocal file logging driverLogentries logging driverJSON File logging driverGraylog Extended Format (GELF) logging driverSyslog logging driverAmazon CloudWatch logs logging driverETW logging driverFluentd logging driverGoogle Cloud logging driverJournald logging driverSplunk logging driverSecurityDocker securityDocker security non-eventsProtect the Docker daemon socketUsing certificates for repository client verificationUse trusted imagesOverviewAutomationDelegationsDeploy NotaryManage content trust keysPlay in a content trust sandboxAntivirus softwareAppArmor security profilesSeccomp security profilesIsolate containers with a user namespaceRootless modeScale your appSwarm mode overviewSwarm mode key conceptsGet started with swarm modeSet up for the tutorialCreate a swarmAdd nodes to the swarmDeploy a serviceInspect the serviceScale the serviceDelete the serviceApply rolling updatesDrain a nodeUse swarm mode routing meshHow swarm mode worksHow nodes workHow services workManage swarm security with PKISwarm task statesRun Docker in swarm modeJoin nodes to a swarmManage nodes in a swarmDeploy services to a swarmStore service configuration dataManage sensitive data with Docker secretsLock your swarmSwarm administration guideRaft consensus in swarm modeExtend DockerManaged plugin systemAccess authorization pluginExtending Docker with pluginsDocker network driver pluginsVolume pluginsPlugin configurationPluginsConfigure networkingNetworking overviewUse bridge networksUse overlay networksUse host networkingUse Macvlan networksDisable networking for a containerNetworking tutorialsBridge network tutorialHost networking tutorialOverlay networking tutorialMacvlan network tutorialConfigure the daemon and containersConfigure the daemon for IPv6Docker and iptablesContainer networkingConfigure Docker to use a proxy serverLegacy networking content(Legacy) Container linksManage application dataStorage overviewVolumesBind mountstmpfs mountsTroubleshoot volume problemsStore data within containersAbout storage driversSelect a storage driverUse the AUFS storage driverUse the Btrfs storage driverUse the Device mapper storage driverUse the OverlayFS storage driverUse the ZFS storage driverUse the VFS storage driverEducational resourcesOpen source at DockerContribute to documentationOther ways to contributeDocumentation archive\n\n\n\n\n\n\n\nEdit this page Request docs changes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn this page:\n\nPrerequisites\nOverview\nSample application\nTest application\nCreate a Dockerfile for Node.js\nBuild image\nTag images\nConclusion\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhy Docker?\nWhat is a Container?\n\n\n\n\nProducts\nDocker Desktop\nDocker Hub\nFeatures\nContainer Runtime\nDeveloper Tools\nKubernetes\n\n\n\n\nDevelopers\nUse Cases\nPlay with Docker\nCommunity\nOpen Source\nDocker Captains\n\n\n\n\nCompany\nAbout Us\nBlog\nCustomers\nPartners\nNewsroom\nCareers\nContact Us\n\n\n\n\n\n\nStatus\nSecurity\nLegal\nContact\n\n\n\n\n\n\n\nCopyright \u00a9 2013-2020 Docker Inc. All rights reserved.\n\n\n\nTwitter\nYoutube\nGitHub\nLinkedin\nFacebook\nSlideshare\nReddit\n\n\n\n\n\n\n\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Python version 2.6 required, which was not found in the registry", "id": 717, "answers": [{"answer_id": 720, "document_id": 407, "question_id": 717, "text": "cp -rec HKLM:\\SOFTWARE\\Python\\ HKCU:\\SOFTWARE", "answer_start": 846, "answer_category": null}], "is_impossible": false}], "context": "Can't download any python Windows modules and install. I wanted to experiment with scrapy framework and stackless but unable to install due to error \"Python version 2.6 required, which was not found in the registry\".\nTrying to install it to Windows 7, 64 bit machine. I realize this question is a year old - but I thought I would contribute one additional bit of info in case anyone else is Googling for this answer.\nThe issue only crops up on Win7 64-bit when you install Python \"for all users\". If you install it \"for just me\", you should not receive these errors. It seems that a lot of installers only look under HKEY_CURRENT_USER for the required registry settings, and not under HKEY_LOCAL_MACHINE. The page linked by APC gives details on how to manually copy the settings to HKEY_CURRENT_USER.\nOr here's the PowerShell command to do this: cp -rec HKLM:\\SOFTWARE\\Python\\ HKCU:\\SOFTWARE\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "RubyGems installation errors both when using 'sudo' and not using sudo", "id": 966, "answers": [{"answer_id": 961, "document_id": 594, "question_id": 966, "text": "gem environment", "answer_start": 2486, "answer_category": null}], "is_impossible": false}], "context": "ounds like it could be a path issue coupled with having multiple versions installed.\n\nAny difference in output between:\n\nsudo gem env\nand\n\ngem env\nShare\nImprove this answer\nFollow\nanswered Apr 21 '10 at 23:06\n\nSundayEdition\n14633 bronze badges\nNo difference in the two, though in the GEM PATHS section: /usr/lib/ruby/gems/1.8 and /home/username/.gem/ruby/1.8. Shouldn't that not be there by default with sudo? \u2013 \nKenny Peng\n Apr 22 '10 at 22:12 \nThey're the same on my system, but only after I went through and 'scrubbed' everything after recompiling 1.9. Things I checked: 1. Deleted the un-used gem command (had one in /usr/bin and /usr/local/bin 2. Checked my ~/.gemrc file and cleaned the path up to point to my gem path in /usr/local/lib. Used my ~/.gem folder as a backup 3. Matched my GEM_HOME and GEM_PATH environment variables up (in .profile/.bash_profile/.bashrc) with the .gemrc file 4. Checked permissions on the gem folders 5. You might also check to see if you have anything in /etc/gemrc \u2013 \nSundayEdition\n Apr 23 '10 at 4:12\nSo I went through these suggestions and they all seem to check out: 1) only one gem executable in /usr/bin, 2) no .gemrc, 3) I never set these variables in my .profile, 4) gem folders had permissions for root, which sudo should be fine with, 5) no /etc/gemrc. The only other hitch is maybe the fact that my home directory lives on AFS screws things up? \u2013 \nKenny Peng\n Apr 23 '10 at 15:52\nCheck this: stackoverflow.com/questions/257616/sudo-changes-path-why Maybe the old version of gem wasn't installed in /usr/bin or /usr/local/bin, and sudo is picking up a different PATH variable (to gem) or a GEM_PATH (which points to your /home/username path) from somewhere? You might check /etc/environment as well and try to find out where sudo is picking up its environment variables from (maybe a root .profile?). \u2013 \nSundayEdition\n Apr 24 '10 at 3:58\n3\nI believe the problem was caused by having my home directory on AFS. RubyGems seems to write metadata into your .gem directory (~/.gem/specs) regardless of whether you are running as root via sudo. But running as root means that you credentials don't carry over and you can't do anything to AFS. I ended up using sudo su and installing things as root proper. It did install into /usr/lib/ruby/gems and wrote some metadata into ~/.gem/specs. Thanks for all the help, I was able to narrow down the scope of the problem a lot. \u2013 \nKenny Peng\n Apr 26 '10 at 17:20 \nShow 1 more comment\n\n0\n\nTry running:\n\ngem environment\nand checking the values for the GEM PATH. More info at http://docs.rubygems.org/read/chapter/10#page31\n\nShare\nImprove this answer\nFollow\nanswered Apr 22 '10 at 6:00\n\nnutcracker\n2,55922 gold badges1515 silver badges1616 bronze badges\nAdd a comment\n\n0\n\nI was running into the same problem myself on Fedora 15, so I ran 'gem install' with the '--backtrace' option to see what was going on.\n\nIt turned out it was failing at /usr/lib/ruby/site_ruby/1.8/rubygems/doc_manager.rb:203 where it tried to chdir to the directory it had previously stored (the home directory of the user I was running sudo as)\n\nI didn't extensively debug to see what the underlying cause was, just rather used a quick workaround so I could continue moving forward. The workaround was simply to cd to the root directory, eg cd /, before running the gem install command.\n\nHope this helps / solves your issue.\n\nShare\nImprove this answer\nFollow", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Programmatically registering program into Add/Remove programs and storing files within an executable", "id": 1790, "answers": [{"answer_id": 1776, "document_id": 1362, "question_id": 1790, "text": "Don't build your own installer, use a proven deployment tool like: NSIS, Inno Setup or WiX. All of which are free and has a lot of features.", "answer_start": 816, "answer_category": null}], "is_impossible": false}], "context": "I am working on a windows c# console application which I want to allow the user to install on to their computer.\nI want to make my own Windows Installer executable as the Setup Deployment tools built into Visual Studio, appear to be somewhat lacking in functionality for customisations and documentation.\nTherefore, because I want to make my own Windows installer, how do I register my program into the Add/Remove Programs window so they can choose to uninstall it again if they wish and it relaunches my installer program to do the removal.\nAlso as well, the executable would obviously need to copy the files into various locations on the PC, i.e. C:\\Program Files so how would I store the executable files within the windows installer executable so I can move them into the right location.\nIs this possible to do?\nDon't build your own installer, use a proven deployment tool like: NSIS, Inno Setup or WiX. All of which are free and has a lot of features.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Pip install not functioning on windows 7 Cygwin install", "id": 1789, "answers": [{"answer_id": 1775, "document_id": 1361, "question_id": 1789, "text": "There's a bug in 64-bit Cygwin which causes ctypes.util to segfault when trying to find libuuid (/usr/bin/cyguuid-1.dll). The fix is to install libuuid-devel from Cygwin setup. I found this from an issue filed against requests.py, but it's noted (and worked around in different ways) in a few other places, too.", "answer_start": 774, "answer_category": null}], "is_impossible": false}], "context": "I'm having a terrible time of getting pip up and running on Cygwin which I just recently installed on my Windows 7 Computer. I am writing in the hope that anyone out there can tell me what I am doing incorrectly in terms of getting these packages installed correctly.\nTo start, I followed the instructions on this site:\nhttp://www.pip-installer.org/en/latest/installing.html\nEssentially I have tried and tried to get pip up and running on my windows 7 Cygwin install but to no avail. I am aware of the fact that I can use other packages to install plugins and so forth but I would really appreciate it if someone had any knowledge on why this was happening so it doesn't plague me when I try to install stuff further on down the line.\nAny help would be greatly appreciated!\nThere's a bug in 64-bit Cygwin which causes ctypes.util to segfault when trying to find libuuid (/usr/bin/cyguuid-1.dll). The fix is to install libuuid-devel from Cygwin setup. I found this from an issue filed against requests.py, but it's noted (and worked around in different ways) in a few other places, too.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Deploy Create-React-App on Nginx", "id": 499, "answers": [{"answer_id": 502, "document_id": 226, "question_id": 499, "text": "In your case, run npm run build to create the build/ directory and then make the files available in a location Nginx can access them. Your build is probably best done on your local machine and then you can securely copy the files across to your server (via SCP, SFTP etc). You could run npm run build on your server, but if you do, resist the temptation to directly serve the build/ directory as the next time you run a build, clients could receive an inconsistent set of resources whilst you're building.", "answer_start": 391, "answer_category": null}], "is_impossible": false}], "context": "I'm attempting to deploy my create-react-app SPA on a Digital Ocean droplet with Ubuntu 14.04 and Nginx. Per the static server deployment instructions, I can get it working when I run serve -s build -p 4000, but the app comes down as soon as I close the terminal. It is not clear to me from the create-react-app repo readme how to keep it running forever, similar to something like forever.\nIn your case, run npm run build to create the build/ directory and then make the files available in a location Nginx can access them. Your build is probably best done on your local machine and then you can securely copy the files across to your server (via SCP, SFTP etc). You could run npm run build on your server, but if you do, resist the temptation to directly serve the build/ directory as the next time you run a build, clients could receive an inconsistent set of resources whilst you're building.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How can I include line numbers in a stack trace without a pdb?", "id": 536, "answers": [{"answer_id": 538, "document_id": 261, "question_id": 536, "text": "During your build process, use Mike Stall's pdb2xml converter, distributed as part of his excellent MDbg managed code debugger, and store them some place secure (e.g., source control).", "answer_start": 292, "answer_category": null}], "is_impossible": false}], "context": "We are currently distributing a WinForms app without .pdb files to conserve space on client machines and download bandwidth. When we get stack traces, we are getting method names but not line numbers. Is there any way to get the line numbers without resorting to distributing the .pdb files?\nDuring your build process, use Mike Stall's pdb2xml converter, distributed as part of his excellent MDbg managed code debugger, and store them some place secure (e.g., source control).\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to wait for server restart using Ansible?", "id": 594, "answers": [{"answer_id": 600, "document_id": 319, "question_id": 594, "text": "You should change the wait_for task to run as local_action, and specify the host you're waiting for.", "answer_start": 76, "answer_category": null}], "is_impossible": false}], "context": "I'm trying to restart the server and then wait, using, But I get the error. You should change the wait_for task to run as local_action, and specify the host you're waiting for.", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "cant install elementtree with pip", "id": 2006, "answers": [{"answer_id": 1992, "document_id": 1597, "question_id": 2006, "text": ";\n\n&lt;/html&gt;\n\n\nIs there any other way to install elementtree?\n    \n\nIt's in the stdlib: http://docs.python.o", "answer_start": 2247, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nTrying to install elementtree package with pip. Getting the following error:\n\n    (taximachine_env)vagrant@dev-gm:/opt/taxi/taxiconsole$ pip install elementtree -vvv\nCollecting elementtree\n  Getting page https://pypi.python.org/simple/elementtree/\n  1 location(s) to search for versions of elementtree:\n  * https://pypi.python.org/simple/elementtree/\n  Getting page https://pypi.python.org/simple/elementtree/\n  Analyzing links from page https://pypi.python.org/simple/elementtree/\n  Could not find a version that satisfies the requirement elementtree (from versions: )\nCleaning up...\nNo matching distribution found for elementtree\nException information:\nTraceback (most recent call last):\n  File \"/opt/taxi/envs/taximachine_env/local/lib/python2.7/site-packages/pip/basecommand.py\", line 211, in main\n    status = self.run(options, args)\n  File \"/opt/taxi/envs/taximachine_env/local/lib/python2.7/site-packages/pip/commands/install.py\", line 294, in run\n    requirement_set.prepare_files(finder)\n  File \"/opt/taxi/envs/taximachine_env/local/lib/python2.7/site-packages/pip/req/req_set.py\", line 334, in prepare_files\n    functools.partial(self._prepare_file, finder))\n  File \"/opt/taxi/envs/taximachine_env/local/lib/python2.7/site-packages/pip/req/req_set.py\", line 321, in _walk_req_to_install\n    more_reqs = handler(req_to_install)\n  File \"/opt/taxi/envs/taximachine_env/local/lib/python2.7/site-packages/pip/req/req_set.py\", line 461, in _prepare_file\n    req_to_install.populate_link(finder, self.upgrade)\n  File \"/opt/taxi/envs/taximachine_env/local/lib/python2.7/site-packages/pip/req/req_install.py\", line 250, in populate_link\n    self.link = finder.find_requirement(self, upgrade)\n  File \"/opt/taxi/envs/taximachine_env/local/lib/python2.7/site-packages/pip/index.py\", line 571, in find_requirement\n    'No matching distribution found for %s' % req\nDistributionNotFound: No matching distribution found for elementtree\n\n\nPyPI link returns the following response:\n    \n\n&lt;head&gt;\n    &lt;title&gt;Links for elementtree&lt;/title&gt;\n    &lt;meta name=\"api-version\" value=\"2\"&gt;\n    &lt;style type=\"text/css\"&gt;&lt;/style&gt;\n&lt;/head&gt;\n\n&lt;body&gt;\n    &lt;h1&gt;Links for elementtree&lt;/h1&gt;\n&lt;/body&gt;\n\n&lt;/html&gt;\n\n\nIs there any other way to install elementtree?\n    \n\nIt's in the stdlib: http://docs.python.org/2/library/xml.etree.elementtree.html. No need to install it with pip.\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to install Android SDK on Ubuntu?", "id": 640, "answers": [{"answer_id": 645, "document_id": 333, "question_id": 640, "text": "There is no need to download any binaries or files or follow difficult installation instructions.All you really needed to do is:sudo apt update && sudo apt install android-sdk", "answer_start": 470, "answer_category": null}], "is_impossible": false}], "context": "For my Ubuntu machine, I downloaded the latest version of Android SDK from this page.After extracting the downloaded .tgz file, I was trying to search for installation instructions and found:Unpack the .zip file you've downloaded. The SDK files are download separately to a user-specified directory.Make a note of the name and location of the SDK directory on your system\u2014you will need to refer to the SDK directory later when using the SDK tools from the command line. There is no need to download any binaries or files or follow difficult installation instructions.All you really needed to do is:sudo apt update && sudo apt install android-sdkThe location of Android SDK on Linux can be any of the following:\n\u2022\t/home/AccountName/Android/Sdk\n\u2022\t/usr/lib/android-sdk\n\u2022\t/Library/Android/sdk/\n\u2022\t/Users/[USER]/Library/Android/sdk\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Emacs 23.1.50.1 hangs ramdomly for 6-8 seconds on Windows XP", "id": 1737, "answers": [{"answer_id": 1724, "document_id": 1309, "question_id": 1737, "text": "You should see the excellent article http://www.hydrus.org.uk/journal/emacs-netlogon.html that saved me from the agony.", "answer_start": 250, "answer_category": null}], "is_impossible": false}], "context": "I have EmacsW32 23.1.50.1 emacs working on my windows XP machine. It hangs randomly for 5 to 8 seconds and quite frustrating.\nAny one has solution?\nI even tried using emacs win32 binaries (23.1) from gnu ftp site and that also hangs for few seconds.\nYou should see the excellent article http://www.hydrus.org.uk/journal/emacs-netlogon.html that saved me from the agony.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "installing ruby 1 9 2 through rvm never works", "id": 1993, "answers": [{"answer_id": 1979, "document_id": 1577, "question_id": 1993, "text": "eed to run:\n[[ -s \"$HOME/.rvm/scripts/rvm\" ]] &amp;&amp; . \"$HO", "answer_start": 2282, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nWhen I do &gt;rvm install 1.9.2 it all goes well.  Then I do &gt;rvm use 1.9.2 and that goes well too. \n\nWhen It comes to ruby -v though..\n\nsam@sjones:~$ rvm install 1.9.2\n/home/sam/.rvm/rubies/ruby-1.9.2-p136, this may take a while depending on your cpu(s)...\n\nruby-1.9.2-p136 - #fetching\nruby-1.9.2-p136 - #downloading ruby-1.9.2-p136, this may take a while depending on your connection...\n% Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                               Dload  Upload   Total   Spent    Left  Speed\n100 8612k  100 8612k    0     0   562k      0  0:00:15  0:00:15 --:--:-- 1305k\nruby-1.9.2-p136 - #extracting ruby-1.9.2-p136 to /home/sam/.rvm/src/ruby-1.9.2-p136\nruby-1.9.2-p136 - #extracted to /home/sam/.rvm/src/ruby-1.9.2-p136\nruby-1.9.2-p136 - #configuring\nruby-1.9.2-p136 - #compiling\nruby-1.9.2-p136 - #installing\nruby-1.9.2-p136 - updating #rubygems for /home/sam/.rvm/gems/ruby-1.9.2-p136@global\nruby-1.9.2-p136 - updating #rubygems for /home/sam/.rvm/gems/ruby-1.9.2-p136\nruby-1.9.2-p136 - adjusting #shebangs for (gem).\nruby-1.9.2-p136 - #importing default gemsets (/home/sam/.rvm/gemsets/)\nInstall of ruby-1.9.2-p136 - #complete\nsam@sjones:~$ rvm use 1.9.2\nUsing /home/sam/.rvm/gems/ruby-1.9.2-p136\nsam@sjones:~$ ruby -v\n-bash: ruby: command not found\n\n\nWhat on earth do I do? I've been trying for a few hours now :P\n\n--Edit--\n\nHere is my rvm info output:\n\n rvm info\n\n system:\n\n system:\n    uname:       \"Linux sjones 2.6.18-194.8.1.el5.028stab070.2 #1 SMP Tue Jul 6 14:55:39 MSD 2010 x86_64 GNU/Linux\"\n bash:        \"/bin/bash =&gt; GNU bash, version 3.2.39(1)-release (x86_64-pc-linux-gnu)\"\n zsh:         \" =&gt; not installed\"\n\n rvm:\n version:      \"rvm 1.2.2 by Wayne E. Seguin (wayneeseguin@gmail.com) [http://rvm.beginrescueend.com/]\"\n\n homes:\n gem:          \"not set\"\n ruby:         \"not set\"\n\n binaries:\n ruby:         \"\"\n irb:          \"/usr/local/bin/irb\"\n gem:          \"\"\n rake:         \"/usr/bin/rake\"\n\n environment:\n PATH:         \"/usr/local/bin:/usr/bin:/bin:/usr/games:/home/sam/.rvm/bin:/home/sam/.rvm/bin\"\n GEM_HOME:     \"\"\n GEM_PATH:     \"\"\n MY_RUBY_HOME: \"\"\n IRBRC:        \"\"\n RUBYOPT:      \"\"\n gemset:       \"\"\n\n    \n\nTry this:\n\nrvm --default use 1.9.2\n\n--EDIT--\nproblem fixed, you need to run:\n[[ -s \"$HOME/.rvm/scripts/rvm\" ]] &amp;&amp; . \"$HOME/.rvm/scripts/rvm\"\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "getting a permission error when installing with bower", "id": 1401, "answers": [{"answer_id": 1390, "document_id": 973, "question_id": 1401, "text": "ried with sudo ?\n\nsudo bower install --allow-root\nsudo bo", "answer_start": 1233, "answer_category": null}], "is_impossible": false}, {"question": "getting a permission error when installing with bower more errors with eaccess", "id": 1402, "answers": [{"answer_id": 1391, "document_id": 973, "question_id": 1402, "text": " it should work.\n\nsudo chown -R $USER:$GROUP ~/.npm\nsudo chown -R $USE", "answer_start": 1603, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nEverytime I try to install something with Bower I get a permission error like this:\n\nAndreass-MacBook-Air:openMedia Andreas$ bower install jquery\n/usr/local/lib/node_modules/bower/node_modules/configstore/index.js:56\n            throw err;\n                  ^\nError: EACCES, permission denied '/Users/Andreas/.config/configstore/bower-github.yml'\nYou don't have access to this file.\n\nat Error (native)\nat Object.fs.openSync (evalmachine.&lt;anonymous&gt;:500:18)\nat Object.fs.readFileSync (evalmachine.&lt;anonymous&gt;:352:15)\nat Object.create.all.get (/usr/local/lib/node_modules/bower/node_modules/configstore/index.js:34:29)\nat Object.Configstore (/usr/local/lib/node_modules/bower/node_modules/configstore/index.js:27:44)\nat readCachedConfig (/usr/local/lib/node_modules/bower/lib/config.js:22:23)\nat defaultConfig (/usr/local/lib/node_modules/bower/lib/config.js:11:24)\nat Object.&lt;anonymous&gt; (/usr/local/lib/node_modules/bower/lib/index.js:40:32)\nat Module._compile (module.js:460:26)\nat Object.Module._extensions..js (module.js:478:10)\nAndreass-MacBook-Air:openMedia Andreas$ \n\n\nI imagine there might be an easy solution to this. But this is my first experience with Bower. Thanks in advance.\n    \n\nTried with sudo ?\n\nsudo bower install --allow-root\nsudo bower install jquery\n\n\nAnother solution is to change the chown of your configstore folder\n\nsudo chown -R Andreas:Andreas /Users/Andreas/.config/configstore/\nbower init\nbower install jquery\n\n\nIf there are more errors with eaccess, expand the chown to your whole home folder\n    \n\nHere is the fix, run below commands. it should work.\n\nsudo chown -R $USER:$GROUP ~/.npm\nsudo chown -R $USER:$GROUP ~/.config\n\n    \n\nChange the json bower configuration file .bowerrc  which looks like\n\n{\n    \"directory\": \"www/lib\"\n}\n\n\nto look like\n\n{\n    \"directory\": \"www/lib\",\n    \"allow_root\": true\n}\n\n    \n\nyou can just copy and paste this on your terminal\n\nsudo chown -R $(whoami) $(npm config get prefix)/{lib/node_modules,bin,share}\n\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Boost Installation", "id": 1730, "answers": [{"answer_id": 1718, "document_id": 1303, "question_id": 1730, "text": "If you want to run with the latest version, you can do the bjam install as mentioned by Ralf, but I suggest you build a 'pseudo' package.", "answer_start": 508, "answer_category": null}], "is_impossible": false}], "context": "I have a question regarding the installation of the boost libraries. Is there a package that I can use the sudo apt-get install to install this package. I searched all of the questions in this forum and using the commands sudo apt-get install libboost1.40-dev I cannot install theh package with this. Also, I can download it from boost.org but I do not know the correct path to install it too. I would prefer to install it using the sudo apt-get install commands if possible. I am using Ubuntu 9.04. Thanks.\nIf you want to run with the latest version, you can do the bjam install as mentioned by Ralf, but I suggest you build a 'pseudo' package.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Install Rails Error \"invalid gem: package is corrupt\"", "id": 1775, "answers": [{"answer_id": 1761, "document_id": 1346, "question_id": 1775, "text": "First you should find out the version.\ngem list psych   # note down latest version\ngem uninstall psych -v 2.0.5  # or whatever version you have installed\nAfter the gems are installed you can upgrade the Psych gem again should you need it.", "answer_start": 117, "answer_category": null}], "is_impossible": false}], "context": "I am running Mac OSX 10.9.1 and I am using RVM to manage Ruby v2.0.0-p353.\nDoes anyone have suggestions to fix this?\nFirst you should find out the version.\ngem list psych   # note down latest version\ngem uninstall psych -v 2.0.5  # or whatever version you have installed\nAfter the gems are installed you can upgrade the Psych gem again should you need it.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "What does Debian package creator do?", "id": 987, "answers": [{"answer_id": 982, "document_id": 607, "question_id": 987, "text": "The main objective of this project is to simplify the creation of Debian packages by providing an utility that parses a simple YAML file and generate all the file structure, DEBIAN control files and freedesktop.org menus and icons (for GUI applications). It provides both a command line and graphical applications.", "answer_start": 157, "answer_category": null}], "is_impossible": false}], "context": "Google Code\nArchiveSkip to content\nSearch this site\n \nSearch\nProjects\nSearch\nAbout\nProject\nSource\nIssues\nWikis\nDownloads\n\npkgcreator\nDebian package creator\n\nThe main objective of this project is to simplify the creation of Debian packages by providing an utility that parses a simple YAML file and generate all the file structure, DEBIAN control files and freedesktop.org menus and icons (for GUI applications). It provides both a command line and graphical applications.\n\nIf pkgcreator is useful to you or you would like to motivate me to keep working on it, please consider making a donation:\n\n\n\nAlso, developers and package maintainers are welcome to join this project.\n\nProject Information\nLicense: GNU GPL v3\n7 stars\nsvn-based source control\nLabels:\ndebian python linux", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "clickonce isnt creating an appname exe config deploy file correct", "id": 1371, "answers": [{"answer_id": 1360, "document_id": 940, "question_id": 1371, "text": "les.\n\nUsing the Solution Explorer, check that the App.Config's build action is still \"None\".\n\nIf it were listed as \"Content\", this might lead to the file being deployed with it's original name (rather than the prefixed name) as you ment", "answer_start": 1104, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI am trying to publish an internal application I have developed.  I have done this multiple times before with no issues, but never with this application.\n\nIn VS2010 I go to my TestManager project, hit publish, and set everything up correctly.  I go to my folder and run the setup.exe.  Installation then fails and gives me a log file stating that it cannot find the TestManager.exe.config.deploy file.\n\nWhen I go into the Application Files directory, it's true that it did not create a TestManager.exe.config.deploy file, but it instead created the config as App.config.deploy.  \n\nI have looked at my other ClickOnce installers (even installers for other projects in the same solution) and verified that it is correctly creating an &lt;appname&gt;.exe.config.deploy and not creating an app.config.deploy.\n\nWhy is Click-Once not creating the app.config file properly for deployment, and how can I fix it?\n    \n\nI'm not clear if this will work, but if you're having ClickOnce file deploy issues with specific files, then the first thing I check is the build action property for those files.\n\nUsing the Solution Explorer, check that the App.Config's build action is still \"None\".\n\nIf it were listed as \"Content\", this might lead to the file being deployed with it's original name (rather than the prefixed name) as you mentioned.\n\nNOTE: Other config files should be marked as Content in order to ensure they end up in the deploy folder.\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "how to install visual studio 2017 and components to another drive", "id": 1962, "answers": [{"answer_id": 1948, "document_id": 1544, "question_id": 1962, "text": "  enter admin privileges in cmd.exe and enter the lines below\n  \n  mklink / J \"C: \\ Program Files (x86) \\ Microsoft Visual Studio \\\n  Shared\" \"F: \\ msvs2017 \\ shared\" mklink / J \"C: \\ Program Files (x86)\n  \\ Microsoft Visual Studio \\ Installer\" F: \\ msvs2017 \\ Installer \"\n  mklink / J \"C: \\ Program Files (x86) \\ Microsoft Visual Studio \\ 2017\n  \\ Enterprise\" \"F: \\ msvs2017 \\ Enterprise\"\n  \n  change \"F: \\ msvs2017\" to the mklink target lines by the drive and\n  directory where it will be installed.", "answer_start": 1614, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI have a small SSD drive with only 2-3 GB free space. I need the .NET desktop and C++ desktop workloads. I tried creating offline installer, that went fine and I tried -installPath option, but it only affects the studio itself and some other components but not the Windows SDK and .NET SDK, those get installed to system drive.\n\nI did install Windows SDK and .NET SDK's to to different drive but I had to use standalone installers for each. I was wondering if it is possible to accomplish with the installer that VS 2017 comes with.\n    \n\nNo, the VS installer only allows you to specify where packages for that instance of VS are installed (i.e. anything that goes into the folder that you do get to choose).  It does not currently allow you to specify where other global packages are installed.\n    \n\nSomeone proposed a hack in this thread\n\nhttps://social.msdn.microsoft.com/Forums/en-US/3e7160ef-505e-4c48-a1aa-78e778c13ee0/install-visual-studio-2017-in-d-drive?forum=vssetup\n\n\n  I was able to do this using a Junction.  First, I went to the\n  installer and found where the files on C would be installed, even when\n  another drive is selected:\n  \n  C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\n  \n  Then, I created a new folder on H, a much larger SSD. I called it\n  VSSHARED.  \n  \n  Then I opened cmd.exe as an administrator and ran:\n  \n  mklink /J \"C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\"\n  \"H\\VSSHARED\"\n  \n  The installer, once run, placed all of the shared information into H\n  because it thought it was a folder in C.\n\n\nAnd right below there's a similar reply:\n\n\n  enter admin privileges in cmd.exe and enter the lines below\n  \n  mklink / J \"C: \\ Program Files (x86) \\ Microsoft Visual Studio \\\n  Shared\" \"F: \\ msvs2017 \\ shared\" mklink / J \"C: \\ Program Files (x86)\n  \\ Microsoft Visual Studio \\ Installer\" F: \\ msvs2017 \\ Installer \"\n  mklink / J \"C: \\ Program Files (x86) \\ Microsoft Visual Studio \\ 2017\n  \\ Enterprise\" \"F: \\ msvs2017 \\ Enterprise\"\n  \n  change \"F: \\ msvs2017\" to the mklink target lines by the drive and\n  directory where it will be installed.\n  \n  have Fun\n\n\nI ignore if there are any downsides to doing this.\n\nEdit: I just noticed someone commented this solution in the comment section. Well, I think it doesn't hurt to have it as an answer.\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Extract MSI from EXE", "id": 693, "answers": [{"answer_id": 697, "document_id": 385, "question_id": 693, "text": "setup.exe /s /x /b\"C:\\Fold", "answer_start": 232, "answer_category": null}], "is_impossible": false}], "context": "I want to extract the MSI of an EXE setup to publish over a network.\nFor example, using Universal Extractor, but it doesn't work for Java Runtime Environment. For InstallShield MSI based projects I have found the following to work:\nsetup.exe /s /x /b\"C:\\FolderInWhichMSIWillBeExtracted\" /v\"/qn\"\nThis command will lead to an extracted MSI in a directory you can freely specify and a silently failed uninstall of the product.\nThe command line basically tells the setup.exe to attempt to uninstall the product (/x) and do so silently (/s). While doing that it should extract the MSI to a specific location (/b).\nThe /v command passes arguments to Windows Installer, in this case the /qn argument. The /qn argument disables any GUI output of the installer.\n\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Rails: Why is bundle install frozen up by sassc 2.4.0", "id": 1336, "answers": [{"answer_id": 1326, "document_id": 905, "question_id": 1336, "text": "There is a problem with sassc version 2.4.0 and rails 6. I,You can change the version in the Gemfile to 2.1.0 and now it install fast and don't stucks on docker build.", "answer_start": 433, "answer_category": null}], "is_impossible": false}], "context": "I'm trying to deploy my rails app with Capistrano, however, the deploy process (cap production deploy) is constantly held up at the line:\n$HOME/.rbenv/bin/rbenv exec bundle install --path /home/deploy/appname/shared/bundle --jobs 4 --without development test --deployment -- quiet\n\nWhen I ssh into the server and run the same command in the latest release without the --quiet flag, I see most gems are installed, but when it gets to\nThere is a problem with sassc version 2.4.0 and rails 6. I,You can change the version in the Gemfile to 2.1.0 and now it install fast and don't stucks on docker build.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "http://timestamp.verisign.com/scripts/timstamp.dll not available", "id": 808, "answers": [{"answer_id": 803, "document_id": 490, "question_id": 808, "text": "Honestly, I would just try again. But you can use any of the following:\n\u2022\thttp://timestamp.globalsign.com/scripts/timstamp.dll,\n\u2022\thttp://timestamp.comodoca.com/authenticode, or\n\u2022\thttp://www.startssl.com/timestamp.\n\u2022\thttp://timestamp.sectigo.com", "answer_start": 241, "answer_category": null}], "is_impossible": false}], "context": "When the following URL is not available, what other timestamp URL can I use in my setup authoring tool? The specific error I get is: SignTool Error: The specified timestamp server either could not be reached or returned an invalid response. Honestly, I would just try again. But you can use any of the following:\n\u2022\thttp://timestamp.globalsign.com/scripts/timstamp.dll,\n\u2022\thttp://timestamp.comodoca.com/authenticode, or\n\u2022\thttp://www.startssl.com/timestamp.\n\u2022\thttp://timestamp.sectigo.com\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "cant open the mysql plugin table please run mysql upgrade to create it", "id": 1372, "answers": [{"answer_id": 1361, "document_id": 941, "question_id": 1372, "text": "To initialize a fresh data directory, you basically (after setting your config file) just have to run either\n\nbin\\mysqld --initialize\n\n\nor \n\nbin\\mysqld --initialize-insecure\n", "answer_start": 779, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI have downloaded mysql ZIP from here https://dev.mysql.com/downloads/file/?id=467269\n\nThen extracted it, renamed my-default.ini to my.ini, set \n\nbasedir = D:\\Apps\\MySQL\\mysql-5.7.17-winx64\ndatadir = D:\\Apps\\MySQL\\data5717\n\n\nthen started \n\nmysqld --console\n\n\nunder admin privileges. All was described here: http://dev.mysql.com/doc/refman/5.7/en/windows-install-archive.html\n\nUnfortunately it prints the following in console:\n\n\n  [ERROR] Can't open the mysql.plugin table. Please run mysql_upgrade to\n  create it.\n\n\nand doesn't work.\n    \n\n\n\nYou probably misunderstood/skipped point 4 in your list, Initialize MySQL. It means to either copy an existing data directory there or to create a new one, see Initializing the Data Directory Manually Using mysqld .\n\nTo initialize a fresh data directory, you basically (after setting your config file) just have to run either\n\nbin\\mysqld --initialize\n\n\nor \n\nbin\\mysqld --initialize-insecure\n\n\nThe latter will set an empty root password.\n    \n\nIf you set datadir to some other location than basedir, like we do, then you have to COPY, not move, the basedir databases there too.\nApparently mysqld looks for some of it's own stuff in the wrong place. \nAfter the copy you have to change the owner and group of everything you copied to mysql.\n\nsudo cp -R /usr/local/mysql/data/* /your/own/data/place\nsudo chown -R mysql:mysql /your/own/data/place\n\nBTW you can't just change the basedir to match the new datadir after the copy.\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How do I include variables in my VagrantFile?", "id": 589, "answers": [{"answer_id": 595, "document_id": 314, "question_id": 589, "text": "You should use the approach of https://puphpet.com.", "answer_start": 264, "answer_category": null}], "is_impossible": false}], "context": "Can anyone guide me to how do I include variables in my VagrantFile? I am trying to inject configs into the Vagrantfile from an external file so that I can distribute the config to my colleagues without having them to hardcode configs directly on the Vagrantfile.\nYou should use the approach of https://puphpet.com.\n\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Get installed applications in a system", "id": 685, "answers": [{"answer_id": 690, "document_id": 378, "question_id": 685, "text": "@\"SOFTWARE\\WOW6432Node\\Microsoft\\Windows\\CurrentVersion\\Uninstall\".", "answer_start": 472, "answer_category": null}], "is_impossible": false}], "context": "How to get the applications installed in the system using c# code? I agree that enumerating through the registry key is the best way.\nNote, however, that the key given, @\"SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Uninstall\", will list all applications in a 32-bit Windows installation, and 64-bit applications in a Windows 64-bit installation.\nIn order to also see 32-bit applications installed on a Windows 64-bit installation, you would also need to enumeration the key @\"SOFTWARE\\WOW6432Node\\Microsoft\\Windows\\CurrentVersion\\Uninstall\".\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "installation appears to be corrupt unsupported visual studio code", "id": 1440, "answers": [{"answer_id": 1429, "document_id": 1012, "question_id": 1440, "text": " \n\nsimply go to your anti virus software setting,\nthen find there quarantined files,\nthere you can see vs code files,\nselect them and restore,\nopen your vs code and see your vs code is running nice", "answer_start": 2390, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI have a problem with my Visual Studio Code. I have a popup that appears in the bottom right corner, to inform that my VS Code is damaged. So this week I uninstalled my VS Code from Windows. I then went to the official Visual Studio Code website to get the installation executable. Once done, I do the installation, everything works again correctly, then 5 min after, there is a new popup.\nIs there a solution that allows me to fix this?\nI have a pc on Windows 10 64bits.\n    \n\nShort answer - try vs-code extension Fix VSCode Checksums.\nAfter installation, open command palette and run 'Fix Checksums: Apply'. Now, just restart your vs-code.\n or\nAlternatively, open command palette and write 'Developer: Reload Window'.\nThat's it.\nThis fixes the issue for me as well (If you ever have any problems/doubts and wanted to undo what you just did, open command palette and execute 'Fix Checksums: Restore').\n\nDetails - I was having the same issue after installing 'SynthWave 84 theme' and 'Enable Neon Dreams' (that glow effect), as mentioned by answers above.\nI searched for it and even though vs-code documentation suggested to re-install the VS Code (as mentioned in the second answer), I was willing to search for another possible solution.\nso, I found this section on 'SynthWave 84' extension official page.\nthus, just wanted to add another possibility to this question, which may help someone.\nI will be following this and if it causes any issue in VS Code updates or anything related, I will provide an update.\nJFYI - I am on macOS Catalina, but I am sure it will be the same for windows as well and vs-code September 2020 (version 1.50)\n    \n\nMaybe you installed an unsupported theme or extension, i.e. JS CSS loader and themes like SynthWave '84, this theme and extension are not supported by Visual Studio Code or change protected files that VScode uses.\n    \n\nI got this issue after installing \"SynthWave 84\" extension.\nOfficial documentation is recommending the reinstallation of VSCode!\nHowever, Took the following steps to resolve this issue as I never wanted to reinstall VSCode!\n\nUninstall \"SynthWave 84\" extension and this restarts VSCode\nRestart VSCode manually again by closing all the VSCode projects\nUpdate VSCode (Click on Settings -&gt; Check for Updates) and this restarts VSCode.\n\nVoila, the issue got resolved without having to reinstall VSCode.\n    \n\nsimply go to your anti virus software setting,\nthen find there quarantined files,\nthere you can see vs code files,\nselect them and restore,\nopen your vs code and see your vs code is running nicely.\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "error conda can only be installed into the root environment", "id": 1968, "answers": [{"answer_id": 1954, "document_id": 1553, "question_id": 1968, "text": "    \n\nCopy root environment to env1.\n\nconda create --name env1 --clone root\n\n\nActivate your environment.\n\nsource activate env1\n\n\nRemove some conda packages which have to be in root environment.\n\nconda remove conda\nconda remove conda-build\nconda remove cond", "answer_start": 1764, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI am getting the following error when I try to install the python package seaborn:\n\nconda install --name dato-env seaborn\nError: 'conda' can only be installed into the root environment\n\n\nThis, of course, is puzzling because I am not trying to install conda.  I am trying to install seaborn.\n\nThis is my setup. I have 3 python environments:\n\n\ndato-env \npy35\nroot\n\n\nI successfully installed seaborn previously (with the command conda install seaborn), but it installed in the root environment (and is not available to my iPython notebooks which are using the dato-env).\n\nI tried to install seaborn in the dato-env environment so that it would be available to my iPython notebook code, but I keep getting the above error saying that I must install conda in the root environment.  (conda is installed in the root environment)\n\nHow do I successfully install seaborn into my dato-env?\n\nThanks in advance for any assistance.\n\nEdit:\n\n&gt; conda --version\nconda 4.0.5\n&gt; conda env list\ndato-env              *  /Users/*******/anaconda/envs/dato-env\npy35                     /Users/*******/anaconda/envs/py35\nroot                     /Users/*******/anaconda\n\n    \n\nIf you clone root you get conda-build and conda-env in your new environment but afaik they shouldn't be there and are not required outside root provided root remains on your path. So if you remove them from your non-root env first your command should work. For example, I had the same error when trying to update anaconda but did not get the error doing it this way:\n\nsource activate my-env\nconda remove conda-build\nconda remove conda-env\nconda update anaconda\n\n\nSee this thread for alternative and background: https://groups.google.com/a/continuum.io/forum/#!topic/anaconda/PkXOIqlEPCU\n    \n\nCopy root environment to env1.\n\nconda create --name env1 --clone root\n\n\nActivate your environment.\n\nsource activate env1\n\n\nRemove some conda packages which have to be in root environment.\n\nconda remove conda\nconda remove conda-build\nconda remove conda-env\n\n\nThen, You can anything like this.\n\nconda update --all\n\n    \n\nI was able to replicate the problem for a number of different packages. \nThe error only occurs when I tried to install packages in envs created using the conda create --clone option and not those created from scratch.  \n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "See when packages were installed / updated using pip", "id": 1150, "answers": [{"answer_id": 1143, "document_id": 727, "question_id": 1150, "text": "we have to resort to using the file creation and modification times, known as ctime and mtime, respectively.", "answer_start": 320, "answer_category": null}], "is_impossible": false}], "context": "I know how to see installed Python packages using pip, just use pip freeze. But is there any way to see the date and time when package is installed or updated with pip? 7\nI was recently looking for this too. But although there are many good answers here, the real issue is that since pip is not keeping logs by default, we have to resort to using the file creation and modification times, known as ctime and mtime, respectively.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Assets link to wrong release after Capistrano deploy", "id": 1242, "answers": [{"answer_id": 1235, "document_id": 814, "question_id": 1242, "text": "The problem just went away, perhaps server restarting did work, although I SWEAR that I had restarted server before and it didn't work before.", "answer_start": 436, "answer_category": null}], "is_impossible": false}], "context": "So I am new with Capistrano and attempting to deploy my application with Capistrano.\nDone a lot of Googling, really hard to figure out, all the docs I find, apply for older version of Capistrano and they seem to break compatibility with every other version.\nBut I stumbled upon a problem I can't seem to figure out by myself.\nAfter Capistrano deploy, app assets link to wrong release folder and App just returns Missing template error.\nThe problem just went away, perhaps server restarting did work, although I SWEAR that I had restarted server before and it didn't work before.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to make index.html not to cache when the site contents are changes in AngularJS website?", "id": 1640, "answers": [{"answer_id": 1628, "document_id": 1214, "question_id": 1640, "text": "By setting content value to 0, browsers will always load the page from the web server.\n<meta http-equiv=\"expires\" content=\"0\">", "answer_start": 435, "answer_category": null}], "is_impossible": false}], "context": "Normally for .js and .css file we will append a version during build like xx.js?v=123, and then after website deploy, we can get the new version of js and CSS. But I don't see a place talking about how to make the index.html file upgrade when website deployment happen. And we do see in IE that the HTML content should have been changed but it still use the old HTML content.\nHowever, I am not sure whether this is the best solution?\n\nBy setting content value to 0, browsers will always load the page from the web server.\n<meta http-equiv=\"expires\" content=\"0\">\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Visual Studio Setup Project - Per User Registry Settings", "id": 1804, "answers": [{"answer_id": 1789, "document_id": 1375, "question_id": 1804, "text": "my solution is going to be to use the User/Machine Hive, and then in the application it checks if the registry entries are in HKCU and if not, copies them from HKLM. I know this probably isn't the most ideal way of doing it, but it has the least amount of changes.", "answer_start": 738, "answer_category": null}], "is_impossible": false}], "context": "I'm trying to maintain a Setup Project in Visual Studio 2003 (yes, it's a legacy application). The problem we have at the moment is that we need to write registry entries to HKCU for every user on the computer. They need to be in the HKCU rather than HKLM because they are the default user settings, and they do change per user. My feeling is that\nThis isn't possible\nThis isn't something the installer should be doing, but something the application should be doing (after all what happens when a user profile is created after the install?).\nWith that in mind, I still want to change as little as possible in the application, so my question is, is it possible to add registry entries for every user in a Visual Studio 2003 setup project?\nmy solution is going to be to use the User/Machine Hive, and then in the application it checks if the registry entries are in HKCU and if not, copies them from HKLM. I know this probably isn't the most ideal way of doing it, but it has the least amount of changes.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Which relational field does Craft support?", "id": 200644, "answers": [{"answer_id": 239403, "document_id": 357719, "question_id": 200644, "text": "Assets Fields\nCategories Fields\nEntries Fields\nTags Fields\nUsers Fields", "answer_start": 316, "answer_category": null}], "is_impossible": false}, {"question": "How to add additional parameters in Craft?", "id": 200645, "answers": [{"answer_id": 239408, "document_id": 357719, "question_id": 200645, "text": "{% for ingredient in drink.ingredients.section('ingredients') %}\n<li>{{ ingredient.title }}</li>\n{% endfor %}\n", "answer_start": 2397, "answer_category": null}], "is_impossible": false}, {"question": "In which situation do I need use element in Craft?", "id": 200646, "answers": [{"answer_id": 239409, "document_id": 357719, "question_id": 200646, "text": " if you don\u2019t care whether the returned elements are the source or the target of a relation with the element(s) you\u2019re passing in", "answer_start": 3390, "answer_category": null}], "is_impossible": false}, {"question": "In which situation do I need use Sourceelement in Craft?", "id": 200647, "answers": [{"answer_id": 239412, "document_id": 357719, "question_id": 200647, "text": " if you want to find elements related to the given element, where the given element is the source of the relation", "answer_start": 3537, "answer_category": null}], "is_impossible": false}, {"question": "In which situation do I need use Targetelement in Craft?", "id": 200648, "answers": [{"answer_id": 239413, "document_id": 357719, "question_id": 200648, "text": "if you want to find elements related to the given element, where the given element is the target of the relation\n", "answer_start": 3669, "answer_category": null}], "is_impossible": false}], "context": "Relations\n\n\nCraft 3 Documentation\n\n\nCraft 2 Documentation\nCraft 3 Documentation\nCraft 2 Class Reference\nCraft 3 Class Reference\n\n\n\n\nRelations\nCraft has a powerful engine for relating elements to one another. You create those relationships using relational field types.\nCraft comes with five relational field types:\n\nAssets Fields\nCategories Fields\nEntries Fields\nTags Fields\nUsers Fields\n\nJust like the other field types, you can add these to your section, user, asset, category group, tag group, and global sets\u2019 field layouts.\nTerminology #\nBefore working with relations in Craft, it\u2019s important to grasp the following terms, as they are relevant to the templating side of things.\nEach relation involves two elements:\n\nSource element - it has the relational field, where you selected the other element.\nTarget element - the one selected by the source.\n\nHow does this look in practice?\nIf we have an entry for a drink recipe where we select the ingredients as relationships (via an Entries Field), we\u2019d label the elements as follows:\n\nDrink Recipe Entry: Source\nIngredients: Target\n\nTo set this up, we create a new field of the Entries Field Type, give it the name Ingredients, check Ingredients as the source (the available elements will be from the Ingredients section), and leave the Limit field blank so we can choose as many ingredients as each recipe dictates.\nNow we can assign the ingredients to each Drink entry via the new Ingredients relation field.\nTemplating #\nOnce we have our relations field set up, we can look at the options for outputting related elements in our templates.\nGetting Target Elements via the Source Element #\nIf you\u2019ve already got a hold of the source element in your template, like in the example below where we\u2019re outputting the Drink entry, you can access its target elements for a particular field in the same way you access any other field\u2019s value: by the handle.\nCalling the source\u2019s relational field handle (ingredients) returns an Element Criteria Model that can output the field\u2019s target elements, in the field-defined order.\nIf we want to output the ingredients list for a drink recipe, we\u2019d use the following:\n{% if drink.ingredients|length %}\n\n<h3>Ingredients</h3>\n\n<ul>\n{% for ingredient in drink.ingredients %}\n<li>{{ ingredient.title }}</li>\n{% endfor %}\n</ul>\n\n{% endif %}\n\nYou can also add any additional parameters supported by the element type:\n{% for ingredient in drink.ingredients.section('ingredients') %}\n<li>{{ ingredient.title }}</li>\n{% endfor %}\n\nThe relatedTo Parameter #\nAssets, Categories, Entries, Users, and Tags each support a relatedTo parameter, enabling all kinds of crazy things.\nIn its simplest form, you can pass in one of these things to it:\n\nA craft\\elements\\Asset, craft\\elements\\Category, craft\\elements\\Entry, craft\\elements\\User, or craft\\elements\\Tag object\nAn element\u2019s ID\nAn array of element objects and/or IDs\n\nBy doing that, Craft will return all of the elements related to the given element(s), regardless of which one\u2019s the source or target.\n{% set relatedDrinks = craft.entries.section('drinks').relatedTo(drink).all() %}\n\nIf you want to be a little more specific, relatedTo also accepts an object that contains the following properties:\n\nelement, sourceElement, or targetElement\nfield (optional)\nsourceLocale (optional)\n\nSet the first property\u2019s key depending on what you want to get back:\n\nUse element if you don\u2019t care whether the returned elements are the source or the target of a relation with the element(s) you\u2019re passing in\nUse sourceElement if you want to find elements related to the given element, where the given element is the source of the relation\nUse targetElement if you want to find elements related to the given element, where the given element is the target of the relation\n\nSet the field property if you want to limit the scope to relations created by a particular field. You can set this to either a field handle or a field ID (or an array of handles and/or IDs).\n{% set ingredients = craft.entries.section('ingredients').relatedTo({\nsourceElement: drink,\nfield: 'ingredients'\n}) %}\n\nSet the sourceLocale property if you want to limit the scope to relations created from a particular field. (Only do this if you set your relational field to be translatable.) You can set this to a locale ID.\n{% set ingredients = craft.entries.section('ingredients').relatedTo({\nsourceElement: drink,\nsourceLocale: craft.locale\n}) %}\n\nGoing Through Matrix #\nIf you want to find elements related to a source element through a Matrix field, just pass the Matrix field\u2019s handle to the field parameter. If that Matrix field has more than one relational field and you want to target a specific one, you can specify the block type field\u2019s handle using a dot notation:\n{% set ingredients = craft.entries.section('ingredients').relatedTo({\nsourceElement: drink,\nfield: 'ingredientsMatrix.relatedIngredient'\n}).all() %}\n\nPassing Multiple Relation Criteria #\nThere might be times when you need to factor multiple types of relations into the mix. For example, outputting all of the current user\u2019s favorite drinks that include espresso:\n{% set espresso = craft.entries.section('ingredients').slug('espresso').first() %}\n\n{% set cocktails = craft.entries.section('drinks').relatedTo(['and',\n{ sourceElement: currentUser, field: 'favoriteDrinks' },\n{ targetElement: espresso, field: 'ingredients' }\n]).all() %}\n\nThat first argument ('and') specified that the query must match all of the relation criteria. You can pass 'or' instead if you want any of the relation criteria to match.\n\n\n\nCraft 3 Documentation\nIntroduction\n\nAbout Craft CMS\nCode of Conduct\nHow to Use the Documentation\n\nInstalling Craft\n\nServer Requirements\nInstallation\n\nUpgrading & Updating Craft\n\nUpgrading from Craft 2\nUpdating Craft 3\nChanges in Craft 3\n\nGetting Started\n\nThe Pieces of Craft\nDirectory Structure\n\nCore Concepts\n\nSections and Entries\nFields\nTemplates\n\nTwig Primer\n\nCategories\nAssets\nUsers\nGlobals\nTags\nRelations\nRouting\nSearching\nSites\nLocalization\nElement Queries\nContent Migrations\nConfiguration\n\nTemplating\n\nGlobal Variables\nFunctions\nFilters\nTags\nQuerying Elements\nElements\nCommon Examples\n\nPlugin Development\n\nIntro to Plugin Dev\nCoding Guidelines\nUpdating Plugins for Craft 3\nChangelogs and Updates\nPlugin Settings\nControl Panel Section\nAsset Bundles\nServices\nExtending Twig\nWidget Types\nField Types\nVolume Types\nUtility Types\nElement Types\nElement Action Types\nPlugin Migrations\nPublishing to the Plugin Store\n\n\n\u00a9 Pixel & Tonic\n\n\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "detecting if an oracle database is installed", "id": 1966, "answers": [{"answer_id": 1952, "document_id": 1551, "question_id": 1966, "text": "$yum grouplist | grep Orac", "answer_start": 2295, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI was wondering if there was a way to tell if an instance of Oracle on a system has a database installed or not?\n\nThis is for an installation script, and I need to verify that there is an actual database in place before proceeding with the loading of my own tablespace onto that database. Has anyone tackled this problem before?\n\nCheers\n    \n\nCheck for the existence of an ORACLE_HOME.  It's also reasonable to expect that this environment should be configured for the installation, so testing the environment variables and exiting with a sensible diagnostic (possibly suggesting they run oraenv) is a good first start.  If you have an ORACLE_HOME, ORACLE_SID or other appropriate environment variable set up, you can then check for the existence of an oracle home and test for database connectivity and permissions.\n    \n\nFor Oracle 10g, on Windows :\n\n\nCheck the registry :\n\n\nThe key HKLM\\SOFTWARE\\ORACLE must exist.\nA subkey must exist that :\n\n\nHas a name starting with KEY_ (like KEY_OraDb10g_home1, the end string being an Oracle home name).\nHas a value whose name starts with ORA_ and ends with _AUTOSTART. (like ORA_XE_AUTOSTART, the middle string being an instance name).\n\n\n\n\nBeware, installing an Oracle client (without a database instance then), creates entries in the registry and can set environment variables (like ORACLE_HOME). This is why the above pattern is a bit complicated.\n\nThis pattern is very likely to work for Oracle 9i also, and possibly Oracle 8i.\n    \n\nYou could use tnsping to check whether the database listener is active, that would be a good indication. Other than that, why not just simply do a test connection? If it's part of an installer process, you could prompt the user to enter the appropriate connection credentials if you don't know what they'll be in advance.\n    \n\nLook up the /etc/oratab file for oracle homes.These homes have the database software installed as well as the database name from that home.Then you can check whether the database is sound or not by starting it.\n    \n\nI'm not sure about Oracle, but for MySQL and PostgreSql I do the following:\n\n$yum grouplist | grep SQL\n\n\nThis returns:\n\nMySQL Database client\nMySQL Database server\nPostgreSQL Database client\nPostgreSQL Database server\n\n\nSo I assume you should try: \n\n$yum grouplist | grep Orac\n\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "cmdline-tools : could not determine SDK root", "id": 795, "answers": [{"answer_id": 791, "document_id": 478, "question_id": 795, "text": "After unzipping the command line tools package, the top-most directory you'll get is cmdline-tools. Rename the unpacked directory from cmdline-tools to tools, and place it under $C:/Android/cmdline-tools", "answer_start": 381, "answer_category": null}], "is_impossible": false}], "context": "C:\\Android\\sdk\\bin>sdkmanager Error: Could not determine SDK root. Error: Either specify it explicitly with --sdk_root= or move this package into its expected location: \\cmdline-tools\\latest\\\nit shows like this, even after specifying the root in env variables. ANDROID_SDK_ROOT C:\\Android\\sdk. Since new updates, there are some changes that are not mentioned in the documentation. After unzipping the command line tools package, the top-most directory you'll get is cmdline-tools. Rename the unpacked directory from cmdline-tools to tools, and place it under $C:/Android/cmdline-tools\nnow it will look like $C:/Android/cmdline-tools/tools\nand it will work perfectly.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "django says no module named blog", "id": 1459, "answers": [{"answer_id": 1448, "document_id": 1030, "question_id": 1459, "text": "You could add &lt;full_path_to_your_project&gt;/myproject/myproject to PYTHONPATH so that you can import blog, but I would not recommend it\n    \n\nI usually add the config path to installed apps to avoid this p", "answer_start": 2345, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI am getting \"ModuleNotFoundError: No module named 'blog'\" error when  add my blog app to the INSTALLED_APPS section of settings.py. I have determined that it has something to do with the way I have added the \"blog\" app under INSTALLED_APPS. When I remove the 'blog' reference from INSTALLED_APPS error goes away. It looks like that Django is unable to find directory for my blog app? \n\nI have done one thing differently and that is use:\n\npython manage.py startapp blog /myproject\n\n\nDifference here is specifying the /myproject directory and not using:\n\npython manage.py startapp blog\n\n\nWhich will place it under root directory myproject. I wanted to avoid adding app directory in the root folder so i stay more organized. But it looks like Django does not like this or i am not referencing this correctly in the INSTALLED_APPS section? \n\nMy project directory is as following:\n\nmyproject/\n\u251c\u2500\u2500 myproject\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 __init__.py\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 __pycache__\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 __init__.cpython-36.pyc\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 settings.cpython-36.pyc\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 urls.cpython-36.pyc\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 wsgi.cpython-36.pyc\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 blog\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 __init__.py\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 admin.py\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 apps.py\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 migrations\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 __init__.py\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 models.py\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 tests.py\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 views.py\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 settings.py\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 urls.py\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 wsgi.py\n\u251c\u2500\u2500 db.sqlite3\n\u2514\u2500\u2500 manage.py\n\n\nInside my settings.py I have setup my app blog:\n\nINSTALLED_APPS = [\n    'django.contrib.admin',\n    'django.contrib.auth',\n    'django.contrib.contenttypes',\n    'django.contrib.sessions',\n    'django.contrib.messages',\n    'django.contrib.staticfiles',\n    'blog',\n]\n\n    \n\nDirectory structure is unusual. More usual and the one that matches your app being named blog would be\n\nmyproject/\n\u251c\u2500\u2500 myproject\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 __pycache__\n\u2502   \u2502   \u251c\u2500\u2500 __init__.cpython-36.pyc\n\u2502   \u2502   \u251c\u2500\u2500 settings.cpython-36.pyc\n\u2502   \u2502   \u251c\u2500\u2500 urls.cpython-36.pyc\n\u2502   \u2502   \u2514\u2500\u2500 wsgi.cpython-36.pyc\n\u251c\u2500\u2500 blog\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 admin.py\n\u2502   \u251c\u2500\u2500 apps.py\n\u2502   \u251c\u2500\u2500 migrations\n\u2502   \u2502   \u2514\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 models.py\n\u2502   \u251c\u2500\u2500 tests.py\n\u2502   \u2514\u2500\u2500 views.py\n\u2502   \u251c\u2500\u2500 settings.py\n\u2502   \u251c\u2500\u2500 urls.py\n\u2502   \u2514\u2500\u2500 wsgi.py\n\u251c\u2500\u2500 db.sqlite3\n\u2514\u2500\u2500 manage.py\n\n    \n\nDjango needs to be able to import your application, usually this means including the full path relative to the root directory 'myproject.blog'.\n\nYou could add &lt;full_path_to_your_project&gt;/myproject/myproject to PYTHONPATH so that you can import blog, but I would not recommend it\n    \n\nI usually add the config path to installed apps to avoid this problem. So installed apps would look like this:\n\nNSTALLED_APPS = [\n'django.contrib.admin',\n'django.contrib.auth',\n'django.contrib.contenttypes',\n'django.contrib.sessions',\n'django.contrib.messages',\n'django.contrib.staticfiles',\n'blog,apps.BlogConfig',\n\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Rails application installer?", "id": 972, "answers": [{"answer_id": 967, "document_id": 598, "question_id": 972, "text": "Capistrano is a remote server automation tool", "answer_start": 20, "answer_category": null}], "is_impossible": false}, {"question": "What does capistrano do?", "id": 973, "answers": [{"answer_id": 968, "document_id": 598, "question_id": 973, "text": "Reliably deploy web application to any number of machines simultaneously, in sequence or as a rolling set\nTo automate audits of any number of machines (checking login logs, enumerating uptimes, and/or applying security patches)\nTo script arbitrary workflows over SSH\nTo automate common tasks in software teams.\nTo drive infrastructure provisioning tools such as chef-solo, Ansible or similar", "answer_start": 213, "answer_category": null}], "is_impossible": false}, {"question": "What is the advatanges of capistrano ?", "id": 974, "answers": [{"answer_id": 969, "document_id": 598, "question_id": 974, "text": "Capistrano is also very scriptable, and can be integrated with any other Ruby software to form part of a larger tool", "answer_start": 606, "answer_category": null}], "is_impossible": false}], "context": "What is Capistrano?\nCapistrano is a remote server automation tool.\nIt supports the scripting and execution of arbitrary tasks, and includes a set of sane-default deployment workflows.\n\nCapistrano can be used to:\n\nReliably deploy web application to any number of machines simultaneously, in sequence or as a rolling set\nTo automate audits of any number of machines (checking login logs, enumerating uptimes, and/or applying security patches)\nTo script arbitrary workflows over SSH\nTo automate common tasks in software teams.\nTo drive infrastructure provisioning tools such as chef-solo, Ansible or similar.\nCapistrano is also very scriptable, and can be integrated with any other Ruby software to form part of a larger tool.\n\nWhat does it look like?\nCapistrano 3.5 / Airbrussh formatter screenshot\n\nWhat else is in the box?\nThere\u2019s lots of cool stuff in the Capistrano toy box:\n\nInterchangeable output formatters (progress, pretty, html, etc)\nEasy to add support for other source control management software.\nA rudimentary multi-console for running Capistrano interactively.\nHost and Role filters for partial deploys, or partial-cluster maintenance.\nRecipes for the Rails asset pipelines, and database migrations.\nSupport for complex environments.\nA sane, expressive API:\ndesc \"Show off the API\"\ntask :ditty do\n\n  on roles(:all) do |host|\n    # Capture output from the remote host, and re-use it\n    # we can reflect on the `host` object passed to the block\n    # and use the `info` logger method to benefit from the\n    # output formatter that is selected.\n    uptime = capture('uptime')\n    if host.roles.include?(:web)\n      info \"Your webserver #{host} has uptime: #{uptime}\"\n    end\n  end\n\n  on roles(:app) do\n    # We can set environmental variables for the duration of a block\n    # and move the process into a directoy, executing arbitrary tasks\n    # such as letting Rails do some heavy lifting.\n    with({:rails_env => :production}) do\n      within('/var/www/my/rails/app') do\n        execute :rails, :runner, 'MyModel.something'\n      end\n    end\n  end\n\n  on roles(:db) do\n    # We can even switch users, provided we have support on the remote\n    # server for switching to that user without being prompted for a\n    # passphrase.\n    as 'postgres' do\n      widgets = capture \"echo 'SELECT * FROM widgets;' | psql my_database\"\n      if widgets.to_i < 50\n        warn \"There are fewer than 50 widgets in the database on #{host}!\"\n      end\n    end\n  end\n\n  on roles(:all) do\n    # We can even use `test` the way the Unix gods intended\n    if test(\"[ -d /some/directory ]\")\n      info \"Phew, it's ok, the directory exists!\"\n    end\n  end\nend", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How Can I Install TypeScript with Visual Studio 2010", "id": 1168, "answers": [{"answer_id": 1161, "document_id": 745, "question_id": 1168, "text": "1.\tClose Visual Studio\n2.\tIntall the Wix Toolset\n3.\tDownload TypeScript for Visual Studio 2012 version 0.9.1.1 and place the installer executable in its own temporary directory.\n4.\tOpen a command window in the temporary directory.\n5.\tExecute the command \"%wix%\\bin\\dark.exe\" -x .\\TypeScriptSetup TypeScriptSetup.0.9.1.1.exe\n6.\tGo to the directory .\\TypeScriptSetup\\AttachedContainer\\packages\\TypeScript_CORE\n7.\tExecute the following commands\nIF NOT DEFINED ProgramFiles(x86) SET ProgramFiles(x86)=%ProgramFiles%\nSET VSDir=%ProgramFiles(x86)%\\Microsoft Visual Studio 10.0\\Common7\\IDE\nSET Devenv=%VSDir%\\devenv.exe", "answer_start": 330, "answer_category": null}], "is_impossible": false}], "context": "Typescript 0.9.5+ have a dependency on Microsoft.VisualStudio.Shell.11.0.dll. Therefore, these instructions will no longer be updated.\nThe following is provided for educational purposes. Please adhere to all licensing and redistribution requirements. For prior versions of TypeScript, please see the edit history for this answer.\n1.\tClose Visual Studio\n2.\tIntall the Wix Toolset\n3.\tDownload TypeScript for Visual Studio 2012 version 0.9.1.1 and place the installer executable in its own temporary directory.\n4.\tOpen a command window in the temporary directory.\n5.\tExecute the command \"%wix%\\bin\\dark.exe\" -x .\\TypeScriptSetup TypeScriptSetup.0.9.1.1.exe\n6.\tGo to the directory .\\TypeScriptSetup\\AttachedContainer\\packages\\TypeScript_CORE\n7.\tExecute the following commands\nIF NOT DEFINED ProgramFiles(x86) SET ProgramFiles(x86)=%ProgramFiles%\nSET VSDir=%ProgramFiles(x86)%\\Microsoft Visual Studio 10.0\\Common7\\IDE\nSET Devenv=%VSDir%\\devenv.exe\nThe solution has been updated for 0.8.3.1 using lessmsi unpacking as mentioned by @Geotarget. This is arguably a \"cleaner\" install than the instructions that were provided for the 0.8.2 release.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "ClickOnce start menu icon", "id": 1053, "answers": [{"answer_id": 1048, "document_id": 634, "question_id": 1053, "text": "Double-click on the properties for your main project.", "answer_start": 107, "answer_category": null}], "is_impossible": false}], "context": "How do I set the icon for my start menu shortcut, when I deploy and install my application with ClickOnce?\nDouble-click on the properties for your main project.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "how to install the latest version of eclipse classic on ubuntu 12 04 ", "id": 1441, "answers": [{"answer_id": 1430, "document_id": 1014, "question_id": 1441, "text": "wing command and it'll insta", "answer_start": 2538, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nPlease how to install the latest Eclipse Classic (4.2) on Ubuntu 12.04 using the terminal? if you can direct me step-by-step, I would be grateful.\n    \n\nStep 1 \u00bb Install java JDK or JRE\n\nsudo apt-get install openjdk-7-jdk\n\n\nStep 2 \u00bb Download latest copy from here\nhttp://www.eclipse.org/downloads/?osType=linux\n\nStep 3 \u00bb Move the downloaded package to /opt directory\n\nsudo mv eclipse-SDK-4.2.2-linux-gtk.tar.gz /opt\n\n\nStep 4 \u00bb Extract the package\n\nsudo tar -xvf /opt/eclipse-SDK-4.2.2-linux-gtk.tar.gz -C /opt\n\n\nStep 5 \u00bb Create a new desktop file eclipse.desktop in /usr/share/applications/ and add the below lines .\n\n[Desktop Entry]\nName=Eclipse \nType=Application\nExec=/opt/eclipse/eclipse\nTerminal=false\nIcon=/opt/eclipse/icon.xpm\nComment=Integrated Development Environment\nNoDisplay=false\nCategories=Development;IDE\nName[en]=eclipse.desktop\n\n\nStep 6 \u00bb Simply drag this eclipse.desktop file to the launcher.\n\nCheck this link : install eclipse in ubuntu 12.04\n    \n\nSee this blog post here, for step-by-step instructions.\n\nThe process is documented step-by-step and in the comments the author has included a script - \n\n#!/bin/sh\n\nECLIPSE=/usr/lib/eclipse/eclipse\n\ninject_update_site(){\nif [ ! -e \"$1\" ] ; then\necho \"W: Cannot find $1\" 2&gt;&amp;1\nreturn 1\nfi\ncat - &gt;&gt;\"$1\" &lt;&lt;EOF\nrepositories/http\\:__download.eclipse.org_releases_indigo/enabled=true\nrepositories/http\\:__download.eclipse.org_releases_indigo/isSystem=false\nrepositories/http\\:__download.eclipse.org_releases_indigo/nickname=Indigo Update Site\nrepositories/http\\:__download.eclipse.org_releases_indigo/uri=http\\://download.eclipse.org/releases/indigo/\nEOF\n\n}\n\nif [ ! -d ~/.eclipse/ ] ; then\n$ECLIPSE -clean -initialize || exit $?\nartifact=$(find ~/.eclipse \\\n-regex .*/profileRegistry/.*/org.eclipse.equinox.p2.artifact.repository.prefs)\nmetadata=$(find ~/.eclipse \\\n-regex .*/profileRegistry/.*/org.eclipse.equinox.p2.metadata.repository.prefs)\nif [ -z \"$artifact\" ] || [ -z \"$metadata\" ]; then\necho \"W: Cannot inject update-sites, cannot find the correct config.\" 2&gt;&amp;1\nelse\n( inject_update_site \"$artifact\" &amp;&amp; \\\ninject_update_site \"$metadata\" &amp;&amp; \\\necho \"I: Injected update sites\" ) || echo \"W: Could not inject update sites.\" 2&gt;&amp;1\nfi\nfi\n\nexec $ECLIPSE \"$@\"\n\n\nwhich works.\n    \n\nMake sure to cd to the /opt folder before executing the tar command.  If you don't then the tar command will create the eclipse directory in whatever your current directory is.  \n    \n\nAfter installing java JDK or JRE run the following command and it'll install eclipse for you,\n\nsudo apt-get install eclipse\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "installing heroku toolbelt in kali sana?", "id": 1001, "answers": [{"answer_id": 996, "document_id": 612, "question_id": 1001, "text": "$ wget https://cli-assets.heroku.com/branches/stable/heroku-linux-386.tar.gz -O heroku.tar.gz\n\n$ mkdir -p /usr/local/lib\n\n$ tar -xvzf heroku.tar.gz -C /usr/local/lib\n\n$ /usr/local/lib/heroku/install\n", "answer_start": 1030, "answer_category": null}], "is_impossible": false}], "context": "3\n\n\n1\ni am Trying to install heroku tool belt in Kali sana from :wget -O- https://toolbelt.heroku.com/install-ubuntu.sh | sh and i keep getting the error below any help? Some packages could not be installed. This may mean that you have requested an impossible situation or if you are using the unstable distribution that some required packages have not yet been created or been moved out of Incoming. The following information may help to resolve the situation:\n\nThe following packages have unmet dependencies: heroku-toolbelt : Depends: heroku (= 3.43.3) but it is not going to be installed E: Unable to correct problems, you have held broken packages.\n\nheroku\ninstallation\nheroku-toolbelt\nShare\nImprove this question\nFollow\nedited May 31 '16 at 8:12\n\n\u0417\u0435\u043b\u0451\u043d\u044b\u0439\n39.7k99 gold badges8080 silver badges9191 bronze badges\nasked May 31 '16 at 8:07\n\nBonface Ochieng\n47344 silver badges44 bronze badges\nAdd a comment\n3 Answers\n\n0\n\nSo I know this thread is old but I just got heroku working on kali and found this thread while searching.\n\n$ wget https://cli-assets.heroku.com/branches/stable/heroku-linux-386.tar.gz -O heroku.tar.gz\n\n$ mkdir -p /usr/local/lib\n\n$ tar -xvzf heroku.tar.gz -C /usr/local/lib\n\n$ /usr/local/lib/heroku/install\n\nAbove is what I used, the 386 part for ARCH as in architecture is 32bit, so you would need amd64 for 64bit.\n\nShare\nImprove this answer\nFollow\nanswered May 21 '17 at 14:32\n\npeeterX\n1811 silver badge55 bronze badges\nAdd a comment\n\n0\n\nsudo apt install software-properties-common # debian only\n\n[nano | echo | leafpad ] \"deb https://cliassets.heroku.com/branches/stable/apt ./\" \"\"\"into\"\"\" /etc/apt/source.list\n\ncurl -L https://cli-assets.heroku.com/apt/release.key | sudo apt-key add - sudo apt update sudo apt upgrade sudo apt-get install heroku\n\nShare\nImprove this answer\nFollow\nanswered Apr 25 '17 at 5:14\n\nsudored\n1\nAdd a comment\n\n0\n\napt-get update\n\nwget -O- https://toolbelt.heroku.com/install-ubuntu.sh | sh\n\nheroku update\n\nthis would update all the plugins to the latest version of the heroku cli...goodluck", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How can I use git-archive to include submodules from a bare repository", "id": 586, "answers": [{"answer_id": 592, "document_id": 311, "question_id": 586, "text": "You should use this python package https://github.com/Kentzo/git-archive-all. You can install it by using: pip install git-archive-all.", "answer_start": 463, "answer_category": null}], "is_impossible": false}], "context": "I'm in the process of setting up a deployment script. The basic process is:\nPush changes to a bare repository on the server\nThen based on new tags will create a new folder for the release.\nUse git archive to move the files into the release directory\nRuns some migrations scripts and puts it live (if all is successful).\nThe issue is my repository contains a submodule, which doesn't get put in the archive, and therefore doesn't get put in the release directory.\nYou should use this python package https://github.com/Kentzo/git-archive-all. You can install it by using: pip install git-archive-all.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to deploy application with Python or another embedded scripting language\uff1f", "id": 52, "answers": [{"answer_id": 55, "document_id": 61, "question_id": 52, "text": "The one important task that only embedders (as opposed to extension writers) of\nthe Python interpreter have to worry about is the initialization, and possibly\nthe finalization, of the Python interpreter.  Most functionality of the\ninterpreter can only be used after the interpreter has been initialized.\nThe basic initialization function is Py_Initialize(). ", "answer_start": 24454, "answer_category": null}], "is_impossible": false}, {"question": "What does Py_Initialize() do? ", "id": 53, "answers": [{"answer_id": 56, "document_id": 61, "question_id": 53, "text": "Py_Initialize() calculates the module search path\nbased upon its best guess for the location of the standard Python interpreter\nexecutable, assuming that the Python library is found in a fixed location\nrelative to the Python interpreter executable.", "answer_start": 25335, "answer_category": null}], "is_impossible": false}, {"question": "How to bebug python?", "id": 54, "answers": [{"answer_id": 57, "document_id": 61, "question_id": 54, "text": "Py_DEBUG is enabled in the Unix build by adding\n--with-pydebug to the ./configure command.", "answer_start": 27993, "answer_category": null}], "is_impossible": false}], "context": "\n\n\nIntroduction\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable of Contents\n\nIntroduction\nCoding standards\nInclude Files\nUseful macros\nObjects, Types and Reference Counts\nReference Counts\nReference Count Details\n\n\nTypes\n\n\nExceptions\nEmbedding Python\nDebugging Builds\n\n\n\nPrevious topic\nPython/C API Reference Manual\nNext topic\nC API Stability\n\nThis Page\n\nReport a Bug\n\nShow Source\n\n\n\n\n\n\n\n\nNavigation\n\n\nindex\n\nmodules |\n\nnext |\n\nprevious |\n\nPython \u00bb\n\n\n\n\n\n\n\n3.10.0 Documentation \u00bb\n\nPython/C API Reference Manual \u00bb\nIntroduction\n\n\n\n\n\n\n\n\n\n|\n\n\n\n\n\n\n\n\nIntroduction\u00b6\nThe Application Programmer\u2019s Interface to Python gives C and C++ programmers\naccess to the Python interpreter at a variety of levels.  The API is equally\nusable from C++, but for brevity it is generally referred to as the Python/C\nAPI.  There are two fundamentally different reasons for using the Python/C API.\nThe first reason is to write extension modules for specific purposes; these\nare C modules that extend the Python interpreter.  This is probably the most\ncommon use.  The second reason is to use Python as a component in a larger\napplication; this technique is generally referred to as embedding Python\nin an application.\nWriting an extension module is a relatively well-understood process, where a\n\u201ccookbook\u201d approach works well.  There are several tools that automate the\nprocess to some extent.  While people have embedded Python in other\napplications since its early existence, the process of embedding Python is\nless straightforward than writing an extension.\nMany API functions are useful independent of whether you\u2019re embedding  or\nextending Python; moreover, most applications that embed Python  will need to\nprovide a custom extension as well, so it\u2019s probably a  good idea to become\nfamiliar with writing an extension before  attempting to embed Python in a real\napplication.\n\nCoding standards\u00b6\nIf you\u2019re writing C code for inclusion in CPython, you must follow the\nguidelines and standards defined in PEP 7.  These guidelines apply\nregardless of the version of Python you are contributing to.  Following these\nconventions is not necessary for your own third party extension modules,\nunless you eventually expect to contribute them to Python.\n\n\nInclude Files\u00b6\nAll function, type and macro definitions needed to use the Python/C API are\nincluded in your code by the following line:\n#define PY_SSIZE_T_CLEAN\n#include <Python.h>\n\n\nThis implies inclusion of the following standard headers: <stdio.h>,\n<string.h>, <errno.h>, <limits.h>, <assert.h> and <stdlib.h>\n(if available).\n\nNote\nSince Python may define some pre-processor definitions which affect the standard\nheaders on some systems, you must include Python.h before any standard\nheaders are included.\nIt is recommended to always define PY_SSIZE_T_CLEAN before including\nPython.h.  See Parsing arguments and building values for a description of this macro.\n\nAll user visible names defined by Python.h (except those defined by the included\nstandard headers) have one of the prefixes Py or _Py.  Names beginning\nwith _Py are for internal use by the Python implementation and should not be\nused by extension writers. Structure member names do not have a reserved prefix.\n\nNote\nUser code should never define names that begin with Py or _Py. This\nconfuses the reader, and jeopardizes the portability of the user code to\nfuture Python versions, which may define additional names beginning with one\nof these prefixes.\n\nThe header files are typically installed with Python.  On Unix, these  are\nlocated in the directories prefix/include/pythonversion/ and\nexec_prefix/include/pythonversion/, where prefix and\nexec_prefix are defined by the corresponding parameters to Python\u2019s\nconfigure script and version is\n'%d.%d' % sys.version_info[:2].  On Windows, the headers are installed\nin prefix/include, where prefix is the installation\ndirectory specified to the installer.\nTo include the headers, place both directories (if different) on your compiler\u2019s\nsearch path for includes.  Do not place the parent directories on the search\npath and then use #include <pythonX.Y/Python.h>; this will break on\nmulti-platform builds since the platform independent headers under\nprefix include the platform specific headers from\nexec_prefix.\nC++ users should note that although the API is defined entirely using C, the\nheader files properly declare the entry points to be extern \"C\". As a result,\nthere is no need to do anything special to use the API from C++.\n\n\nUseful macros\u00b6\nSeveral useful macros are defined in the Python header files.  Many are\ndefined closer to where they are useful (e.g. Py_RETURN_NONE).\nOthers of a more general utility are defined here.  This is not necessarily a\ncomplete listing.\n\n\nPy_UNREACHABLE()\u00b6\nUse this when you have a code path that cannot be reached by design.\nFor example, in the default: clause in a switch statement for which\nall possible values are covered in case statements.  Use this in places\nwhere you might be tempted to put an assert(0) or abort() call.\nIn release mode, the macro helps the compiler to optimize the code, and\navoids a warning about unreachable code.  For example, the macro is\nimplemented with __builtin_unreachable() on GCC in release mode.\nA use for Py_UNREACHABLE() is following a call a function that\nnever returns but that is not declared _Py_NO_RETURN.\nIf a code path is very unlikely code but can be reached under exceptional\ncase, this macro must not be used.  For example, under low memory condition\nor if a system call returns a value out of the expected range.  In this\ncase, it\u2019s better to report the error to the caller.  If the error cannot\nbe reported to caller, Py_FatalError() can be used.\n\nNew in version 3.7.\n\n\n\n\nPy_ABS(x)\u00b6\nReturn the absolute value of x.\n\nNew in version 3.3.\n\n\n\n\nPy_MIN(x, y)\u00b6\nReturn the minimum value between x and y.\n\nNew in version 3.3.\n\n\n\n\nPy_MAX(x, y)\u00b6\nReturn the maximum value between x and y.\n\nNew in version 3.3.\n\n\n\n\nPy_STRINGIFY(x)\u00b6\nConvert x to a C string.  E.g. Py_STRINGIFY(123) returns\n\"123\".\n\nNew in version 3.4.\n\n\n\n\nPy_MEMBER_SIZE(type, member)\u00b6\nReturn the size of a structure (type) member in bytes.\n\nNew in version 3.6.\n\n\n\n\nPy_CHARMASK(c)\u00b6\nArgument must be a character or an integer in the range [-128, 127] or [0,\n255].  This macro returns c cast to an unsigned char.\n\n\n\nPy_GETENV(s)\u00b6\nLike getenv(s), but returns NULL if -E was passed on the\ncommand line (i.e. if Py_IgnoreEnvironmentFlag is set).\n\n\n\nPy_UNUSED(arg)\u00b6\nUse this for unused arguments in a function definition to silence compiler\nwarnings. Example: int func(int a, int Py_UNUSED(b)) { return a; }.\n\nNew in version 3.4.\n\n\n\n\nPy_DEPRECATED(version)\u00b6\nUse this for deprecated declarations.  The macro must be placed before the\nsymbol name.\nExample:\nPy_DEPRECATED(3.8) PyAPI_FUNC(int) Py_OldFunction(void);\n\n\n\nChanged in version 3.8: MSVC support was added.\n\n\n\n\nPyDoc_STRVAR(name, str)\u00b6\nCreates a variable with name name that can be used in docstrings.\nIf Python is built without docstrings, the value will be empty.\nUse PyDoc_STRVAR for docstrings to support building\nPython without docstrings, as specified in PEP 7.\nExample:\nPyDoc_STRVAR(pop_doc, \"Remove and return the rightmost element.\");\n\nstatic PyMethodDef deque_methods[] = {\n// ...\n{\"pop\", (PyCFunction)deque_pop, METH_NOARGS, pop_doc},\n// ...\n}\n\n\n\n\n\nPyDoc_STR(str)\u00b6\nCreates a docstring for the given input string or an empty string\nif docstrings are disabled.\nUse PyDoc_STR in specifying docstrings to support\nbuilding Python without docstrings, as specified in PEP 7.\nExample:\nstatic PyMethodDef pysqlite_row_methods[] = {\n{\"keys\", (PyCFunction)pysqlite_row_keys, METH_NOARGS,\nPyDoc_STR(\"Returns the keys of the row.\")},\n{NULL, NULL}\n};\n\n\n\n\n\nObjects, Types and Reference Counts\u00b6\nMost Python/C API functions have one or more arguments as well as a return value\nof type PyObject*.  This type is a pointer to an opaque data type\nrepresenting an arbitrary Python object.  Since all Python object types are\ntreated the same way by the Python language in most situations (e.g.,\nassignments, scope rules, and argument passing), it is only fitting that they\nshould be represented by a single C type.  Almost all Python objects live on the\nheap: you never declare an automatic or static variable of type\nPyObject, only pointer variables of type PyObject* can  be\ndeclared.  The sole exception are the type objects; since these must never be\ndeallocated, they are typically static PyTypeObject objects.\nAll Python objects (even Python integers) have a type and a\nreference count.  An object\u2019s type determines what kind of object it is\n(e.g., an integer, a list, or a user-defined function; there are many more as\nexplained in The standard type hierarchy).  For each of the well-known types there is a macro\nto check whether an object is of that type; for instance, PyList_Check(a) is\ntrue if (and only if) the object pointed to by a is a Python list.\n\nReference Counts\u00b6\nThe reference count is important because today\u2019s computers have a  finite (and\noften severely limited) memory size; it counts how many  different places there\nare that have a reference to an object.  Such a  place could be another object,\nor a global (or static) C variable, or  a local variable in some C function.\nWhen an object\u2019s reference count  becomes zero, the object is deallocated.  If\nit contains references to  other objects, their reference count is decremented.\nThose other  objects may be deallocated in turn, if this decrement makes their\nreference count become zero, and so on.  (There\u2019s an obvious problem  with\nobjects that reference each other here; for now, the solution is  \u201cdon\u2019t do\nthat.\u201d)\nReference counts are always manipulated explicitly.  The normal way is  to use\nthe macro Py_INCREF() to increment an object\u2019s reference count by one,\nand Py_DECREF() to decrement it by   one.  The Py_DECREF() macro\nis considerably more complex than the incref one, since it must check whether\nthe reference count becomes zero and then cause the object\u2019s deallocator to be\ncalled. The deallocator is a function pointer contained in the object\u2019s type\nstructure.  The type-specific deallocator takes care of decrementing the\nreference counts for other objects contained in the object if this is a compound\nobject type, such as a list, as well as performing any additional finalization\nthat\u2019s needed.  There\u2019s no chance that the reference count can overflow; at\nleast as many bits are used to hold the reference count as there are distinct\nmemory locations in virtual memory (assuming sizeof(Py_ssize_t) >= sizeof(void*)).\nThus, the reference count increment is a simple operation.\nIt is not necessary to increment an object\u2019s reference count for every  local\nvariable that contains a pointer to an object.  In theory, the  object\u2019s\nreference count goes up by one when the variable is made to  point to it and it\ngoes down by one when the variable goes out of  scope.  However, these two\ncancel each other out, so at the end the  reference count hasn\u2019t changed.  The\nonly real reason to use the  reference count is to prevent the object from being\ndeallocated as  long as our variable is pointing to it.  If we know that there\nis at  least one other reference to the object that lives at least as long as\nour variable, there is no need to increment the reference count  temporarily.\nAn important situation where this arises is in objects  that are passed as\narguments to C functions in an extension module  that are called from Python;\nthe call mechanism guarantees to hold a  reference to every argument for the\nduration of the call.\nHowever, a common pitfall is to extract an object from a list and hold on to it\nfor a while without incrementing its reference count. Some other operation might\nconceivably remove the object from the list, decrementing its reference count\nand possibly deallocating it. The real danger is that innocent-looking\noperations may invoke arbitrary Python code which could do this; there is a code\npath which allows control to flow back to the user from a Py_DECREF(), so\nalmost any operation is potentially dangerous.\nA safe approach is to always use the generic operations (functions  whose name\nbegins with PyObject_, PyNumber_, PySequence_ or PyMapping_).\nThese operations always increment the reference count of the object they return.\nThis leaves the caller with the responsibility to call Py_DECREF() when\nthey are done with the result; this soon becomes second nature.\n\nReference Count Details\u00b6\nThe reference count behavior of functions in the Python/C API is best  explained\nin terms of ownership of references.  Ownership pertains to references, never\nto objects (objects are not owned: they are always shared).  \u201cOwning a\nreference\u201d means being responsible for calling Py_DECREF on it when the\nreference is no longer needed.  Ownership can also be transferred, meaning that\nthe code that receives ownership of the reference then becomes responsible for\neventually decref\u2019ing it by calling Py_DECREF() or Py_XDECREF()\nwhen it\u2019s no longer needed\u2014or passing on this responsibility (usually to its\ncaller). When a function passes ownership of a reference on to its caller, the\ncaller is said to receive a new reference.  When no ownership is transferred,\nthe caller is said to borrow the reference. Nothing needs to be done for a\nborrowed reference.\nConversely, when a calling function passes in a reference to an  object, there\nare two possibilities: the function steals a  reference to the object, or it\ndoes not.  Stealing a reference means that when you pass a reference to a\nfunction, that function assumes that it now owns that reference, and you are not\nresponsible for it any longer.\nFew functions steal references; the two notable exceptions are\nPyList_SetItem() and PyTuple_SetItem(), which  steal a reference\nto the item (but not to the tuple or list into which the item is put!).  These\nfunctions were designed to steal a reference because of a common idiom for\npopulating a tuple or list with newly created objects; for example, the code to\ncreate the tuple (1, 2, \"three\") could look like this (forgetting about\nerror handling for the moment; a better way to code this is shown below):\nPyObject *t;\n\nt = PyTuple_New(3);\nPyTuple_SetItem(t, 0, PyLong_FromLong(1L));\nPyTuple_SetItem(t, 1, PyLong_FromLong(2L));\nPyTuple_SetItem(t, 2, PyUnicode_FromString(\"three\"));\n\n\nHere, PyLong_FromLong() returns a new reference which is immediately\nstolen by PyTuple_SetItem().  When you want to keep using an object\nalthough the reference to it will be stolen, use Py_INCREF() to grab\nanother reference before calling the reference-stealing function.\nIncidentally, PyTuple_SetItem() is the only way to set tuple items;\nPySequence_SetItem() and PyObject_SetItem() refuse to do this\nsince tuples are an immutable data type.  You should only use\nPyTuple_SetItem() for tuples that you are creating yourself.\nEquivalent code for populating a list can be written using PyList_New()\nand PyList_SetItem().\nHowever, in practice, you will rarely use these ways of creating and populating\na tuple or list.  There\u2019s a generic function, Py_BuildValue(), that can\ncreate most common objects from C values, directed by a format string.\nFor example, the above two blocks of code could be replaced by the following\n(which also takes care of the error checking):\nPyObject *tuple, *list;\n\ntuple = Py_BuildValue(\"(iis)\", 1, 2, \"three\");\nlist = Py_BuildValue(\"[iis]\", 1, 2, \"three\");\n\n\nIt is much more common to use PyObject_SetItem() and friends with items\nwhose references you are only borrowing, like arguments that were passed in to\nthe function you are writing.  In that case, their behaviour regarding reference\ncounts is much saner, since you don\u2019t have to increment a reference count so you\ncan give a reference away (\u201chave it be stolen\u201d).  For example, this function\nsets all items of a list (actually, any mutable sequence) to a given item:\nint\nset_all(PyObject *target, PyObject *item)\n{\nPy_ssize_t i, n;\n\nn = PyObject_Length(target);\nif (n < 0)\nreturn -1;\nfor (i = 0; i < n; i++) {\nPyObject *index = PyLong_FromSsize_t(i);\nif (!index)\nreturn -1;\nif (PyObject_SetItem(target, index, item) < 0) {\nPy_DECREF(index);\nreturn -1;\n}\nPy_DECREF(index);\n}\nreturn 0;\n}\n\n\nThe situation is slightly different for function return values.   While passing\na reference to most functions does not change your  ownership responsibilities\nfor that reference, many functions that  return a reference to an object give\nyou ownership of the reference. The reason is simple: in many cases, the\nreturned object is created  on the fly, and the reference you get is the only\nreference to the  object.  Therefore, the generic functions that return object\nreferences, like PyObject_GetItem() and  PySequence_GetItem(),\nalways return a new reference (the caller becomes the owner of the reference).\nIt is important to realize that whether you own a reference returned  by a\nfunction depends on which function you call only \u2014 the plumage (the type of\nthe object passed as an argument to the function) doesn\u2019t enter into it!\nThus, if you  extract an item from a list using PyList_GetItem(), you\ndon\u2019t own the reference \u2014 but if you obtain the same item from the same list\nusing PySequence_GetItem() (which happens to take exactly the same\narguments), you do own a reference to the returned object.\nHere is an example of how you could write a function that computes the sum of\nthe items in a list of integers; once using  PyList_GetItem(), and once\nusing PySequence_GetItem().\nlong\nsum_list(PyObject *list)\n{\nPy_ssize_t i, n;\nlong total = 0, value;\nPyObject *item;\n\nn = PyList_Size(list);\nif (n < 0)\nreturn -1; /* Not a list */\nfor (i = 0; i < n; i++) {\nitem = PyList_GetItem(list, i); /* Can't fail */\nif (!PyLong_Check(item)) continue; /* Skip non-integers */\nvalue = PyLong_AsLong(item);\nif (value == -1 && PyErr_Occurred())\n/* Integer too big to fit in a C long, bail out */\nreturn -1;\ntotal += value;\n}\nreturn total;\n}\n\n\nlong\nsum_sequence(PyObject *sequence)\n{\nPy_ssize_t i, n;\nlong total = 0, value;\nPyObject *item;\nn = PySequence_Length(sequence);\nif (n < 0)\nreturn -1; /* Has no length */\nfor (i = 0; i < n; i++) {\nitem = PySequence_GetItem(sequence, i);\nif (item == NULL)\nreturn -1; /* Not a sequence, or other failure */\nif (PyLong_Check(item)) {\nvalue = PyLong_AsLong(item);\nPy_DECREF(item);\nif (value == -1 && PyErr_Occurred())\n/* Integer too big to fit in a C long, bail out */\nreturn -1;\ntotal += value;\n}\nelse {\nPy_DECREF(item); /* Discard reference ownership */\n}\n}\nreturn total;\n}\n\n\n\n\n\nTypes\u00b6\nThere are few other data types that play a significant role in  the Python/C\nAPI; most are simple C types such as int,  long,\ndouble and char*.  A few structure types  are used to\ndescribe static tables used to list the functions exported  by a module or the\ndata attributes of a new object type, and another is used to describe the value\nof a complex number.  These will  be discussed together with the functions that\nuse them.\n\n\n\nExceptions\u00b6\nThe Python programmer only needs to deal with exceptions if specific  error\nhandling is required; unhandled exceptions are automatically  propagated to the\ncaller, then to the caller\u2019s caller, and so on, until they reach the top-level\ninterpreter, where they are reported to the  user accompanied by a stack\ntraceback.\nFor C programmers, however, error checking always has to be explicit.  All\nfunctions in the Python/C API can raise exceptions, unless an explicit claim is\nmade otherwise in a function\u2019s documentation.  In general, when a function\nencounters an error, it sets an exception, discards any object references that\nit owns, and returns an error indicator.  If not documented otherwise, this\nindicator is either NULL or -1, depending on the function\u2019s return type.\nA few functions return a Boolean true/false result, with false indicating an\nerror.  Very few functions return no explicit error indicator or have an\nambiguous return value, and require explicit testing for errors with\nPyErr_Occurred().  These exceptions are always explicitly documented.\nException state is maintained in per-thread storage (this is  equivalent to\nusing global storage in an unthreaded application).  A  thread can be in one of\ntwo states: an exception has occurred, or not. The function\nPyErr_Occurred() can be used to check for this: it returns a borrowed\nreference to the exception type object when an exception has occurred, and\nNULL otherwise.  There are a number of functions to set the exception state:\nPyErr_SetString() is the most common (though not the most general)\nfunction to set the exception state, and PyErr_Clear() clears the\nexception state.\nThe full exception state consists of three objects (all of which can  be\nNULL): the exception type, the corresponding exception  value, and the\ntraceback.  These have the same meanings as the Python result of\nsys.exc_info(); however, they are not the same: the Python objects represent\nthe last exception being handled by a Python  try \u2026\nexcept statement, while the C level exception state only exists while\nan exception is being passed on between C functions until it reaches the Python\nbytecode interpreter\u2019s  main loop, which takes care of transferring it to\nsys.exc_info() and friends.\nNote that starting with Python 1.5, the preferred, thread-safe way to access the\nexception state from Python code is to call the function sys.exc_info(),\nwhich returns the per-thread exception state for Python code.  Also, the\nsemantics of both ways to access the exception state have changed so that a\nfunction which catches an exception will save and restore its thread\u2019s exception\nstate so as to preserve the exception state of its caller.  This prevents common\nbugs in exception handling code caused by an innocent-looking function\noverwriting the exception being handled; it also reduces the often unwanted\nlifetime extension for objects that are referenced by the stack frames in the\ntraceback.\nAs a general principle, a function that calls another function to  perform some\ntask should check whether the called function raised an  exception, and if so,\npass the exception state on to its caller.  It  should discard any object\nreferences that it owns, and return an  error indicator, but it should not set\nanother exception \u2014 that would overwrite the exception that was just raised,\nand lose important information about the exact cause of the error.\nA simple example of detecting exceptions and passing them on is shown in the\nsum_sequence() example above.  It so happens that this example doesn\u2019t\nneed to clean up any owned references when it detects an error.  The following\nexample function shows some error cleanup.  First, to remind you why you like\nPython, we show the equivalent Python code:\ndef incr_item(dict, key):\ntry:\nitem = dict[key]\nexcept KeyError:\nitem = 0\ndict[key] = item + 1\n\n\nHere is the corresponding C code, in all its glory:\nint\nincr_item(PyObject *dict, PyObject *key)\n{\n/* Objects all initialized to NULL for Py_XDECREF */\nPyObject *item = NULL, *const_one = NULL, *incremented_item = NULL;\nint rv = -1; /* Return value initialized to -1 (failure) */\n\nitem = PyObject_GetItem(dict, key);\nif (item == NULL) {\n/* Handle KeyError only: */\nif (!PyErr_ExceptionMatches(PyExc_KeyError))\ngoto error;\n\n/* Clear the error and use zero: */\nPyErr_Clear();\nitem = PyLong_FromLong(0L);\nif (item == NULL)\ngoto error;\n}\nconst_one = PyLong_FromLong(1L);\nif (const_one == NULL)\ngoto error;\n\nincremented_item = PyNumber_Add(item, const_one);\nif (incremented_item == NULL)\ngoto error;\n\nif (PyObject_SetItem(dict, key, incremented_item) < 0)\ngoto error;\nrv = 0; /* Success */\n/* Continue with cleanup code */\n\nerror:\n/* Cleanup code, shared by success and failure path */\n\n/* Use Py_XDECREF() to ignore NULL references */\nPy_XDECREF(item);\nPy_XDECREF(const_one);\nPy_XDECREF(incremented_item);\n\nreturn rv; /* -1 for error, 0 for success */\n}\n\n\nThis example represents an endorsed use of the goto statement  in C!\nIt illustrates the use of PyErr_ExceptionMatches() and\nPyErr_Clear() to handle specific exceptions, and the use of\nPy_XDECREF() to dispose of owned references that may be NULL (note the\n'X' in the name; Py_DECREF() would crash when confronted with a\nNULL reference).  It is important that the variables used to hold owned\nreferences are initialized to NULL for this to work; likewise, the proposed\nreturn value is initialized to -1 (failure) and only set to success after\nthe final call made is successful.\n\n\nEmbedding Python\u00b6\nThe one important task that only embedders (as opposed to extension writers) of\nthe Python interpreter have to worry about is the initialization, and possibly\nthe finalization, of the Python interpreter.  Most functionality of the\ninterpreter can only be used after the interpreter has been initialized.\nThe basic initialization function is Py_Initialize(). This initializes\nthe table of loaded modules, and creates the fundamental modules\nbuiltins, __main__, and sys.  It also\ninitializes the module search path (sys.path).\nPy_Initialize() does not set the \u201cscript argument list\u201d  (sys.argv).\nIf this variable is needed by Python code that will be executed later, it must\nbe set explicitly with a call to  PySys_SetArgvEx(argc, argv, updatepath)\nafter the call to Py_Initialize().\nOn most systems (in particular, on Unix and Windows, although the details are\nslightly different), Py_Initialize() calculates the module search path\nbased upon its best guess for the location of the standard Python interpreter\nexecutable, assuming that the Python library is found in a fixed location\nrelative to the Python interpreter executable.  In particular, it looks for a\ndirectory named lib/pythonX.Y relative to the parent directory\nwhere the executable named python is found on the shell command search\npath (the environment variable PATH).\nFor instance, if the Python executable is found in\n/usr/local/bin/python, it will assume that the libraries are in\n/usr/local/lib/pythonX.Y.  (In fact, this particular path is also\nthe \u201cfallback\u201d location, used when no executable file named python is\nfound along PATH.)  The user can override this behavior by setting the\nenvironment variable PYTHONHOME, or insert additional directories in\nfront of the standard path by setting PYTHONPATH.\nThe embedding application can steer the search by calling\nPy_SetProgramName(file) before calling  Py_Initialize().  Note that\nPYTHONHOME still overrides this and PYTHONPATH is still\ninserted in front of the standard path.  An application that requires total\ncontrol has to provide its own implementation of Py_GetPath(),\nPy_GetPrefix(), Py_GetExecPrefix(), and\nPy_GetProgramFullPath() (all defined in Modules/getpath.c).\nSometimes, it is desirable to \u201cuninitialize\u201d Python.  For instance,  the\napplication may want to start over (make another call to\nPy_Initialize()) or the application is simply done with its  use of\nPython and wants to free memory allocated by Python.  This can be accomplished\nby calling Py_FinalizeEx().  The function Py_IsInitialized() returns\ntrue if Python is currently in the initialized state.  More information about\nthese functions is given in a later chapter. Notice that Py_FinalizeEx()\ndoes not free all memory allocated by the Python interpreter, e.g. memory\nallocated by extension modules currently cannot be released.\n\n\nDebugging Builds\u00b6\nPython can be built with several macros to enable extra checks of the\ninterpreter and extension modules.  These checks tend to add a large amount of\noverhead to the runtime so they are not enabled by default.\nA full list of the various types of debugging builds is in the file\nMisc/SpecialBuilds.txt in the Python source distribution. Builds are\navailable that support tracing of reference counts, debugging the memory\nallocator, or low-level profiling of the main interpreter loop.  Only the most\nfrequently-used builds will be described in the remainder of this section.\nCompiling the interpreter with the Py_DEBUG macro defined produces\nwhat is generally meant by a debug build of Python.\nPy_DEBUG is enabled in the Unix build by adding\n--with-pydebug to the ./configure command.\nIt is also implied by the presence of the\nnot-Python-specific _DEBUG macro.  When Py_DEBUG is enabled\nin the Unix build, compiler optimization is disabled.\nIn addition to the reference count debugging described below, extra checks are\nperformed, see Python Debug Build.\nDefining Py_TRACE_REFS enables reference tracing\n(see the configure --with-trace-refs option).\nWhen defined, a circular doubly linked list of active objects is maintained by adding two extra\nfields to every PyObject.  Total allocations are tracked as well.  Upon\nexit, all existing references are printed.  (In interactive mode this happens\nafter every statement run by the interpreter.)\nPlease refer to Misc/SpecialBuilds.txt in the Python source distribution\nfor more detailed information.\n\n\n\n\n\n\n\n\nTable of Contents\n\nIntroduction\nCoding standards\nInclude Files\nUseful macros\nObjects, Types and Reference Counts\nReference Counts\nReference Count Details\n\n\nTypes\n\n\nExceptions\nEmbedding Python\nDebugging Builds\n\n\n\nPrevious topic\nPython/C API Reference Manual\nNext topic\nC API Stability\n\nThis Page\n\nReport a Bug\n\nShow Source\n\n\n\n\n\n\n\n\n\nNavigation\n\n\nindex\n\nmodules |\n\nnext |\n\nprevious |\n\nPython \u00bb\n\n\n\n\n\n\n\n3.10.0 Documentation \u00bb\n\nPython/C API Reference Manual \u00bb\nIntroduction\n\n\n\n\n\n\n\n\n\n|\n\n\n\n\n\u00a9 Copyright 2001-2021, Python Software Foundation.\n\nThis page is licensed under the Python Software Foundation License Version 2.\n\nExamples, recipes, and other code in the documentation are additionally licensed under the Zero Clause BSD License.\n\nSee History and License for more information.\n\n\nThe Python Software Foundation is a non-profit corporation.\nPlease donate.\n\n\n\nLast updated on Oct 07, 2021.\nFound a bug?\n\n\nCreated using Sphinx 3.2.1.\n\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "maven deploy:deploy-file fails (409 Conflict), yet artifact uploads successfully", "id": 1722, "answers": [{"answer_id": 1710, "document_id": 1295, "question_id": 1722, "text": "Ensure that you include -SNAPSHOT as part of your version if you are publishing to the snapshot repository.\nAnd remove -SNAPSHOT in case you are publishing it to a non-snapshot repository.", "answer_start": 541, "answer_category": null}], "is_impossible": false}], "context": "I found the problem. Two problems actually:\n\u2022\tI only had the release repository setup, and I was attempting to save a snapshot release in the release repository. Artifactory was setup to only allow releases in the release repository. This can be modified in the Artifactory setting, but I decided against this.\n\u2022\tMy pom.xml has a different version in it than I was trying to save it to. For example, the pom.xml said version 2.0 and I was trying to save the release as 2.0.2. Artifactory rejected the pom (but not the jar) for this reason.\n\nEnsure that you include -SNAPSHOT as part of your version if you are publishing to the snapshot repository.\nAnd remove -SNAPSHOT in case you are publishing it to a non-snapshot repository.\nI have a project in Jenkins where I use the promotion plugin to deploy my artifacts in Maven via the deploy:deploy-file goal.\nThis works for several other projects I have in Maven, but it fails for this project. The funny thing is that the file (but not the pom.xml) uploads anyway. I've verified this by removing the artifact from our Maven repository, then running the promotion. The artifact is in our repository after the promotion.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to check for DLL dependency?", "id": 1859, "answers": [{"answer_id": 1845, "document_id": 1430, "question_id": 1859, "text": "You can try Dependencies (Dec 2019) which is a modern rewrite of the famous Dependency Walker (last update in 2006)", "answer_start": 485, "answer_category": null}], "is_impossible": false}], "context": "Sometimes when I'm doing a little project I'm not careful enough and accidentally add a dependency for a DLL that I am not aware of. When I ship this program to a friend or other people, \"it doesn't work\" because \"some DLL\" is missing. This is of course because the program can find the DLL on my system, but not on theirs.\nIs there a way to scan an executable for DLL dependencies or execute the program in a \"clean\" DLL-free environment for testing to prevent these oops situations?\nYou can try Dependencies (Dec 2019) which is a modern rewrite of the famous Dependency Walker (last update in 2006)\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "how to install gprof on os x", "id": 1981, "answers": [{"answer_id": 1967, "document_id": 1566, "question_id": 1981, "text": "brew install google-perftools graphviz ghostscript gv\nbrew link --overwrite ghostscript", "answer_start": 706, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI'm trying to use gprof to profile a c++ application I've written but I can't figure out for the life of me how to download and install it. I've googled everything I can think of and can't even find a download link. Someone please help!\n    \n\nIt seems there are two components to gprof.  One is a part of the GCC compiler itself, invoked by using the -pg argument.  The other is the gprof command, which apparently is part of GNU binutils.  I'll leave it to you to figure out how to install GCC and binutils on OSX...\n    \n\nI did not find a MacOS solution for gprof and gcov did not work for me, but gperftools (Google Performance Tools) do work. Here is how to install them on MacOS:\n\nbrew install google-perftools graphviz ghostscript gv\nbrew link --overwrite ghostscript\n\n\nNext, run the profiler on a program:\n\nCPUPROFILE=program_name.prof DYLD_INSERT_LIBRARIES=/usr/local/Cellar/gperftools/2.6.3/lib/libprofiler.dylib ./program_name\npprof --pdf program_name program_name.prof &gt; program_name.pdf\n\n\nYou can find more options for gperftools here. Finally, open the program_name.pdf file in a PDF viewer such as Preview to enjoy the fancy graphviz output.\n\nObviously, running the profiler on a program can be automated very easily with a Bash script, as there only is the one program_name parameter and the shared library location is constant. Here is an example script called profile.sh that does exactly that, but includes compiling and adds a second variable so that you can compile .cpp files with a different name:\n\n#!/bin/bash\ng++-7 -fopenmp -O3 -o $1 $2.cpp\nCPUPROFILE=$1.prof DYLD_INSERT_LIBRARIES=/usr/local/Cellar/gperftools/2.6.3/lib/libprofiler.dylib ./$1\npprof --pdf $1 $1.prof &gt; $1.pdf\necho \"Profiling results: $1.pdf\"\n\n\nNext, modify the permissions so it can run from any folder:\n\nchmod +x profile.sh\n\n\nThe script can be invoked from the command line as follows, automating the full process:\n\n./profile.sh program_name cpp_name\n\n\nYou may want to separate the compilation and profiling steps, which is easy enough to do by removing the g++-7 line in the shell script.\n    \n\nSince gprof does not work on OS X at the moment, use Google Performance Tools, now known as gperftools.\n\nAlso gcov works \"out of the box\" if you have gcc installed.\n\n$ gcc -fprofile-arcs -ftest-coverage your_program.c\n$ a.out\n$ gcov your_program.c\n\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to add an instance to a Amazon cluster? ", "id": 456, "answers": [{"answer_id": 465, "document_id": 189, "question_id": 456, "text": "For this to work, you need follow the steps outlined in Launching an Amazon ECS Container Instance, be it manually or via automation. Be aware of step 10.:", "answer_start": 464, "answer_category": null}], "is_impossible": false}], "context": "When creating a new cluster using Amazon ECS console, how to add a new ec2 instance to the new cluster. In other words, when launching a new ec2 instance, what config is needed in order to allocate it to a user created cluster under Amazon ECS.\nAmazon ECS Container Instances are added indirectly, it's the job of the Amazon ECS Container Agent on each instance to register itself with the cluster created and named by you, see concepts and lifecycle for details.\nFor this to work, you need follow the steps outlined in Launching an Amazon ECS Container Instance, be it manually or via automation. Be aware of step 10.:\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "ImportError: No module named site on Windows", "id": 656, "answers": [{"answer_id": 661, "document_id": 349, "question_id": 656, "text": "Setting the PYTHONPATH / PYTHONHOME variables", "answer_start": 1673, "answer_category": null}], "is_impossible": false}], "context": "I am trying to install Python for the first time. I downloaded the following installer from the Python website: Python 2.7.1 Windows Installer (Windows binary -- does not include source). I then ran the installer, selected 'All Users' and all was fine. I installed Python into the default location:\nC:\\Python27\nNext, to test that Python was installed correctly, I navigated to my Python Directory, and ran the \"python\" command in the windows cmd prompt. It returns me the following error:\nImportError: No module named site\nWhen I do 'python -v' I get the following:\n#installing zipimport hook\nimport zipimport # builtin #installed zipimport hook\n#ImportError: No module named site #clear builtin._\n#clear sys.path #clear sys.argv\n#clear sys.ps1 #clear sys.ps2\n#clear sys.exitfunc #clear sys.exc_type\n#clear sys.exc_value #clear sys.exc_traceback\n#clear sys.last_type #clear sys.last_value\n#clear sys.last_traceback #clear sys.path_hooks\n#clear sys.path_importer_cache #clear sys.meta_path\n#clear sys.flags #clear sys.float_info\n#restore sys.stdin #restore sys.stdout\n#restore sys.stderr #cleanup main\n#cleanup[1] zipimport #cleanup[1] signal\n#cleanup[1] exceptions #cleanup[1] _warnings\n#cleanup sys #cleanup builtin\n#cleanup ints: 6 unfreed ints #cleanup floats\nWhen I do dir C:\\Python27\\Lib\\site.py* I get the following:\nC:\\Users\\Mimminito>dir C:\\Python27\\Lib\\site.py*\nVolume in drive C has no label.\nVolume Serial Number is DAB9-A863\nDirectory of C:\\Python27\\Lib\n13/11/2010 20:08 20,389 site.py\n1 File(s) 20,389 bytes\n0 Dir(s) 694,910,976 bytes free\nAny ideas?\nI've been looking into this problem for myself for almost a day and finally had a breakthrough. Try this:\n1.\tSetting the PYTHONPATH / PYTHONHOME variables\nMake sure you don't have a trailing '\\' on the PYTHON* vars, this seems to break it aswel.Right click the Computer icon in the start menu, go to properties. On the left tab, go to Advanced system settings. In the window that comes up, go to the Advanced tab, then at the bottom click Environment Variables. Click in the list of user variables and start typing Python, and repeat for System variables, just to make certain that you don't have mis-set variables for PYTHONPATH or PYTHONHOME. Next, add new variables (I did in System rather than User, although it may work for User too): PYTHONPATH, set to C:\\Python27\\Lib. PYTHONHOME, set to C:\\Python27.\nHope this helps!\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Installation of VB6 on Windows 7 / 8 / 10", "id": 1184, "answers": [{"answer_id": 1177, "document_id": 760, "question_id": 1184, "text": "What I have done and never came across any issues, is to install VB6, ignore the errors and then proceed to install the latest service pack, currently SP6", "answer_start": 493, "answer_category": null}], "is_impossible": false}], "context": "I have been having problems installing VB6 on Windows 7. I realize it is a legacy IDE and my research on the net hasn't been that much of help. Every time I attempt to install VB6 on Windows 7, besides the alert notifying me of compatibility issues, the setup runs and fails near the end, to which the installation process fails ultimately. And when another attempt is made, certain dll files are supposedly unreachable. I've installed and use VB6 for legacy projects many times on Windows 7.\nWhat I have done and never came across any issues, is to install VB6, ignore the errors and then proceed to install the latest service pack, currently SP6.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to manually install an artifact in Maven 2?", "id": 664, "answers": [{"answer_id": 669, "document_id": 357, "question_id": 664, "text": "mvn install:install-file \\\n  -DgroupId=javax.transaction \\\n  -DartifactId=jta \\\n  -Dpackaging=jar \\\n  -Dversion=1.0.1B \\\n  -Dfile=jta-1.0.1B.jar \\\n  -DgeneratePom=true", "answer_start": 315, "answer_category": null}], "is_impossible": false}], "context": "I've encountered some errors when I tried to install an artifact manually with Maven 2. I wanted to install a jar from a local directory with the command\nmvn install:install-file -Dfile=jta-1.0.1B.jar\nBut Maven gave a build error.\nyou need to indicate the groupId, the artifactId and the version for your artifact:\nmvn install:install-file \\\n  -DgroupId=javax.transaction \\\n  -DartifactId=jta \\\n  -Dpackaging=jar \\\n  -Dversion=1.0.1B \\\n  -Dfile=jta-1.0.1B.jar \\\n  -DgeneratePom=true\nIf you ever get similar errors when using Windows PowerShell, you should try Windows' simple command-line. I didn't find out what caused this, but PowerShell seems to interpret some of Maven's parameters.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "npm install private github repositories by dependency in package.json", "id": 1883, "answers": [{"answer_id": 1869, "document_id": 1454, "question_id": 1883, "text": "The following worked just fine in all scenarios i needed :\n\"dependencies\": {\n\"GitRepo\": \"git+https://<token-from-github>:x-oauth-basic@github.com/<user>/<GitRepo>.git\"\n}", "answer_start": 498, "answer_category": null}], "is_impossible": false}], "context": "I'm trying to install github private repository by npm that includes other private github repositories as dependency.\nHave tried a lot of ways and posts but none is working. Here is what i'm doing :\nnpm install git+https://github.com/myusername/mygitrepository.git\nin package.json is like :\n\"dependencies\": {\n    \"repository1name\": \"git+https://github.com/myusername/repository1.git\",\n    \"repository2name\": \"git+https://github.com/myusername/repository2.git\"\n}\nWhat is the the right way to do it?\nThe following worked just fine in all scenarios i needed :\n\"dependencies\": {\n\"GitRepo\": \"git+https://<token-from-github>:x-oauth-basic@github.com/<user>/<GitRepo>.git\"\n}\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Unable to install R package in Ubuntu 11.04", "id": 1617, "answers": [{"answer_id": 1604, "document_id": 1191, "question_id": 1617, "text": "You need to install the ubuntu package libxml2-dev So in a shell prompt type:\nsudo apt-get update\nsudo apt-get install libxml2-dev", "answer_start": 257, "answer_category": null}], "is_impossible": false}], "context": "I'm new to Linux and R.\nI installed R 2.12 in Ubuntu 11.04. Today I tried to install a new package, so I ran the following command:\ninstall.packages('XML')\nBut the installation failed.\nBut it gives the same error information.\nAnyone can give me any advice?\nYou need to install the ubuntu package libxml2-dev So in a shell prompt type:\nsudo apt-get update\nsudo apt-get install libxml2-dev\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to deploy Node.js application with deep node_modules structure on Windows?", "id": 1678, "answers": [{"answer_id": 1665, "document_id": 1251, "question_id": 1678, "text": "npm v3(released recently) solves this issue by flattening out the dependencies.. Check the release notes here in https://github.com/npm/npm/releases/tag/v3.0.0 under flat flat section.", "answer_start": 0, "answer_category": null}], "is_impossible": false}], "context": "npm v3(released recently) solves this issue by flattening out the dependencies.. Check the release notes here in https://github.com/npm/npm/releases/tag/v3.0.0 under flat flat section.\nAnd the last comment on this issue https://github.com/npm/npm/issues/3697\nI've run into a curious issue - apparently some Node.js module have so deep folder hierarchies that Windows copy command (or PowerShell's Copy-Item which is what we're actually using) hits the infamous \"path too long\" error when path is over 250 chars long.\nFor example, this is a folder hierarchy that a single Node module can create:\nnode_modules\\nodemailer\\node_modules\\simplesmtp\\node_modules\\\nxoauth2\\node_modules\\request\\node_modules\\form-data\\node_modules\\\ncombined-stream\\node_modules\\delayed-stream\\...\nIt seems insane but is a reality with Node modules.\nWe need to use copy-paste during deployment (we're not using a \"clever\" target platform like Heroku where Git deployment would be an option) and this is a serious limitation on Windows.\nIsn't there a npm command or something that would compact the node_modules folder or maybe include only what's actually necessary at runtime? (Node modules usually contain test folders etc. which we don't need to deploy.) Any other ideas how to work around it? Not using Windows is unfortunately not an option :)\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "difference in details between make install and make altinstall", "id": 1980, "answers": [{"answer_id": 1966, "document_id": 1565, "question_id": 1980, "text": "n.1)\n\nTLDR: altinstall skips creating the python link and the manual pages links, install will hide the system binaries and ", "answer_start": 2207, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nHere is my case:\n\nI am using Ubuntu\u00a010.04 (Lucid Lynx). The system's default Python is v2.6.5, but I need Python v2.7. So I downloaded the source from python.org and tried to install it.\n\nThe first time I installed it, I ran:\n\ncd Python2.7.4\n./configure --prefix=/usr\nmake\nsu root\nmake install\n\n\nThis installs Python 2.7 to my system. It will create a link, \"python\", in /usr/bin linking to python2.7 also in /usr/bin. So when I type &gt;python, the system will start Python 2.7.4 for me just like when I type &gt;python2.7.\n\nBut when I install this way:\n\ncd Python2.7.4\n./configure --prefix=/usr\nmake\nsu root\nmake altinstall\n\n\nThe link \"python\" in /usr/bin still exists and links to python2.6 which is the default system version. Of course, I can remove it and create a new soft link linking to python2.7.\n\nWhat is the difference between the command \"make install\" and \"make altinstall\", except for the link in /usr/bin?\n    \n\nLet's take a look at the generated Makefile!\nFirst, the install target:\ninstall:         altinstall bininstall maninstall\n\nIt does everything altinstall does, along with bininstall and maninstall\nHere's bininstall; it just creates the python and other symbolic links.\n# Install the interpreter by creating a symlink chain:\n#  $(PYTHON) -&gt; python2 -&gt; python$(VERSION))\n# Also create equivalent chains for other installed files\nbininstall:     altbininstall\n        -if test -f $(DESTDIR)$(BINDIR)/$(PYTHON) -o -h $(DESTDIR)$(BINDIR)/$(PYTHON); \\\n        then rm -f $(DESTDIR)$(BINDIR)/$(PYTHON); \\\n        else true; \\\n        fi\n        (cd $(DESTDIR)$(BINDIR); $(LN) -s python2$(EXE) $(PYTHON))\n        -rm -f $(DESTDIR)$(BINDIR)/python2$(EXE)\n        (cd $(DESTDIR)$(BINDIR); $(LN) -s python$(VERSION)$(EXE) python2$(EXE))\n        ... (More links created)\n\nAnd here's maninstall, it just creates \"unversioned\" links to the Python manual pages.\n# Install the unversioned manual pages\nmaninstall:     altmaninstall\n        -rm -f $(DESTDIR)$(MANDIR)/man1/python2.1\n        (cd $(DESTDIR)$(MANDIR)/man1; $(LN) -s python$(VERSION).1 python2.1)\n        -rm -f $(DESTDIR)$(MANDIR)/man1/python.1\n        (cd $(DESTDIR)$(MANDIR)/man1; $(LN) -s python2.1 python.1)\n\nTLDR: altinstall skips creating the python link and the manual pages links, install will hide the system binaries and manual pages.\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "remove trailing slash from paths in wix", "id": 1452, "answers": [{"answer_id": 1441, "document_id": 1025, "question_id": 1452, "text": "er]\\Foobar Plugin\" Type=\"string\"/&gt;\n    &lt;/RegistryKey&gt;\n  &lt;/RegistryKey&gt;\n&lt;/DirectoryRef&gt;\n\n\nA", "answer_start": 2829, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI am using WiX to install a plugin for a software that I am not controlling. To install the plugin, I have to put the target folder in a registry key:\n\n&lt;Directory Id=\"TARGETDIR\" Name=\"SourceDir\"&gt;\n  &lt;Directory Id=\"LocalAppDataFolder\"&gt;\n    &lt;Directory Id=\"APPROOTFOLDER\" Name=\"Foobar Plugin\" /&gt;\n  &lt;/Directory&gt;\n&lt;/Directory&gt;\n\n...\n\n&lt;DirectoryRef Id=\"APPROOTFOLDER\"&gt;\n  &lt;Component Id=\"register\" Guid=\"240C21CC-D53B-45A7-94BD-6833CF1568BE\"&gt;\n    &lt;RegistryKey Root=\"HKCU\" Key=\"Software\\ACME\\Plugins\\FooBar\"&gt;\n      &lt;RegistryValue Name=\"InstallDir\" Value=\"[APPROOTFOLDER]\" Type=\"string\"/&gt;\n    &lt;/RegistryKey&gt;\n  &lt;/RegistryKey&gt;\n&lt;/DirectoryRef&gt;\n\n\nAfter the installation, the registry key HKCU\\Software\\ACME\\Plugins\\FooBar\\InstallDir will contain the installation target path, but with a trailing \"\\\". Unfortunately, for some strange reasons, the host application (the provides the plugin architecture) crashes due to that. If there is no trailing slash, everything works fine!\n\nIs there a way in WiX to get rid of the trailing slash?\n\nOne solution I was thinking of is simply adding a \".\" at the end of the path, however, this seems not to work in my scenario :( ..\n    \n\nYou should not be using scripts in custom actions, but if you could limit down to only a few lines and to something as simple as this example, you should be Ok... \n\n&lt;CustomAction Id=\"VBScriptCommand\" Script=\"vbscript\"&gt;\n  &lt;![CDATA[         \n    value = Session.Property(\"INSTALLFOLDER\")\n\n    If Right(value, 1) = \"\\\" Then\n      value = Left(value, Len(value) - 1) \n    End If\n\n    Session.Property(\"SOME_PROPERTY\") = value      \n  ]]&gt;\n&lt;/CustomAction&gt;\n\n&lt;InstallExecuteSequence&gt;\n  &lt;Custom Action=\"VBScriptCommand\" After=\"CostFinalize\"&gt;NOT REMOVE&lt;/Custom&gt;\n&lt;/InstallExecuteSequence&gt;\n\n    \n\nThe only string manipulation you really have in Windows Installer is the manipulation of formatted data types. See MSDN for more information.\n\nWindows Installer provides a trailing directory separator by design, so there isn't any way to remove this aside from a custom action. I'd suggest lodging a bug with the developers of the source package you're developing a plugin for, if you're encountering this error then other developers likely are too.\n    \n\nYou can always do something like this:\n\n&lt;Directory Id=\"TARGETDIR\" Name=\"SourceDir\"&gt;\n  &lt;Directory Id=\"LocalAppDataFolder\"&gt;\n    &lt;Directory Id=\"APPROOTFOLDER\" Name=\"Foobar Plugin\" /&gt;\n  &lt;/Directory&gt;\n&lt;/Directory&gt;\n\n...\n\n&lt;DirectoryRef Id=\"APPROOTFOLDER\"&gt;\n  &lt;Component Id=\"register\" Guid=\"240C21CC-D53B-45A7-94BD-6833CF1568BE\"&gt;\n    &lt;RegistryKey Root=\"HKCU\" Key=\"Software\\ACME\\Plugins\\FooBar\"&gt;\n      &lt;RegistryValue Name=\"InstallDir\" Value=\"[LocalAppDataFolder]\\Foobar Plugin\" Type=\"string\"/&gt;\n    &lt;/RegistryKey&gt;\n  &lt;/RegistryKey&gt;\n&lt;/DirectoryRef&gt;\n\n\nAnd don't allow the user to change the final folder\n    \n\nAs far as I know, Windows Installer doesn't provide any string manipulation natively, so this is going to require a custom action.\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Create Custom Action to Start Application", "id": 957, "answers": [{"answer_id": 952, "document_id": 590, "question_id": 957, "text": "On the File menu, click New Project.\n\nIn the New Project dialog box, select Windows, and then click Class Library. In the Name box, type OpenWeb.\n\nThe project is added to Solution Explorer.\n\nOn the Project menu, click Add Class, and then in the Add New Item dialog box, select Installer Class. Accept the default name of Installer1, and then click Add.\n\nSwitch to code view by clicking click here to switch to code view on the design surface (or by right-clicking the design surface and clicking View Code).\n\nIn the Code Editor, add the following code (which opens a Web browser) to the Installer1 code file, under the constructor.", "answer_start": 606, "answer_category": null}], "is_impossible": false}, {"question": "How deploy another computer?", "id": 958, "answers": [{"answer_id": 953, "document_id": 590, "question_id": 958, "text": "In Solution Explorer, right-click the setup project and click Open Folder in Windows Explorer.\n\nNavigate to the project output, and copy Custom Action Installer.msi, Setup.exe, and all other files and subdirectories in the directory to another computer.\n", "answer_start": 4482, "answer_category": null}], "is_impossible": false}, {"question": "How to uninstall visual studio application?", "id": 959, "answers": [{"answer_id": 954, "document_id": 590, "question_id": 959, "text": "In Control Panel, double-click Add or Remove Programs.\n\nIn the Add or Remove Programs dialog box, select Custom Action Installer and click Remove.", "answer_start": 5302, "answer_category": null}], "is_impossible": false}], "context": "Walkthrough: Creating a Custom Action\n11/01/2012\n4 minutes to read\nThe following walkthrough demonstrates the process of creating a DLL custom action to direct a user to a Web page at the end of an installation. You can use custom actions to run code after the installation finishes.\n\n Note\n\nYour computer might show different names or locations for some of the Visual Studio user interface elements in the following instructions. The Visual Studio edition that you have and the settings that you use determine these elements. For more information, see Visual Studio Settings.\n\nTo create the custom action\nOn the File menu, click New Project.\n\nIn the New Project dialog box, select Windows, and then click Class Library. In the Name box, type OpenWeb.\n\nThe project is added to Solution Explorer.\n\nOn the Project menu, click Add Class, and then in the Add New Item dialog box, select Installer Class. Accept the default name of Installer1, and then click Add.\n\nSwitch to code view by clicking click here to switch to code view on the design surface (or by right-clicking the design surface and clicking View Code).\n\nIn the Code Editor, add the following code (which opens a Web browser) to the Installer1 code file, under the constructor.\n\nVB\n\nCopy\n<Security.Permissions.SecurityPermission(Security.Permissions.SecurityAction.Demand)>\nPublic Overrides Sub Install(ByVal stateSaver As System.Collections.IDictionary)\n    MyBase.Install(stateSaver)\nEnd Sub\n\n<Security.Permissions.SecurityPermission(Security.Permissions.SecurityAction.Demand)>\nPublic Overrides Sub Commit(\n  ByVal savedState As System.Collections.IDictionary)\n\n    MyBase.Commit(savedState)\n    System.Diagnostics.Process.Start(\"https://www.microsoft.com\")\nEnd Sub\n\n<Security.Permissions.SecurityPermission(Security.Permissions.SecurityAction.Demand)>\nPublic Overrides Sub Rollback(ByVal savedState As System.Collections.IDictionary)\n    MyBase.Rollback(savedState)\nEnd Sub\n\n<Security.Permissions.SecurityPermission(Security.Permissions.SecurityAction.Demand)>\nPublic Overrides Sub Uninstall(ByVal savedState As System.Collections.IDictionary)\n    MyBase.Uninstall(savedState)\nEnd Sub\n Note\n\nIf you type Public Overrides and then type space, IntelliSense will provide a list of methods and properties; you can select Commit from the list and get the complete declaration. Repeat for the Install, Rollback, and Uninstall methods.\n\nIn Solution Explorer, right-click Class1 code file, and then click Delete (because it is unnecessary).\n\nTo add a deployment project\nOn the File menu, point to Add, and then click New Project.\n\nIn the Add New Project dialog box, expand the Other Project Types node, expand the Setup and Deployment Projects, click Visual Studio Installer, and then click Setup Project. In the Name box, type Custom Action Installer.\n\nThe project is added to Solution Explorer and the File System Editor is displayed.\n\nIn the File System Editor, select Application Folder in the left pane. On the Action menu, point to Add, and then click Project Output.\n\nIn the Add Project Output Group dialog box, OpenWeb will be displayed in the Project list. Select Primary Output.\n\nPrimary Output from OpenWeb (Active) appears in the Application Folder.\n\nTo add the custom action\nSelect the Custom Action Installer project in Solution Explorer. On the View menu, point to Editor, and then click Custom Actions.\n\nThe Custom Actions Editor is displayed.\n\nIn the Custom Actions Editor, select the Commit node. On the Action menu, click Add Custom Action.\n\nIn the Select Item in Project dialog box, double-click the Application Folder. Select Primary output from OpenWeb.\n\nPrimary output from OpenWeb appears under the Commit node in the Custom Actions Editor.\n\nIn the Properties window, make sure that the InstallerClass property is set to True (this is the default).\n\nIn the Custom Actions Editor, select the Install node and add Primary output from OpenWeb to this node as you did for the Commit node.\n\nOn the Build menu, click Build Custom Action Installer.\n\nTo install on your development computer\nSelect the Custom Action Installer project in Solution Explorer. On the Project menu, click Install.\n\nThis will run the installer and install Custom Action Installer on your development computer. At the end of installation, Internet Explorer should start and should open the Microsoft.com Web site.\n\n Note\n\nYou must have install permissions on the computer in order to run the installer.\n\nTo deploy to another computer\nIn Solution Explorer, right-click the setup project and click Open Folder in Windows Explorer.\n\nNavigate to the project output, and copy Custom Action Installer.msi, Setup.exe, and all other files and subdirectories in the directory to another computer.\n\n Note\n\nTo install on a computer that is not on a network, copy the files to traditional media such as CD-ROM.\n\nOn the target computer, double-click Setup.exe to run the installer.\n\nAt the end of installation, Internet Explorer should start and should open the Microsoft.com Web site.\n\n Note\n\nYou must have install permissions on the computer in order to run the installer.\n\n Note\n\nIf the .NET Framework is not already installed on the target computer, this deployment will install it, and this installation might take several minutes.\n\nTo uninstall the application\nIn Control Panel, double-click Add or Remove Programs.\n\nIn the Add or Remove Programs dialog box, select Custom Action Installer and click Remove.\n\n Tip\n\nTo uninstall from your development computer, with the Custom Action Installer project open and selected in Solution Explorer, from the Project menu, click Uninstall.", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Deploy ASP.NET MVC on IIS 5.1 (Windows XP)", "id": 497, "answers": [{"answer_id": 500, "document_id": 224, "question_id": 497, "text": "You need to add a wildcard mapping in IIS 5. In IIS 6 you have a specific section to add wildcard mappings. ", "answer_start": 256, "answer_category": null}], "is_impossible": false}], "context": "Deploying ASP.NET MVC seems to be painful. I want to deploy my ASP.NET MVC application on Windows XP (IIS 5.1), but can't seem to find how to do it. When I type the application name into the web browser address bar I get a \"website not available\" message.\nYou need to add a wildcard mapping in IIS 5. In IIS 6 you have a specific section to add wildcard mappings. \n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Maven: Trying to Deploy with credentials in settings.xml file", "id": 1723, "answers": [{"answer_id": 1711, "document_id": 1296, "question_id": 1723, "text": "You need to provide the repositoryId=VeggieCorp (not id) property so that maven knows from which <server> configuration it has to read the credentials.\n$ mvn deploy:deploy-file \\\n -Durl=http://repo.veggiecorp.com/artifactory/ext-release-local \\\n -Dfile=crypto.jar \\\n -DpomFile=pom.xml \\\n -DrepositoryId=VeggieCorp", "answer_start": 0, "answer_category": null}], "is_impossible": false}], "context": "You need to provide the repositoryId=VeggieCorp (not id) property so that maven knows from which <server> configuration it has to read the credentials.\n$ mvn deploy:deploy-file \\\n -Durl=http://repo.veggiecorp.com/artifactory/ext-release-local \\\n -Dfile=crypto.jar \\\n -DpomFile=pom.xml \\\n -DrepositoryId=VeggieCorp\n\nthis seemed to be working last week and now it doesn't.\n\u2022\tWe use Artifactory as our Maven repository.\n\u2022\tI am deploying a jar and pom using the deploy:deploy-file goal\n\u2022\tOur Artifactory repository requires authentication to deploy.\n\n\u2022\t(I've reformatted the output to make it easier to see. I'm getting a 401 \"Unauthorized\" error)\n\u2022\tSo, what am I doing wrong? Why can't I use my .settings.xml file to do my credentials? The proxy part does work because it can download the needed plugins from the main Maven repository.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How do I transfer a local Magento install onto my live server?", "id": 1120, "answers": [{"answer_id": 1113, "document_id": 697, "question_id": 1120, "text": "I assume these two steps are obvious:\ncopy all of your local files to production server\ndump your magento local db and import it into your production server db", "answer_start": 247, "answer_category": null}], "is_impossible": false}], "context": "I have spent a long time building a store with Magento on my local development PC.\nNow that I am happy with the result, I would like to upload it to my live production server.\nWhat steps must I complete to ensure this move is as easy as possible?\nI assume these two steps are obvious:\ncopy all of your local files to production server\ndump your magento local db and import it into your production server db\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to install Python packages from the tar.gz file without using pip install", "id": 923, "answers": [{"answer_id": 918, "document_id": 575, "question_id": 923, "text": "pip install relative_path_to_seaborn.tar.gz    \npip install absolute_path_to_seaborn.tar.gz    \npip install file:///absolute_path_to_seaborn.tar.gz    ", "answer_start": 936, "answer_category": null}], "is_impossible": false}, {"question": "how to install python packages from the tar gz file without using pip install?", "id": 924, "answers": [{"answer_id": 919, "document_id": 575, "question_id": 924, "text": "git clone https://github.com/mwaskom/seaborn.git\ncd seaborn\npython setup.py install", "answer_start": 4180, "answer_category": null}], "is_impossible": false}], "context": "Long story short my work computer has network constraints which means trying to use pip install in cmd just leads to timing out/not finding package errors.\n\nFor example; when I try to pip install seaborn: enter image description here\n\nInstead I have tried to download the tar.gz file of the packages I want, however, I do not know how to install them. I've extracted the files from the tar.gz file and there is a \"setup\" file within but it's not doing much for me.\n\nIf someone could explain how to install python packages in this manner without using pip install on windows that would be amazing.\n\npython\ninstallation\npackages\nShare\nImprove this question\nFollow\nasked Mar 15 '16 at 14:37\n\nyenoolnairb\n1,08311 gold badge88 silver badges1111 bronze badges\nAdd a comment\n7 Answers\n\n148\n\nYou may use pip for that without using the network. See in the docs (search for \"Install a particular source archive file\"). Any of those should work:\n\npip install relative_path_to_seaborn.tar.gz    \npip install absolute_path_to_seaborn.tar.gz    \npip install file:///absolute_path_to_seaborn.tar.gz    \nOr you may uncompress the archive and use setup.py directly with either pip or python:\n\ncd directory_containing_tar.gz\ntar -xvzf seaborn-0.10.1.tar.gz\npip install seaborn-0.10.1\npython setup.py install\nOf course, you should also download required packages and install them the same way before you proceed.\n\nShare\nImprove this answer\nFollow\nedited May 14 '20 at 10:43\nanswered Mar 15 '16 at 14:44\n\nJ\u00e9r\u00f4me\n10.1k33 gold badges4444 silver badges7979 bronze badges\nThis gives the following error: You must give at least one requirement to install (maybe you meant \"pip install file:///absolute path..\"?) \u2013 \nyenoolnairb\n Mar 15 '16 at 14:54\nand I have actually entered the path in case you're wondering! \u2013 \nyenoolnairb\n Mar 15 '16 at 14:55\nIf you can download on another machine and bring them over, it's possible to download each dependency you need and install them in this manner in such an order that the desired package has all dependencies met and does not need a network connection anymore. I.e., A depends on B. So install B, then A. \u2013 \nHawkins\n Mar 26 '18 at 17:19 \nAdd a comment\n\n28\n\nYou can install a tarball without extracting it first. Just navigate to the directory containing your .tar.gz file from your command prompt and enter this command:\n\npip install my-tarball-file-name.tar.gz\nI am running python 3.4.3 and this works for me. I can't tell if this would work on other versions of python though.\n\nShare\nImprove this answer\nFollow\nedited Feb 28 '19 at 0:43\nanswered May 23 '16 at 14:01\n\nS\u043d\u0430\u0111\u043e\u0448\u0192\u0430\u04fd\n14.4k1212 gold badges6969 silver badges8686 bronze badges\n3\nRunning this on python 3.4.3 and pip 9.0.1 I get an error relating to a temp file: [Errno 2] No such file or directory: '/tmp/pip-anjip21-build/setup.py running on Jessie (raspberry pi 3) \u2013 \nMagic_Matt_Man\n Feb 17 '17 at 11:09 \n@Magic See the examples in official docs. So it should work. From the error message you showed it seems the tarball you are trying to install isn't following proper file naming conventions. \u2013 \nS\u043d\u0430\u0111\u043e\u0448\u0192\u0430\u04fd\n Feb 28 '19 at 0:54 \n1\npip install *.tar.gz will install all the packages in the directory... just trying to help:) \u2013 \nLev\n Aug 7 '19 at 15:57\nAdd a comment\n\n9\n\nThanks to the answers below combined I've got it working.\n\nFirst needed to unpack the tar.gz file into a folder.\nThen before running python setup.py install had to point cmd towards the correct folder. I did this by pushd C:\\Users\\absolutefilepathtotarunpackedfolder\nThen run python setup.py install\nThanks Tales Padua & Hugo Honorem\n\nShare\nImprove this answer\nFollow\nanswered Mar 15 '16 at 15:15\n\nyenoolnairb\n1,08311 gold badge88 silver badges1111 bronze badges\nCan you edit the PATH variables of your machine? If so, you can add python to it, and use python anywhere without need to use pushd \u2013 \nTales P\u00e1dua\n Mar 15 '16 at 16:21\n1\nActually you don't have to extract the tar.gz file to install it. Take a look at my answer \u2013 \nS\u043d\u0430\u0111\u043e\u0448\u0192\u0430\u04fd\n May 23 '16 at 14:04\nAdd a comment\n\n7\n\nInstall it by running\n\npython setup.py install\nBetter yet, you can download from github. Install git via apt-get install git and then follow this steps:\n\ngit clone https://github.com/mwaskom/seaborn.git\ncd seaborn\npython setup.py install\nShare\nImprove this answer\nFollow\nedited Mar 15 '16 at 15:04\nanswered Mar 15 '16 at 14:48\n\nTales P\u00e1dua\n1,19111 gold badge1313 silver badges3535 bronze badges\nNo such file or directory This could be to do with the setup of my files and folders. Python isn't gonna be in the default place Python would normally install \u2013 \nyenoolnairb\n Mar 15 '16 at 14:56 \nunpack from tar, go to the folder where the setup.py is, and then run the command above \u2013 \nTales P\u00e1dua\n Mar 15 '16 at 14:58\nDo I need to amend the code above to point it directly to where the unpacked tar is? Cos I'm still getting no such file or directory \u2013 \nyenoolnairb\n Mar 15 '16 at 14:59 \nUnpack the tar.gz to somewhere. Then, from the terminal, go to the folder where you extracted it. You need to be on the folder where setup.py is, and then run this command \u2013 \nTales P\u00e1dua\n Mar 15 '16 at 15:01\nUnfortunatley github is blocked on my work computer!! I've got it working now though; thanks \u2013 \nyenoolnairb\n Mar 15 '16 at 15:15\nShow 1 more comment\n\n5\n\nIf you don't wanted to use PIP install atall, then you could do the following:\n\n1) Download the package 2) Use 7 zip for unzipping tar files. ( Use 7 zip again until you see a folder by the name of the package you are looking for. Ex: wordcloud)\n\nFolder by Name 'wordcloud'\n\n3) Locate Python library folder where python is installed and paste the 'WordCloud' folder itself there\n\nPython Library folder\n\n4) Success !! Now you can import the library and start using the package.\n\nenter image description here\n\nShare\nImprove this answer\nFollow\nanswered Jan 3 '18 at 22:02\n\nSVK\n8861010 silver badges2222 bronze badges\n1\nThis works perfect for situations where using pip generates error(10054, 'An existing connection was forcibly closed by the remote host') \u2013 \nadin\n Apr 30 '19 at 19:10\n1\nI was getting a whole host of different errors using the other solutions -- this was the only solution that worked for me \u2013 \nBill Wallis\n Apr 20 at 8:23 \nAdd a comment\n\n5\n\nFor those of you using python3 you can use:\n\npython3 setup.py install\nShare\nImprove this answer\nFollow\nedited Apr 25 '19 at 17:54\n\nmx0\n5,0151010 gold badges4242 silver badges4949 bronze badges\nanswered May 17 '17 at 15:50\n\nShersha Fn\n1,26522 gold badges2121 silver badges3333 bronze badges\nAdd a comment\n\n0\n\nIs it possible for you to use sudo apt-get install python-seaborn instead? Basically tar.gz is just a zip file containing a setup, so what you want to do is to unzip it, cd to the place where it is downloaded and use gunzip -c seaborn-0.7.0.tar.gz | tar xf - for linux. Change dictionary into the new seaborn unzipped file and execute python setup.py install", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Create an application setup in visual studio 2013", "id": 677, "answers": [{"answer_id": 682, "document_id": 370, "question_id": 677, "text": "You can now create installers in VS2013, download the extension here from the visualstudiogallery.", "answer_start": 616, "answer_category": null}], "is_impossible": false}], "context": "I already have a project which is ready to build. Currently, I am using visual studio 2013.\nBut, I don't know how to create an MSI setup in visual studio 2013, but for visual studio 2010 there are plenty of tutorials out there discussing how to create a setup in visual studio 2010.\n\u2022\tDoes this mean I need to install visual studio 2010 in order to create an application setup for my project?\n\u2022\tWhat is the easiest way to create an application setup in visual studio 2013?\nMicrosoft has listened to the cry for supporting installers (MSI) in Visual Studio and release the Visual Studio Installer Projects Extension. You can now create installers in VS2013, download the extension here from the visualstudiogallery.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to configure SSIS 2012 project to run under different environment configurations?", "id": 527, "answers": [{"answer_id": 529, "document_id": 252, "question_id": 527, "text": "SSIS 2012 project deployment model provides greater flexibility to create Environments and configure environment specific values, which can mapped to project parameters. Here is a sample that illustrates how you can execute a package deployed to Integration Services Catalog against multiple environments. Ideally, production environment should be on its own server. This example uses all the environments on the same server for simplicity.", "answer_start": 182, "answer_category": null}], "is_impossible": false}], "context": "What would be the best logical way of configuring 2012 SSIS project using the Project Deployment Model?\nConsider a scenario of an SSIS Project MyImport-Project having three packages\nSSIS 2012 project deployment model provides greater flexibility to create Environments and configure environment specific values, which can mapped to project parameters. Here is a sample that illustrates how you can execute a package deployed to Integration Services Catalog against multiple environments. Ideally, production environment should be on its own server. This example uses all the environments on the same server for simplicity.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "npm wont install packages locally whats wrong", "id": 1921, "answers": [{"answer_id": 1908, "document_id": 1494, "question_id": 1921, "text": "you can fix it by either deleting that directory or manually creating a node_modules dir in your project folder", "answer_start": 1712, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI want to install packages locally, but npm is always installing packages to the global location. I'm running the following command:\n\nnpm install serialport\n\n\nI do not have a .npmrc command and I'm not using the -g flag, so I don't know why it's not installing locally. Here's a snippet from the config dump showing that global is false:\n    $ npm config ls -l | grep global\n    global = false\n    globalconfig = \"/usr/local/etc/npmrc\"\n    globalignorefile = \"/usr/local/etc/npmignore\"\n\nAnd the packages are still being installed like this\n\nserialport@0.7.3 ../../../../node_modules/serialport\n\n\nSo unless I am totally wrong about what \"local\" means, this seems wrong. I was under the impression that \"local\" meant in the current working directory so that I could do a \"require\" in my main code file. See: http://blog.nodejs.org/2011/03/23/npm-1-0-global-vs-local-installation/ as referenced in a previous npm related question.\n\nCan someone please give me some hints on this? Thank you very much.\n\nP.S. It's not specific to the serialport module. It's happening with all of them.\n    \n\nMost of my answer can be found: http://npmjs.org/doc/folders.html#More-Information\n\nWhat I understand is that npm will try to install it in a sensible location. So if you have a project/node_modules directory and you are in /project and do npm install it will use product/node_modules.\n\nNow if you accidentally did a cd project/css and do npm install then npm will traverse up until it finds your node_modules directory. This is to prevent you from accidentally installing it in your project/css.\n\nSo in your case you have a node_module directory somewhere in the path of your project. So my guess is that you can fix it by either deleting that directory or manually creating a node_modules dir in your project folder.\n    \n\nIf you have a package.json file on the folder you are trying to install the package, then it will create the node_modules folder correctly.\n\nBasic package.json\n\n{\n    \"name\": \"application-name\",\n    \"version\": \"0.0.1\"\n}\n\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Error installing Angular using npm due to require-from-string", "id": 1739, "answers": [{"answer_id": 1726, "document_id": 1311, "question_id": 1739, "text": "some problem with NPM registry some of the packages got deleted. They are restoring it... it will be available shortly", "answer_start": 397, "answer_category": null}], "is_impossible": false}], "context": "How to resolve this error as I am not able to install Angular.\nPlease see the below exception:\nC:\\Users\\absin\\node>npm install -g @angular/cli\nnpm ERR! code ETARGET\nnpm ERR! notarget No matching version found for require-from-string@^1.1.0\nnpm ERR! notarget In most cases you or one of your dependencies are requesting\nnpm ERR! notarget a package version that doesn't exist.\nSo how to resolve it?\nsome problem with NPM registry some of the packages got deleted. They are restoring it... it will be available shortly\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to make an MSI that simply wraps an EXE file", "id": 1210, "answers": [{"answer_id": 1203, "document_id": 786, "question_id": 1210, "text": "I think the easiest way to create a .MSI file is to use WiX.", "answer_start": 758, "answer_category": null}], "is_impossible": false}], "context": "After way too many experiments, I've come to the conclusion that Windows Installer is simply bad technology. But the customers want MSI files.\nSo, how can I create an MSI file that extracts an EXE file to a temporary directory and runs it with options same or similar as were passed to the EXE file?\nOptions to an MSI are explained in Msiexec (command-line options) (low level \"run\" of an MSI is msiexec option package.msi).\nEDIT: mjmarsh's WiX solution looks like it works. I just haven't had a chance to try it yet (crunch time). If it works, I'll be accepting it.\nEDIT: it does not work. Missing piece: attended/unattended does not seem to be available.\nAnyway, the only to make this work at all would be for the custom action to kill its parent process!\nI think the easiest way to create a .MSI file is to use WiX.\nJoshua, I understand your frustration very well. MSI is quirky to say the least - a completely new way to think of deployment. Still, applied correctly MSI offers the best possible deployment, especially for corporate customers.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "problem installing delphi 2007 on windows 7 64 bit enterprise", "id": 2007, "answers": [{"answer_id": 1993, "document_id": 1598, "question_id": 2007, "text": "Save the installer (i.e. setup.exe) to an empty directory\n  before running it. For example:\n  c:\\temp\\delphi2007.", "answer_start": 1651, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI have an issue installing Delphi 2007 RAD Studio Professional onto Windows 7 64 Bit Enterprise.\n\nEverything is fine until I enter the serial number for the installation. After I enter it and click the next button in the installation wizard I get an installation script error at line 906. OKing that error closes the installer.\n\nI have done some Googling but have not found anything similar yet (though I have found a lot of references to people who have installed it on various versions of 64 bit windows and who have had other issues).\n\nI have been able to install Delphi 2009 on the same machine with no problems.\n\nI'd really like to get 2007 installed as this is currently our primary Delphi development IDE.\n    \n\nI had problems installing Delphi 2007 from my original DVD from CodeGear.  However, after logging into the CodeGear/Embarcadero website, I was able to download the latest DVD ISO (with all updates already applied) for Delphi 2007 and that resolved all of my installation issues.\n\nI'd suggest trying that. Also, you will need to uninstall your broken installation if possible. Try using Revo Uninstaller (free) to help with getting rid of all the extra stuff left around from a broken installation.\n    \n\nYou mention you previously installed Delphi 2009 on the same machine. Could it be you are running into this issue? I quote:\n\n\n  Symptom: Attempting to install Delphi\n  2007 or C++Builder 2007 results in\n  Install Script Error 1580.\n  \n  Cause: This problem occurs when the installer is run from a directory\n  that contains a slip file for a\n  different product than the one being\n  installed.\n  \n  Solution: Save the installer (i.e. setup.exe) to an empty directory\n  before running it. For example:\n  c:\\temp\\delphi2007.\n\n    \n\nIf you're trying to install into the default folder (%PROGRAMFILES%\\CodeGear\\RAD Studio\\5.0), you could be having trouble with permissions. Are you running the installer as an Administrator?\n\nYou should try re-running the installation as Administrator, and install to somewhere other than %PROGRAMFILES%. I have D2007, D2009, and D2010 all running on Win7 64 Home Premium in a folder called E:\\Delphi, to which I can safely give read/write access to everyone. (Later versions of Delphi are sensitive to access rights and UAC under Vista and Win7, but D7 and 2007 not so much; both of those try to write information to the ($BDS) folders, which is verboten under Vista and Win7).\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Fabric + buildout as opposed to Fabric + pip + virtualenv", "id": 1287, "answers": [{"answer_id": 1279, "document_id": 858, "question_id": 1287, "text": "Summary: Pip only installs python packages and there's more you need to do, obviously. You can do most of the extra work in buildout with the advantage that buildout does it for you both locally and on the server.", "answer_start": 513, "answer_category": null}], "is_impossible": false}], "context": "I've recently started playing around with Mezzanine, a django-based CMS. I recently just managed to configure Fabric to get it uploading to my host, webfaction.com, as its a bit more involved automatically creating the website on the shared hosting, and I wanted to automate that process.\nAltogether, that system uses Fabric for template uploads of config files, and pip + virtualenv for handling the python packages.\nHowever, I just recently read about buildout, and how some people swear by it for deployment,.\nSummary: Pip only installs python packages and there's more you need to do, obviously. You can do most of the extra work in buildout with the advantage that buildout does it for you both locally and on the server.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to install / start neo4j on mac?", "id": 955, "answers": [{"answer_id": 950, "document_id": 589, "question_id": 955, "text": " brew install neo4j", "answer_start": 1012, "answer_category": null}], "is_impossible": false}, {"question": "How to install / start neo4j on docker?", "id": 956, "answers": [{"answer_id": 951, "document_id": 589, "question_id": 956, "text": "docker run \\\n       --publish=7474:7474 \\\n       --publish=7687:7687 \\\n       --name neo4j \\\n       --volume=$HOME/neo4j/data:/data \\\n       neo4j:3.3", "answer_start": 1310, "answer_category": null}], "is_impossible": false}], "context": "\n\nJ\u00e1nos\n28.2k2525 gold badges136136 silver badges281281 bronze badges\n\"Server with PID 311 was java\". What do you mean exactly by \"java\"? \u2013 \nfbiville\n Jun 22 '14 at 9:28\n1\nI opened Monitor and I scrolled down to PID 311, and a process called java, without any other info was there. \u2013 \nJ\u00e1nos\n Jun 22 '14 at 9:30\nHmmm ok. So as I answered, my guess is this is totally specific to your setup, not something you usually have to do. \u2013 \nfbiville\n Jun 22 '14 at 9:31\n1\nWell, the installation is actually pretty straightforward. You just happened to have a conflicting executing process, that is the specific part of your setup. Unfortunately, we could not get more information about this process. It really usually does not happen. \u2013 \nfbiville\n Jun 22 '14 at 9:44 \n1\nThis happens to me too. Each time there\u2019s a problem I have to quit the Java process and restart the server. I haven\u2019t changed any preferences and am using a mac \u2013 \nAdam Carter\n Dec 17 '14 at 16:52\nShow 1 more comment\n3 Answers\n\n16\n\nIt is easier to use: brew install neo4j\n\nShare\nImprove this answer\nFollow\nedited Jul 20 '15 at 21:14\n\njcoppens\n5,05766 gold badges2525 silver badges4040 bronze badges\nanswered Jul 20 '15 at 21:07\n\nPablo Araya Romero\n16111 silver badge33 bronze badges\nAdd a comment\n\n2\n\nNowadays it can also be installed using Docker:\n\ndocker run \\\n       --publish=7474:7474 \\\n       --publish=7687:7687 \\\n       --name neo4j \\\n       --volume=$HOME/neo4j/data:/data \\\n       neo4j:3.3\nNote the 3.3 at the end of the command. You can change it to run another version of Neo4j. Take a look here for the list of available versions: https://hub.docker.com/_/neo4j?tab=tags\n\nShare\nImprove this answer\nFollow\nedited Jun 19 '19 at 14:37\nanswered Dec 31 '18 at 14:33\n\nMohammad Banisaeid\n1,8982323 silver badges3434 bronze badges\nAdd a comment\n\n0\n\nIt seems to me the issue you faced was just due to your particular local setup.\n\nShare\nImprove this answer\nFollow\nanswered Jun 22 '14 at 9:27\n\nfbiville\n7,19066 gold badges4949 silver badges7474 bronze badges\nYep, duplicate conflicting install. Either binary download or homebrew \u2013 \nMichael Hunger\n Jun 22 '14 at 11:54", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "MSBuildExtensionsPath32 not set correctly?", "id": 549, "answers": [{"answer_id": 551, "document_id": 274, "question_id": 549, "text": "MSBuildExtensionsPath32 is set internally by MSBuild. (BuildEngine.BuildPropertyGroup.SetExtensionsPathProperties)\nBut you could override it by setting an environment variable.\nSET MSBuildExtensionsPath=\"C:\\Program Files\\MSBuild\"", "answer_start": 185, "answer_category": null}], "is_impossible": false}], "context": "For the life of me, I cannot find where this value is actually set. It SHOULD be pointing at C:\\Program Files\\MSBuild, but on our build box, it's pointing at C:. How can I change this?\nMSBuildExtensionsPath32 is set internally by MSBuild. (BuildEngine.BuildPropertyGroup.SetExtensionsPathProperties)\nBut you could override it by setting an environment variable.\nSET MSBuildExtensionsPath=\"C:\\Program Files\\MSBuild\"\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Salesforce - How to Deploy between Environments (Sandboxes, Live etc)", "id": 573, "answers": [{"answer_id": 579, "document_id": 298, "question_id": 573, "text": "You should use the Force.com Migration Tool.", "answer_start": 439, "answer_category": null}], "is_impossible": false}], "context": "We're looking into setting up a proper deployment process.\nFrom what I've read there seems to be 4 methods of doing this.\nCopy & Paste -- We don't want to do this\nUsing the \"Package\" mechanism built into the Salesforce Web Interface\nEclipse Force IDE \"Deploy to Server\" option\nAnt Script (haven't tried this one yet)\nDoes anyone have advice on the limitation of the various methods .\nCan you include everything in a Web Interface package?\nYou should use the Force.com Migration Tool.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Glassfish DeploymentException: Error in linking security policy for", "id": 1709, "answers": [{"answer_id": 1697, "document_id": 1282, "question_id": 1709, "text": "\u2022\tStoped the Glassfish server\n\u2022\tDeleted all the content from glassfishhome/glassfish/domains/yourdomainname/generated\n\u2022\tStarted Glassfish", "answer_start": 792, "answer_category": null}], "is_impossible": false}], "context": "I have been trying to deploy my web application (war) from Glassfish AdminConsole but I keep getting the following error message -\nException while loading the app : Error in linking security policy for MyApp-war -- Inconsistent Module State.\nBut it deploys without any problem when I do it from Netbeans. (I don't know if Netbeans is doing something that I am missing before deploying the application.)\nAnd I also tried with the latest version of Glassfish (i.e. V3.1.1 (build 12) ), and I can deploy the same application without any problem from AdminConsole.\nI am using Glassfish 3.1 (build 43) and Netbeans 7.0.\nIs there any Security Policy setting that I have to have before deploying my application with this version of Glassfish?\nThe same thing was happening to me.\nHere is what I did:\n\u2022\tStoped the Glassfish server\n\u2022\tDeleted all the content from glassfishhome/glassfish/domains/yourdomainname/generated\n\u2022\tStarted Glassfish\nIt worked for me. But what still sucks is that everytime I need to deploy I need to follow this procedure again...\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Python deployment with virtualenv", "id": 29, "answers": [{"answer_id": 31, "document_id": 45, "question_id": 29, "text": "venv is the standard tool for creating virtual environments, and has\nbeen part of Python since Python 3.3. Starting with Python 3.4, it\ndefaults to installing pip into all created virtual environments.\nvirtualenv is a third party alternative (and predecessor) to\nvenv.", "answer_start": 2009, "answer_category": null}], "is_impossible": false}, {"question": "Tensorflow installation error: not a supported wheel on this platform", "id": 115, "answers": [{"answer_id": 123, "document_id": 45, "question_id": 115, "text": "The standard packaging tools are all designed to be used from the command\nline.\nThe following command will install the latest version of a module and its\ndependencies from the Python Package Index:\npython -m pip install SomePackage\n", "answer_start": 3536, "answer_category": null}], "is_impossible": false}, {"question": "How to install pip?", "id": 134, "answers": [{"answer_id": 142, "document_id": 45, "question_id": 134, "text": "It is possible that pip does not get installed by default. One potential fix is:\npython -m ensurepip --default-pip", "answer_start": 7317, "answer_category": null}], "is_impossible": false}], "context": "\n\n\n\n\n\nInstalling Python Modules\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable of Contents\n\nInstalling Python Modules\nKey terms\nBasic usage\nHow do I \u2026?\n\u2026 install pip in versions of Python prior to Python 3.4?\n\u2026 install packages just for the current user?\n\u2026 install scientific Python packages?\n\u2026 work with multiple versions of Python installed in parallel?\n\n\nCommon installation issues\nInstalling into the system Python on Linux\nPip not installed\nInstalling binary extensions\n\n\n\n\n\nPrevious topic\nDistributing Python Modules\nNext topic\nPython HOWTOs\n\nThis Page\n\nReport a Bug\n\nShow Source\n\n\n\n\n\n\n\n\nNavigation\n\n\nindex\n\nmodules |\n\nnext |\n\nprevious |\n\nPython \u00bb\n\n\n\n\n\n\n\n3.10.0 Documentation \u00bb\n\nInstalling Python Modules\n\n\n\n\n\n\n\n\n\n|\n\n\n\n\n\n\n\n\nInstalling Python Modules\u00b6\n\nEmail\ndistutils-sig@python.org\n\n\nAs a popular open source development project, Python has an active\nsupporting community of contributors and users that also make their software\navailable for other Python developers to use under open source license terms.\nThis allows Python users to share and collaborate effectively, benefiting\nfrom the solutions others have already created to common (and sometimes\neven rare!) problems, as well as potentially contributing their own\nsolutions to the common pool.\nThis guide covers the installation part of the process. For a guide to\ncreating and sharing your own Python projects, refer to the\ndistribution guide.\n\nNote\nFor corporate and other institutional users, be aware that many\norganisations have their own policies around using and contributing to\nopen source software. Please take such policies into account when making\nuse of the distribution and installation tools provided with Python.\n\n\nKey terms\u00b6\n\npip is the preferred installer program. Starting with Python 3.4, it\nis included by default with the Python binary installers.\nA virtual environment is a semi-isolated Python environment that allows\npackages to be installed for use by a particular application, rather than\nbeing installed system wide.\nvenv is the standard tool for creating virtual environments, and has\nbeen part of Python since Python 3.3. Starting with Python 3.4, it\ndefaults to installing pip into all created virtual environments.\nvirtualenv is a third party alternative (and predecessor) to\nvenv. It allows virtual environments to be used on versions of\nPython prior to 3.4, which either don\u2019t provide venv at all, or\naren\u2019t able to automatically install pip into created environments.\nThe Python Package Index is a public\nrepository of open source licensed packages made available for use by\nother Python users.\nthe Python Packaging Authority is the group of\ndevelopers and documentation authors responsible for the maintenance and\nevolution of the standard packaging tools and the associated metadata and\nfile format standards. They maintain a variety of tools, documentation,\nand issue trackers on both GitHub and\nBitbucket.\ndistutils is the original build and distribution system first added to\nthe Python standard library in 1998. While direct use of distutils is\nbeing phased out, it still laid the foundation for the current packaging\nand distribution infrastructure, and it not only remains part of the\nstandard library, but its name lives on in other ways (such as the name\nof the mailing list used to coordinate Python packaging standards\ndevelopment).\n\n\nChanged in version 3.5: The use of venv is now recommended for creating virtual environments.\n\n\nSee also\nPython Packaging User Guide: Creating and using virtual environments\n\n\n\nBasic usage\u00b6\nThe standard packaging tools are all designed to be used from the command\nline.\nThe following command will install the latest version of a module and its\ndependencies from the Python Package Index:\npython -m pip install SomePackage\n\n\n\nNote\nFor POSIX users (including macOS and Linux users), the examples in\nthis guide assume the use of a virtual environment.\nFor Windows users, the examples in this guide assume that the option to\nadjust the system PATH environment variable was selected when installing\nPython.\n\nIt\u2019s also possible to specify an exact or minimum version directly on the\ncommand line. When using comparator operators such as >, < or some other\nspecial character which get interpreted by shell, the package name and the\nversion should be enclosed within double quotes:\npython -m pip install SomePackage==1.0.4    # specific version\npython -m pip install \"SomePackage>=1.0.4\"  # minimum version\n\n\nNormally, if a suitable module is already installed, attempting to install\nit again will have no effect. Upgrading existing modules must be requested\nexplicitly:\npython -m pip install --upgrade SomePackage\n\n\nMore information and resources regarding pip and its capabilities can be\nfound in the Python Packaging User Guide.\nCreation of virtual environments is done through the venv module.\nInstalling packages into an active virtual environment uses the commands shown\nabove.\n\nSee also\nPython Packaging User Guide: Installing Python Distribution Packages\n\n\n\nHow do I \u2026?\u00b6\nThese are quick answers or links for some common tasks.\n\n\u2026 install pip in versions of Python prior to Python 3.4?\u00b6\nPython only started bundling pip with Python 3.4. For earlier versions,\npip needs to be \u201cbootstrapped\u201d as described in the Python Packaging\nUser Guide.\n\nSee also\nPython Packaging User Guide: Requirements for Installing Packages\n\n\n\n\u2026 install packages just for the current user?\u00b6\nPassing the --user option to python -m pip install will install a\npackage just for the current user, rather than for all users of the system.\n\n\n\u2026 install scientific Python packages?\u00b6\nA number of scientific Python packages have complex binary dependencies, and\naren\u2019t currently easy to install using pip directly. At this point in\ntime, it will often be easier for users to install these packages by\nother means\nrather than attempting to install them with pip.\n\nSee also\nPython Packaging User Guide: Installing Scientific Packages\n\n\n\n\u2026 work with multiple versions of Python installed in parallel?\u00b6\nOn Linux, macOS, and other POSIX systems, use the versioned Python commands\nin combination with the -m switch to run the appropriate copy of\npip:\npython2   -m pip install SomePackage  # default Python 2\npython2.7 -m pip install SomePackage  # specifically Python 2.7\npython3   -m pip install SomePackage  # default Python 3\npython3.4 -m pip install SomePackage  # specifically Python 3.4\n\n\nAppropriately versioned pip commands may also be available.\nOn Windows, use the py Python launcher in combination with the -m\nswitch:\npy -2   -m pip install SomePackage  # default Python 2\npy -2.7 -m pip install SomePackage  # specifically Python 2.7\npy -3   -m pip install SomePackage  # default Python 3\npy -3.4 -m pip install SomePackage  # specifically Python 3.4\n\n\n\n\n\nCommon installation issues\u00b6\n\nInstalling into the system Python on Linux\u00b6\nOn Linux systems, a Python installation will typically be included as part\nof the distribution. Installing into this Python installation requires\nroot access to the system, and may interfere with the operation of the\nsystem package manager and other components of the system if a component\nis unexpectedly upgraded using pip.\nOn such systems, it is often better to use a virtual environment or a\nper-user installation when installing packages with pip.\n\n\nPip not installed\u00b6\nIt is possible that pip does not get installed by default. One potential fix is:\npython -m ensurepip --default-pip\n\n\nThere are also additional resources for installing pip.\n\n\nInstalling binary extensions\u00b6\nPython has typically relied heavily on source based distribution, with end\nusers being expected to compile extension modules from source as part of\nthe installation process.\nWith the introduction of support for the binary wheel format, and the\nability to publish wheels for at least Windows and macOS through the\nPython Package Index, this problem is expected to diminish over time,\nas users are more regularly able to install pre-built extensions rather\nthan needing to build them themselves.\nSome of the solutions for installing scientific software\nthat are not yet available as pre-built wheel files may also help with\nobtaining other binary extensions without needing to build them locally.\n\nSee also\nPython Packaging User Guide: Binary Extensions\n\n\n\n\n\n\n\n\n\n\nTable of Contents\n\nInstalling Python Modules\nKey terms\nBasic usage\nHow do I \u2026?\n\u2026 install pip in versions of Python prior to Python 3.4?\n\u2026 install packages just for the current user?\n\u2026 install scientific Python packages?\n\u2026 work with multiple versions of Python installed in parallel?\n\n\nCommon installation issues\nInstalling into the system Python on Linux\nPip not installed\nInstalling binary extensions\n\n\n\n\n\nPrevious topic\nDistributing Python Modules\nNext topic\nPython HOWTOs\n\nThis Page\n\nReport a Bug\n\nShow Source\n\n\n\n\n\n\n\n\n\nNavigation\n\n\nindex\n\nmodules |\n\nnext |\n\nprevious |\n\nPython \u00bb\n\n\n\n\n\n\n\n3.10.0 Documentation \u00bb\n\nInstalling Python Modules\n\n\n\n\n\n\n\n\n\n|\n\n\n\n\n\u00a9 Copyright 2001-2021, Python Software Foundation.\n\nThis page is licensed under the Python Software Foundation License Version 2.\n\nExamples, recipes, and other code in the documentation are additionally licensed under the Zero Clause BSD License.\n\nSee History and License for more information.\n\n\nThe Python Software Foundation is a non-profit corporation.\nPlease donate.\n\n\n\nLast updated on Oct 07, 2021.\nFound a bug?\n\n\nCreated using Sphinx 3.2.1.\n\n\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Error during Xcode Component Installation", "id": 701, "answers": [{"answer_id": 705, "document_id": 392, "question_id": 701, "text": "$ cd /usr/local/lib/node_modules\n\ncurl -L https://www.npmjs.com/install.sh | sh", "answer_start": 427, "answer_category": null}], "is_impossible": false}], "context": "i tried to install grunt on a mac with Yosemite. node is already installed in the newest version. if i type \"node -v\" in the terminal i get the line v0.12.5. thats good. but when i want to install something with npm i get only a error...\ni tried \"sudo npm install -g grunt-cli\", \"sudo npm install npm -g\" and also with \"npm -v\" i get always this error...\nGo to the global node_modules directory (npm root -g if you don't know)\n$ cd /usr/local/lib/node_modules\n\ncurl -L https://www.npmjs.com/install.sh | sh\nI have been trying feverishly to get to the latest version of node and npm on my centos 7 machine.\nUnfortunately, I have been plagued with this error for near a week now. And I have finally found a solution that works.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "how to generate an installer package for mac app", "id": 1413, "answers": [{"answer_id": 1402, "document_id": 989, "question_id": 1413, "text": "macOS does not normally use installers. Applications are packaged in app containers with the extension .app. This container is \"executable\" but you're also able to dig in and see what is inside. This is also the format distributed through App Store.\n\nYou can create .pkg or .dmg \"installers\" if necessary, however this is clearly not something apple aims to be standard. I would advise to use the .app pattern and any scripts needed should be self contained and executed on first run.\n\nYou can use .dmg to distribute your application outside of App Store (this is still fairly normal).\n", "answer_start": 1406, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nHow can I create a single installer package for an OS X binary as well as a few configuration and script files?\nFinal folders should look like this:\n\nAny help would be appreciated. Thanks.\n    \n\nInstallers are great if you want various things to be placed in different spots \u2013 app here, documentation there, support files over here, etc.  They're also great for providing configurability of the installation experience (optional extras), or hand-holding for an unusual type of installation that the user might not otherwise understand, or extra work (configuration scripts, permissions modifications, authentication, compatibility checking, etc.) that need to run during the installation process.  There is nothing wrong with installers, contrary to the answer from @d00dle, although there is also nothing wrong with distributing your app through the App Store, or as a dmg.\n\nFor setting up your own installers, I highly recommend a program called Packages (http://s.sudre.free.fr/Software/Packages/about.html).  I am in no way connected to it, but I use it to build the installer for an app that I work on.  It greatly smoothes the process of making a complex installer, and has an excellent GUI interface.\n    \n\nThere's also macOS Installer Builder, which is a CLI you can use to create an installer wizard for your .pkg: https://github.com/KosalaHerath/macos-installer-builder\n    \n\nmacOS does not normally use installers. Applications are packaged in app containers with the extension .app. This container is \"executable\" but you're also able to dig in and see what is inside. This is also the format distributed through App Store.\n\nYou can create .pkg or .dmg \"installers\" if necessary, however this is clearly not something apple aims to be standard. I would advise to use the .app pattern and any scripts needed should be self contained and executed on first run.\n\nYou can use .dmg to distribute your application outside of App Store (this is still fairly normal).\n\nmacOS also includes a terminal program called productbuild that builds a product archive for the macOS Installer or the Mac App Store. Enter man productbuild into the Terminal on a Mac for the manual page.\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Override env values defined in container spec", "id": 558, "answers": [{"answer_id": 560, "document_id": 283, "question_id": 558, "text": "Above clearly states the env will take precedence than envFrom. So, for overriding, don't use envFrom, but define the value twice within env.", "answer_start": 197, "answer_category": null}], "is_impossible": false}], "context": "I have a configmap where I have defined the following key-value mapping in the data section.\nthen in the definition of my container (in the deployment / statefulset manifest) I have the following.\nAbove clearly states the env will take precedence than envFrom. So, for overriding, don't use envFrom, but define the value twice within env.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Android App Installation Failed: Package com.my.app has no certificates at entry AndroidManifest.xml", "id": 1163, "answers": [{"answer_id": 1156, "document_id": 740, "question_id": 1163, "text": "Try to signed your APK with version 1. Version 2 signed APK only compatible from 7.0 so it will not working on below 7.0 devices or try to generate v1 and v2 version APK", "answer_start": 310, "answer_category": null}], "is_impossible": false}], "context": "in Android Studio I generated a signed APK with keystore and so on. While installing the APK onto a device it fails with 'App could not be installed' and in Android Monitor I saw the following line:\nPackage com.my.app has no certificates at entry AndroidManifest.xml; ignoring!\nDoes anybody know what's wrong?\nTry to signed your APK with version 1. Version 2 signed APK only compatible from 7.0 so it will not working on below 7.0 devices or try to generate v1 and v2 version APK.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Can't run Curl command inside my Docker Container", "id": 866, "answers": [{"answer_id": 861, "document_id": 546, "question_id": 866, "text": "apt-get update; apt-get install curl", "answer_start": 1223, "answer_category": null}], "is_impossible": false}], "context": "I created a docker container from my OS X VM Docker host. I created it using the run command and created the container based off the ubuntu:xenial image off docker hub.\n\nI'm now connected to my container after it's created and logged in as root and at the command prompt inside my container.\n\nI tried to install homebrew and for some reason, I can't run the command to install Homebrew:\n\nruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\"\nwhen I run that I get a bash:\n\ncurl: command not found\n\nNot sure why I'm not able to use curl here inside my container.\n\ndocker\ninstallation\nShare\nImprove this question\nFollow\nedited May 17 '20 at 21:45\n\nMukesh M\n1,89266 gold badges2525 silver badges3535 bronze badges\nasked Jan 2 '16 at 23:13\n\nPositiveGuy\n12.7k1616 gold badges5757 silver badges109109 bronze badges\nWhy not just install curl from zip ? Is curl anywhere on your disk ? \u2013 \nMarged\n Jan 2 '16 at 23:18\nbased on Ubuntu image you can use apt-get install curl -y. RUN apt-get update && apt-get upgrade -y && apt-get install curl -y CMD /bin/bash \u2013 \nali Falahati\n Jul 22 '20 at 13:37\nAdd a comment\n6 Answers\n\n192\n\ncurl: command not found\n\nis a big hint, you have to install it with :\n\napt-get update; apt-get install curl\nShare\nImprove this answer\nFollow\nedited May 21 '18 at 12:46\n\nTundra Fizz\n42033 gold badges1010 silver badges2424 bronze badges\nanswered Jan 2 '16 at 23:22\n\nGilles Quenot\n149k3333 gold badges205205 silver badges200200 bronze badges\n4\nHi, why would I want to use sudo? The container does not know sudo anyway..Nirojan's answer works \u2013 \nJan Sila\n Oct 30 '17 at 9:57\n3\nI think you have to remove the sudo for this to work in docker \u2013 \njorfus\n Dec 5 '17 at 19:27\nu've assumed that it will be a debian image. right answer in this case , but what about smaller unix images, any help ? \u2013 \nsaurshaz\n Mar 1 '18 at 12:38 \nI dunno what you mean by smaller image. If it's debian based, it should just works \u2013 \nGilles Quenot\n Mar 1 '18 at 13:22\nNeeded this one. \u2013 \nMuhammad Haseeb\n Aug 27 '20 at 6:24\nShow 1 more comment\n\n\n54\n\nRan into this same issue while using the CURL command inside my Dockerfile. As Gilles pointed out, we have to install curl first. These are the commands to be added in the 'Dockerfile'.\n\nFROM ubuntu:16.04\n\n# Install prerequisites\nRUN apt-get update && apt-get install -y \\\ncurl\nCMD /bin/bash\nShare\nImprove this answer\nFollow\nedited Jul 12 '19 at 7:20\nanswered Mar 6 '17 at 2:13\n\nNirojan Selvanathan\n7,63855 gold badges4747 silver badges6868 bronze badges\nAdd a comment\n\n25\n\nSo I added curl AFTER my docker container was running.\n\n(This was for debugging the container...I did not need a permanent addition)\n\nI ran my image\n\ndocker run -d -p 8899:8080 my-image:latest\n(the above makes my \"app\" available on my machine on port 8899) (not important to this question)\n\nThen I listed and created terminal into the running container.\n\ndocker ps\ndocker exec -it my-container-id-here /bin/sh\nIf the exec command above does not work, check this SOF article:\n\nError: Cannot Start Container: stat /bin/sh: no such file or directory\"\n\nthen I ran:\n\napk\njust to prove it existed in the running container, then i ran:\n\napk add curl\nand got the below:\n\napk add curl\n\nfetch http://dl-cdn.alpinelinux.org/alpine/v3.8/main/x86_64/APKINDEX.tar.gz\n\nfetch http://dl-cdn.alpinelinux.org/alpine/v3.8/community/x86_64/APKINDEX.tar.gz\n\n(1/5) Installing ca-certificates (20171114-r3)\n\n(2/5) Installing nghttp2-libs (1.32.0-r0)\n\n(3/5) Installing libssh2 (1.8.0-r3)\n\n(4/5) Installing libcurl (7.61.1-r1)\n\n(5/5) Installing curl (7.61.1-r1)\n\nExecuting busybox-1.28.4-r2.trigger\n\nExecuting ca-certificates-20171114-r3.trigger\n\nOK: 18 MiB in 35 packages\n\nthen i ran curl:\n\n/ # curl\ncurl: try 'curl --help' or 'curl --manual' for more information\n/ # \nNote, to get \"out\" of the drilled-in-terminal-window, I had to open a new terminal window and stop the running container:\n\ndocker ps\ndocker stop my-container-id-here\nAPPEND:\n\nIf you don't have \"apk\" (which depends on which base image you are using), then try to use \"another\" installer. From other answers here, you can try:\n\napt-get -qq update\n\napt-get -qq -y install curl\nShare\nImprove this answer\nFollow\nedited Nov 18 '20 at 16:51\nanswered Jan 25 '19 at 10:34\n\ngranadaCoder\n22.4k88 gold badges8888 silver badges120120 bronze badges\nThanks for the step by step explanation! \u2013 \nFred\n Jun 11 at 5:58\nAdd a comment\n\n\n15\n\nThis is happening because there is no package cache in the image, you need to run:\n\napt-get -qq update\nbefore installing packages, and if your command is in a Dockerfile, you'll then need:\n\napt-get -qq -y install curl\nAfter that install ZSH and GIT Core:\n\napt-get install zsh\napt-get install git-core\nGetting zsh to work in ubuntu is weird since sh does not understand the source command. So, you do this to install zsh:\n\nwget https://github.com/robbyrussell/oh-my-zsh/raw/master/tools/install.sh -O - | zsh\nand then you change your shell to zsh:\n\nchsh -s `which zsh`\nand then restart:\n\nsudo shutdown -r 0\nThis problem is explained in depth in this issue.\n\nShare\nImprove this answer\nFollow\nedited Jan 16 '20 at 8:30\nanswered Jul 3 '16 at 22:45\n\nAhmad Awais\n25.8k44 gold badges6868 silver badges5454 bronze badges\n2\nInstalling zsh is not relevant to the solution. \u2013 \nnonameable\n Jul 18 '19 at 13:37\nAdd a comment\n\n7\n\nIf you are using an Alpine based image, you have to\n\nRUN\n... \\\napk add --no-cache curl \\\ncurl ...\n...\nShare\nImprove this answer\nFollow\nanswered May 12 '19 at 4:47\n\nhannes ach\n11.2k66 gold badges4444 silver badges6969 bronze badges\n1\nWhy --no-cache? \u2013 \nMarSoft\n Jun 16 '19 at 0:08\n4\nHm, understood: we don't want cached package file to be left in our image. Nice touch. \u2013 \nMarSoft\n Jun 16 '19 at 0:08\nAdd a comment\n\n6\n\nYou don't need to install curl to download the file into Docker container, use ADD command, e.g.\n\nADD https://raw.githubusercontent.com/Homebrew/install/master/install /tmp\nRUN ruby -e /tmp/install\nNote: Add above lines to your Dockerfile file.\n\nAnother example which installs Azure CLI:\n\nADD https://aka.ms/InstallAzureCLIDeb /tmp\nRUN bash /tmp/InstallAzureCLIDeb\nShare\nImprove this answer\nFollow\nedited Aug 5 '20 at 16:56\nanswered Aug 5 '20 at 16:52\n\nkenorb\n127k7070 gold badges617617 silver badges652652 bronze badges\nless time consuming compared to the accepted answer. Just one question though. Does ADD command download the file locally and then adds the file to the image or the download happens within the image? \u2013 \nRaja Anbazhagan\n Aug 5 '20 at 16:56\n@RajaAnbazhagan It's downloaded during docker build process of your Dockerfile (which runs on your host). Since it's built directly by Docker process, I assume it's downloading the file directly to your image. \u2013 \nkenorb\n Aug 5 '20 at 16:58 ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Teamcity SetParameter doesn't seem to be working", "id": 1344, "answers": [{"answer_id": 1334, "document_id": 913, "question_id": 1344, "text": "Write-Host \"##teamcity[setParameter name='TestParameter' value='2']\"", "answer_start": 311, "answer_category": null}], "is_impossible": false}], "context": "As the first step in a build configuration I am trying to dynamically change a parameter and use it in the subsequent steps. Reading online, it seems that the way to do this is to call ##teamcity[setParameter. But this doesn't seem to be working. It doesn't even change the value in the same step.\nYou can try:\nWrite-Host \"##teamcity[setParameter name='TestParameter' value='2']\"\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "SQL Server 2012 can't start because of a login failure", "id": 746, "answers": [{"answer_id": 747, "document_id": 434, "question_id": 746, "text": "Right click on the Service in the Services mmc\n\u2022\tClick Properties\n\u2022\tClick on the Log On tab\n\u2022\tThe password fields will appear to have entries in them...\n\u2022\tBlank out both Password fields\n\u2022\tClick \"OK\"", "answer_start": 403, "answer_category": null}], "is_impossible": false}], "context": "I recently installed Microsoft SQL Server 2012 on a fresh Windows 7 installation, but whenever I want to run the server, I get the following error:\nError 1069: The service did not start due to a logon failure.\nThe answer to this may be identical to the problem with full blown SQL Server (NTService\\MSSQLSERVER) and this is to reset the password. The ironic thing is, there is no password.\nSteps are:\n\u2022\tRight click on the Service in the Services mmc\n\u2022\tClick Properties\n\u2022\tClick on the Log On tab\n\u2022\tThe password fields will appear to have entries in them...\n\u2022\tBlank out both Password fields\n\u2022\tClick \"OK\"\nThis should re-grant access to the service and it should start up again. Weird?\nNOTE: if the problem comes back after a few hours or days, then you probably have a group policy which is overriding your settings and it's coming and taking the right away again.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Capistrano - \"cannot load such file --deploy\"", "id": 1048, "answers": [{"answer_id": 1043, "document_id": 629, "question_id": 1048, "text": "You have the correct 2.x version of Capistrano in your project's Gemfile, you should be using bundle exec cap (to use your project's version) instead of just cap (which will use the globally-installed 3.x version).", "answer_start": 206, "answer_category": null}], "is_impossible": false}], "context": "Working on an existing rails project that is set up to deploy with Capistrano. Trying to determine my first step in figuring out this error. Can anyone point me in the right direction of what I need to do?\nYou have the correct 2.x version of Capistrano in your project's Gemfile, you should be using bundle exec cap (to use your project's version) instead of just cap (which will use the globally-installed 3.x version).\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "GitHub workflows - how to keep project up-to-date and passwords secure?", "id": 1294, "answers": [{"answer_id": 1286, "document_id": 865, "question_id": 1294, "text": "You should only push a config- sample in the repo and gitignore the \"real\" config, because every instance will have its own config. After you deployed the repo on your production system you copy/create the config based on the sample, fill in your \"personal\" settings and you are done.", "answer_start": 741, "answer_category": null}], "is_impossible": false}], "context": " - how to keep project up-to-date and passwords secure?\nI'm about to opensource a PHP based website on GitHub\nContained within it are my MySql DB passwords and API keys (contained within a separate config file)\nI can remove these to upload to GitHub, but how would I then make future changes and deploy to the production server? - currently the I pull the latest commit from a private git repo.\nNow I'm using a public repo, I can't pull the latest committed changes as the passwords / API keys will be missing.\nWould I use a combination of the 2 repositories and git-ignore the config file (i.e. push to both, but leave out the config for the public repo)? Or should I not be using Git to deploy to the production server in the first place?\nYou should only push a config- sample in the repo and gitignore the \"real\" config, because every instance will have its own config. After you deployed the repo on your production system you copy/create the config based on the sample, fill in your \"personal\" settings and you are done.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Website needs force refresh after deploy", "id": 595, "answers": [{"answer_id": 601, "document_id": 320, "question_id": 595, "text": "You can append a variable to the end of each of your resources that changes with each deploy. For example you can name your stylesheets:styles.css?id=1.", "answer_start": 209, "answer_category": null}], "is_impossible": false}], "context": "After deploying a new version of a website the browser loads everything from its cache from the old webpage until a force refresh is done. Images are old, cookies are old, and some AJAX parts are not working.\nYou can append a variable to the end of each of your resources that changes with each deploy. For example you can name your stylesheets:styles.css?id=1.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to install python-dateutil on Windows?", "id": 800, "answers": [{"answer_id": 796, "document_id": 483, "question_id": 800, "text": "If dateutil is missing install it via:\npip install python-dateutil\nOr on Ubuntu:\nsudo apt-get install python-dateutil", "answer_start": 297, "answer_category": null}], "is_impossible": false}], "context": "I'm trying to convert some date/times to UTC, which I thought would be dead simple in Python - batteries included, right? Well, it would be simple except that Python (2.6) doesn't include any tzinfo classes. No problem, a quick search turns up python-dateutil which should do exactly what I need.\nIf dateutil is missing install it via:\npip install python-dateutil\nOr on Ubuntu:\nsudo apt-get install python-dateutil\n\nThe problem is that I need to install it on Windows. I was able to upack the .tar.gz2 distribution using 7-zip, but now I'm left with a collection of files and no guidance on how to proceed. When I try to run setup.py I get the error \"No module named setuptools\".\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Advantages of bundledDependencies over normal dependencies in npm", "id": 1818, "answers": [{"answer_id": 1803, "document_id": 1389, "question_id": 1818, "text": "Other advantage is that you can put your internal dependencies (application components) there and then just require them in your app as if they were independent modules instead of cluttering your lib/ and publishing them to npm.", "answer_start": 328, "answer_category": null}], "is_impossible": false}], "context": "npm allows us to specify bundledDependencies, but what are the advantages of doing so? I guess if we want to make absolutely sure we get the right version even if the module we reference gets deleted, or perhaps there is a speed benefit with bundling?\nAnyone know the advantages of bundledDependencies over normal dependencies?\nOther advantage is that you can put your internal dependencies (application components) there and then just require them in your app as if they were independent modules instead of cluttering your lib/ and publishing them to npm.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to write Kafka consumers - single threaded vs multi threaded", "id": 1128, "answers": [{"answer_id": 1121, "document_id": 705, "question_id": 1128, "text": "Your existing solution is best. Handing off to another thread will cause problems with offset management. Spring kafka allows you to run multiple threads in each instance, as long as you have enough partitions.", "answer_start": 453, "answer_category": null}], "is_impossible": false}], "context": "I have written a single Kafka consumer (using Spring Kafka), that reads from a single topic and is a part of a consumer group. Once a message is consumed, it will perform all downstream operations and move on to the next message offset. I have packaged this as a WAR file and my deployment pipeline pushes this out to a single instance. Using my deployment pipeline, I could potentially deploy this artifact to multiple instances in my deployment pool.\nYour existing solution is best. Handing off to another thread will cause problems with offset management. Spring kafka allows you to run multiple threads in each instance, as long as you have enough partitions.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to make an installer for my C# application?", "id": 853, "answers": [{"answer_id": 848, "document_id": 533, "question_id": 853, "text": "The WiX toolset lets developers create installers for Windows Installer, the Windows installation engine.", "answer_start": 22, "answer_category": null}], "is_impossible": false}], "context": "About the WiX toolset\nThe WiX toolset lets developers create installers for Windows Installer, the Windows installation engine.\n\nThe core of WiX is a set of build tools that build Windows Installer packages using the same build concepts as the rest of your product: source code is compiled and then linked to create executables; in this case .exe setup bundles, .msi installation packages, .msm merge modules, and .msp patches. The WiX command-line build tools work with any automated build system. Also, MSBuild is supported from the command line, Visual Studio, and Team Build.\n\nWiX includes several extensions that offer functionality beyond that of Windows Installer. For example, WiX can install IIS web sites, create SQL Server databases, and register exceptions in the Windows Firewall, among others.\n\nWith Burn, the WiX bootstrapper, you can create setup bundles that install prerequisites like the .NET Framework and other runtimes along with your own product. Burn lets you download packages or combine them into a single downloadable .exe.\n\nThe WiX SDK includes managed and native libraries that make it easier to write code that works with Windows Installer, including custom actions in both C# and C++.\n\nRecent news\nWiX Toolset v4-preview.0 available  2021/05/17\nWiX v4-preview.0 is now available for experimentation. Read more about it at Rob's blog.\n\nWiX Toolset v3.11.2 released  2019/09/18\nWiX v3.11.2 is a minor security release of WiX. If your application directly references Microsoft.Deployment.Compression.Cab.dll or Microsoft.Deployment.Compression.Zip.dll to decompress cabinet or zip files to a folder, you should upgrade to this release.\n\nRead more about the release at FireGiant's blog.\n\nWiX Toolset Visual Studio Extension v1.0 released (with VS2019 support)  2019/07/16\nToday the WiX Toolset Visual Studio Extension v1.0 (Votive v1.0) was published to the Visual Studio marketplace. This includes support for Visual Studio 2019. Huge thanks to the team at FireGiant for their persistence with the publishing process.\n\nYou can find it all on the Downloads page.\n\n\nOther sources of news\nThe following blogs may have additional news about the progress of the WiX toolset:\n\nRob Mensching\nBob Arnson\nHeath Stewart\nFireGiant's Setup Matters", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to use CMAKE_INSTALL_PREFIX", "id": 1580, "answers": [{"answer_id": 1569, "document_id": 1157, "question_id": 1580, "text": "You should use should use (see the docs):\ncmake -DCMAKE_INSTALL_PREFIX=/usr ..", "answer_start": 174, "answer_category": null}], "is_impossible": false}], "context": "I want to generate Makefile with install target, making installation to /usr instead of default /usr/local. Assuming that build directory is done in the source subdirectory.\nYou should use should use (see the docs):\ncmake -DCMAKE_INSTALL_PREFIX=/usr ..\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "You can include the local.config settings in the web.config using configSource ", "id": 513, "answers": [{"answer_id": 515, "document_id": 238, "question_id": 513, "text": "You can include the local.config settings in the web.config using configSource (http://weblogs.asp.net/fmarguerie/archive/2007/04/26/using-configsource-to-split-configuration-files.aspx).", "answer_start": 502, "answer_category": null}], "is_impossible": false}], "context": "Anyone have any good tips on handling differences in web.config settings between environments? I've considered creating a 'config' folder in our source control system but outside of the web hierarchy, and having the deployment process copy the appropriate config files (web.dev.config,web.staging.config, web.production.config) into the web folder upon deployment. I've also seen posts on how to programmatically change the config settings (WCF endpoints, connection strings, etc) when the app starts.\nYou can include the local.config settings in the web.config using configSource (http://weblogs.asp.net/fmarguerie/archive/2007/04/26/using-configsource-to-split-configuration-files.aspx).\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to request the creation of all types of self-contained application packages for the platform when deploying java applications?\n", "id": 620, "answers": [{"answer_id": 625, "document_id": 329, "question_id": 620, "text": "add nativeBundles=\"all\" to the <fx:deploy> task", "answer_start": 518, "answer_category": null}], "is_impossible": false}, {"question": "How to specify the exact package format when deploying java applications?", "id": 621, "answers": [{"answer_id": 626, "document_id": 329, "question_id": 621, "text": " Use the value image to produce a basic package, exe to request an EXE installer, dmg to request a DMG installer, and so on.", "answer_start": 1201, "answer_category": null}], "is_impossible": false}, {"question": "How to produce native packages using the Java Packager tool when deploying java applications?", "id": 622, "answers": [{"answer_id": 627, "document_id": 329, "question_id": 622, "text": "Self-contained application packages are built by default when you use the -makeall command. You can request specific formats using the -native option with the -deploy command.", "answer_start": 1516, "answer_category": null}], "is_impossible": false}, {"question": "How to substitute a built-in resource with my customized version when deploying java applications?", "id": 623, "answers": [{"answer_id": 628, "document_id": 329, "question_id": 623, "text": "Learn what resources are used.\n\u2022\tDrop custom resources into a location where the packaging tool looks for them.", "answer_start": 2101, "answer_category": null}], "is_impossible": false}, {"question": "How to get more insight into what resources are being used when deploying java applications?", "id": 624, "answers": [{"answer_id": 629, "document_id": 329, "question_id": 624, "text": "enable verbose mode by adding the verbose=\"true\" attribute to <fx:deploy>, or pass the -v option to the javapackager -deploy command.", "answer_start": 2364, "answer_category": null}], "is_impossible": false}, {"question": "How to replace the application icon when deploying java applications?", "id": 625, "answers": [{"answer_id": 630, "document_id": 329, "question_id": 625, "text": "copy your custom icon to ./package/macosx/DemoApp.icns in the directory from which javapackager is run (typically, the root project directory).", "answer_start": 3522, "answer_category": null}], "is_impossible": false}], "context": "7.3 Basics\nMultiple package formats are possible. Built-in support is provided for several types of packages. You can also assemble your own packages by post-processing a self-contained application packaged as a folder, for example if you want to distribute your application as a ZIP file.\n\n7.3.2 Basic Build\nThe easiest way to produce a self-contained application is to modify the deployment task. To request the creation of all types of self-contained application packages for the platform on which you are running, add nativeBundles=\"all\" to the <fx:deploy> task, as shown in Example 7-1.\nExample 7-1 Simple Deployment Task to Create All Self-contained Application Packages\n<fx:deploy width=\"${javafx.run.width}\" height=\"${javafx.run.height}\"\n           nativeBundles=\"all\"\n           outdir=\"${basedir}/${dist.dir}\" outfile=\"${application.title}\">\n    <fx:application name=\"${application.title}\" mainClass=\"${javafx.main.class}\"/>\n    <fx:resources>\n        <fx:fileset dir=\"${basedir}/${dist.dir}\" includes=\"*.jar\"/>\n    </fx:resources>\n    <fx:info title=\"${application.title}\" vendor=\"${application.vendor}\"/>\n</fx:deploy>\nYou can also specify the exact package format that you want to produce. Use the value image to produce a basic package, exe to request an EXE installer, dmg to request a DMG installer, and so on. For the full list of attribute values, see the nativeBundles attribute in the <fx:deploy> entry in the Ant Task Reference.\nYou can also produce native packages using the Java Packager tool. Self-contained application packages are built by default when you use the -makeall command. You can request specific formats using the -native option with the -deploy command. See the javapackager command reference for Windows or for Solaris, Linux, or OS X.\n7.3.3 Customizing the Package Using Drop-In Resources\nThe packaging tools use several built-in resources to produce a package, such as the application icon or configuration files. One way to customize the resulting package is to substitute a built-in resource with your customized version.\nThe following actions are needed:\n\u2022\tLearn what resources are used.\n\u2022\tDrop custom resources into a location where the packaging tool looks for them.\nThe following sections explain how to use custom resources.\n7.3.3.1 Preparing Custom Resources\nTo get more insight into what resources are being used, enable verbose mode by adding the verbose=\"true\" attribute to <fx:deploy>, or pass the -v option to the javapackager -deploy command.\nVerbose mode includes the following actions:\n\u2022\tThe following items are printed:\no\tList of configuration resources that are used for the package that you are generating\no\tRole of each resource\no\tExpected custom resource name\n\u2022\tA copy of the configuration files and resources used to create the self contained package are saved to a temporary folder. You can use these files as a starting point for customization\nNow you can grab a copy of the configuration files and tune them to your needs. For example, you can take the configuration file Info.plist and add localized package names.\n\nNote:\nIt is recommended that you disable verbose mode after you are done customizing, or add a custom cleanup action to remove sample configuration files.\n\n7.3.3.2 Substituting a Built-In Resource\nPackaging tools look for customized resources on the class path before reverting to built-in resources. The Java Packager has \".\" (the current working directory) added to the class path by default. Therefore, to replace the application icon, copy your custom icon to ./package/macosx/DemoApp.icns in the directory from which javapackager is run (typically, the root project directory).\nThe class path for Java Ant tasks is defined when task definitions are loaded. You must add an additional path to the lookup before the path ant-javafx.jar.\nExample 7-4 shows how to add \".\" to the class path. For a more detailed code example, see Example 6-1.\nExample 7-4 Enabling Resource Customization for JavaFX Ant Tasks\n<taskdef resource=\"com/sun/javafx/tools/ant/antlib.xml\"\n         uri=\"javafx:com.sun.javafx.tools.ant\"\n         classpath=\".:path/to/sdk/lib/ant-javafx.jar\"/>\nAfter you provide a customized resource, verbose build output reports that the resource is used. For example, if you added a custom icon to an application, then the verbose output reports the addition", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "how to Find the JDK Registry Key and UninstallString Value?", "id": 387, "answers": [{"answer_id": 394, "document_id": 161, "question_id": 387, "text": "Go to Start and type regedit.\n\nIn the Registry Editor, go to HKEY_LOCAL_MACHINE/Software/Microsoft/Windows/CurrentVersion/Uninstall.\n\nUnder the Uninstall folder, you will find many registry entries within curly brackets.\n\nClick Edit and then Find.\n\n\nNote:\n\nHighlight Uninstall folder before performing search for a particular registry.\n\n\nEnter version string as value to find corresponding registry key. Follow format 1.8.0.xxx. For example, enter jre1.8.0.251.\n\nThe registry key is highlighted on the right-hand side of the pane and values of various uninstall strings are displayed on the left-hand pane.\n\nNote the value of the UninstallString.", "answer_start": 1483, "answer_category": null}], "is_impossible": false}, {"question": "what to do if I meet corrupt cabinet file whren installing jdk for windows?", "id": 388, "answers": [{"answer_id": 395, "document_id": 161, "question_id": 388, "text": " Check the file size against the expected file size listed in these instructions. If sizes do not match, try downloading the bundle again. (A cabinet file contains compressed application, data, resource, and DLL files.)\n", "answer_start": 2974, "answer_category": null}], "is_impossible": false}, {"question": "what to do if I meet system error during decompression whren installing jdk for windows?", "id": 389, "answers": [{"answer_id": 396, "document_id": 161, "question_id": 389, "text": " you might not have enough space on the disk that contains your TEMP directory.\n", "answer_start": 3299, "answer_category": null}], "is_impossible": false}, {"question": "what to do if I meet \u201cThis program cannot be run in DOS mode\u201d whren installing jdk for windows?", "id": 390, "answers": [{"answer_id": 397, "document_id": 161, "question_id": 390, "text": "Open the MS-DOS shell or Command Prompt window.\n\nRight-click the title bar.\n\nSelect Properties.\n\nChoose the Program tab.\n\nClick the Advanced button.\n\nEnsure that the item \"Prevent MS-DOS-based programs from detecting Windows\" is not selected.\n\nSelect OK.\n\nSelect OK again.\n\nExit the MS-DOS shell.\n\nRestart your computer.", "answer_start": 3509, "answer_category": null}], "is_impossible": false}, {"question": "how to avoid error 1722 when intalling java for windows?", "id": 391, "answers": [{"answer_id": 398, "document_id": 161, "question_id": 391, "text": " ensure that the user and system locales are identical, and that the installation path contains only characters that are part of the system locale's code page. User and system locales can be set in the Regional Options or Regional Settings control panel.", "answer_start": 5832, "answer_category": null}], "is_impossible": false}, {"question": "how to use the command line for uninstalling the JDK?", "id": 392, "answers": [{"answer_id": 399, "document_id": 161, "question_id": 392, "text": "Use the following command to uninstall the JDK in silent mode:\nmsiexec.exe/X{UninstallString} \nFor example, to uninstall Java 8 update 251, run the command:\n\nmsiexec.exe/X{26A24AE4-039D-4CA4-87B4-2F64180251F0}", "answer_start": 590, "answer_category": null}], "is_impossible": false}], "context": "This page describes how to install and uninstall JDK 8 for Windows.\n\nThe page has these topics:\n\n\"System Requirements\"\n\n\"Installation Instructions Notation\"\n\n\"Installation Instructions\"\n\n\"Uninstalling the JDK\"\n\n\"Installed Directory Tree\"\n\n\"Installation Troubleshooting\"\n\nSee \"JDK 8 and JRE 8 Installation Start Here\" for general information about installing JDK 8 and JRE 8.\n\nUninstalling the JDK\nTo uninstall the JDK, use the \"Add/Remove Programs\" utility in the Microsoft Windows Control Panel.\n\nUninstalling the JDK in Silent Mode\nYou can use the command line for uninstalling the JDK.\n\nUse the following command to uninstall the JDK in silent mode:\nmsiexec.exe/X{UninstallString} \nFor example, to uninstall Java 8 update 251, run the command:\n\nmsiexec.exe/X{26A24AE4-039D-4CA4-87B4-2F64180251F0}\nTo find the UninstallString, see \"Finding the JDK Registry Key and UninstallString\".\n\n\nNote:\n\nThis command can be run from anywhere.\n\nThe msiexec.exe executable is located in the windows system directory.\n\nA reboot is required only if some files are in use during uninstallation; it is not necessary everytime. However, to manually suppress reboot while uninstalling, append REBOOT=R option to the command.\n\nAppend /l \"C:\\<path>setup.log\" option to the command if you want to create a log file describing the uninstallation status.\n\n\nWindows Installer dialog appears prompting you for confirmation. Click Yes to uninstall JDK.\n\nFinding the JDK Registry Key and UninstallString Value\nGo to Start and type regedit.\n\nIn the Registry Editor, go to HKEY_LOCAL_MACHINE/Software/Microsoft/Windows/CurrentVersion/Uninstall.\n\nUnder the Uninstall folder, you will find many registry entries within curly brackets.\n\nClick Edit and then Find.\n\n\nNote:\n\nHighlight Uninstall folder before performing search for a particular registry.\n\n\nEnter version string as value to find corresponding registry key. Follow format 1.8.0.xxx. For example, enter jre1.8.0.251.\n\nThe registry key is highlighted on the right-hand side of the pane and values of various uninstall strings are displayed on the left-hand pane.\n\nNote the value of the UninstallString.\n\nInstalled Directory Tree\nSee http://docs.oracle.com/javase/8/docs/technotes/tools/windows/jdkfiles.html for a description of the directory structure of the JDK. (Note that the file structure of the JRE is identical to that of the JDK's jre directory.)\n\nInstallation Troubleshooting\nBelow are some tips for working around problems that are sometimes seen during or following an installation:\n\n\"Corrupt Cabinet File\"\n\n\"System Error During Decompression\"\n\n\"Program Cannot Be Run in DOS Mode\"\n\n\"Private Versus Public JRE\"\n\n\"Source Files in Notepad\"\n\n\"Characters That Are Not Part of the System Code Page\"\n\nFor more troubleshooting information, see http://docs.oracle.com/javase/8/docs/technotes/guides/tsgdesktop/index.html.\n\nCorrupt Cabinet File\nIf you see the error message \"corrupt cabinet file,\" then the file you have downloaded is corrupted. Check the file size against the expected file size listed in these instructions. If sizes do not match, try downloading the bundle again. (A cabinet file contains compressed application, data, resource, and DLL files.)\n\nSystem Error During Decompression\nIf you see the error message \"system error during decompression,\" then you might not have enough space on the disk that contains your TEMP directory.\n\nProgram Cannot Be Run in DOS Mode\nIf you see the error message \"This program cannot be run in DOS mode,\" then do the following:\n\nOpen the MS-DOS shell or Command Prompt window.\n\nRight-click the title bar.\n\nSelect Properties.\n\nChoose the Program tab.\n\nClick the Advanced button.\n\nEnsure that the item \"Prevent MS-DOS-based programs from detecting Windows\" is not selected.\n\nSelect OK.\n\nSelect OK again.\n\nExit the MS-DOS shell.\n\nRestart your computer.\n\nPrivate Versus Public JRE\nInstalling the JDK also installs a private JRE and optionally a public copy. The private JRE is required to run the tools included with the JDK. It has no registry settings and is contained entirely in a jre directory (typically at C:\\Program Files\\jdk1.8.0\\jre) whose location is known only to the JDK. On the other hand, the public JRE can be used by other Java applications, is contained outside the JDK (typically at C:\\Program Files\\Java\\jre1.8.0), is registered with the Windows registry (at HKEY_LOCAL_MACHINE\\SOFTWARE\\JavaSoft), can be removed using Add/Remove Programs, might be registered with browsers, and might have the java.exe file copied to the Windows system directory (which would make it the default system Java platform).\n\nSource Files in Notepad\nIn Microsoft Windows, when you create a new file in Microsoft Notepad and then save it for the first time, Notepad usually adds the .txt extension to the file name. Therefore, a file you name Test.java is saved as Test.java.txt. It is important to note that you cannot see the .txt extension unless you turn on the viewing of file extensions (in Microsoft Windows Explorer, unselect \"Hide file extensions for known file types\" under Folder Options). To prevent the .txt extension, enclose the file name in quotation marks, such as \"Test.java\", when typing it into the Save As dialog box.\n\nOn the other hand, Microsoft WordPad does not add the .txt extension if you specify another extension. However, you must save the file as \"Text Document\".\n\nCharacters That Are Not Part of the System Code Page\nIt is possible to name directories using characters that are not part of the system locale's code page. If such a directory is part of the installation path, then generic error 1722 occurs, and installation is not completed. Error 1722 is a Windows Installer error code. It indicates that the installation process has failed. The exact reason for this error is not known at this time.\n\nTo prevent this problem, ensure that the user and system locales are identical, and that the installation path contains only characters that are part of the system locale's code page. User and system locales can be set in the Regional Options or Regional Settings control panel.\n\nThe associated bug number is 4895647.", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Was my version of Java installed with RPM?", "id": 182, "answers": [{"answer_id": 189, "document_id": 106, "question_id": 182, "text": "rpm -q --list jre | grep \"bin/java\"\nNote: If a version of Java is installed on the system, but is not listed by the RPM query, then it is not an RPM installation.", "answer_start": 638, "answer_category": null}], "is_impossible": false}, {"question": "How do I upgrade Java using RPM?", "id": 183, "answers": [{"answer_id": 190, "document_id": 106, "question_id": 183, "text": "Type: rpm -Uvh <package-file>\nFor example: for upgrade to Java 1.7.0, type: rpm -Uvh jre1.7.0", "answer_start": 924, "answer_category": null}], "is_impossible": false}, {"question": "if I have two versions of Java on my machine running through RPM, how do I remove one of them?", "id": 184, "answers": [{"answer_id": 191, "document_id": 106, "question_id": 184, "text": " rpm -q jre\nNote: Sample results returned by above query\njre-1.5.0\njre-1.6.0_07\nTo uninstall 1.5.0, type: rpm -e jre-1.5.0", "answer_start": 1290, "answer_category": null}], "is_impossible": false}], "context": "What are the options for Java installed through RPM?\nThis article applies to:\nPlatform(s): Red Hat Linux, SUSE Linux, Oracle Linux, Oracle Enterprise Linux, SLES\nJava version(s): 7.0, 8.0\nThis page answers most frequently asked questions about installing Java on Linux using RPM method.\nWas my version of Java installed with RPM?\nHow do I upgrade Java using RPM?\nIf I have two versions of Java on my machine running through RPM, how do I remove one of them?\n \n\nThe RPM database can be queried to determine what packages have been installed. This command will try to locate any Java installed on the system.\nOpen new Terminal Window\nType: rpm -q --list jre | grep \"bin/java\"\nNote: If a version of Java is installed on the system, but is not listed by the RPM query, then it is not an RPM installation.\n\n \n\nThe recommended method of upgrading Java using RPM is to use the upgrade option.\nFor example:\nOpen new Terminal Window\nType: rpm -Uvh <package-file>\nFor example: for upgrade to Java 1.7.0, type: rpm -Uvh jre1.7.0\nThe vh options are used to provide the user with feedback during the installation process.\n\n \n\nTo uninstall a RPM installation, you need to know the package\u2019s full name, version number, and release names.\nDetermine installed versions of Java\nOpen new Terminal Window\nType: rpm -q jre\nNote: Sample results returned by above query\njre-1.5.0\njre-1.6.0_07\nTo uninstall 1.5.0, type: rpm -e jre-1.5.0", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "SQL Server 2005 and 2008 on same developer machine?", "id": 1795, "answers": [{"answer_id": 1781, "document_id": 1367, "question_id": 1795, "text": "Yes this is possible. You will have to create a named instance not used by another version of SQL Server as per the previous answer and version 3.5 of .Net installed. Works great!!\nHere the list of prerequisites:\n.NET Framework 3.5 SP1\nWindows Installer 4.5\nWindows PowerShell 1.0", "answer_start": 672, "answer_category": null}], "is_impossible": false}], "context": "Has anyone tried installing SQL Server 2008 Developer on a machine that already has 2005 Developer installed?\nI am unsure if I should do this, and I need to keep 2005 on this machine for the foreseeable future in order to test our application easily. Since I sometimes need to take backup files of databases and make available for other people in the company I cannot just replace 2005 with 2008 as I suspect (but do not know) that the databases aren't 100% backwards compatible.\nIt doesn't say more than just yes you can do this and I kinda suspected that this was doable anyway, but I need to know if there are anything I need to know before I start installing.\nAnyone?\nYes this is possible. You will have to create a named instance not used by another version of SQL Server as per the previous answer and version 3.5 of .Net installed. Works great!!\nHere the list of prerequisites:\n.NET Framework 3.5 SP1\nWindows Installer 4.5\nWindows PowerShell 1.0\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "is there a way to force conda install to install the packages and its dependenci", "id": 1398, "answers": [{"answer_id": 1387, "document_id": 970, "question_id": 1398, "text": "it. But, here is one which will require you to replace conda usage with another Python package designed for such issue: Poetry (https://python-poet", "answer_start": 1085, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI have a miniconda environment that I am installing packages in. I want to install a package and I understand that there can be some conflicts, however to resolve those conflicts either requires a missing package install or downgrade/upgrade of another and it has become a long rabbit hole of trying to downgrade, upgrade and install packages. Is there a way to force conda to do this all automatically as it currently does not. Example install is:\n\nconda install psycopg2=2.7.5=py35h74b6da3_2\n\n\nwhich is the package, the version and the python I'm using, however I get a Error that never seems to end.\n\nUnsatisfiableError: The follow specifications were found to be in conflict:\n-defaults/win-64::qt==5.9.7 -&gt;openssl[version='1.1.*,&gt;1=1.1.1a,&lt;1.1.2a']\n-openssl=1.0.2r\n\n\nAnd then tells me to look at its dependencies and then it list more packages that need install and I'm not sure when it ends...\n\nAny help would be great.\n    \n\nthis is a long running problem in Python package management. Thus, as far as I know, Anaconda doesn't provide a solution for it. But, here is one which will require you to replace conda usage with another Python package designed for such issue: Poetry (https://python-poetry.org/)\n\nInstall Poetry: curl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py | python \nCreate a virtual environment for your project using Pyenv, or conda or whatever: conda create -n myenv python=3.6\nActivate your virtual environment: conda activate myenv\nCreate a Poetry projet with poetry init\nAdd your package dependencies to your virtual environment with poetry add my-package\n\nPoetry handles package dependencies for you, and offers useful commands like poetry update to easily update package versions when available without breaking the whole thing. You can also install the environment from an existing pyproject.toml (capturing dependencies) file using poetry install.\nPoetry is becoming a standard in Python ecosystem (&gt;= 13k stars on Github).\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Composer how to specify install directory", "id": 830, "answers": [{"answer_id": 825, "document_id": 512, "question_id": 830, "text": "from the command prompt, type cd C:\\xampp\\htdocs then press enter. After that, type following code on the command prompt and press enter\ncomposer create-project laravel/laravel my-laravel-project", "answer_start": 591, "answer_category": null}], "is_impossible": false}], "context": "However when I use the above line this installs my project in the c:\\users\\guti directory however I want it to be installed in c:\\xampp\\htdocs.\nI've seen another question on stackoverflow and it talks about amending the composer.json file but I'm not sure whether this is the route I should be taking as I've done a computer search for this file and this file exists as part of the laravel framework so does it make sense to amend this file?\nAny composer experts on hand to point me in the right direction? You should specify your destination directory where you want to create the project, from the command prompt, type cd C:\\xampp\\htdocs then press enter. After that, type following code on the command prompt and press enter\ncomposer create-project laravel/laravel my-laravel-project\nHere my-laravel-project will be your project folder. The composer.json file is used to setup some configurations (for Dependency management) during installation of the package via packagist. Visit getcomposer.org for more information.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "What is a schema in Mongoose", "id": 194, "answers": [{"answer_id": 201, "document_id": 109, "question_id": 194, "text": "Each schema maps to a MongoDB\ncollection and defines the shape of the documents within that collection.", "answer_start": 828, "answer_category": null}], "is_impossible": false}, {"question": "What if I overwrite default Mongoose document method?", "id": 195, "answers": [{"answer_id": 202, "document_id": 109, "question_id": 195, "text": "unpredictable results", "answer_start": 4592, "answer_category": null}], "is_impossible": false}], "context": "\n\n\nMongoose v6.0.10: SchemasmongoosemongooseVersion 6.0.10Version 5.13.10Version 4.13.21Quick StartGuidesSchemasSchemaTypesConnectionsModelsDocumentsSubdocumentsQueriesValidationMiddlewarePopulateDiscriminatorsPluginsTransactionsTypeScriptAPIMongooseSchemaConnectionDocumentModelQueryAggregateSchemaTypeVirtualTypeMigration GuideVersion CompatibilityFAQFurther ReadingFor EnterpriseSponsors\n\n\n\nSchemas\n\n\n\n\nIf you haven't yet done so, please take a minute to read the quickstart to get an idea of how Mongoose works.\nIf you are migrating from 5.x to 6.x please take a moment to read the migration guide.\n\nDefining your schema\nCreating a model\nIds\nInstance methods\nStatics\nQuery Helpers\nIndexes\nVirtuals\nAliases\nOptions\nWith ES6 Classes\nPluggable\nFurther Reading\n\nDefining your schema\nEverything in Mongoose starts with a Schema. Each schema maps to a MongoDB\ncollection and defines the shape of the documents within that collection.\nimport mongoose from 'mongoose';\nconst { Schema } = mongoose;\n\nconst blogSchema = new Schema({\ntitle:  String, // String is shorthand for {type: String}\nauthor: String,\nbody:   String,\ncomments: [{ body: String, date: Date }],\ndate: { type: Date, default: Date.now },\nhidden: Boolean,\nmeta: {\nvotes: Number,\nfavs:  Number\n}\n});\nIf you want to add additional keys later, use the\nSchema#add method.\nEach key in our code blogSchema defines a property in our documents which\nwill be cast to its associated SchemaType.\nFor example, we've defined a property title which will be cast to the\nString SchemaType and property date\nwhich will be cast to a Date SchemaType.\nNotice above that if a property only requires a type, it can be specified using\na shorthand notation (contrast the title property above with the date\nproperty).\nKeys may also be assigned nested objects containing further key/type definitions\nlike the meta property above.  This will happen whenever a key's value is a POJO\nthat doesn't have a type property.\nIn these cases, Mongoose only creates actual schema paths for leaves\nin the tree. (like meta.votes and meta.favs above),\nand the branches do not have actual paths.  A side-effect of this is that meta\nabove cannot have its own validation.  If validation is needed up the tree, a path\nneeds to be created up the tree - see the Subdocuments section\nfor more information on how to do this.  Also read the Mixed\nsubsection of the SchemaTypes guide for some gotchas.\nThe permitted SchemaTypes are:\n\nString\nNumber\nDate\nBuffer\nBoolean\nMixed\nObjectId\nArray\nDecimal128\nMap\n\nRead more about SchemaTypes here.\nSchemas not only define the structure of your document and casting of\nproperties, they also define document instance methods,\nstatic Model methods, compound indexes,\nand document lifecycle hooks called middleware.\nCreating a model\nTo use our schema definition, we need to convert our blogSchema into a\nModel we can work with.\nTo do so, we pass it into mongoose.model(modelName, schema):\nconst Blog = mongoose.model('Blog', blogSchema);\n// ready to go!\nIds\nBy default, Mongoose adds an _id property to your schemas.\nconst schema = new Schema();\n\nschema.path('_id'); // ObjectId { ... }\nWhen you create a new document with the automatically added\n_id property, Mongoose creates a new _id of type ObjectId\nto your document.\nconst Model = mongoose.model('Test', schema);\n\nconst doc = new Model();\ndoc._id instanceof mongoose.Types.ObjectId; // true\nYou can also overwrite Mongoose's default _id with your\nown _id. Just be careful: Mongoose will refuse to save a\ndocument that doesn't have an _id, so you're responsible\nfor setting _id if you define your own _id path.\nconst schema = new Schema({ _id: Number });\nconst Model = mongoose.model('Test', schema);\n\nconst doc = new Model();\nawait doc.save(); // Throws \"document must have an _id before saving\"\n\ndoc._id = 1;\nawait doc.save(); // works\nInstance methods\nInstances of Models are documents. Documents have\nmany of their own built-in instance methods.\nWe may also define our own custom document instance methods.\n// define a schema\nconst animalSchema = new Schema({ name: String, type: String });\n\n// assign a function to the \"methods\" object of our animalSchema\nanimalSchema.methods.findSimilarTypes = function(cb) {\nreturn mongoose.model('Animal').find({ type: this.type }, cb);\n};\nNow all of our animal instances have a findSimilarTypes method available\nto them.\nconst Animal = mongoose.model('Animal', animalSchema);\nconst dog = new Animal({ type: 'dog' });\n\ndog.findSimilarTypes((err, dogs) => {\nconsole.log(dogs); // woof\n});\n\nOverwriting a default mongoose document method may lead to unpredictable results. See this for more details.\nThe example above uses the Schema.methods object directly to save an instance method. You can also use the Schema.method() helper as described here.\nDo not declare methods using ES6 arrow functions (=>). Arrow functions explicitly prevent binding this, so your method will not have access to the document and the above examples will not work.\n\nStatics\nYou can also add static functions to your model. There are two equivalent\nways to add a static:\n\nAdd a function property to schema.statics\nCall the Schema#static() function\n\n// Assign a function to the \"statics\" object of our animalSchema\nanimalSchema.statics.findByName = function(name) {\nreturn this.find({ name: new RegExp(name, 'i') });\n};\n// Or, equivalently, you can call `animalSchema.static()`.\nanimalSchema.static('findByBreed', function(breed) { return this.find({ breed }); });\n\nconst Animal = mongoose.model('Animal', animalSchema);\nlet animals = await Animal.findByName('fido');\nanimals = animals.concat(await Animal.findByBreed('Poodle'));\nDo not declare statics using ES6 arrow functions (=>). Arrow functions explicitly prevent binding this, so the above examples will not work because of the value of this.\nQuery Helpers\nYou can also add query helper functions, which are like instance methods\nbut for mongoose queries. Query helper methods let you extend mongoose's\nchainable query builder API.\nanimalSchema.query.byName = function(name) {\nreturn this.where({ name: new RegExp(name, 'i') })\n};\n\nconst Animal = mongoose.model('Animal', animalSchema);\n\nAnimal.find().byName('fido').exec((err, animals) => {\nconsole.log(animals);\n});\n\nAnimal.findOne().byName('fido').exec((err, animal) => {\nconsole.log(animal);\n});\nIndexes\nMongoDB supports secondary indexes.\nWith mongoose, we define these indexes within our Schema at the path level or the schema level.\nDefining indexes at the schema level is necessary when creating\ncompound indexes.\nconst animalSchema = new Schema({\nname: String,\ntype: String,\ntags: { type: [String], index: true } // field level\n});\n\nanimalSchema.index({ name: 1, type: -1 }); // schema level\nWhen your application starts up, Mongoose automatically calls createIndex for each defined index in your schema.\nMongoose will call createIndex for each index sequentially, and emit an 'index' event on the model when all the createIndex calls succeeded or when there was an error.\nWhile nice for development, it is recommended this behavior be disabled in production since index creation can cause a significant performance impact.\nDisable the behavior by setting the autoIndex option of your schema to false, or globally on the connection by setting the option autoIndex to false.\nmongoose.connect('mongodb://user:pass@localhost:port/database', { autoIndex: false });\n// or\nmongoose.createConnection('mongodb://user:pass@localhost:port/database', { autoIndex: false });\n// or\nanimalSchema.set('autoIndex', false);\n// or\nnew Schema({..}, { autoIndex: false });\nMongoose will emit an index event on the model when indexes are done\nbuilding or an error occurred.\n// Will cause an error because mongodb has an _id index by default that\n// is not sparse\nanimalSchema.index({ _id: 1 }, { sparse: true });\nconst Animal = mongoose.model('Animal', animalSchema);\n\nAnimal.on('index', error => {\n// \"_id index cannot be sparse\"\nconsole.log(error.message);\n});\nSee also the Model#ensureIndexes method.\nVirtuals\nVirtuals are document properties that\nyou can get and set but that do not get persisted to MongoDB. The getters\nare useful for formatting or combining fields, while setters are useful for\nde-composing a single value into multiple values for storage.\n// define a schema\nconst personSchema = new Schema({\nname: {\nfirst: String,\nlast: String\n}\n});\n\n// compile our model\nconst Person = mongoose.model('Person', personSchema);\n\n// create a document\nconst axl = new Person({\nname: { first: 'Axl', last: 'Rose' }\n});\nSuppose you want to print out the person's full name. You could do it yourself:\nconsole.log(axl.name.first + ' ' + axl.name.last); // Axl Rose\nBut concatenating the first and\nlast name every time can get cumbersome.\nAnd what if you want to do some extra processing on the name, like\nremoving diacritics? A\nvirtual property getter lets you\ndefine a fullName property that won't get persisted to MongoDB.\npersonSchema.virtual('fullName').get(function() {\nreturn this.name.first + ' ' + this.name.last;\n});\nNow, mongoose will call your getter function every time you access the\nfullName property:\nconsole.log(axl.fullName); // Axl Rose\nIf you use toJSON() or toObject() mongoose will not include virtuals\nby default. This includes the output of calling JSON.stringify()\non a Mongoose document, because JSON.stringify() calls toJSON().\nPass { virtuals: true } to either\ntoObject() or toJSON().\nYou can also add a custom setter to your virtual that will let you set both\nfirst name and last name via the fullName virtual.\npersonSchema.virtual('fullName').\nget(function() {\nreturn this.name.first + ' ' + this.name.last;\n}).\nset(function(v) {\nthis.name.first = v.substr(0, v.indexOf(' '));\nthis.name.last = v.substr(v.indexOf(' ') + 1);\n});\n\naxl.fullName = 'William Rose'; // Now `axl.name.first` is \"William\"\nVirtual property setters are applied before other validation. So the example\nabove would still work even if the first and last name fields were\nrequired.\nOnly non-virtual properties work as part of queries and for field selection.\nSince virtuals are not stored in MongoDB, you can't query with them.\nYou can learn more about virtuals here.\nAliases\nAliases are a particular type of virtual where the getter and setter\nseamlessly get and set another property. This is handy for saving network\nbandwidth, so you can convert a short property name stored in the database\ninto a longer name for code readability.\nconst personSchema = new Schema({\nn: {\ntype: String,\n// Now accessing `name` will get you the value of `n`, and setting `name` will set the value of `n`\nalias: 'name'\n}\n});\n\n// Setting `name` will propagate to `n`\nconst person = new Person({ name: 'Val' });\nconsole.log(person); // { n: 'Val' }\nconsole.log(person.toObject({ virtuals: true })); // { n: 'Val', name: 'Val' }\nconsole.log(person.name); // \"Val\"\n\nperson.name = 'Not Val';\nconsole.log(person); // { n: 'Not Val' }\nYou can also declare aliases on nested paths. It is easier to use nested\nschemas and subdocuments, but you can also declare\nnested path aliases inline as long as you use the full nested path\nnested.myProp as the alias.\nconst childSchema = new Schema({\nn: {\ntype: String,\nalias: 'name'\n}\n}, { _id: false });\n\nconst parentSchema = new Schema({\n// If in a child schema, alias doesn't need to include the full nested path\nc: childSchema,\nname: {\nf: {\ntype: String,\n// Alias needs to include the full nested path if declared inline\nalias: 'name.first'\n}\n}\n});\nOptions\nSchemas have a few configurable options which can be passed to the\nconstructor or to the set method:\nnew Schema({..}, options);\n\n// or\n\nconst schema = new Schema({..});\nschema.set(option, value);\nValid options:\n\nautoIndex\nautoCreate\nbufferCommands\nbufferTimeoutMS\ncapped\ncollection\ndiscriminatorKey\nid\n_id\nminimize\nread\nwriteConcern\nshardKey\nstrict\nstrictQuery\ntoJSON\ntoObject\ntypeKey\nuseNestedStrict\nvalidateBeforeSave\nversionKey\noptimisticConcurrency\ncollation\nselectPopulatedPaths\nskipVersioning\ntimestamps\nstoreSubdocValidationError\n\noption: autoIndex\nBy default, Mongoose's init() function\ncreates all the indexes defined in your model's schema by calling\nModel.createIndexes()\nafter you successfully connect to MongoDB. Creating indexes automatically is\ngreat for development and test environments. But index builds can also create\nsignificant load on your production database. If you want to manage indexes\ncarefully in production, you can set autoIndex to false.\nconst schema = new Schema({..}, { autoIndex: false });\nconst Clock = mongoose.model('Clock', schema);\nClock.ensureIndexes(callback);\nThe autoIndex option is set to true by default. You can change this\ndefault by setting mongoose.set('autoIndex', false);\noption: autoCreate\nBefore Mongoose builds indexes, it calls Model.createCollection()\nto create the underlying collection in MongoDB if autoCreate is set to true.\nCalling createCollection()\nsets the collection's default collation\nbased on the collation option and establishes the collection as\na capped collection if you set the capped schema option. Like\nautoIndex, setting autoCreate to true is helpful for development and\ntest environments.\nUnfortunately, createCollection() cannot change an existing collection.\nFor example, if you add capped: 1024 to your schema and the existing\ncollection is not capped, createCollection() will throw an error.\nGenerally, autoCreate should be false for production environments.\nconst schema = new Schema({..}, { autoCreate: true, capped: 1024 });\nconst Clock = mongoose.model('Clock', schema);\n// Mongoose will create the capped collection for you.\nUnlike autoIndex, autoCreate is false by default. You can change this\ndefault by setting mongoose.set('autoCreate', true);\noption: bufferCommands\nBy default, mongoose buffers commands when the connection goes down until\nthe driver manages to reconnect. To disable buffering, set bufferCommands\nto false.\nconst schema = new Schema({..}, { bufferCommands: false });\nThe schema bufferCommands option overrides the global bufferCommands option.\nmongoose.set('bufferCommands', true);\n// Schema option below overrides the above, if the schema option is set.\nconst schema = new Schema({..}, { bufferCommands: false });\noption: bufferTimeoutMS\nIf bufferCommands is on, this option sets the maximum amount of time Mongoose buffering will wait before\nthrowing an error. If not specified, Mongoose will use 10000 (10 seconds).\n// If an operation is buffered for more than 1 second, throw an error.\nconst schema = new Schema({..}, { bufferTimeoutMS: 1000 });\noption: capped\nMongoose supports MongoDBs capped\ncollections. To specify the underlying MongoDB collection be capped, set\nthe capped option to the maximum size of the collection in\nbytes.\nnew Schema({..}, { capped: 1024 });\nThe capped option may also be set to an object if you want to pass\nadditional options like max\nor autoIndexId.\nIn this case you must explicitly pass the size option, which is required.\nnew Schema({..}, { capped: { size: 1024, max: 1000, autoIndexId: true } });\noption: collection\nMongoose by default produces a collection name by passing the model name to\nthe utils.toCollectionName method.\nThis method pluralizes the name. Set this option if you need a different name\nfor your collection.\nconst dataSchema = new Schema({..}, { collection: 'data' });\noption: discriminatorKey\nWhen you define a discriminator, Mongoose adds a path to your\nschema that stores which discriminator a document is an instance of. By default, Mongoose\nadds an __t path, but you can set discriminatorKey to overwrite this default.\nconst baseSchema = new Schema({}, { discriminatorKey: 'type' });\nconst BaseModel = mongoose.model('Test', baseSchema);\n\nconst personSchema = new Schema({ name: String });\nconst PersonModel = BaseModel.discriminator('Person', personSchema);\n\nconst doc = new PersonModel({ name: 'James T. Kirk' });\n// Without `discriminatorKey`, Mongoose would store the discriminator\n// key in `__t` instead of `type`\ndoc.type; // 'Person'\noption: id\nMongoose assigns each of your schemas an id virtual getter by default\nwhich returns the document's _id field cast to a string, or in the case of\nObjectIds, its hexString. If you don't want an id getter added to your\nschema, you may disable it by passing this option at schema construction time.\n// default behavior\nconst schema = new Schema({ name: String });\nconst Page = mongoose.model('Page', schema);\nconst p = new Page({ name: 'mongodb.org' });\nconsole.log(p.id); // '50341373e894ad16347efe01'\n\n// disabled id\nconst schema = new Schema({ name: String }, { id: false });\nconst Page = mongoose.model('Page', schema);\nconst p = new Page({ name: 'mongodb.org' });\nconsole.log(p.id); // undefined\noption: _id\nMongoose assigns each of your schemas an _id field by default if one\nis not passed into the Schema constructor.\nThe type assigned is an ObjectId\nto coincide with MongoDB's default behavior. If you don't want an _id\nadded to your schema at all, you may disable it using this option.\nYou can only use this option on subdocuments. Mongoose can't\nsave a document without knowing its id, so you will get an error if\nyou try to save a document without an _id.\n// default behavior\nconst schema = new Schema({ name: String });\nconst Page = mongoose.model('Page', schema);\nconst p = new Page({ name: 'mongodb.org' });\nconsole.log(p); // { _id: '50341373e894ad16347efe01', name: 'mongodb.org' }\n\n// disabled _id\nconst childSchema = new Schema({ name: String }, { _id: false });\nconst parentSchema = new Schema({ children: [childSchema] });\n\nconst Model = mongoose.model('Model', parentSchema);\n\nModel.create({ children: [{ name: 'Luke' }] }, (error, doc) => {\n// doc.children[0]._id will be undefined\n});\noption: minimize\nMongoose will, by default, \"minimize\" schemas by removing empty objects.\nconst schema = new Schema({ name: String, inventory: {} });\nconst Character = mongoose.model('Character', schema);\n\n// will store `inventory` field if it is not empty\nconst frodo = new Character({ name: 'Frodo', inventory: { ringOfPower: 1 }});\nawait frodo.save();\nlet doc = await Character.findOne({ name: 'Frodo' }).lean();\ndoc.inventory; // { ringOfPower: 1 }\n\n// will not store `inventory` field if it is empty\nconst sam = new Character({ name: 'Sam', inventory: {}});\nawait sam.save();\ndoc = await Character.findOne({ name: 'Sam' }).lean();\ndoc.inventory; // undefined\nThis behavior can be overridden by setting minimize option to false. It\nwill then store empty objects.\nconst schema = new Schema({ name: String, inventory: {} }, { minimize: false });\nconst Character = mongoose.model('Character', schema);\n\n// will store `inventory` if empty\nconst sam = new Character({ name: 'Sam', inventory: {} });\nawait sam.save();\ndoc = await Character.findOne({ name: 'Sam' }).lean();\ndoc.inventory; // {}\nTo check whether an object is empty, you can use the $isEmpty() helper:\nconst sam = new Character({ name: 'Sam', inventory: {} });\nsam.$isEmpty('inventory'); // true\n\nsam.inventory.barrowBlade = 1;\nsam.$isEmpty('inventory'); // false\noption: read\nAllows setting query#read options at the\nschema level, providing us a way to apply default\nReadPreferences\nto all queries derived from a model.\nconst schema = new Schema({..}, { read: 'primary' });            // also aliased as 'p'\nconst schema = new Schema({..}, { read: 'primaryPreferred' });   // aliased as 'pp'\nconst schema = new Schema({..}, { read: 'secondary' });          // aliased as 's'\nconst schema = new Schema({..}, { read: 'secondaryPreferred' }); // aliased as 'sp'\nconst schema = new Schema({..}, { read: 'nearest' });            // aliased as 'n'\nThe alias of each pref is also permitted so instead of having to type out\n'secondaryPreferred' and getting the spelling wrong, we can simply pass 'sp'.\nThe read option also allows us to specify tag sets. These tell the\ndriver from which members\nof the replica-set it should attempt to read. Read more about tag sets\nhere and\nhere.\nNOTE: you may also specify the driver read pref strategy\noption when connecting:\n// pings the replset members periodically to track network latency\nconst options = { replset: { strategy: 'ping' }};\nmongoose.connect(uri, options);\n\nconst schema = new Schema({..}, { read: ['nearest', { disk: 'ssd' }] });\nmongoose.model('JellyBean', schema);\noption: writeConcern\nAllows setting write concern\nat the schema level.\nconst schema = new Schema({ name: String }, {\nwriteConcern: {\nw: 'majority',\nj: true,\nwtimeout: 1000\n}\n});\noption: shardKey\nThe shardKey option is used when we have a sharded MongoDB architecture.\nEach sharded collection is given a shard key which must be present in all\ninsert/update operations. We just need to set this schema option to the same\nshard key and we\u2019ll be all set.\nnew Schema({ .. }, { shardKey: { tag: 1, name: 1 }})\nNote that Mongoose does not send the shardcollection command for you. You\nmust configure your shards yourself.\noption: strict\nThe strict option, (enabled by default), ensures that values passed to our\nmodel constructor that were not specified in our schema do not get saved to\nthe db.\nconst thingSchema = new Schema({..})\nconst Thing = mongoose.model('Thing', thingSchema);\nconst thing = new Thing({ iAmNotInTheSchema: true });\nthing.save(); // iAmNotInTheSchema is not saved to the db\n\n// set to false..\nconst thingSchema = new Schema({..}, { strict: false });\nconst thing = new Thing({ iAmNotInTheSchema: true });\nthing.save(); // iAmNotInTheSchema is now saved to the db!!\nThis also affects the use of doc.set() to set a property value.\nconst thingSchema = new Schema({..})\nconst Thing = mongoose.model('Thing', thingSchema);\nconst thing = new Thing;\nthing.set('iAmNotInTheSchema', true);\nthing.save(); // iAmNotInTheSchema is not saved to the db\nThis value can be overridden at the model instance level by passing a second\nboolean argument:\nconst Thing = mongoose.model('Thing');\nconst thing = new Thing(doc, true);  // enables strict mode\nconst thing = new Thing(doc, false); // disables strict mode\nThe strict option may also be set to \"throw\" which will cause errors\nto be produced instead of dropping the bad data.\nNOTE: Any key/val set on the instance that does not exist in your schema\nis always ignored, regardless of schema option.\nconst thingSchema = new Schema({..})\nconst Thing = mongoose.model('Thing', thingSchema);\nconst thing = new Thing;\nthing.iAmNotInTheSchema = true;\nthing.save(); // iAmNotInTheSchema is never saved to the db\noption: strictQuery\nMongoose supports a separate strictQuery option to avoid strict mode for query filters.\nThis is because empty query filters cause Mongoose to return all documents in the model, which can cause issues.\nconst mySchema = new Schema({ field: Number }, { strict: true });\nconst MyModel = mongoose.model('Test', mySchema);\n// Mongoose will filter out `notInSchema: 1` because `strict: true`, meaning this query will return\n// _all_ documents in the 'tests' collection\nMyModel.find({ notInSchema: 1 });\nThe strict option does apply to updates.\nThe strictQuery option is just for query filters.\n// Mongoose will strip out `notInSchema` from the update if `strict` is\n// not `false`\nMyModel.updateMany({}, { $set: { notInSchema: 1 } });\nMongoose has a separate strictQuery option to toggle strict mode for the filter parameter to queries.\nconst mySchema = new Schema({ field: Number }, {\nstrict: true,\nstrictQuery: false // Turn off strict mode for query filters\n});\nconst MyModel = mongoose.model('Test', mySchema);\n// Mongoose will strip out `notInSchema: 1` because `strictQuery` is false\nMyModel.find({ notInSchema: 1 });\nIn general, we do not recommend passing user-defined objects as query filters:\n// Don't do this!\nconst docs = await MyModel.find(req.query);\n\n// Do this instead:\nconst docs = await MyModel.find({ name: req.query.name, age: req.query.age }).setOptions({ sanitizeFilter: true });\nIn Mongoose 6, strictQuery is equal to strict by default.\nHowever, you can override this behavior globally:\n// Set `strictQuery` to `false`, so Mongoose doesn't strip out non-schema\n// query filter properties by default.\n// This does **not** affect `strict`.\nmongoose.set('strictQuery', false);\noption: toJSON\nExactly the same as the toObject option but only applies when\nthe document's toJSON method is called.\nconst schema = new Schema({ name: String });\nschema.path('name').get(function (v) {\nreturn v + ' is my name';\n});\nschema.set('toJSON', { getters: true, virtuals: false });\nconst M = mongoose.model('Person', schema);\nconst m = new M({ name: 'Max Headroom' });\nconsole.log(m.toObject()); // { _id: 504e0cd7dd992d9be2f20b6f, name: 'Max Headroom' }\nconsole.log(m.toJSON()); // { _id: 504e0cd7dd992d9be2f20b6f, name: 'Max Headroom is my name' }\n// since we know toJSON is called whenever a js object is stringified:\nconsole.log(JSON.stringify(m)); // { \"_id\": \"504e0cd7dd992d9be2f20b6f\", \"name\": \"Max Headroom is my name\" }\nTo see all available toJSON/toObject options, read this.\noption: toObject\nDocuments have a toObject method\nwhich converts the mongoose document into a plain JavaScript object. This\nmethod accepts a few options. Instead of applying these options on a\nper-document basis, we may declare the options at the schema level and have\nthem applied to all of the schema's documents by default.\nTo have all virtuals show up in your console.log output, set the\ntoObject option to { getters: true }:\nconst schema = new Schema({ name: String });\nschema.path('name').get(function(v) {\nreturn v + ' is my name';\n});\nschema.set('toObject', { getters: true });\nconst M = mongoose.model('Person', schema);\nconst m = new M({ name: 'Max Headroom' });\nconsole.log(m); // { _id: 504e0cd7dd992d9be2f20b6f, name: 'Max Headroom is my name' }\nTo see all available toObject options, read this.\noption: typeKey\nBy default, if you have an object with key 'type' in your schema, mongoose\nwill interpret it as a type declaration.\n// Mongoose interprets this as 'loc is a String'\nconst schema = new Schema({ loc: { type: String, coordinates: [Number] } });\nHowever, for applications like geoJSON,\nthe 'type' property is important. If you want to control which key mongoose\nuses to find type declarations, set the 'typeKey' schema option.\nconst schema = new Schema({\n// Mongoose interpets this as 'loc is an object with 2 keys, type and coordinates'\nloc: { type: String, coordinates: [Number] },\n// Mongoose interprets this as 'name is a String'\nname: { $type: String }\n}, { typeKey: '$type' }); // A '$type' key means this object is a type declaration\noption: validateBeforeSave\nBy default, documents are automatically validated before they are saved to\nthe database. This is to prevent saving an invalid document. If you want to\nhandle validation manually, and be able to save objects which don't pass\nvalidation, you can set validateBeforeSave to false.\nconst schema = new Schema({ name: String });\nschema.set('validateBeforeSave', false);\nschema.path('name').validate(function (value) {\nreturn value != null;\n});\nconst M = mongoose.model('Person', schema);\nconst m = new M({ name: null });\nm.validate(function(err) {\nconsole.log(err); // Will tell you that null is not allowed.\n});\nm.save(); // Succeeds despite being invalid\noption: versionKey\nThe versionKey is a property set on each document when first created by\nMongoose. This keys value contains the internal\nrevision\nof the document. The versionKey option is a string that represents the\npath to use for versioning. The default is __v. If this conflicts with\nyour application you can configure as such:\nconst schema = new Schema({ name: 'string' });\nconst Thing = mongoose.model('Thing', schema);\nconst thing = new Thing({ name: 'mongoose v3' });\nawait thing.save(); // { __v: 0, name: 'mongoose v3' }\n\n// customized versionKey\nnew Schema({..}, { versionKey: '_somethingElse' })\nconst Thing = mongoose.model('Thing', schema);\nconst thing = new Thing({ name: 'mongoose v3' });\nthing.save(); // { _somethingElse: 0, name: 'mongoose v3' }\nNote that Mongoose's default versioning is not a full optimistic concurrency\nsolution. Mongoose's default versioning only operates on arrays as shown below.\n// 2 copies of the same document\nconst doc1 = await Model.findOne({ _id });\nconst doc2 = await Model.findOne({ _id });\n\n// Delete first 3 comments from `doc1`\ndoc1.comments.splice(0, 3);\nawait doc1.save();\n\n// The below `save()` will throw a VersionError, because you're trying to\n// modify the comment at index 1, and the above `splice()` removed that\n// comment.\ndoc2.set('comments.1.body', 'new comment');\nawait doc2.save();\nIf you need optimistic concurrency support for save(), you can set the optimisticConcurrency option\nDocument versioning can also be disabled by setting the versionKey to\nfalse.\nDO NOT disable versioning unless you know what you are doing.\nnew Schema({..}, { versionKey: false });\nconst Thing = mongoose.model('Thing', schema);\nconst thing = new Thing({ name: 'no versioning please' });\nthing.save(); // { name: 'no versioning please' }\nMongoose only updates the version key when you use save().\nIf you use update(), findOneAndUpdate(), etc. Mongoose will not\nupdate the version key. As a workaround, you can use the below middleware.\nschema.pre('findOneAndUpdate', function() {\nconst update = this.getUpdate();\nif (update.__v != null) {\ndelete update.__v;\n}\nconst keys = ['$set', '$setOnInsert'];\nfor (const key of keys) {\nif (update[key] != null && update[key].__v != null) {\ndelete update[key].__v;\nif (Object.keys(update[key]).length === 0) {\ndelete update[key];\n}\n}\n}\nupdate.$inc = update.$inc || {};\nupdate.$inc.__v = 1;\n});\noption: optimisticConcurrency\nOptimistic concurrency is a strategy to ensure\nthe document you're updating didn't change between when you loaded it using find() or findOne(), and when\nyou update it using save().\nFor example, suppose you have a House model that contains a list of photos, and a status that represents\nwhether this house shows up in searches. Suppose that a house that has status 'APPROVED' must have at least\ntwo photos. You might implement the logic of approving a house document as shown below:\nasync function markApproved(id) {\nconst house = await House.findOne({ _id });\nif (house.photos.length < 2) {\nthrow new Error('House must have at least two photos!');\n}\n\nhouse.status = 'APPROVED';\nawait house.save();\n}\nThe markApproved() function looks right in isolation, but there might be a potential issue: what if another\nfunction removes the house's photos between the findOne() call and the save() call? For example, the below\ncode will succeed:\nconst house = await House.findOne({ _id });\nif (house.photos.length < 2) {\nthrow new Error('House must have at least two photos!');\n}\n\nconst house2 = await House.findOne({ _id });\nhouse2.photos = [];\nawait house2.save();\n\n// Marks the house as 'APPROVED' even though it has 0 photos!\nhouse.status = 'APPROVED';\nawait house.save();\nIf you set the optimisticConcurrency option on the House model's schema, the above script will throw an\nerror.\nconst House = mongoose.model('House', Schema({\nstatus: String,\nphotos: [String]\n}, { optimisticConcurrency: true }));\n\nconst house = await House.findOne({ _id });\nif (house.photos.length < 2) {\nthrow new Error('House must have at least two photos!');\n}\n\nconst house2 = await House.findOne({ _id });\nhouse2.photos = [];\nawait house2.save();\n\n// Throws 'VersionError: No matching document found for id \"...\" version 0'\nhouse.status = 'APPROVED';\nawait house.save();\noption: collation\nSets a default collation\nfor every query and aggregation. Here's a beginner-friendly overview of collations.\nconst schema = new Schema({\nname: String\n}, { collation: { locale: 'en_US', strength: 1 } });\n\nconst MyModel = db.model('MyModel', schema);\n\nMyModel.create([{ name: 'val' }, { name: 'Val' }]).\nthen(() => {\nreturn MyModel.find({ name: 'val' });\n}).\nthen((docs) => {\n// `docs` will contain both docs, because `strength: 1` means\n// MongoDB will ignore case when matching.\n});\noption: skipVersioning\nskipVersioning allows excluding paths from versioning (i.e., the internal\nrevision will not be incremented even if these paths are updated). DO NOT\ndo this unless you know what you're doing. For subdocuments, include this\non the parent document using the fully qualified path.\nnew Schema({..}, { skipVersioning: { dontVersionMe: true } });\nthing.dontVersionMe.push('hey');\nthing.save(); // version is not incremented\noption: timestamps\nThe timestamps option tells mongoose to assign createdAt and updatedAt fields\nto your schema. The type assigned is Date.\nBy default, the names of the fields are createdAt and updatedAt. Customize\nthe field names by setting timestamps.createdAt and timestamps.updatedAt.\nconst thingSchema = new Schema({..}, { timestamps: { createdAt: 'created_at' } });\nconst Thing = mongoose.model('Thing', thingSchema);\nconst thing = new Thing();\nawait thing.save(); // `created_at` & `updatedAt` will be included\n\n// With updates, Mongoose will add `updatedAt` to `$set`\nawait Thing.updateOne({}, { $set: { name: 'Test' } });\n\n// If you set upsert: true, Mongoose will add `created_at` to `$setOnInsert` as well\nawait Thing.findOneAndUpdate({}, { $set: { name: 'Test2' } });\n\n// Mongoose also adds timestamps to bulkWrite() operations\n// See https://mongoosejs.com/docs/api.html#model_Model.bulkWrite\nawait Thing.bulkWrite([\ninsertOne: {\ndocument: {\nname: 'Jean-Luc Picard',\nship: 'USS Stargazer'\n// Mongoose will add `created_at` and `updatedAt`\n}\n},\nupdateOne: {\nfilter: { name: 'Jean-Luc Picard' },\nupdate: {\n$set: {\nship: 'USS Enterprise'\n// Mongoose will add `updatedAt`\n}\n}\n}\n]);\nBy default, Mongoose uses new Date() to get the current time.\nIf you want to overwrite the function\nMongoose uses to get the current time, you can set the\ntimestamps.currentTime option. Mongoose will call the\ntimestamps.currentTime function whenever it needs to get\nthe current time.\nconst schema = Schema({\ncreatedAt: Number,\nupdatedAt: Number,\nname: String\n}, {\n// Make Mongoose use Unix time (seconds since Jan 1, 1970)\ntimestamps: { currentTime: () => Math.floor(Date.now() / 1000) }\n});\noption: useNestedStrict\nWrite operations like update(), updateOne(), updateMany(),\nand findOneAndUpdate() only check the top-level\nschema's strict mode setting.\nconst childSchema = new Schema({}, { strict: false });\nconst parentSchema = new Schema({ child: childSchema }, { strict: 'throw' });\nconst Parent = mongoose.model('Parent', parentSchema);\nParent.update({}, { 'child.name': 'Luke Skywalker' }, (error) => {\n// Error because parentSchema has `strict: throw`, even though\n// `childSchema` has `strict: false`\n});\n\nconst update = { 'child.name': 'Luke Skywalker' };\nconst opts = { strict: false };\nParent.update({}, update, opts, function(error) {\n// This works because passing `strict: false` to `update()` overwrites\n// the parent schema.\n});\nIf you set useNestedStrict to true, mongoose will use the child schema's\nstrict option for casting updates.\nconst childSchema = new Schema({}, { strict: false });\nconst parentSchema = new Schema({ child: childSchema },\n{ strict: 'throw', useNestedStrict: true });\nconst Parent = mongoose.model('Parent', parentSchema);\nParent.update({}, { 'child.name': 'Luke Skywalker' }, error => {\n// Works!\n});\n\n\noption: selectPopulatedPaths\n\n\nBy default, Mongoose will automatically select() any populated paths for\nyou, unless you explicitly exclude them.\nconst bookSchema = new Schema({\ntitle: 'String',\nauthor: { type: 'ObjectId', ref: 'Person' }\n});\nconst Book = mongoose.model('Book', bookSchema);\n\n// By default, Mongoose will add `author` to the below `select()`.\nawait Book.find().select('title').populate('author');\n\n// In other words, the below query is equivalent to the above\nawait Book.find().select('title author').populate('author');\nTo opt out of selecting populated fields by default, set selectPopulatedPaths\nto false in your schema.\nconst bookSchema = new Schema({\ntitle: 'String',\nauthor: { type: 'ObjectId', ref: 'Person' }\n}, { selectPopulatedPaths: false });\nconst Book = mongoose.model('Book', bookSchema);\n\n// Because `selectPopulatedPaths` is false, the below doc will **not**\n// contain an `author` property.\nconst doc = await Book.findOne().select('title').populate('author');\n\n\noption: storeSubdocValidationError\n\n\nFor legacy reasons, when there is a validation error in subpath of a\nsingle nested schema, Mongoose will record that there was a validation error\nin the single nested schema path as well. For example:\nconst childSchema = new Schema({ name: { type: String, required: true } });\nconst parentSchema = new Schema({ child: childSchema });\n\nconst Parent = mongoose.model('Parent', parentSchema);\n\n// Will contain an error for both 'child.name' _and_ 'child'\nnew Parent({ child: {} }).validateSync().errors;\nSet the storeSubdocValidationError to false on the child schema to make\nMongoose only reports the parent error.\nconst childSchema = new Schema({\nname: { type: String, required: true }\n}, { storeSubdocValidationError: false }); // <-- set on the child schema\nconst parentSchema = new Schema({ child: childSchema });\n\nconst Parent = mongoose.model('Parent', parentSchema);\n\n// Will only contain an error for 'child.name'\nnew Parent({ child: {} }).validateSync().errors;\nWith ES6 Classes\nSchemas have a loadClass() method\nthat you can use to create a Mongoose schema from an ES6 class:\n\nES6 class methods become Mongoose methods\nES6 class statics become Mongoose statics\nES6 getters and setters become Mongoose virtuals\n\nHere's an example of using loadClass() to create a schema from an ES6 class:\nclass MyClass {\nmyMethod() { return 42; }\nstatic myStatic() { return 42; }\nget myVirtual() { return 42; }\n}\n\nconst schema = new mongoose.Schema();\nschema.loadClass(MyClass);\n\nconsole.log(schema.methods); // { myMethod: [Function: myMethod] }\nconsole.log(schema.statics); // { myStatic: [Function: myStatic] }\nconsole.log(schema.virtuals); // { myVirtual: VirtualType { ... } }\nPluggable\nSchemas are also pluggable which allows us to package up reusable features into\nplugins that can be shared with the community or just between your projects.\nFurther Reading\nHere's an alternative introduction to Mongoose schemas.\nTo get the most out of MongoDB, you need to learn the basics of MongoDB schema design.\nSQL schema design (third normal form) was designed to minimize storage costs,\nwhereas MongoDB schema design is about making common queries as fast as possible.\nThe 6 Rules of Thumb for MongoDB Schema Design blog series\nis an excellent resource for learning the basic rules for making your queries\nfast.\nUsers looking to master MongoDB schema design in Node.js should look into\nThe Little MongoDB Schema Design Book\nby Christian Kvalheim, the original author of the MongoDB Node.js driver.\nThis book shows you how to implement performant schemas for a laundry list\nof use cases, including e-commerce, wikis, and appointment bookings.\nNext Up\nNow that we've covered Schemas, let's take a look at SchemaTypes.\n\n\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Moving app to production mode in Symfony 2", "id": 578, "answers": [{"answer_id": 584, "document_id": 303, "question_id": 578, "text": "You might also run into permission issues with the cache directory. I would actually first make sure everything works in development mode on the server before switching to production mode. And if all you get is blank screens in production mode then set debug to true. And of course know how to check your error logs.", "answer_start": 219, "answer_category": null}], "is_impossible": false}], "context": "Can someone help me to move my Symfony 2 application into production mode?\nCurrently, the application is running properly in /app_dev.php.\nI'm googling, but I'm not finding a definite guide for deployment in Symfony 2.\nYou might also run into permission issues with the cache directory. I would actually first make sure everything works in development mode on the server before switching to production mode. And if all you get is blank screens in production mode then set debug to true. And of course know how to check your error logs.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Creating an installer for Linux application", "id": 1169, "answers": [{"answer_id": 1162, "document_id": 746, "question_id": 1169, "text": "1.\tUsing autoconf,\n2.\tUsing CMake,\n3.\tUsing a Java installer like IZPack\n4.\tetc", "answer_start": 326, "answer_category": null}], "is_impossible": false}], "context": "I'm developing a small cross-platform application and I need some advice on how to install it in Linux. I am using InnoSetup in Windows and an application bundle in OSX but I have no idea how to get my app installed in Linux, are there any opensource installer creators for Linux?\nThanks.\n you do have other options, such as:\n1.\tUsing autoconf,\n2.\tUsing CMake,\n3.\tUsing a Java installer like IZPack\n4.\tetc\nI leave the repo up because its full of useful bits, however I quickly gave up on the idea of an install sheild type program for Linux distributions. You'll find that its much, much better to produce installable packages for .deb , .rpm and (possibly) slackware .tgz formats.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "unable to install pip in ubuntu", "id": 1404, "answers": [{"answer_id": 1393, "document_id": 976, "question_id": 1404, "text": "run these commands in a terminal (command prompt) in Ubuntu \n\n\n  which pip\n\n\n# to get the location of the pip file already installed\n\n\n  cp location provided by which pip location where to copy pip file", "answer_start": 1813, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI tried to install pip several times but couldn't. This is how my terminal looks like while trying to install pip:\n\naman@aman-HP-Pavilion-Notebook:~$ sudo apt-get install python-pip\n\nReading package lists... Done\nBuilding dependency tree       \nReading state information... Done\npython-pip is already the newest version.\n0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.\n\naman@aman-HP-Pavilion-Notebook:~$ pip\n\nThe program 'pip' is currently not installed. You can install it by typing:\nsudo apt-get install python-pip\n\naman@aman-HP-Pavilion-Notebook:~$ sudo pip\n\nsudo: pip: command not found\n\naman@aman-HP-Pavilion-Notebook:~$\n\n    \n\nTry with this commands:\n\nsudo apt-get purge --auto-remove python-pip\n\nsudo apt-get update\n\nsudo apt-get -y install python-pip\n\ncurl \"https://bootstrap.pypa.io/get-pip.py\" -o \"get-pip.py\"\n\npython get-pip.py\n\n\nverify:\n\npip --help\n\npip -V\n\n\nThe issue might be package available in Ubuntu 14.04 Universe repository.\n\nIf that doesn't work, download the .deb from here:\n\nhttps://packages.ubuntu.com/trusty/all/python-pip/download\n\nand right click on the .deb and install it\n    \n\nthe problem is probably you have two versions of python 2 installed on your PC in different locations and pip of one of the copies of python got removed somehow. so when you try to install the pip for a copy, system says that there is a pip already installed in another location and also the /usr/local/bin folder is also missing the pip file used to installed pip.\n\ni just solved this issue so i will try to explain it in a way so that beginners also understand it properly\n\ncopy the already installed pip file to the location where it is missing instead of installing it through commands as command will detect the already installed pip.\n\nthe solution is very simple:\n\nrun these commands in a terminal (command prompt) in Ubuntu \n\n\n  which pip\n\n\n# to get the location of the pip file already installed\n\n\n  cp location provided by which pip location where to copy pip file\n\n\nhope it helps.\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How do I target a specific commit SHA with capistrano deploy\uff1f", "id": 577, "answers": [{"answer_id": 583, "document_id": 302, "question_id": 577, "text": "For older versions of Capistrano, you can deploy a particular git commit/tree/branch/tag by doing this:cap -s branch=80655da8d80aaaf92ce5357e7828dc09adb00993 deploy", "answer_start": 250, "answer_category": null}], "is_impossible": false}], "context": "How do I target a specific commit SHA with capistrano deploy\uff1f\nI am wondering how I can target a specific commit SHA in Git for deployment, using Capistrano?\nFor Capistrano 2.9 until 3.0:cap -S revision=80655da8d80aaaf92ce5357e7828dc09adb00993 deploy\nFor older versions of Capistrano, you can deploy a particular git commit/tree/branch/tag by doing this:cap -s branch=80655da8d80aaaf92ce5357e7828dc09adb00993 deploy\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How do I install and use cURL on Windows?", "id": 1564, "answers": [{"answer_id": 1553, "document_id": 1141, "question_id": 1564, "text": "Assuming you got it from https://curl.haxx.se/download.html, just unzip it wherever you want. No need to install. If you are going to use SSL, you need to download the OpenSSL DLLs, available from curl's website.", "answer_start": 349, "answer_category": null}], "is_impossible": false}], "context": "I am having trouble getting cURL to run on Windows.\nI have downloaded a cURL zip file from here, but it seems to contain source code, not an executable.\nDo I need to compile cURL to run it? If yes, then how do I do that?\nWhere can I find .exe downloads for cURL ?\nI have looked for documentation on installing cURL, but there is little to be found.\nAssuming you got it from https://curl.haxx.se/download.html, just unzip it wherever you want. No need to install. If you are going to use SSL, you need to download the OpenSSL DLLs, available from curl's website.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Maven equivalent for python", "id": 1659, "answers": [{"answer_id": 1647, "document_id": 1233, "question_id": 1659, "text": "It's good to use virtualenv to create standalone project environment and use pip/easy_install to management dependencies.", "answer_start": 278, "answer_category": null}], "is_impossible": false}], "context": "I'm a java developer/python beginner, and I'm missing my maven features, particularly dependency management and build automation (I mean you don't build, but how to create a package for deployment?)\nIs there a python equivalent to achieve these features?\nNote: I use python 2.x\nIt's good to use virtualenv to create standalone project environment and use pip/easy_install to management dependencies.\nThanks.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to replace the default logo in NSIS installer?", "id": 833, "answers": [{"answer_id": 828, "document_id": 515, "question_id": 833, "text": "put this anywhere in your .nsi (after inclusion of MUI.nsh):\n!define MUI_ICON path_to_icon_file.ico", "answer_start": 152, "answer_category": null}], "is_impossible": false}], "context": "I would like to replace the default logo in my NSIS installer (see image below), but I cannot find anything about this on the internet. Can it be done? put this anywhere in your .nsi (after inclusion of MUI.nsh):\n!define MUI_ICON path_to_icon_file.ico For the sake of completion, this is the full code I used to change the logo (using Anders method) and the executable icon (using CharlesB method). Also included MUI2.nsh as mentioned by Yuri Korolov.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to build a minimal WiX installer UI without a license page?", "id": 757, "answers": [{"answer_id": 757, "document_id": 444, "question_id": 757, "text": "for the simplest minimal UI:\n2.\tWelcomeDlg\n3.\tInstallation progress\n4.\tExit Dialog", "answer_start": 193, "answer_category": null}], "is_impossible": false}], "context": "I would like to use the WixUI_Minimal installer, but I don't want the license page. How can I do this? See the answer to a related question, WiX script with only Welcome and Completed screens, for the simplest minimal UI:\n2.\tWelcomeDlg\n3.\tInstallation progress\n4.\tExit Dialog\n\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Slow initial server startup when using Phusion Passenger and Rails", "id": 1681, "answers": [{"answer_id": 1668, "document_id": 1254, "question_id": 1681, "text": "You can also use PassengerMinInstances:\nhttp://www.modrails.com/documentation/Users%20guide%20Apache.html#PassengerMinInstances\nThis can be combined with PassengerPreStart", "answer_start": 118, "answer_category": null}], "is_impossible": false}], "context": "To jump on the band-wagon of Phusion Passenger we've setup a staging server for a small rails app to test things out.\nYou can also use PassengerMinInstances:\nhttp://www.modrails.com/documentation/Users%20guide%20Apache.html#PassengerMinInstances\nThis can be combined with PassengerPreStart\n\nSo far it has been very nice to use, it makes installing/configuring and deploying apps a breeze. The problem is the site we're using doesn't get hit very often and it seems to shut down the servers in the background. Meaning when someone goes to the site they have a really long wait until it starts up a new server to handle the request. We've read through the documentation, tried quite a few different set-ups (smart/smart-lv2 modes, passengeridletime etc) and still haven't found a real solution.\nAfter ploughing through Google results we can't really find useful information. Currently we have a cron job that makes a request every-so-often in an attempt to keep the servers running.\nIs anyone else experiencing this problem and do you have any advice for a fix?\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Using Ansible to download a single file from a private github repo to a remote host", "id": 1122, "answers": [{"answer_id": 1115, "document_id": 699, "question_id": 1122, "text": "You should follow the steps:\nSimple and robust - will just work\nDoes not \"contaminate\" production servers with irrelevant files (other configuration files)\nDoes not load production servers with I/O to GitHub (probably negligible)", "answer_start": 227, "answer_category": null}], "is_impossible": false}], "context": "Example scenario: config files for a certain service are kept under version control on a private github repo. I want to write a playbook that fetches one of these files on the remote node and puts it into the desired location.\nYou should follow the steps:\nSimple and robust - will just work\nDoes not \"contaminate\" production servers with irrelevant files (other configuration files)\nDoes not load production servers with I/O to GitHub (probably negligible)\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Removing files when uninstalling WiX", "id": 678, "answers": [{"answer_id": 683, "document_id": 371, "question_id": 678, "text": "Use RemoveFolderEx element from Util extension in WiX", "answer_start": 426, "answer_category": null}], "is_impossible": false}], "context": "When uninstalling my application, I'd like to configure the Wix setup to remove all the files that were added after the original installation. It seems like the uninstaller removes only the directories and files that were originally installed from the MSI file and it leaves everything else that was added later in the application folder. In another words, I'd like to purge the directory when uninstalling. How do I do that? Use RemoveFolderEx element from Util extension in WiX.\nWith this approach, all the subdirectories are also removed (as opposed to using RemoveFile element directly). This element adds temporary rows to RemoveFile and RemoveFolder table in the MSI database.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Deploying application with Python or another embedded scripting language", "id": 1070, "answers": [{"answer_id": 1063, "document_id": 648, "question_id": 1070, "text": "You should link your application to the python library (pythonXX.lib on Windows) and add the following to your main() function.\nPy_NoSiteFlag = 1;  // Disable importing site.py\nPy_Initialize();    // Create a python interpreter", "answer_start": 467, "answer_category": null}], "is_impossible": false}], "context": "I'm thinking about using Python as an embedded scripting language in a hobby project written in C++. I would not like to depend on separately installed Python distribution. Python documentation seems to be quite clear about general usage, but I couldn't find a clear answer to this.\nIs it feasible to deploy a Python interpreter + standard library with my application? Would some other language like Lua, Javascript (Spidermonkey), Ruby, etc. be better for this use?\nYou should link your application to the python library (pythonXX.lib on Windows) and add the following to your main() function.\nPy_NoSiteFlag = 1;  // Disable importing site.py\nPy_Initialize();    // Create a python interpreter\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Django newbie deployment question - ImportError: Could not import settings 'settings'", "id": 46, "answers": [{"answer_id": 48, "document_id": 58, "question_id": 46, "text": "Because a settings file is a Python module, the following apply:\n\nIt doesn\u2019t allow for Python syntax errors.\n\nIt can assign settings dynamically using normal Python syntax.\nFor example:\nMY_SETTING = [str(i) for i in range(30)]", "answer_start": 631, "answer_category": null}], "is_impossible": false}], "context": "\n\n\nDjango settings\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDjango 3.2.9.dev documentation\n\nHome  |\nTable of contents  |\nIndex  |\nModules\n\n\n\u00ab previous\n|\nup\n|\nnext \u00bb\n\n\n\n\n\n\nDjango settings\u00b6\nA Django settings file contains all the configuration of your Django\ninstallation. This document explains how settings work and which settings are\navailable.\n\nThe basics\u00b6\nA settings file is just a Python module with module-level variables.\nHere are a couple of example settings:\nALLOWED_HOSTS = ['www.example.com']\nDEBUG = False\nDEFAULT_FROM_EMAIL = 'webmaster@example.com'\n\n\n\nNote\nIf you set DEBUG to False, you also need to properly set\nthe ALLOWED_HOSTS setting.\n\nBecause a settings file is a Python module, the following apply:\n\nIt doesn\u2019t allow for Python syntax errors.\n\nIt can assign settings dynamically using normal Python syntax.\nFor example:\nMY_SETTING = [str(i) for i in range(30)]\n\n\n\nIt can import values from other settings files.\n\n\n\n\nDesignating the settings\u00b6\n\n\nDJANGO_SETTINGS_MODULE\u00b6\n\nWhen you use Django, you have to tell it which settings you\u2019re using. Do this\nby using an environment variable, DJANGO_SETTINGS_MODULE.\nThe value of DJANGO_SETTINGS_MODULE should be in Python path syntax,\ne.g. mysite.settings. Note that the settings module should be on the\nPython import search path.\n\nThe django-admin utility\u00b6\nWhen using django-admin, you can either set the\nenvironment variable once, or explicitly pass in the settings module each time\nyou run the utility.\nExample (Unix Bash shell):\nexport DJANGO_SETTINGS_MODULE=mysite.settings\ndjango-admin runserver\n\n\nExample (Windows shell):\nset DJANGO_SETTINGS_MODULE=mysite.settings\ndjango-admin runserver\n\n\nUse the --settings command-line argument to specify the settings manually:\ndjango-admin runserver --settings=mysite.settings\n\n\n\n\nOn the server (mod_wsgi)\u00b6\nIn your live server environment, you\u2019ll need to tell your WSGI\napplication what settings file to use. Do that with os.environ:\nimport os\n\nos.environ['DJANGO_SETTINGS_MODULE'] = 'mysite.settings'\n\n\nRead the Django mod_wsgi documentation for more information and other common\nelements to a Django WSGI application.\n\n\n\nDefault settings\u00b6\nA Django settings file doesn\u2019t have to define any settings if it doesn\u2019t need\nto. Each setting has a sensible default value. These defaults live in the\nmodule django/conf/global_settings.py.\nHere\u2019s the algorithm Django uses in compiling settings:\n\nLoad settings from global_settings.py.\nLoad settings from the specified settings file, overriding the global\nsettings as necessary.\n\nNote that a settings file should not import from global_settings, because\nthat\u2019s redundant.\n\nSeeing which settings you\u2019ve changed\u00b6\nThe command python manage.py diffsettings displays differences between the\ncurrent settings file and Django\u2019s default settings.\nFor more, see the diffsettings documentation.\n\n\n\nUsing settings in Python code\u00b6\nIn your Django apps, use settings by importing the object\ndjango.conf.settings. Example:\nfrom django.conf import settings\n\nif settings.DEBUG:\n# Do something\n\n\nNote that django.conf.settings isn\u2019t a module \u2013 it\u2019s an object. So\nimporting individual settings is not possible:\nfrom django.conf.settings import DEBUG  # This won't work.\n\n\nAlso note that your code should not import from either global_settings or\nyour own settings file. django.conf.settings abstracts the concepts of\ndefault settings and site-specific settings; it presents a single interface.\nIt also decouples the code that uses settings from the location of your\nsettings.\n\n\nAltering settings at runtime\u00b6\nYou shouldn\u2019t alter settings in your applications at runtime. For example,\ndon\u2019t do this in a view:\nfrom django.conf import settings\n\nsettings.DEBUG = True   # Don't do this!\n\n\nThe only place you should assign to settings is in a settings file.\n\n\nSecurity\u00b6\nBecause a settings file contains sensitive information, such as the database\npassword, you should make every attempt to limit access to it. For example,\nchange its file permissions so that only you and your Web server\u2019s user can\nread it. This is especially important in a shared-hosting environment.\n\n\nAvailable settings\u00b6\nFor a full list of available settings, see the settings reference.\n\n\nCreating your own settings\u00b6\nThere\u2019s nothing stopping you from creating your own settings, for your own\nDjango apps, but follow these guidelines:\n\nSetting names must be all uppercase.\nDon\u2019t reinvent an already-existing setting.\n\nFor settings that are sequences, Django itself uses lists, but this is only\na convention.\n\n\nUsing settings without setting DJANGO_SETTINGS_MODULE\u00b6\nIn some cases, you might want to bypass the DJANGO_SETTINGS_MODULE\nenvironment variable. For example, if you\u2019re using the template system by\nitself, you likely don\u2019t want to have to set up an environment variable\npointing to a settings module.\nIn these cases, you can configure Django\u2019s settings manually. Do this by\ncalling:\n\n\ndjango.conf.settings.configure(default_settings, **settings)\u00b6\n\nExample:\nfrom django.conf import settings\n\nsettings.configure(DEBUG=True)\n\n\nPass configure() as many keyword arguments as you\u2019d like, with each keyword\nargument representing a setting and its value. Each argument name should be all\nuppercase, with the same name as the settings described above. If a particular\nsetting is not passed to configure() and is needed at some later point,\nDjango will use the default setting value.\nConfiguring Django in this fashion is mostly necessary \u2013 and, indeed,\nrecommended \u2013 when you\u2019re using a piece of the framework inside a larger\napplication.\nConsequently, when configured via settings.configure(), Django will not\nmake any modifications to the process environment variables (see the\ndocumentation of TIME_ZONE for why this would normally occur). It\u2019s\nassumed that you\u2019re already in full control of your environment in these\ncases.\n\nCustom default settings\u00b6\nIf you\u2019d like default values to come from somewhere other than\ndjango.conf.global_settings, you can pass in a module or class that\nprovides the default settings as the default_settings argument (or as the\nfirst positional argument) in the call to configure().\nIn this example, default settings are taken from myapp_defaults, and the\nDEBUG setting is set to True, regardless of its value in\nmyapp_defaults:\nfrom django.conf import settings\nfrom myapp import myapp_defaults\n\nsettings.configure(default_settings=myapp_defaults, DEBUG=True)\n\n\nThe following example, which uses myapp_defaults as a positional argument,\nis equivalent:\nsettings.configure(myapp_defaults, DEBUG=True)\n\n\nNormally, you will not need to override the defaults in this fashion. The\nDjango defaults are sufficiently tame that you can safely use them. Be aware\nthat if you do pass in a new default module, it entirely replaces the Django\ndefaults, so you must specify a value for every possible setting that might be\nused in that code you are importing. Check in\ndjango.conf.settings.global_settings for the full list.\n\n\nEither configure() or DJANGO_SETTINGS_MODULE is required\u00b6\nIf you\u2019re not setting the DJANGO_SETTINGS_MODULE environment\nvariable, you must call configure() at some point before using any code\nthat reads settings.\nIf you don\u2019t set DJANGO_SETTINGS_MODULE and don\u2019t call\nconfigure(), Django will raise an ImportError exception the first time\na setting is accessed.\nIf you set DJANGO_SETTINGS_MODULE, access settings values somehow,\nthen call configure(), Django will raise a RuntimeError indicating\nthat settings have already been configured. There is a property for this\npurpose:\nFor example:\nfrom django.conf import settings\nif not settings.configured:\nsettings.configure(myapp_defaults, DEBUG=True)\n\n\nAlso, it\u2019s an error to call configure() more than once, or to call\nconfigure() after any setting has been accessed.\nIt boils down to this: Use exactly one of either configure() or\nDJANGO_SETTINGS_MODULE. Not both, and not neither.\n\n\nCalling django.setup() is required for \u201cstandalone\u201d Django usage\u00b6\nIf you\u2019re using components of Django \u201cstandalone\u201d \u2013 for example, writing a\nPython script which loads some Django templates and renders them, or uses the\nORM to fetch some data \u2013 there\u2019s one more step you\u2019ll need in addition to\nconfiguring settings.\nAfter you\u2019ve either set DJANGO_SETTINGS_MODULE or called\nconfigure(), you\u2019ll need to call django.setup() to load your\nsettings and populate Django\u2019s application registry. For example:\nimport django\nfrom django.conf import settings\nfrom myapp import myapp_defaults\n\nsettings.configure(default_settings=myapp_defaults, DEBUG=True)\ndjango.setup()\n\n# Now this script or any imported module can use any part of Django it needs.\nfrom myapp import models\n\n\nNote that calling django.setup() is only necessary if your code is truly\nstandalone. When invoked by your Web server, or through django-admin, Django will handle this for you.\n\ndjango.setup() may only be called once.\nTherefore, avoid putting reusable application logic in standalone scripts\nso that you have to import from the script elsewhere in your application.\nIf you can\u2019t avoid that, put the call to django.setup() inside an\nif block:\nif __name__ == '__main__':\nimport django\ndjango.setup()\n\n\n\n\nSee also\n\nThe Settings Reference\nContains the complete list of core and contrib app settings.\n\n\n\n\n\n\n\n\n\n\n\nTable of Contents\n\nDjango settings\nThe basics\nDesignating the settings\nThe django-admin utility\nOn the server (mod_wsgi)\n\n\nDefault settings\nSeeing which settings you\u2019ve changed\n\n\nUsing settings in Python code\nAltering settings at runtime\nSecurity\nAvailable settings\nCreating your own settings\nUsing settings without setting DJANGO_SETTINGS_MODULE\nCustom default settings\nEither configure() or DJANGO_SETTINGS_MODULE is required\nCalling django.setup() is required for \u201cstandalone\u201d Django usage\n\n\n\n\n\nPrevious topic\nSerializing Django objects\nNext topic\nSignals\n\nThis Page\n\nShow Source\n\n\n\nQuick search\n\n\n\n\n\n\n\n\n\n\n\n\nLast update:\nOct 05, 2021\n\n\n\n\n\u00ab previous\n|\nup\n|\nnext \u00bb\n\n\n\n\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How To Compile An Electron Application To A .exe", "id": 1705, "answers": [{"answer_id": 1693, "document_id": 1278, "question_id": 1705, "text": "You need to use Electron Packager.\nInstall it using:\n# for use in npm scripts\nnpm install electron-packager --save-dev\n\n# for use from cli\nnpm install electron-packager -g\nAnd package or deploy using:\nelectron-packager <sourcedir> <appname> --platform=win32 --arch=x86_64", "answer_start": 662, "answer_category": null}], "is_impossible": false}], "context": "I've been learning how to create applications in Electron and I need help compiling a simple project to a Windows executable. The program is a clone from this Github repo: https://github.com/electron/electron-quick-start. On the repo readme it shows how to run the program:\n# Clone this repository\ngit clone https://github.com/electron/electron-quick-start\n# Go into the repository\ncd electron-quick-start\n# Install dependencies\nnpm install\n# Run the app\nnpm start\nThis works fine, but I can't figure out how to simply compile it. I've looked all over google, you would think that something as simple as deploying an application would be well known information.\nYou need to use Electron Packager.\nInstall it using:\n# for use in npm scripts\nnpm install electron-packager --save-dev\n\n# for use from cli\nnpm install electron-packager -g\nAnd package or deploy using:\nelectron-packager <sourcedir> <appname> --platform=win32 --arch=x86_64\nIf you would like to keep it with the Electron Installation, see Application Distribution.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How can I get the latest JRE / JDK as a zip file rather than EXE or MSI installer?", "id": 1557, "answers": [{"answer_id": 1546, "document_id": 1134, "question_id": 1557, "text": "You can download a Java Portable from PortableApps.com. It will not change your system settings. You can put it on your USB stick.", "answer_start": 375, "answer_category": null}], "is_impossible": false}], "context": "I like to be sure that everything will work just by copying the contents of the Java folder and setting the environment variables.\nI usually run the installer in a virtual machine, zip the \\java folder contents, go back to a snapshot of the virtual machine, and then unzip the compressed file.\nI couldn't find a place where the latest JRE / JDK is available as a zip file...\nYou can download a Java Portable from PortableApps.com. It will not change your system settings. You can put it on your USB stick.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "R not finding package even after package installation", "id": 720, "answers": [{"answer_id": 723, "document_id": 410, "question_id": 720, "text": "Do .libPaths(), close every R runing, check in the first directory, remove the zoo package restart R and install zoo again", "answer_start": 0, "answer_category": null}], "is_impossible": false}], "context": "Do .libPaths(), close every R runing, check in the first directory, remove the zoo package restart R and install zoo again. Of course you need to have sufficient rights. When I try uploading the package, I get the error again that the zoo package doesn't exist.\nI have no idea what's happening. I exited the GUI and restarted it, same problem. I have always worked with this package, and I have no idea why this is happening now.\n\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How can I install the Beautiful Soup module on the Mac?", "id": 797, "answers": [{"answer_id": 793, "document_id": 480, "question_id": 797, "text": "pip install beautifulsoup4", "answer_start": 157, "answer_category": null}], "is_impossible": false}], "context": "I read this without finding the solution: http://docs.python.org/install/index.html. I think the current right way to do this is by pip like Pramod comments\npip install beautifulsoup4\nbecause of last changes in Python, see discussion here. This was not so in the past. you install many, many different third-party packages simply by one command at a shell.\n\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Is it possible to answer dialog questions when installing under docker?", "id": 863, "answers": [{"answer_id": 858, "document_id": 543, "question_id": 863, "text": "debconf-set-selections ", "answer_start": 88, "answer_category": null}], "is_impossible": false}], "context": "jammy (1) debconf-set-selections.1.gz\n\nProvided by: debconf_1.5.79_all bug\n\nNAME\n       debconf-set-selections - insert new values into the debconf database\n\nSYNOPSIS\n        debconf-set-selections file\n        debconf-get-selections | ssh newhost debconf-set-selections\n\nDESCRIPTION\n       debconf-set-selections can be used to pre-seed the debconf database with answers, or to\n       change answers in the database. Each question will be marked as seen to prevent debconf\n       from asking the question interactively.\n\n       Reads from a file if a filename is given, otherwise from stdin.\n\nWARNING\n       Only use this command to seed debconf values for packages that will be or are installed.\n       Otherwise you can end up with values in the database for uninstalled packages that will\n       not go away, or with worse problems involving shared values. It is recommended that this\n       only be used to seed the database if the originating machine has an identical install.\n\nDATA FORMAT\n       The data is a series of lines. Lines beginning with a # character are comments. Blank\n       lines are ignored. All other lines set the value of one question, and should contain four\n       values, each separated by one character of whitespace. The first value is the name of the\n       package that owns the question. The second is the name of the question, the third value is\n       the type of this question, and the fourth value (through the end of the line) is the value\n       to use for the answer of the question.\n\n       Alternatively, the third value can be \"seen\"; then the preseed line only controls whether\n       the question is marked as seen in debconf's database. Note that preseeding a question's\n       value defaults to marking that question as seen, so to override the default value without\n       marking a question seen, you need two lines.\n\n       Lines can be continued to the next line by ending them with a \"\\\" character.\n\nEXAMPLES\n        # Force debconf priority to critical.\n        debconf debconf/priority select critical\n\n        # Override default frontend to readline, but allow user to select.\n        debconf debconf/frontend select readline\n        debconf debconf/frontend seen false\n\nOPTIONS\n       --verbose, -v\n           verbose output\n\n       --checkonly, -c\n           only check the input file format, do not save changes to database\n\nSEE ALSO\n       debconf-get-selections(1) (available in the debconf-utils package)\n\nAUTHOR\n       Petter Reinholdtsen <pere@hungry.com>\n\n                                            2021-10-29                  DEBCONF-SET-SELECTIONS(1)", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Close running version of program before installing update (Inno Setup)", "id": 1734, "answers": [{"answer_id": 1721, "document_id": 1306, "question_id": 1734, "text": "you can add an AppMutex value in your Inno Setup installer and it will display a message telling the user to stop the program.", "answer_start": 420, "answer_category": null}], "is_impossible": false}], "context": "This should be simple, I need to stop any previous version of my program from running when the installer starts.\nMost people suggested making an exe which does this and calling it before Inno Setup starts. I created an exe using AutoIt which kills all processes of my program. The problem is I don't know how to get Inno Setup to call it before it installs anything.\nHow do I call an executable before installing files?\nyou can add an AppMutex value in your Inno Setup installer and it will display a message telling the user to stop the program.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "inno setup how to get dynamically path to file", "id": 1919, "answers": [{"answer_id": 1906, "document_id": 1490, "question_id": 1919, "text": "You can get .iss folder by using predefined variable \n\n\n  SourcePath\n\n\nUsage would be like: {#SourcePath}\\???\\bin\\x86\\Release\\???.exe", "answer_start": 162, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI'm making a setup script in Inno and I was wondering, how can I get non \"hardcoded\" path. Here is example:\n\n\n\nThanks in advance!\n\nSOLUTION:\n\nYou can get .iss folder by using predefined variable \n\n\n  SourcePath\n\n\nUsage would be like: {#SourcePath}\\???\\bin\\x86\\Release\\???.exe\n\nThanks all who contributed!\n    \n\nThe reference about the source directory says (emphasized by me):\n\n\n  By default, the Setup Compiler expects to find files referenced in the script's [Files] section Source parameters, and files referenced\n  in the [Setup] section, under the same directory the script file is\n  located if they do not contain fully qualified pathnames. To specify\n  a different source directory, create a SourceDir directive in the\n  script's [Setup] section.\n\n\nThis includes also option to specify relative path to the files. So let's assume that you have the following file structure and you didn't specify a different path in the SourceDir directive:\n\nC:\\Deploy\\Script.iss\nC:\\Deploy\\MyProg.exe\nC:\\Deploy\\SubFolder\\MyOtherProg.exe\nC:\\Folder\\SomeFile.txt\n\n\nNow if you'd like to include the MyProg.exe into the setup compiled from the Script.iss script, you could specify just the file name without the path, since the MyProg.exe file is stored in the same folder as the script, so you could write just:\n\n[Files]\nSource: \"MyProg.exe\"; DestDir: \"{app}\"\n\n\nAnd you can use a relative path to the MyOtherProg.exe which is stored in the subfolder of the folder where the Script.iss script is stored this way:\n\n[Files]\nSource: \"SubFolder\\MyOtherProg.exe\"; DestDir: \"{app}\"\n\n\nAs well as you can use a relative path to include the SomeFile.txt stored in a subfolder of the parent folder where the script is stored:\n\n[Files]\nSource: \"..\\Folder\\SomeFile.txt\"; DestDir: \"{app}\"\n\n\nMore about relative path conventions you can read in this chapter.\n    \n\nLike OP has said in his own question,\n\nYou can get .iss folder by using predefined variable\nSourcePath\nUsage would be like: {#SourcePath}\\???\\bin\\x86\\Release\\???.exe\n\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Good Git deployment using branches strategy with Heroku?", "id": 524, "answers": [{"answer_id": 526, "document_id": 249, "question_id": 524, "text": "In the Gemcutter project we simply have a production branch. Any changes that we want to see on the production site get merged into that branch, and then deployed with:\ngit push heroku production:master", "answer_start": 374, "answer_category": null}], "is_impossible": false}], "context": "What is a good deployment strategy to use with Git + Heroku (Ruby on Rails)?\nCurrently, the way I work with my origin Git repository: All features (or 'stories') are first checked out as branches, then get merged with master and pushed to origin.\nAnything pushed to origin/master triggers a script that pulls the new rails code to the staging area (simple rails webserver).\nIn the Gemcutter project we simply have a production branch. Any changes that we want to see on the production site get merged into that branch, and then deployed with:\ngit push heroku production:master\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Error in loading rgl package with Mac OS X", "id": 1738, "answers": [{"answer_id": 1725, "document_id": 1310, "question_id": 1738, "text": "You should download the XQuartz at: https://www.xquartz.org.", "answer_start": 364, "answer_category": null}], "is_impossible": false}], "context": "I am trying to install rgl package (0.92.858) for R (2.14.2) under Mac OS X (Lion 10.7.3). When I try to load it (library(rgl)), I get the error:\nI have a Mac with Norwegian keyboard. Maybe this has something to do with the issue? However, I get the same error, even though I change the language to U.S. Has anyone experienced similar problem? Any way solving it?\nYou should download the XQuartz at: https://www.xquartz.org.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Python pip install module is not found. How to link python to pip location?", "id": 814, "answers": [{"answer_id": 809, "document_id": 496, "question_id": 814, "text": "you can try to export the PYTHONPATH environment variable:\nexport PYTHONPATH=\"${PYTHONPATH}:/usr/local/lib/python2.7/site-packages:/usr/lib/python2.7/site-packages\"", "answer_start": 86, "answer_category": null}], "is_impossible": false}], "context": "As a quick workaround, and assuming that you are on a bash-like terminal (Linux/OSX), you can try to export the PYTHONPATH environment variable:\nexport PYTHONPATH=\"${PYTHONPATH}:/usr/local/lib/python2.7/site-packages:/usr/lib/python2.7/site-packages\"\nFor Python 2.7\nNotice how its in the site-packages directory in the usr/local/lib.\nAll of my pip installs are being installed in that directory but python does not seem to be picking them up when i try importing them.\nHow do I set the path so that python also looks there as well as core directory?\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How do I update a Python package?", "id": 1565, "answers": [{"answer_id": 1554, "document_id": 1142, "question_id": 1565, "text": "You might want to look into a Python package manager like pip. If you don't want to use a Python package manager, you should be able to download M2Crypto and build/compile/install over the old installation.", "answer_start": 386, "answer_category": null}], "is_impossible": false}], "context": "I'm running Ubuntu 9:10 and a package called M2Crypto is installed (version is 0.19.1). I need to download, build and install the latest version of the M2Crypto package (0.20.2).\nThe 0.19.1 package has files in a number of locations including (/usr/share/pyshared and /usr/lib/pymodules.python2.6).\nHow can I completely uninstall version 0.19.1 from my system before installing 0.20.2?\nYou might want to look into a Python package manager like pip. If you don't want to use a Python package manager, you should be able to download M2Crypto and build/compile/install over the old installation.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How do I install Eclipse with C++ in Ubuntu 12.10 (Quantal Quetzal)?", "id": 1137, "answers": [{"answer_id": 1130, "document_id": 714, "question_id": 1137, "text": "sudo apt-get install eclipse eclipse-cdt g++", "answer_start": 517, "answer_category": null}], "is_impossible": false}], "context": "I just installed Ubuntu 12.10, and I tried to install Eclipse and C++, but I failed miserably.\nI started with an installation from the Software Center, Eclipse worked, but only in Java. Then I started googling for installation guides and tutorials, but after hours of downloads and installations, the C++ in Eclipse still doesn't work.\nThere is a package called eclipse-cdt in the Ubuntu 12.10 repositories, this is what you want. If you haven't got g++ already, you need to install that as well, so all you need is:\nsudo apt-get install eclipse eclipse-cdt g++\nWhether you messed up your system with your previous installation attempts depends heavily on how you did it. If you did it the safe way for trying out new packages not from repositories (i.e., only installed in your home folder, no sudos blindly copied from installation manuals...) you're definitely fine. Otherwise, you may well have thousands of stray files all over your file system now. In that case, run all uninstall scripts you can find for the things you installed, then install using apt-get and hope for the best.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "pip installation usr local opt python bin python2 7 bad interpreter no such f?", "id": 926, "answers": [{"answer_id": 921, "document_id": 577, "question_id": 926, "text": "brew link --overwrite python", "answer_start": 1001, "answer_category": null}], "is_impossible": false}], "context": "How to symlink python in Homebrew?\nAsked 9 years ago\nActive 9 years ago\nViewed 38k times\n\n22\n\n\n9\nFor some reason it's no symlinking when I run `brew link python.' I'm getting the following error and I do what it tells me to do but it's not working. I have tried doing what it tells me to do but maybe I'm not putting the formula_name right. Also, when I do 'which python' it doesn't point to the Homebrew python and I need help fixing that as well.\n\n    Linking /usr/local/Cellar/python/2.7.3...Warning: Could not link python. \n    Unlinking....\n\n   Error: Could not symlink file: /usr/local/Cellar/python/2.7.3/bin/smtpd.py\n   Target /usr/local/bin/smtpd2.py already exists. You may need to delete it.\n   To force the link and delete this file, do:\n      brew link --overwrite formula_name\npython\nsymlink\nhomebrew\nShare\nImprove this question\nFollow\nedited Nov 13 '12 at 1:21\nasked Nov 13 '12 at 1:07\n\nJennifer\n24722 gold badges44 silver badges99 bronze badges\nAdd a comment\n1 Answer\n\n57\n\nDid you try brew link --overwrite python?\n\nShare\nImprove this answer\nFollow\nanswered Nov 13 '12 at 1:35\n\nMichael Mior\n26.6k88 gold badges8282 silver badges110110 bronze badges\nThis worked for me to. Does this problem happen because of a prior python install? \u2013 \nalgorithmicCoder\n Dec 4 '12 at 2:01\nMost likely, yes. If you've installed Python manually Homebrew is smart enough to know and warn you before it destroys anything. \u2013 \nMichael Mior\n Dec 4 '12 at 3:17\n2\nthis doesn't work for me actually. you have to remove each conflicting file eg. smtpd2.py then remove each one that it complains about (idle 2to3 etc.) until they are all gone. then it will do it. --overwrite doesn't have any effect \u2013 \nChris Sattinger\n Apr 1 '13 at 13:45\n1\n@felix - maybe you have permissions issues - chowning /usr/local recursively to your user first would help, and overwrite option would then probably work. \u2013 \nRichVel\n Sep 24 '13 at 11:15\n1\nMaybe first do brew link --overwrite --dry-run python to know beforehand which files will be removed \u2013 \nUrosh T.\n May 30 '20 at 19:24\nShow 1 more comment", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "npm install express error no compatible version found", "id": 1470, "answers": [{"answer_id": 1459, "document_id": 1043, "question_id": 1470, "text": "west version:\n\nsudo add-apt-repository ppa:chris-lea/node.js\nsudo apt-get update\nsudo apt-get install nodejs -y # newer nodejs package includes npm\n\n\nAnd then try again on npm install express.\n    \n\nUpdate your npm\n\nnpm ", "answer_start": 2732, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nMy OS is Ubuntu 13.04, 32 bit.\n\nI am trying to install express with this command:\n\n$ npm install express\n\n\nAnd this is the error I get:\n\nnpm http GET https://registry.npmjs.org/express\nnpm http GET https://registry.npmjs.org/express\nnpm http 304 https://registry.npmjs.org/express\nnpm ERR! Error: No compatible version found: express@'&gt;=4.0.0-0'\nnpm ERR! Valid install targets:\nnpm ERR! [\"0.14.0\",\"0.14.1\",\"1.0.0\",\"1.0.1\",\"1.0.2\",\"1.0.3\",\"1.0.4\",\"1.0.5\",\"1.0.6\",\"1.0.7\",\"1.0.8\",\"2.0.0\",\"2.1.0\",\"2.1.1\",\"2.2.0\",\"2.2.1\",\"2.2.2\",\"2.3.0\",\"2.3.1\",\"2.3.2\",\"2.3.3\",\"2.3.4\",\"2.3.5\",\"2.3.6\",\"2.3.7\",\"2.3.8\",\"2.3.9\",\"2.3.10\",\"2.3.11\",\"2.3.12\",\"2.4.0\",\"2.4.1\",\"2.4.2\",\"2.4.3\",\"2.4.4\",\"2.4.5\",\"2.4.6\",\"2.4.7\",\"2.5.0\",\"2.5.1\",\"2.5.2\",\"2.5.3\",\"2.5.4\",\"2.5.5\",\"2.5.6\",\"2.5.7\",\"2.5.8\",\"2.5.9\",\"2.5.10\",\"2.5.11\",\"3.0.0\",\"3.0.1\",\"3.0.2\",\"3.0.3\",\"3.0.4\",\"3.0.5\",\"3.0.6\",\"3.1.0\",\"3.1.1\",\"3.1.2\",\"3.2.0\",\"3.2.1\",\"3.2.2\",\"3.2.3\",\"3.2.4\",\"3.2.5\",\"3.2.6\",\"3.3.0\",\"3.3.1\",\"3.3.2\",\"3.3.3\",\"3.3.4\",\"3.3.5\",\"3.3.6\",\"1.0.0-beta\",\"1.0.0-beta2\",\"1.0.0-rc\",\"1.0.0-rc2\",\"1.0.0-rc3\",\"1.0.0-rc4\",\"2.0.0-beta\",\"2.0.0-beta2\",\"2.0.0-beta3\",\"2.0.0-rc\",\"2.0.0-rc2\",\"2.0.0-rc3\",\"3.0.0-alpha1\",\"3.0.0-alpha2\",\"3.0.0-alpha3\",\"3.0.0-alpha4\",\"3.0.0-alpha5\",\"3.0.0-beta1\",\"3.0.0-beta2\",\"3.0.0-beta3\",\"3.0.0-beta4\",\"3.0.0-beta6\",\"3.0.0-beta7\",\"3.0.0-rc1\",\"3.0.0-rc2\",\"3.0.0-rc3\",\"3.0.0-rc4\",\"3.0.0-rc5\",\"3.3.7\"]\nnpm ERR!     at installTargetsError (/home/admin/.nodes/0.10.17/lib/node_modules/npm/lib/cache.js:719:10)\nnpm ERR!     at next (/home/admin/.nodes/0.10.17/lib/node_modules/npm/lib/cache.js:698:17)\nnpm ERR!     at /home/admin/.nodes/0.10.17/lib/node_modules/npm/lib/cache.js:675:5\nnpm ERR!     at saved (/home/admin/.nodes/0.10.17/lib/node_modules/npm/node_modules/npm-registry-client/lib/get.js:142:7)\nnpm ERR!     at /home/admin/.nodes/0.10.17/lib/node_modules/npm/node_modules/graceful-fs/polyfills.js:133:7\nnpm ERR!     at Object.oncomplete (fs.js:107:15)\nnpm ERR! If you need help, you may report this log at:\nnpm ERR!     &lt;http://github.com/isaacs/npm/issues&gt;\nnpm ERR! or email it to:\nnpm ERR!     &lt;npm-@googlegroups.com&gt;\n\nnpm ERR! System Linux 3.8.0-29-generic\nnpm ERR! command \"/home/admin/.nodes/current/bin/node\" \"/home/admin/.nodes/current/bin/npm\" \"install\" \"express\"\nnpm ERR! cwd /home/admin/M101JS/Week 2/hw2/hw2-3/blog\nnpm ERR! node -v v0.10.17\nnpm ERR! npm -v 1.3.8\nnpm ERR! \nnpm ERR! Additional logging details can be found in:\nnpm ERR!     /home/admin/M101JS/Week 2/hw2/hw2-3/blog/npm-debug.log\nnpm ERR! not ok code 0\n\n\nI am new to NodeJS and NPM, so I don't understand the response.\nDoes anyone have an idea how I can fix it?\n    \n\nYou want to run the latest node (currently 0.10.22). Try these steps to get the newest version:\n\nsudo add-apt-repository ppa:chris-lea/node.js\nsudo apt-get update\nsudo apt-get install nodejs -y # newer nodejs package includes npm\n\n\nAnd then try again on npm install express.\n    \n\nUpdate your npm\n\nnpm install -g npm\n\n\nThis was a bug in an earlier version of npm: https://github.com/npm/npm/issues/4984\n\nYou may get warnings when you update. Run the update one more time and you should see a clean npm install.\n    \n\nWhile the solution by @Jim worked well, if you feel the need to update NodeJS, you can do so via the n package:\n\nsudo npm cache clean -f\nsudo npm install -g n\nsudo n stable\n\n    \n\nIf you are sitting in corporate network then worth pointing registry to your corporate registry/artifactory and trying. \n\nnpm config set registry \n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Credentials when Installing Windows Service", "id": 705, "answers": [{"answer_id": 709, "document_id": 396, "question_id": 705, "text": "Add this code to your private void InitializeComponent() method in projectInstaller.Designer.cs file in your windows service project.\nthis.serviceProcessInstaller1.Account = System.ServiceProcess.ServiceAccount.LocalSystem;", "answer_start": 576, "answer_category": null}], "is_impossible": false}], "context": "I am attempting to install a C# windows service project using a VisualStudio.Net deployment project.\nTo run the deployment project I right-click and select \"install\" from the context menu, the install wizard runs and eventually prompts me with a \"Set Service Login\" dialog which asks for username & password.\nWhen I install a service using the sc utility from the command line, I don't have to provide credentials.\nDo I have to create a login just for this service? I'd prefer to use \"Local System\" or \"Network Service\" (not sure what the difference is) as other services do.\nAdd this code to your private void InitializeComponent() method in projectInstaller.Designer.cs file in your windows service project.\nthis.serviceProcessInstaller1.Account = System.ServiceProcess.ServiceAccount.LocalSystem;\nThat's what I needed. Thanks! \n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Symfony 2 without SSH access", "id": 1342, "answers": [{"answer_id": 1332, "document_id": 911, "question_id": 1342, "text": "You should:\nCreate a copy of the system somewhere, ideally with identical DB connection params like the production system.\nRun all the necessary tasks with the --env=prod parameter, if your DB settings allow it.\nClone the created production database to the production system (with phpMyAdmin). You can clone the schema from the production database, run app/console doctrine:schema:update --dump-sql locally and then run the generated SQL on the production server.\nCopy all the files, excluding the dirs in app/cache and app/log", "answer_start": 124, "answer_category": null}], "is_impossible": false}], "context": "I have a developed a small web-app in Symfony 2 and Doctrine 2.\nCan i deploy it to a web-host that doesn't give SSH access?\nYou should:\nCreate a copy of the system somewhere, ideally with identical DB connection params like the production system.\nRun all the necessary tasks with the --env=prod parameter, if your DB settings allow it.\nClone the created production database to the production system (with phpMyAdmin). You can clone the schema from the production database, run app/console doctrine:schema:update --dump-sql locally and then run the generated SQL on the production server.\nCopy all the files, excluding the dirs in app/cache and app/log\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Why we use caching?", "id": 630, "answers": [{"answer_id": 635, "document_id": 331, "question_id": 630, "text": "caching is usually just a performance-enhancing tool", "answer_start": 1728, "answer_category": null}], "is_impossible": false}, {"question": "How to clear Stache?", "id": 631, "answers": [{"answer_id": 636, "document_id": 331, "question_id": 631, "text": "To clear the Stache (and refresh any content or settings), you can delete local/storage/framework/cache/stache/ or run php please clear:stache via the command line.", "answer_start": 2948, "answer_category": null}], "is_impossible": false}, {"question": "When should I turn off Stache?", "id": 632, "answers": [{"answer_id": 637, "document_id": 331, "question_id": 632, "text": "You cannot, nor should you want to ever turn it off. ", "answer_start": 3178, "answer_category": null}], "is_impossible": false}, {"question": "How to invalidate System Cache?", "id": 633, "answers": [{"answer_id": 638, "document_id": 331, "question_id": 633, "text": "If you find the need to invalidate this cache for some reason, you can wipe the contents of the local/storage/framework/cache folder or run php please clear:cache. ", "answer_start": 3925, "answer_category": null}], "is_impossible": false}, {"question": "How to turn off System Cache?", "id": 634, "answers": [{"answer_id": 639, "document_id": 331, "question_id": 634, "text": "Nope", "answer_start": 4142, "answer_category": null}], "is_impossible": false}, {"question": "What is Blink and Flash?", "id": 635, "answers": [{"answer_id": 640, "document_id": 331, "question_id": 635, "text": "Blink and Flash are two types of Request-based caches used by Statamic and addons", "answer_start": 4164, "answer_category": null}], "is_impossible": false}, {"question": "What does Blink do?", "id": 636, "answers": [{"answer_id": 641, "document_id": 331, "question_id": 636, "text": "Blink stores data for the remainder of the current request, allowing other features, tags, or template logic to reuse that data for performance gains", "answer_start": 4247, "answer_category": null}], "is_impossible": false}, {"question": "What does Flash do?", "id": 637, "answers": [{"answer_id": 642, "document_id": 331, "question_id": 637, "text": "Flash sets data for one-time use in the next request, like success messages and whatnot", "answer_start": 4438, "answer_category": null}], "is_impossible": false}, {"question": "How to invalidate Blink and Flash?", "id": 638, "answers": [{"answer_id": 643, "document_id": 331, "question_id": 638, "text": "Refresh the page. It\u2019s gone. And maybe even back again, who knows? We can\u2019t tell from here", "answer_start": 4550, "answer_category": null}], "is_impossible": false}, {"question": "What does Half Measure do?", "id": 698, "answers": [{"answer_id": 702, "document_id": 331, "question_id": 698, "text": "run every request through the full Statamic bootstrapping process but will serve all request data from a cache, speeding up load times often by half or more", "answer_start": 6245, "answer_category": null}], "is_impossible": false}], "context": "\n\n\n\n\n\n\n\nCaching\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDocumentation\n\nGetting Started\n\nInstalling\nUpdating\nMigrating from v1\nStatamic Tour\nSettings\nLicensing\n\n\nContent Types\n\nOverview\nPages\nCollections\nTaxonomies\nGlobals\nAssets\nUsers\n\n\nURLs and Routing\n\nHow URLs Work\nThe Cascade\nRouting\n\n\nTemplates & Tags\n\nAntlers\nTheming\nTags\u2026\nVariables\u2026\nModifiers\u2026\nConditions\nForms\nSearch\n\n\nControl Panel\n\nOverview\nFieldsets\nFieldtypes\u2026\nWidgets\u2026\n\n\nWorking With Data\n\nYAML\n\nLocalization\nCaching\nDebugging\n\n\n\nUsers & Auth\n\nUsers\nPermissions\nOAuth\nProtecting Content\nDatabase Driver\n\n\nOther Things\n\n\nEmail\nEnvironments\nKnowledge Base\nTesting\n\n\nAddons\n\nBuilding Addons\u2026\nAPI Reference\u2026\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhy Statamic?\nPricing\nDocs\nMarketplace\nForum\nTry for Free\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDocumentation\n\nGetting Started\n\nInstalling\nUpdating\nMigrating from v1\nStatamic Tour\nSettings\nLicensing\n\n\nContent Types\n\nOverview\nPages\nCollections\nTaxonomies\nGlobals\nAssets\nUsers\n\n\nURLs and Routing\n\nHow URLs Work\nThe Cascade\nRouting\n\n\nTemplates & Tags\n\nAntlers\nTheming\nTags\u2026\nVariables\u2026\nModifiers\u2026\nConditions\nForms\nSearch\n\n\nControl Panel\n\nOverview\nFieldsets\nFieldtypes\u2026\nWidgets\u2026\n\n\nWorking With Data\n\nYAML\n\nLocalization\nCaching\nDebugging\n\n\n\nUsers & Auth\n\nUsers\nPermissions\nOAuth\nProtecting Content\nDatabase Driver\n\n\nOther Things\n\n\nEmail\nEnvironments\nKnowledge Base\nTesting\n\n\nAddons\n\nBuilding Addons\u2026\nAPI Reference\u2026\n\n\n\n\n\n\nCaching\nA flat-file CMS wouldn't be worth its salt without a few different caching mechanisms. In this guide we peel back the various layers like an onion and make at least one Shrek\u00a0joke.\n\nAn introduction to flat-file caching\nIn most \u201ctraditional\u201d applications (namely those involving a relational database for content and data storage), caching is usually just a performance-enhancing tool. Statamic on the other hand uses a caching layer as a data-source, much in the way a Redis or NoDB application might. We call this layer the \u201cStache\u201d.\nThe Stache Datastore\nIf Statamic were to search through and read all of your content and settings files every single request, you would probably rage quit the internet or (\u256f\u00b0\u25a1\u00b0\uff09\u256f\ufe35 \u253b\u2501\u253b), unless your site was very small.\nStatamic watches for changes to your content and settings and compiles a number of ephemeral data structures that are used to power your website not unlike an API. It is this Stache that allows fast content querying, relationships, routing, search indexing, URL traversing, and really everything else that makes Statamic useful. Think of your content as the permanence layer. As long as it exists, the Stache can always be rebuilt.\nHow is it invalidated?\nSince the Stache is temporary and self-replicating, you can delete it to invalidate it or any content inside it anytime if you have need. Statamic will do this automatically when we detect changes to files or explicitly do so from the Control Panel, but since you can customize this behavior, it\u2019s good for you to know how to do it yourself.\nTo clear the Stache (and refresh any content or settings), you can delete local/storage/framework/cache/stache/ or run php please clear:stache via the command line.\nCan I turn it off?\nIt is always enabled and is powered by magic. You cannot, nor should you want to ever turn it off. If you were Aquaman, would you give away your powers? Didn\u2019t think so. Everyone wants to talk to fish.\nSystem Cache\nThe system cache is used by Statamic and addons to store data for defined lengths of time, much like sessions or cookies might do on the browser side. It uses Laravel\u2019s cache API. As an example, the Image Transform feature uses this cache to store all the manipulated images and their various sizes.\nHow is it invalidated?\nEach item in the system cache invalidates itself over time. Some features, like the Search Index Throttle, can have configured lengths of time, and others pre-set. It\u2019s highly unlikely you\u2019ll have to deal with this directly unless you\u2019re building addons.\nIf you find the need to invalidate this cache for some reason, you can wipe the contents of the local/storage/framework/cache folder or run php please clear:cache. It will rebuild itself as needed.\nCan I turn it off?\nNope.\nBlink and Flash\nBlink and Flash are two types of Request-based caches used by Statamic and addons.\nBlink stores data for the remainder of the current request, allowing other features, tags, or template logic to reuse that data for performance gains. It\u2019s similar in concept to SQL caching.\nFlash sets data for one-time use in the next request, like success messages and whatnot.\nHow is it invalidated?\nRefresh the page. It\u2019s gone. And maybe even back again, who knows? We can\u2019t tell from here.\nTemplate Fragments\nThere are times when you may want to simply cache a section of a template to gain some performance. That\u2019s what the Cache Tag is for. Wrap your markup in &lbrace&lbrace cache &rbrace&rbrace tags and you\u2019re off on your way to a snappier, zippier, peppier site.\n&lbrace&lbrace cache for=\"1 hour\" &rbrace&rbrace\n<!-- SO MUCH STUFF HAPPENING HERE DONKEY!\n...\n1,000 lines later...\n-->\n&lbrace&lbrace /cache &rbrace&rbrace\n\nCan I turn it off?\nIf you want to disable all your cache tags, you can set cache_tags_enabled: false in site/settings/caching.yaml.\nThis could be useful during development or for debugging.\nHow is it invalidated?\nIf you specify the cache length, it\u2019ll invalidate itself after that length of time.\nLeave it off, and it\u2019ll remain cached until you manually clear your cache or change the template between the tags.\nStatic Caching\nNow we get to the performance part of the show. There is absolutely nothing faster on the web than static pages (except static pages without javascript and big giant header images, of course). And to that end, Statamic can cache static pages and pass off routing to Apache or Nginx through reverse proxying. It sounds much harder than is.\nCertain features, like forms with server-side validation, don\u2019t work with static page caching. As long as you understand that, you can leverage this feature for literally the fastest sites possible. Let\u2019s take a look!\nThere are 2 stages of Static caching. Half Measure, and Full Measure.\nHalf Measure\nHead to System \u00bb Caching or site/settings/caching.yaml and turn on Static Caching. This will still run every request through the full Statamic bootstrapping process but will serve all request data from a cache, speeding up load times often by half or more. This is an easy, one-and-done setting.\nstatic_caching_enabled: true\n\nFull Measure\nStep one: Head to System \u00bb Caching or site/settings/caching.yaml and turn on Static Caching and set the type to File. This will start generating full static HTML pages in a static/ directory in your webroot.\nstatic_caching_enabled: true\nstatic_caching_type: file\n\nStep two: Enable the rewrite rule for whichever server you\u2019re running. It\u2019s commented out in each of the sample server config files included with Statamic.\nApache:\nRewriteCond %&lbraceDOCUMENT_ROOT&rbrace/static/%&lbraceREQUEST_URI&rbrace_%&lbraceQUERY_STRING&rbrace\\.html -s\nRewriteCond %&lbraceREQUEST_METHOD&rbrace GET\nRewriteRule .* static/%&lbraceREQUEST_URI&rbrace_%&lbraceQUERY_STRING&rbrace\\.html [L,T=text/html]\n\nNginx:\nlocation / &lbrace\ntry_files /static$&lbraceuri&rbrace_$&lbraceargs&rbrace.html $uri /index.php?$args;\n&rbrace\n\nIIS:\n<rule name=\"Static Caching\" stopProcessing=\"true\">\n<match url=\"^(.*)\"  />\n<action type=\"Rewrite\" url=\"/static/&lbraceR:1&rbrace_&lbraceQUERY_STRING&rbrace.html\"  />\n</rule>\n\nStep three: Watch your site fly!\n\nQuery strings\nBy default, static caching will include any query strings in the URL. For example, visiting /about and /about?something\nwill result in two different pages. This is useful for being able to cache query string dependent pages, like paginated sections. However, it would also cause the cache being broken by someone appending ?whatever to the URL.\nTo disable pagination, set static_caching_ignore_query_strings: true. If you\u2019re using full measure static caching, you will need to adjust your rewrite rules to ignore the query.\nFor example:\n\nRemove %&lbraceQUERY_STRING&rbrace from htaccess if using Apache\nRemove $&lbraceargs&rbrace from your config if using NGINX\nRemove &lbraceQUERY_STRING&rbrace from web.config if using IIS\n\nIf you require to cache pages with query strings, you may do so in site/settings/caching.yaml\nstatic_caching_ignore_query_strings: false\n\nExcluding pages\nYou may add a list of URLs you wish to exclude from being cached. You may want to exclude pages that need to always be dynamic, such\nas forms and listings with sort=\"random\".\nstatic_caching_exclude:\n- /contact\n\nWildcards may be used at the end of URLs.\nstatic_caching_exclude:\n- /blog/*  # Excludes /blog/post-name, but not /blog\n- /news*   # Exclude /news, /news/article, and /newspaper\n\nNote: Query strings will be omitted from exclusion rules automatically, regardless of whether wildcards are used.\nFor example, choosing to ignore /blog will also ignore /blog?page=2, etc.\nInvalidation\nTime limit\nWhen using half-measure, you\u2019re able to set the number of minutes before the cached pages automatically expire.\nstatic_caching_default_cache_length: 5\n\nFor full-measure, since the generated html files will be served before PHP ever gets hit, the cache length option is not available.\nWhen saving\nWhen saving content, the corresponding item\u2019s URL will be flushed from the static cache automatically.\nYou may also set specific rules for invalidating other pages when content is saved. For example:\nstatic_caching_invalidation:\ncollections:\nblog:\nurls:\n- /blog\n- /blog/category/*\n- /\ntaxonomies:\ntags:\nurls:\n- /blog\n- /blog/category/*\n- /\npages:\n/about:\nurls:\n- *\n\nThis says:\n\n\u201cwhen an entry in the blog collection is saved, we should invalidate the /blog page, any pages beginning with /blog/category/, and the home page.\u201d\n\u201cwhen a term in the tags taxonomy is saved, we should invalidate those same pages\u201d\n\u201cwhen the /about page is saved, invalidate all pages\u201d\n\nOf course, you may add as many collections, taxonomies, and pages as you need.\nYou may also choose to invalidate the entire static cache by specifying all.\nstatic_caching_invalidation: all\n\nBy force\nTo clear the static file cache you can run php please clear:static (and/or delete the appropriate static file locations).\nFile Locations\nWhen using full measure caching, the static html files are stored in the static directory.\nYou may customize this by changing the static_caching_file_path variable.\nstatic_caching_file_path: static_files\n\nYou will need to update your appropriate server rewrite rules.\nLocalization\nWhen using full measure caching combined with localization, you may need to save the html files within each appropriate locale\u2019s webroot.\nYou may do this by placing an array in the file locations with the various locales.\nstatic_caching_file_path:\nen: static\nfr: fr/static\n\n\n\nLast modified on July 8, 2019\n\nImprove Content\n\n\n\n\n\nTable of Contents\n\nAn introduction to flat-file caching\nThe Stache Datastore\nSystem Cache\nTemplate Fragments\nStatic Caching\n\n\n\n\n\n\n\nStatamic\n\nBlog\nPricing\nChangelog\nLegacy v1\n\n\n\nDevelopers\n\nDocumentation\nMarketplace\nCommunity\nDiscord Chat\nForum\nScreencasts\n\n\n\nCompany\n\nAbout\nPartners\nTestimonials\nContact\n\n\n\nResources\n\nShowcase\nSupport\nPrivacy\nTerms\nLicense\n\n\n\nJoin the Newsletter\nWe send a monthly email with announcements, tips, news, popular addons, and other important info.\n\n\n\nJoin\n\n\n\n\n\n\n\n\n\n\n\n\u00a9Statamic 2020\nBuilt by the team at Wilderborn since 2012.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Microsoft SQL Server stuck at \"Install_SQLSupport_CPU64_Action\"", "id": 1787, "answers": [{"answer_id": 1773, "document_id": 1359, "question_id": 1787, "text": "The solution: You will need to browse to this installation path: C:\\SQLServer2017Media\\<YOUR_SQL_ENU>\\1033_ENU_LP\\x64\\Setup\nThen while the setup is stuck at \u201cInstall_SQLSupport_CPU64_Action\u201d run SQLSUPPORT.msi", "answer_start": 218, "answer_category": null}], "is_impossible": false}], "context": "While installing SQL Server 2017 Developer Edition, I got stuck at \"Install_SQLSupport_CPU64_Action\", this happened to me for the second time, once at work and once at home.\nAfter searching online I found no solution.\nThe solution: You will need to browse to this installation path: C:\\SQLServer2017Media\\<YOUR_SQL_ENU>\\1033_ENU_LP\\x64\\Setup\nThen while the setup is stuck at \u201cInstall_SQLSupport_CPU64_Action\u201d run SQLSUPPORT.msi\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How do you maintain development code and production code?", "id": 143, "answers": [{"answer_id": 151, "document_id": 89, "question_id": 143, "text": "These days, the question would be seen in a context using Git, and 10 years of using that distributed development workflow (collaborating mainly through GitHub) shows the general best practices.", "answer_start": 238, "answer_category": null}], "is_impossible": false}], "context": "What are the best practices and rules-of-thumb to follow while maintaining code? Is it good practice to have only the production ready code in the development branch, or should untested latest code be available in the development branch?\nThese days, the question would be seen in a context using Git, and 10 years of using that distributed development workflow (collaborating mainly through GitHub) shows the general best practices.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Execute a Python script post install using distutils / setuptools", "id": 1196, "answers": [{"answer_id": 1189, "document_id": 772, "question_id": 1196, "text": "The way to address these deficiences is:\n1.\tGet the full path to the Python interpreter executing setup.py from sys.executable.\n2.\tClasses inheriting from distutils.cmd.Command", "answer_start": 294, "answer_category": null}], "is_impossible": false}], "context": "I'm trying to add a post-install task to Python distutils as described in How to extend distutils with a simple post install script?. The task is supposed to execute a Python script in the installed lib directory. This script generates additional Python modules the installed package requires.\nThe way to address these deficiences is:\n1.\tGet the full path to the Python interpreter executing setup.py from sys.executable.\n2.\tClasses inheriting from distutils.cmd.Command\nNote however that the --dry-run option is currently broken and does not work as intended anyway.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "ReactJS, Express application self hosting on internal dedicated IP server", "id": 1330, "answers": [{"answer_id": 1320, "document_id": 899, "question_id": 1330, "text": "Just had a look at the following link which contains most of the details related to most common server setup practice. Hopefully this will answer the question related to the server environment setup.\nhttps://www.digitalocean.com/community/tutorials/5-common-server-setups-for-your-web-application\nIn the Related Articles Section (at end of the above article) there are lots of information on setting on NodeJS appliction on Ubunto etc. Hopefully the discussion there will clarify the concepts in more depth. e.g. How To Deploy a Node.js and MongoDB Application with Rancher on Ubuntu 16.04", "answer_start": 205, "answer_category": null}], "is_impossible": false}], "context": "I know we can host our ReactJS application on Amazon, Microsoft Azure, Heroku etc. But what are the important steps and security precautions required to do in order to setup on an internal hosting server.\nJust had a look at the following link which contains most of the details related to most common server setup practice. Hopefully this will answer the question related to the server environment setup.\nhttps://www.digitalocean.com/community/tutorials/5-common-server-setups-for-your-web-application\nIn the Related Articles Section (at end of the above article) there are lots of information on setting on NodeJS appliction on Ubunto etc. Hopefully the discussion there will clarify the concepts in more depth. e.g. How To Deploy a Node.js and MongoDB Application with Rancher on Ubuntu 16.04\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to update an installed Windows service?", "id": 1231, "answers": [{"answer_id": 1224, "document_id": 807, "question_id": 1231, "text": "you can use a framework such as Google Omaha. This is the technology which Google use to update Chrome", "answer_start": 384, "answer_category": null}], "is_impossible": false}], "context": "I have written a Windows service in C#.\nI have since installed it on my machine, and it runs just fine.\nWhen you install a service, does the exe get copied somewhere? Or does it point to my bin folder?\nThis is for me to know that when I update my code from time to time, do I have to uninstall and re-install my service to update it?\nIf you want to update your Service automatically, you can use a framework such as Google Omaha. This is the technology which Google use to update Chrome. It works well with Services because it runs silently in the background, just like a Service. This article gives more information about using Omaha to auto-update a Service.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Git command to checkout any branch and overwrite local changes", "id": 1691, "answers": [{"answer_id": 1679, "document_id": 1264, "question_id": 1691, "text": "git fetch --all\ngit reset --hard origin/abranch\ngit checkout abranch", "answer_start": 576, "answer_category": null}], "is_impossible": false}], "context": "Is there a Git command (or a short sequence of commands) that will safely and surely do the following?\n\u2022\tGet rid of any local changes.\n\u2022\tFetch the given branch from origin if necessary\n\u2022\tCheckout the given branch?\nCurrently I'm stuck with:\ngit fetch -p\ngit stash\ngit stash drop\ngit checkout $branch\ngit pull\nbut it's bothering me because I'm asked for password two times (by fetch and pull). Generally I would be happy with any solution as long as the password is needed only once.\nYou could follow a solution similar to \"How do I force \u201cgit pull\u201d to overwrite local files?\":\ngit fetch --all\ngit reset --hard origin/abranch\ngit checkout abranch \nThat would involve only one fetch.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Is it possible to include subdirectories using dist utils (setup.py) as part of package data?", "id": 1232, "answers": [{"answer_id": 1225, "document_id": 808, "question_id": 1232, "text": "You'll have to use a MANIFEST.in file for that.\nI believe you'll want something like this:\n$ cat MANIFEST.in\nrecursive-include examples/ *.py", "answer_start": 225, "answer_category": null}], "is_impossible": false}], "context": " my project always has people adding examples and I want it to be easy to list them from within my application. I can get it work for any FILE within examples, but not re-curse down through sub-directories. Is this possible? You'll have to use a MANIFEST.in file for that.\nI believe you'll want something like this:\n$ cat MANIFEST.in\nrecursive-include examples/ *.py\nI came across this post and spent some time figuring out how to add specific sub-modules to my package, so I will post my solution here.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to automatically install Emacs packages by specifying a list of package names?", "id": 1566, "answers": [{"answer_id": 1555, "document_id": 1143, "question_id": 1566, "text": "Emacs 25.1+ will automatically keep track of user-installed packages in the customizable package-selected-packages variable. package-install will update the customize variable, and you can install all selected packages with the package-install-selected-packages function.", "answer_start": 351, "answer_category": null}], "is_impossible": false}], "context": "I am using package to manage my Emacs extensions. In order to synchronize my Emacs settings on different computers, I'd like a way to specify a list of package names in .emacs file and then package could automatically search and install the packages, so that I don't need to install them manually by calling M-x package-list-packages. How to do that?\nEmacs 25.1+ will automatically keep track of user-installed packages in the customizable package-selected-packages variable. package-install will update the customize variable, and you can install all selected packages with the package-install-selected-packages function.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Exclude all transitive dependencies of a single dependency", "id": 1833, "answers": [{"answer_id": 1819, "document_id": 1404, "question_id": 1833, "text": "For maven2 there isn't a way to do what you describe. For maven 3, there is. If you are using maven 3 please see another answer for this question: https://stackoverflow.com/questions/547805/exclude-all-transitive-dependencies-of-a-single-dependency/7556707#7556707", "answer_start": 291, "answer_category": null}], "is_impossible": false}], "context": "In Maven2, to exclude a single transitive dependency.\t\nThe problem with this approach is that I have to do this for every transitive dependency contributed by sample-artifactB.\nIs there a way to use some sort of wildcard to exclude all transitive dependencies at once instead of one-by-one?\nFor maven2 there isn't a way to do what you describe. For maven 3, there is. If you are using maven 3 please see another answer for this question: https://stackoverflow.com/questions/547805/exclude-all-transitive-dependencies-of-a-single-dependency/7556707#7556707\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "npm install from github in specific environment", "id": 1958, "answers": [{"answer_id": 1944, "document_id": 1540, "question_id": 1958, "text": "Check out https://www.npmjs.org/doc/cli/npm-shrinkwrap.html#Building-shrinkwrapped-packages for more info on shrink-wrapping.\n    \n\nYou can use two different package.json files, one for staging and one for development and production. These files can be stored on the corresponding SCM branches.", "answer_start": 1071, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nMy team uses a private npm registry. We install packages from this private registry when developing locally and when deploying to production. When deploying to staging, however, we'd like to pull from GitHub instead. I thought this would be possible using a preinstall script which rewrites package.json to use the appropriate git+ssh:// URLs in place of the version numbers if NODE_ENV=staging.\n\nThis appears not to work, possibly because npm ignores the changes made to package.json (having already required it).\n\nPerhaps I'm going about this in completely the wrong way. What is the recommended way of achieving this?\n    \n\nYou could try running npm-install, which will resolve and install all of your dependencies, then npm-shrinkwrap, which will generate the file npm-shrinkwrap.json.\n\nYou can run your pre-install script on npm-shrinkwrap.json and insert 'git+ssh://' URL's as needed, which already has all of the resolved dependencies you need. This should give you a shrink-wrapped package.json file which points to your github repositories.\n\nCheck out https://www.npmjs.org/doc/cli/npm-shrinkwrap.html#Building-shrinkwrapped-packages for more info on shrink-wrapping.\n    \n\nYou can use two different package.json files, one for staging and one for development and production. These files can be stored on the corresponding SCM branches.\n\nYou can specify dependencies for npm in many ways.\n\nGit URL\n\n{\n  \"dependencies\": {\n    \"private_dependency\": git://user@hostname:project.git#ref\n  }\n}\n\n\nSimply supply the URL to the dependency. #ref is a git reference. If you omit this it will default to master.\n\nGitHub URL\n\n{\n  \"dependencies\": {\n    \"private_dependency\": user/project\n  }\n}\n\n\nTarball URL\n\n{\n  \"dependencies\": {\n    \"private_dependency\": example.com/tarball.tar.gz\n  }\n}\n\n\nFor more information about supported formats see here.\n    \n\nHaving a setup similar to yours, we are using roco to run various tasks such as old-tags-cleanup, test-run, etc. If configured properly, it will come down to roco staging deploy and roco production deploy :)\n    \n\nFor private repository you have to put it under dependencies.\n\n{\n    \"private\": true\n    \"name\": \"foo\",\n    \"dependencies\": {\n        \"private-repo\": \"git+ssh://gitolite@my.server:my/project\",\n    }\n    [...]\n}\n\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Facebook login - how to develop on both localhost and in production?", "id": 519, "answers": [{"answer_id": 521, "document_id": 244, "question_id": 519, "text": "There is a better way. You just need to add valid callback URL's for your localhost to Settings > Advanced > OAuth Settings.\nThis method allows you still have your production website URL while allowing oauth from dev, staging, production, etc.", "answer_start": 379, "answer_category": null}], "is_impossible": false}], "context": "I am developing a website which uses the Facebook login. Now, I want to be able to get this to work both in my production environment, as well as in my development environment.\nOn Facebook, I can give ONE site url, which Facebook can redirect to. This worked great during my development phase, but now I want it both to work in production, but also while developing my solution.\nThere is a better way. You just need to add valid callback URL's for your localhost to Settings > Advanced > OAuth Settings.\nThis method allows you still have your production website URL while allowing oauth from dev, staging, production, etc.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "pip install failing with oserror errno 13 permission denied on directory?", "id": 901, "answers": [{"answer_id": 896, "document_id": 567, "question_id": 901, "text": "pip install --user -r requirements.txt", "answer_start": 4005, "answer_category": null}], "is_impossible": false}], "context": "138\n\n\n17\npip install -r requirements.txt fails with the exception below OSError: [Errno 13] Permission denied: '/usr/local/lib/.... What's wrong and how do I fix this? (I am trying to setup Django)\n\nInstalling collected packages: amqp, anyjson, arrow, beautifulsoup4, billiard, boto, braintree, celery, cffi, cryptography, Django, django-bower, django-braces, django-celery, django-crispy-forms, django-debug-toolbar, django-disqus, django-embed-video, django-filter, django-merchant, django-pagination, django-payments, django-storages, django-vote, django-wysiwyg-redactor, easy-thumbnails, enum34, gnureadline, idna, ipaddress, ipython, kombu, mock, names, ndg-httpsclient, Pillow, pyasn1, pycparser, pycrypto, PyJWT, pyOpenSSL, python-dateutil, pytz, requests, six, sqlparse, stripe, suds-jurko\nCleaning up...\nException:\nTraceback (most recent call last):\n  File \"/usr/lib/python2.7/dist-packages/pip/basecommand.py\", line 122, in main\n    status = self.run(options, args)\n  File \"/usr/lib/python2.7/dist-packages/pip/commands/install.py\", line 283, in run\n    requirement_set.install(install_options, global_options, root=options.root_path)\n  File \"/usr/lib/python2.7/dist-packages/pip/req.py\", line 1436, in install\n    requirement.install(install_options, global_options, *args, **kwargs)\n  File \"/usr/lib/python2.7/dist-packages/pip/req.py\", line 672, in install\n    self.move_wheel_files(self.source_dir, root=root)\n  File \"/usr/lib/python2.7/dist-packages/pip/req.py\", line 902, in move_wheel_files\n    pycompile=self.pycompile,\n  File \"/usr/lib/python2.7/dist-packages/pip/wheel.py\", line 206, in move_wheel_files\n    clobber(source, lib_dir, True)\n  File \"/usr/lib/python2.7/dist-packages/pip/wheel.py\", line 193, in clobber\n    os.makedirs(destsubdir)\n  File \"/usr/lib/python2.7/os.py\", line 157, in makedirs\n    mkdir(name, mode)\nOSError: [Errno 13] Permission denied: '/usr/local/lib/python2.7/dist-packages/amqp-1.4.6.dist-info'\npython\npermissions\npip\ninstallation\nShare\nImprove this question\nFollow\nedited Jul 2 '19 at 4:43\nasked Jul 20 '15 at 8:58\n\nRunLoop\n19.5k2121 gold badges9191 silver badges150150 bronze badges\nRelated (macOS/homebrew specific) stackoverflow.com/questions/33004708/\u2026 \u2013 \nwim\n Aug 13 '19 at 2:59\nAdd a comment\n8 Answers\n\n330\n\nRather than using sudo with pip install, It's better to first try pip install --user. If this fails then take a look at the top post here.\n\nThe reason you shouldn't use sudo is as follows:\n\nWhen you run pip with sudo, you are running arbitrary Python code from the Internet as a root user, which is quite a big security risk. If someone puts up a malicious project on PyPI and you install it, you give an attacker root access to your machine.\n\nShare\nImprove this answer\nFollow\nedited May 27 at 9:25\nanswered Feb 3 '17 at 10:24\n\nbert\n3,74033 gold badges1616 silver badges2828 bronze badges\n7\nGood observation. That, after all, goes for all sudo x install, for all x (including x = make). \u2013 \nTobia Tesan\n Feb 14 '17 at 17:02 \n2\nThis also solved my problem. What does adding --user do? \u2013 \nMiles Johnson\n Jul 24 '17 at 21:21\n2\n@MilesJohnson Adding --user installs the package in your home directory, rather than the root. Installing something to this location doesn't require any extra privileges. \u2013 \nbert\n Jul 24 '17 at 21:22 \n1\nAdditionally, if you are on a remote server behind a proxy, \"sudo\" prevents you from fetching the packages from internet repositories and/or git repositories of the remote server's network. \u2013 \nAtaxias\n Nov 2 '17 at 15:34\n2\nAll mention of sudo was removed a year ago. This answer is obsolete - please revise and update it. You also need to mention per-user vs system-wide installs, and permissions. Don't use your answer to directly criticize other answers, that will tend to obsolete quickly. \u2013 \nsmci\n Jul 1 '19 at 21:26\nShow 3 more comments\n\n\n90\n\nOption a) Create a virtualenv, activate it and install:\nvirtualenv .venv\nsource .venv/bin/activate\npip install -r requirements.txt\nOption b) Install in your homedir:\npip install --user -r requirements.txt\nMy recommendation use safe (a) option, so that requirements of this project do not interfere with other projects requirements.\n\nShare\nImprove this answer\nFollow\nedited Jul 1 '19 at 21:21\n\nsmci\n27.9k1717 gold badges105105 silver badges141141 bronze badges\nanswered Jul 20 '15 at 9:02\n\nhectorcanto\n1,6571111 silver badges1717 bronze badges\n2\nI got an error like this sudo:pip: command not found on my aws ec2 instance when running this command. Please help. \u2013 \nuser3768495\n Nov 19 '16 at 19:45\n2\n@user3768495 Probably, pip is not installed by default. Which distro is your EC2? Also, python2 might not be installed, so either you install python2 or use pip3. Be careful with this though. \u2013 \nhectorcanto\n Jan 16 '17 at 11:53 \n35\nI've read this is not recommended in multiple places now. Seems like we should caution against sudo usage when running pip (see Bert's answer) \u2013 \nJustus Eapen\n Mar 11 '17 at 18:51\n3\n@JustusEapen: I don't know how I feel about that. I don't think the proper answer to OP's question is a manual on basic computer hygiene, including \"don't run shady code with superuser permissions\" and \"brush your teeth regularly\". I find the optimal answer should point out that packages can be installed on a per-user or system-wide basis, and that installing system-wide, as OP wished (there are perfectly cromulent reasons to do so) requires super user permission. Cautioning against installing packages on the system path is probably some else's job on some other SO question. \u2013 \nTobia Tesan\n Jul 6 '17 at 16:47\n8\ndownvoting because of sudo advise. even though it works now, it is going to give you a lot of headaches in the future. \u2013 \nGerald\n Sep 9 '17 at 5:30 \nShow 6 more comments\n\n28\n\nYou are trying to install a package on the system-wide path without having the permission to do so.\n\nIn general, you can use sudo to temporarily obtain superuser permissions at your responsibility in order to install the package on the system-wide path:\n\n sudo pip install -r requirements.txt\nFind more about sudo here.\n\nActually, this is a bad idea and there's no good use case for it, see @wim's comment.\n\nIf you don't want to make system-wide changes, you can install the package on your per-user path using the --user flag.\n\nAll it takes is:\n\n pip install --user runloop requirements.txt\nFinally, for even finer grained control, you can also use a virtualenv, which might be the superior solution for a development environment, especially if you are working on multiple projects and want to keep track of each one's dependencies.\n\nAfter activating your virtualenv with\n\n$ my-virtualenv/bin/activate\n\nthe following command will install the package inside the virtualenv (and not on the system-wide path):\n\npip install -r requirements.txt\n\nShare\nImprove this answer\nFollow\nedited Jun 20 '20 at 9:12\n\nCommunityBot\n111 silver badge\nanswered Jul 20 '15 at 9:02\n\nTobia Tesan\n1,8881414 silver badges2727 bronze badges\n4\nRunning pip with root comes with security risks \u2013 \nNrzonline\n Jul 6 '17 at 10:21\nRunning anything that runs code from the Internet as root comes with security risks. \u2013 \nTobia Tesan\n Sep 2 '18 at 10:36\nThis is nearly the best answer, but still needs updating. a) Now we have pyenv/pipenv, should mention those, in preference to virtualenv (or conda-env) b) sudo considered harmful, and see the other answers why. So put the env-based answer first, and the sudo one last, with a big disclaimer. \u2013 \nsmci\n Jul 1 '19 at 21:29\n1\nsudo pip install -r requirements.txt is never right. The system's python environment belongs to the system, period. If you do install more python stuff into the system, do it with package manager only (e.g. sudo yum install, apt-get, etc...) since those repos should have safe and compatible versions of libraries avail. \u2013 \nwim\n Aug 12 '19 at 22:32 \n1\n@TobiaTesan The old sudo make install, usually compiled + linked code, is not really analogous with a sudo pip install since installing to the system Python env can invalidate dependencies. Suppose there is a system service python-frobnicator, which has a dependency on froblib (this will also be in the package manager and pinned to a compatible version), and then you sudo pip install some other app or lib that has a dependency on \"froblib > 1.2\". Pip will happily \"upgrade\" the system version of froblib with a newer one, which may be incompatible/untested and break the system. \u2013 \nwim\n Oct 26 '19 at 19:12 \nShow 4 more comments\n\n27\n\nJust clarifying what worked for me after much pain in linux (ubuntu based) on permission denied errors, and leveraging from Bert's answer above, I now use ...\n\n$ pip install --user <package-name>\nor if running pip on a requirements file ...\n\n$ pip install --user -r requirements.txt\nand these work reliably for every pip install including creating virtual environments.\n\nHowever, the cleanest solution in my further experience has been to install python-virtualenv and virtualenvwrapper with sudo apt-get install at the system level.\n\nThen, inside virtual environments, use pip install without the --user flag AND without sudo. Much cleaner, safer, and easier overall.\n\nShare\nImprove this answer\nFollow\nedited Oct 6 '17 at 12:16\nanswered May 27 '17 at 7:22\n\nThom Ives\n3,00411 gold badge2525 silver badges2525 bronze badges\nI get a \"Can not perform a '--user' install. User site-packages are not visible in this virtualenv.\" error when trying to use pip install --user -r requirements.txt \u2013 \nAmir A. Shabani\n Feb 22 '19 at 15:23\n@AmirA.Shabani the answer has been edited since your question. It now says \u00ab inside virtual environments, use pip install without the --user flag AND without sudo \u00bb \u2013 \nDaishi\n Dec 15 '19 at 19:16 \nAdd a comment\n\n6\n\nUser doesn't have write permission for some Python installation paths. You can give the permission by:\n\nsudo chown -R $USER /absolute/path/to/directory\nSo you should give permission, then try to install it again, if you have new paths you should also give permission:\n\nsudo chown -R $USER /usr/local/lib/python2.7/", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "is there a yarn alternative for npm audit?", "id": 1870, "answers": [{"answer_id": 1856, "document_id": 1441, "question_id": 1870, "text": "This maybe the solution of your question: https://github.com/yarnpkg/yarn/releases/tag/v1.12.0", "answer_start": 208, "answer_category": null}], "is_impossible": false}], "context": "need pinned resolution feature of yarn, but also want to audit with npm audit? Is there a yarn alternative to npm audit? Or, alternately, will pinning resolutions of dependencies of dependencies work in npm?\nThis maybe the solution of your question: https://github.com/yarnpkg/yarn/releases/tag/v1.12.0\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Java Jar file: use resource errors: URI is not hierarchical", "id": 319, "answers": [{"answer_id": 328, "document_id": 132, "question_id": 319, "text": "You can open an InputStream in this way:InputStream in = Model.class.getClassLoader().getResourceAsStream(\"/data.sav\");", "answer_start": 153, "answer_category": null}], "is_impossible": false}], "context": "When you run from the ide you don't have any error, because you don't run a jar file. In the IDE classes and resources are extracted on the file system.\nYou can open an InputStream in this way:InputStream in = Model.class.getClassLoader().getResourceAsStream(\"/data.sav\");\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Rails 3.1 application deployment tutorial", "id": 30, "answers": [{"answer_id": 32, "document_id": 46, "question_id": 30, "text": "This guide covers the configuration and initialization features available to Rails applications. By referring to this guide, you will be able to:\nAdjust the behavior of your Rails applications\nAdd additional code to be run at application start time\nLocations for Initialization CodeRunning Code Before RailsConfiguring Rails ComponentsRails General Configuration Configuring Assets Configuring Generators Configuring Middleware Configuring i18n Configuring Active Record Configuring Action Controller Configuring Action Dispatch Configuring Action View Configuring Action Mailer Configuring Active Resource Configuring Active SupportRails Environment SettingsUsing Initializer FilesInitialization eventsRails::Railtie#initializer Initializers", "answer_start": 440, "answer_category": null}], "is_impossible": false}], "context": "Ruby on Rails Guides: Configuring Rails Applications\nActive Record Validations and Callbacks\nRails Routing from the Outside In\nPerformance Testing Rails Applications\nRails Command Line Tools and Rake Tasks\nCreating and Customizing Rails Generators\nRuby on Rails Guides Guidelines\nRuby on Rails 3.2 Release Notes\nRuby on Rails 3.1 Release Notes\nRuby on Rails 3.0 Release Notes\nRuby on Rails 2.3 Release Notes\nRuby on Rails 2.2 Release Notes\nThis guide covers the configuration and initialization features available to Rails applications. By referring to this guide, you will be able to:\nAdjust the behavior of your Rails applications\nAdd additional code to be run at application start time\nLocations for Initialization CodeRunning Code Before RailsConfiguring Rails ComponentsRails General Configuration Configuring Assets Configuring Generators Configuring Middleware Configuring i18n Configuring Active Record Configuring Action Controller Configuring Action Dispatch Configuring Action View Configuring Action Mailer Configuring Active Resource Configuring Active SupportRails Environment SettingsUsing Initializer FilesInitialization eventsRails::Railtie#initializer Initializers\n1 Locations for Initialization Code\nRails offers four standard spots to place initialization code:\nEnvironment-specific configuration files\nIn the rare event that your application needs to run some code before Rails itself is loaded, put it above the call to require 'rails/all' in config/application.rb.\nIn general, the work of configuring Rails means configuring the components of Rails, as well as configuring Rails itself. The configuration file config/application.rb and environment-specific configuration files (such as config/environments/production.rb) allow you to specify the various settings that you want to pass down to all of the components.\nFor example, the default config/application.rb file includes this setting:\nconfig.filter_parameters += [:password]\nThis is a setting for Rails itself. If you want to pass settings to individual Rails components, you can do so via the same config object in config/application.rb:\nconfig.active_record.observers = [:hotel_observer, :review_observer]\nRails will use that particular setting to configure Active Record.\n3.1 Rails General Configuration\nThese configuration methods are to be called on a Rails::Railtie object, such as a subclass of Rails::Engine or Rails::Application.\nconfig.after_initialize takes a block which will be run after Rails has finished initializing the application. That includes the initialization of the framework itself, plugins, engines, and all the application\u2019s initializers in config/initializers. Note that this block will be run for rake tasks. Useful for configuring values set up by other initializers:\nActionView::Base.sanitized_allowed_tags.delete 'div'\nconfig.allow_concurrency should be true to allow concurrent (threadsafe) action processing. False by default. You probably don\u2019t want to call this one directly, though, because a series of other adjustments need to be made for threadsafe mode to work properly. Can also be enabled with threadsafe!.\nconfig.asset_host sets the host for the assets. Useful when CDNs are used for hosting assets, or when you want to work around the concurrency constraints builtin in browsers using different domain aliases. Shorter version of config.action_controller.asset_host.\nconfig.asset_path lets you decorate asset paths. This can be a callable, a string, or be nil which is the default. For example, the normal path for blog.js would be /javascripts/blog.js, let that absolute path be path. If config.asset_path is a callable, Rails calls it when generating asset paths passing path as argument. If config.asset_path is a string, it is expected to be a sprintf format string with a %s where path will get inserted. In either case, Rails outputs the decorated path. Shorter version of config.action_controller.asset_path.\nconfig.asset_path = proc { |path| \"/blog/public#{path}\" }\nThe config.asset_path configuration is ignored if the asset pipeline is enabled, which is the default.\nconfig.autoload_once_paths accepts an array of paths from which Rails will autoload constants that won\u2019t be wiped per request. Relevant if config.cache_classes is false, which is the case in development mode by default. Otherwise, all autoloading happens only once. All elements of this array must also be in autoload_paths. Default is an empty array.\nconfig.autoload_paths accepts an array of paths from which Rails will autoload constants. Default is all directories under app.\nconfig.cache_classes controls whether or not application classes and modules should be reloaded on each request. Defaults to false in development mode, and true in test and production modes. Can also be enabled with threadsafe!.\nconfig.action_view.cache_template_loading controls whether or not templates should be reloaded on each request. Defaults to whatever is set for config.cache_classes.\nconfig.cache_store configures which cache store to use for Rails caching. Options include one of the symbols :memory_store, :file_store, :mem_cache_store, or an object that implements the cache API. Defaults to :file_store if the directory tmp/cache exists, and to :memory_store otherwise.\nconfig.colorize_logging specifies whether or not to use ANSI color codes when logging information. Defaults to true.\nconfig.consider_all_requests_local is a flag. If true then any error will cause detailed debugging information to be dumped in the HTTP response, and the Rails::Info controller will show the application runtime context in /rails/info/properties. True by default in development and test environments, and false in production mode. For finer-grained control, set this to false and implement local_request? in controllers to specify which requests should provide debugging information on errors.\nconfig.dependency_loading is a flag that allows you to disable constant autoloading setting it to false. It only has effect if config.cache_classes is true, which it is by default in production mode. This flag is set to false by config.threadsafe!.\nconfig.eager_load_paths accepts an array of paths from which Rails will eager load on boot if cache classes is enabled. Defaults to every folder in the app directory of the application.\nconfig.encoding sets up the application-wide encoding. Defaults to UTF-8.\nconfig.exceptions_app sets the exceptions application invoked by the ShowException middleware when an exception happens. Defaults to ActionDispatch::PublicExceptions.new(Rails.public_path).\nconfig.file_watcher the class used to detect file updates in the filesystem when config.reload_classes_only_on_change is true. Must conform to ActiveSupport::FileUpdateChecker API.\nconfig.filter_parameters used for filtering out the parameters that you don\u2019t want shown in the logs, such as passwords or credit card numbers.\nconfig.force_ssl forces all requests to be under HTTPS protocol by using Rack::SSL middleware.\nconfig.log_level defines the verbosity of the Rails logger. This option defaults to :debug for all modes except production, where it defaults to :info.\nconfig.log_tags accepts a list of methods that respond to request object. This makes it easy to tag log lines with debug information like subdomain and request id \u2014 both very helpful in debugging multi-user production applications.\nconfig.logger accepts a logger conforming to the interface of Log4r or the default Ruby Logger class. Defaults to an instance of ActiveSupport::BufferedLogger, with auto flushing off in production mode.\nconfig.middleware allows you to configure the application\u2019s middleware. This is covered in depth in the Configuring Middleware section below.\nconfig.plugins accepts the list of plugins to load. The default is nil in which case all plugins will be loaded. If this is set to [], no plugins will be loaded. Otherwise, plugins will be loaded in the order specified. This option lets you enforce some particular loading order, useful when dependencies between plugins require it. For that use case, put first the plugins you want to be loaded in a certain order, and then the special symbol :all to have the rest loaded without the need to specify them.\nconfig.preload_frameworks enables or disables preloading all frameworks at startup. Enabled by config.threadsafe!. Defaults to nil, so is disabled.\nconfig.reload_classes_only_on_change enables or disables reloading of classes only when tracked files change. By default tracks everything on autoload paths and is set to true. If config.cache_classes is true, this option is ignored.\nconfig.reload_plugins enables or disables plugin reloading. Defaults to false.\nconfig.secret_token used for specifying a key which allows sessions for the application to be verified against a known secure key to prevent tampering. Applications get config.secret_token initialized to a random key in config/initializers/secret_token.rb.\nconfig.serve_static_assets configures Rails itself to serve static assets. Defaults to true, but in the production environment is turned off as the server software (e.g. Nginx or Apache) used to run the application should serve static assets instead. Unlike the default setting set this to true when running (absolutely not recommended!) or testing your app in production mode using WEBrick. Otherwise you won\u00b4t be able use page caching and requests for files that exist regularly under the public directory will anyway hit your Rails app.\nconfig.session_store is usually set up in config/initializers/session_store.rb and specifies what class to use to store the session. Possible values are :cookie_store which is the default, :mem_cache_store, and :disabled. The last one tells Rails not to deal with sessions. Custom session stores can also be specified:\nconfig.session_store :my_custom_store\nThis custom store must be defined as ActionDispatch::Session::MyCustomStore. In addition to symbols, they can also be objects implementing a certain API, like ActiveRecord::SessionStore, in which case no special namespace is required.\nconfig.threadsafe! enables allow_concurrency, cache_classes, dependency_loading and preload_frameworks to make the application threadsafe.\nThreadsafe operation is incompatible with the normal workings of development mode Rails. In particular, automatic dependency loading and class reloading are automatically disabled when you call config.threadsafe!.\nconfig.time_zone sets the default time zone for the application and enables time zone awareness for Active Record.\nconfig.whiny_nils enables or disables warnings when a certain set of methods are invoked on nil and it does not respond to them. Defaults to true in development and test environments.\nRails 3.1, by default, is set up to use the sprockets gem to manage assets within an application. This gem concatenates and compresses assets in order to make serving them much less painful.\nconfig.assets.enabled a flag that controls whether the asset pipeline is enabled. It is explicitly initialized in config/application.rb.\nconfig.assets.compress a flag that enables the compression of compiled assets. It is explicitly set to true in config/production.rb.\nconfig.assets.css_compressor defines the CSS compressor to use. It is set by default by sass-rails. The unique alternative value at the moment is :yui, which uses the yui-compressor gem.\nconfig.assets.js_compressor defines the JavaScript compressor to use. Possible values are :closure, :uglifier and :yui which require the use of the closure-compiler, uglifier or yui-compressor gems respectively.\nconfig.assets.paths contains the paths which are used to look for assets. Appending paths to this configuration option will cause those paths to be used in the search for assets.\nconfig.assets.precompile allows you to specify additional assets (other than application.css and application.js) which are to be precompiled when rake assets:precompile is run.\nconfig.assets.prefix defines the prefix where assets are served from. Defaults to /assets.\nconfig.assets.digest enables the use of MD5 fingerprints in asset names. Set to true by default in production.rb.\nconfig.assets.debug disables the concatenation and compression of assets. Set to false by default in development.rb.\nconfig.assets.manifest defines the full path to be used for the asset precompiler\u2019s manifest file. Defaults to using config.assets.prefix.\nconfig.assets.cache_store defines the cache store that Sprockets will use. The default is the Rails file store.\nconfig.assets.version is an option string that is used in MD5 hash generation. This can be changed to force all files to be recompiled.\nconfig.assets.compile is a boolean that can be used to turn on live Sprockets compilation in production.\nconfig.assets.logger accepts a logger conforming to the interface of Log4r or the default Ruby Logger class. Defaults to the same configured at config.logger. Setting config.assets.logger to false will turn off served assets logging.\nRails 3 allows you to alter what generators are used with the config.generators method. This method takes a block:\nThe full set of methods that can be used in this block are as follows:\nassets allows to create assets on generating a scaffold. Defaults to true.\nforce_plural allows pluralized model names. Defaults to false.\nhelper defines whether or not to generate helpers. Defaults to true.\nintegration_tool defines which integration tool to use. Defaults to nil.\njavascripts turns on the hook for javascripts in generators. Used in Rails for when the scaffold generator is ran. Defaults to true.\njavascript_engine configures the engine to be used (for eg. coffee) when generating assets. Defaults to nil.\norm defines which orm to use. Defaults to false and will use Active Record by default.\nperformance_tool defines which performance tool to use. Defaults to nil.\nresource_controller defines which generator to use for generating a controller when using rails generate resource. Defaults to :controller.\nscaffold_controller different from resource_controller, defines which generator to use for generating a scaffolded controller when using rails generate scaffold. Defaults to :scaffold_controller.\nstylesheets turns on the hook for stylesheets in generators. Used in Rails for when the scaffold generator is ran, but this hook can be used in other generates as well. Defaults to true.\nstylesheet_engine configures the stylesheet engine (for eg. sass) to be used when generating assets. Defaults to :css.\ntest_framework defines which test framework to use. Defaults to false and will use Test::Unit by default.\ntemplate_engine defines which template engine to use, such as ERB or Haml. Defaults to :erb.\nEvery Rails application comes with a standard set of middleware which it uses in this order in the development environment:\nRack::SSL forces every request to be under HTTPS protocol. Will be available if config.force_ssl is set to true.  Options passed to this can be configured by using config.ssl_options.\nActionDispatch::Static is used to serve static assets. Disabled if config.serve_static_assets is true.\nRack::Lock wraps the app in mutex so it can only be called by a single thread at a time. Only enabled if config.action_controller.allow_concurrency is set to false, which it is by default.\nActiveSupport::Cache::Strategy::LocalCache serves as a basic memory backed cache. This cache is not thread safe and is intended only for serving as a temporary memory cache for a single thread.\nRack::Runtime sets an X-Runtime header, containing the time (in seconds) taken to execute the request.\nRails::Rack::Logger notifies the logs that the request has began. After request is complete, flushes all the logs.\nActionDispatch::ShowExceptions rescues any exception returned by the application and renders nice exception pages if the request is local or if config.consider_all_requests_local is set to true. If config.action_dispatch.show_exceptions is set to false, exceptions will be raised regardless.\nActionDispatch::RequestId makes a unique X-Request-Id header available to the response and enables the ActionDispatch::Request#uuid method.\nActionDispatch::RemoteIp checks for IP spoofing attacks. Configurable with the config.action_dispatch.ip_spoofing_check and config.action_dispatch.trusted_proxies settings.\nRack::Sendfile intercepts responses whose body is being served from a file and replaces it with a server specific X-Sendfile header. Configurable with config.action_dispatch.x_sendfile_header.\nActionDispatch::Callbacks runs the prepare callbacks before serving the request.\nActiveRecord::ConnectionAdapters::ConnectionManagement cleans active connections after each request, unless the rack.test key in the request environment is set to true.\nActiveRecord::QueryCache caches all SELECT queries generated in a request. If any INSERT or UPDATE takes place then the cache is cleaned.\nActionDispatch::Cookies sets cookies for the request.\nActionDispatch::Session::CookieStore is responsible for storing the session in cookies. An alternate middleware can be used for this by changing the config.action_controller.session_store to an alternate value. Additionally, options passed to this can be configured by using config.action_controller.session_options.\nActionDispatch::Flash sets up the flash keys. Only available if config.action_controller.session_store is set to a value.\nActionDispatch::ParamsParser parses out parameters from the request into params.\nRack::MethodOverride allows the method to be overridden if params[:_method] is set. This is the middleware which supports the PUT and DELETE HTTP method types.\nActionDispatch::Head converts HEAD requests to GET requests and serves them as so.\nActionDispatch::BestStandardsSupport enables \u201cbest standards support\u201d so that IE8 renders some elements correctly.\nBesides these usual middleware, you can add your own by using the config.middleware.use method:\nconfig.middleware.use Magical::Unicorns\nThis will put the Magical::Unicorns middleware on the end of the stack. You can use insert_before if you wish to add a middleware before another.\nconfig.middleware.insert_before ActionDispatch::Head, Magical::Unicorns\nThere\u2019s also insert_after which will insert a middleware after another:\nconfig.middleware.insert_after ActionDispatch::Head, Magical::Unicorns\nMiddlewares can also be completely swapped out and replaced with others:\nconfig.middleware.swap ActionDispatch::BestStandardsSupport, Magical::Unicorns\nThey can also be removed from the stack completely:\nconfig.middleware.delete ActionDispatch::BestStandardsSupport\nconfig.i18n.default_locale sets the default locale of an application used for i18n. Defaults to :en.\nconfig.i18n.load_path sets the path Rails uses to look for locale files. Defaults to config/locales/*.{yml,rb}.\nconfig.active_record includes a variety of configuration options:\nconfig.active_record.logger accepts a logger conforming to the interface of Log4r or the default Ruby Logger class, which is then passed on to any new database connections made. You can retrieve this logger by calling logger on either an Active Record model class or an Active Record model instance. Set to nil to disable logging.\nconfig.active_record.primary_key_prefix_type lets you adjust the naming for primary key columns. By default, Rails assumes that primary key columns are named id (and this configuration option doesn\u2019t need to be set.) There are two other choices:\n:table_name would make the primary key for the Customer class customerid\n:table_name_with_underscore would make the primary key for the Customer class customer_id\nconfig.active_record.table_name_prefix lets you set a global string to be prepended to table names. If you set this to northwest_, then the Customer class will look for northwest_customers as its table. The default is an empty string.\nconfig.active_record.table_name_suffix lets you set a global string to be appended to table names. If you set this to _northwest, then the Customer class will look for customers_northwest as its table. The default is an empty string.\nconfig.active_record.pluralize_table_names specifies whether Rails will look for singular or plural table names in the database. If set to true (the default), then the Customer class will use the customers table. If set to false, then the Customer class will use the customer table.\nconfig.active_record.default_timezone determines whether to use Time.local (if set to :local) or Time.utc (if set to :utc) when pulling dates and times from the database. The default is :utc for Rails, although Active Record defaults to :local when used outside of Rails.\nconfig.active_record.schema_format controls the format for dumping the database schema to a file. The options are :ruby (the default) for a database-independent version that depends on migrations, or :sql for a set of (potentially database-dependent) SQL statements.\nconfig.active_record.timestamped_migrations controls whether migrations are numbered with serial integers or with timestamps. The default is true, to use timestamps, which are preferred if there are multiple developers working on the same application.\nconfig.active_record.lock_optimistically controls whether Active Record will use optimistic locking and is true by default.\nconfig.active_record.whitelist_attributes will create an empty whitelist of attributes available for mass-assignment security for all models in your app.\nconfig.active_record.identity_map controls whether the identity map is enabled, and is false by default.\nconfig.active_record.auto_explain_threshold_in_seconds configures the threshold for automatic EXPLAINs (nil disables this feature). Queries exceeding the threshold get their query plan logged. Default is 0.5 in development mode.\nconfig.active_record.cache_timestamp_format controls the format of the timestamp value in the cache key. Default is :number.\nThe MySQL adapter adds one additional configuration option:\nActiveRecord::ConnectionAdapters::MysqlAdapter.emulate_booleans controls whether Active Record will consider all tinyint(1) columns in a MySQL database to be booleans and is true by default.\nThe schema dumper adds one additional configuration option:\nActiveRecord::SchemaDumper.ignore_tables accepts an array of tables that should not be included in any generated schema file. This setting is ignored unless config.active_record.schema_format == :ruby.\n3.7 Configuring Action Controller\nconfig.action_controller includes a number of configuration settings:\nconfig.action_controller.asset_host sets the host for the assets. Useful when CDNs are used for hosting assets rather than the application server itself.\nconfig.action_controller.asset_path takes a block which configures where assets can be found. Shorter version of config.action_controller.asset_path.\nconfig.action_controller.page_cache_directory should be the document root for the web server and is set using Base.page_cache_directory = \u201c/document/root\u201d. For Rails, this directory has already been set to Rails.public_path (which is usually set to Rails.root + \u201c/public\u201d). Changing this setting can be useful to avoid naming conflicts with files in public/, but doing so will likely require configuring your web server to look in the new location for cached files.\nconfig.action_controller.page_cache_extension configures the extension used for cached pages saved to page_cache_directory. Defaults to .html.\nconfig.action_controller.perform_caching configures whether the application should perform caching or not. Set to false in development mode, true in production.\nconfig.action_controller.default_charset specifies the default character set for all renders. The default is \u201cutf-8\u201d.\nconfig.action_controller.logger accepts a logger conforming to the interface of Log4r or the default Ruby Logger class, which is then used to log information from Action Controller. Set to nil to disable logging.\nconfig.action_controller.request_forgery_protection_token sets the token parameter name for RequestForgery. Calling protect_from_forgery sets it to :authenticity_token by default.\nconfig.action_controller.allow_forgery_protection enables or disables CSRF protection. By default this is false in test mode and true in all other modes.\nconfig.action_controller.relative_url_root can be used to tell Rails that you are deploying to a subdirectory. The default is ENV['RAILS_RELATIVE_URL_ROOT'].\nThe caching code adds two additional settings:\nActionController::Base.page_cache_directory sets the directory where Rails will create cached pages for your web server. The default is Rails.public_path (which is usually set to Rails.root + \u201c/public\u201d).\nActionController::Base.page_cache_extension sets the extension to be used when generating pages for the cache (this is ignored if the incoming request already has an extension). The default is .html.\nThe Active Record session store can also be configured:\nActiveRecord::SessionStore::Session.table_name sets the name of the table used to store sessions. Defaults to sessions.\nActiveRecord::SessionStore::Session.primary_key sets the name of the ID column used in the sessions table. Defaults to session_id.\nActiveRecord::SessionStore::Session.data_column_name sets the name of the column which stores marshaled session data. Defaults to data.\n3.8 Configuring Action Dispatch\nconfig.action_dispatch.session_store sets the name of the store for session data. The default is :cookie_store; other valid options include :active_record_store, :mem_cache_store or the name of your own custom class.\nconfig.action_dispatch.tld_length sets the TLD (top-level domain) length for the application. Defaults to 1.\nActionDispatch::Callbacks.before takes a block of code to run before the request.\nActionDispatch::Callbacks.to_prepare takes a block to run after ActionDispatch::Callbacks.before, but before the request. Runs for every request in development mode, but only once for production or environments with cache_classes set to true.\nActionDispatch::Callbacks.after takes a block of code to run after the request.\nThere are only a few configuration options for Action View, starting with three on ActionView::Base:\nconfig.action_view.field_error_proc provides an HTML generator for displaying errors that come from Active Record. The default is\nProc.new { |html_tag, instance| %Q(<div class=\"field_with_errors\">#{html_tag}</div>).html_safe }\nconfig.action_view.default_form_builder tells Rails which form builder to use by default. The default is ActionView::Helpers::FormBuilder. If you want your form builder class to be loaded after initialization (so it\u2019s reloaded on each request in development), you can pass it as a String\nconfig.action_view.erb_trim_mode gives the trim mode to be used by ERB. It defaults to '-'. See the ERB documentation for more information.\nconfig.action_view.javascript_expansions is a hash containing expansions that can be used for the JavaScript include tag. By default, this is defined as:\nconfig.action_view.javascript_expansions = { :defaults => %w(jquery jquery_ujs) }\nHowever, you may add to this by defining others:\nconfig.action_view.javascript_expansions[:prototype] = ['prototype', 'effects', 'dragdrop', 'controls']\nAnd can reference in the view with the following code:\n<%= javascript_include_tag :prototype %>\nconfig.action_view.stylesheet_expansions works in much the same way as javascript_expansions, but has no default key. Keys defined for this hash can be referenced in the view like such:\n<%= stylesheet_link_tag :special %>\nconfig.action_view.cache_asset_ids With the cache enabled, the asset tag helper methods will make fewer expensive file system calls (the default implementation checks the file system timestamp). However this prevents you from modifying any asset files while the server is running.\nconfig.action_view.embed_authenticity_token_in_remote_forms This is by default set to true. If you set it to false, authenticity_token will not be added to forms with :remote => true by default. You can force authenticity_token to be added to such remote form by passing :authenticity_token => true option.\nThere are a number of settings available on config.action_mailer:\nconfig.action_mailer.logger accepts a logger conforming to the interface of Log4r or the default Ruby Logger class, which is then used to log information from Action Mailer. Set to nil to disable logging.\nconfig.action_mailer.smtp_settings allows detailed configuration for the :smtp delivery method. It accepts a hash of options, which can include any of these options:\n:address \u2013 Allows you to use a remote mail server. Just change it from its default \u201clocalhost\u201d setting.\n:port \u2013 On the off chance that your mail server doesn\u2019t run on port 25, you can change it.\n:domain \u2013 If you need to specify a HELO domain, you can do it here.\n:user_name \u2013 If your mail server requires authentication, set the username in this setting.\n:password \u2013 If your mail server requires authentication, set the password in this setting.\n:authentication \u2013 If your mail server requires authentication, you need to specify the authentication type here. This is a symbol and one of :plain, :login, :cram_md5.\nconfig.action_mailer.sendmail_settings allows detailed configuration for the sendmail delivery method. It accepts a hash of options, which can include any of these options:\n:location \u2013 The location of the sendmail executable. Defaults to /usr/sbin/sendmail.\n:arguments \u2013 The command line arguments. Defaults to -i -t.\nconfig.action_mailer.raise_delivery_errors specifies whether to raise an error if email delivery cannot be completed. It defaults to true.\nconfig.action_mailer.delivery_method defines the delivery method. The allowed values are :smtp (default), :sendmail, and :test.\nconfig.action_mailer.perform_deliveries specifies whether mail will actually be delivered and is true by default. It can be convenient to set it to false for testing.\nconfig.action_mailer.default configures Action Mailer defaults. These default to:\n:parts_order  => [ \"text/plain\", \"text/enriched\", \"text/html\" ]\nconfig.action_mailer.observers registers observers which will be notified when mail is delivered.\nconfig.action_mailer.observers = [\"MailObserver\"]\nconfig.action_mailer.interceptors registers interceptors which will be called before mail is sent.\nconfig.action_mailer.interceptors = [\"MailInterceptor\"]\n3.11 Configuring Active Resource\nThere is a single configuration setting available on config.active_resource:\nconfig.active_resource.logger accepts a logger conforming to the interface of Log4r or the default Ruby Logger class, which is then used to log information from Active Resource. Set to nil to disable logging.\n3.12 Configuring Active Support\nThere are a few configuration options available in Active Support:\nconfig.active_support.bare enables or disables the loading of active_support/all when booting Rails. Defaults to nil, which means active_support/all is loaded.\nconfig.active_support.escape_html_entities_in_json enables or disables the escaping of HTML entities in JSON serialization. Defaults to true.\nconfig.active_support.use_standard_json_time_format enables or disables serializing dates to ISO 8601 format. Defaults to false.\nActiveSupport::BufferedLogger.silencer is set to false to disable the ability to silence logging in a block. The default is true.\nActiveSupport::Cache::Store.logger specifies the logger to use within cache store operations.\nActiveSupport::Deprecation.behavior alternative setter to config.active_support.deprecation which configures the behavior of deprecation warnings for Rails.\nActiveSupport::Deprecation.silence takes a block in which all deprecation warnings are silenced.\nActiveSupport::Deprecation.silenced sets whether or not to display deprecation warnings.\nActiveSupport::Logger.silencer is set to false to disable the ability to silence logging in a block. The default is true.\nSome parts of Rails can also be configured externally by supplying environment variables. The following environment variables are recognized by various parts of Rails:\nENV[\"RAILS_ENV\"] defines the Rails environment (production, development, test, and so on) that Rails will run under.\nENV[\"RAILS_RELATIVE_URL_ROOT\"] is used by the routing code to recognize URLs when you deploy your application to a subdirectory.\nENV[\"RAILS_ASSET_ID\"] will override the default cache-busting timestamps that Rails generates for downloadable assets.\nENV[\"RAILS_CACHE_ID\"] and ENV[\"RAILS_APP_VERSION\"] are used to generate expanded cache keys in Rails\u2019 caching code. This allows you to have multiple separate caches from the same application.\nAfter loading the framework and any gems and plugins in your application, Rails turns to loading initializers. An initializer is any Ruby file stored under config/initializers in your application. You can use initializers to hold configuration settings that should be made after all of the frameworks, plugins and gems are loaded, such as options to configure settings for these parts.\nYou can use subfolders to organize your initializers if you like, because Rails will look into the whole file hierarchy from the initializers folder on down.\nIf you have any ordering dependency in your initializers, you can control the load order by naming. For example, 01_critical.rb will be loaded before 02_normal.rb.\nRails has 5 initialization events which can be hooked into (listed in the order that they are ran):\nbefore_configuration: This is run as soon as the application constant inherits from Rails::Application. The config calls are evaluated before this happens.\nbefore_initialize: This is run directly before the initialization process of the application occurs with the :bootstrap_hook initializer near the beginning of the Rails initialization process.\nto_prepare: Run after the initializers are ran for all Railties (including the application itself), but before eager loading and the middleware stack is built. More importantly, will run upon every request in development, but only once (during boot-up) in production and test.\nbefore_eager_load: This is run directly before eager loading occurs, which is the default behaviour for the production environment and not for the development environment.\nafter_initialize: Run directly after the initialization of the application, but before the application initializers are run.\nTo define an event for these hooks, use the block syntax within a Rails::Application, Rails::Railtie or Rails::Engine subclass:\nclass Application < Rails::Application\n# initialization code goes here\nAlternatively, you can also do it through the config method on the Rails.application object:\nRails.application.config.before_initialize do\n# initialization code goes here\nSome parts of your application, notably observers and routing, are not yet set up at the point where the after_initialize block is called.\nRails has several initializers that run on startup that are all defined by using the initializer method from Rails::Railtie. Here\u2019s an example of the initialize_whiny_nils initializer from Active Support:\ninitializer \"active_support.initialize_whiny_nils\" do |app|\nrequire 'active_support/whiny_nil' if app.config.whiny_nils\nThe initializer method takes three arguments with the first being the name for the initializer and the second being an options hash (not shown here) and the third being a block. The :before key in the options hash can be specified to specify which initializer this new initializer must run before, and the :after key will specify which initializer to run this initializer after.\nInitializers defined using the initializer method will be ran in the order they are defined in, with the exception of ones that use the :before or :after methods.\nYou may put your initializer before or after any other initializer in the chain, as long as it is logical. Say you have 4 initializers called \u201cone\u201d through \u201cfour\u201d (defined in that order) and you define \u201cfour\u201d to go before \u201cfour\u201d but after \u201cthree\u201d, that just isn\u2019t logical and Rails will not be able to determine your initializer order.\nThe block argument of the initializer method is the instance of the application itself, and so we can access the configuration on it by using the config method as done in the example.\nBecause Rails::Application inherits from Rails::Railtie (indirectly), you can use the initializer method in config/application.rb to define initializers for the application.\nBelow is a comprehensive list of all the initializers found in Rails in the order that they are defined (and therefore run in, unless otherwise stated).\nServes as a placeholder so that :load_environment_config can be defined to run before it.\nload_active_support Requires active_support/dependencies which sets up the basis for Active Support. Optionally requires active_support/all if config.active_support.bare is un-truthful, which is the default.\npreload_frameworks Loads all autoload dependencies of Rails automatically if config.preload_frameworks is true or \u201ctruthful\u201d. By default this configuration option is disabled. In Rails, when internal classes are referenced for the first time they are autoloaded. :preload_frameworks loads all of this at once on initialization.\ninitialize_logger Initializes the logger (an ActiveSupport::BufferedLogger object) for the application and makes it accessible at Rails.logger, provided that no initializer inserted before this point has defined Rails.logger.\ninitialize_cache If RAILS_CACHE isn\u2019t set yet, initializes the cache by referencing the value in config.cache_store and stores the outcome as RAILS_CACHE. If this object responds to the middleware method, its middleware is inserted before Rack::Runtime in the middleware stack.\nset_clear_dependencies_hook Provides a hook for active_record.set_dispatch_hooks to use, which will run before this initializer. This initializer \u2014 which runs only if cache_classes is set to false \u2014 uses ActionDispatch::Callbacks.after to remove the constants which have been referenced during the request from the object space so that they will be reloaded during the following request.\ninitialize_dependency_mechanism If config.cache_classes is true, configures ActiveSupport::Dependencies.mechanism to require dependencies rather than load them.\nbootstrap_hook Runs all configured before_initialize blocks.\ni18n.callbacks In the development environment, sets up a to_prepare callback which will call I18n.reload! if any of the locales have changed since the last request. In production mode this callback will only run on the first request.\nactive_support.initialize_whiny_nils Requires active_support/whiny_nil if config.whiny_nils is true. This file will output errors such as:\nCalled id for nil, which would mistakenly be 4 -- if you really wanted the id of nil, use object_id\nYou have a nil object when you didn't expect it!\nYou might have expected an instance of Array.\nThe error occurred while evaluating nil.each\nactive_support.deprecation_behavior Sets up deprecation reporting for environments, defaulting to :log for development, :notify for production and :stderr for test. If a value isn\u2019t set for config.active_support.deprecation then this initializer will prompt the user to configure this line in the current environment\u2019s config/environments file. Can be set to an array of values.\nactive_support.initialize_time_zone Sets the default time zone for the application based on the config.time_zone setting, which defaults to \u201cUTC\u201d.\naction_dispatch.configure Configures the ActionDispatch::Http::URL.tld_length to be set to the value of config.action_dispatch.tld_length.\naction_view.cache_asset_ids Sets ActionView::Helpers::AssetTagHelper::AssetPaths.cache_asset_ids to false when Active Support loads, but only if config.cache_classes is too.\naction_view.javascript_expansions Registers the expansions set up by config.action_view.javascript_expansions and config.action_view.stylesheet_expansions to be recognized by Action View and therefore usable in the views.\naction_view.set_configs Sets up Action View by using the settings in config.action_view by send\u2019ing the method names as setters to ActionView::Base and passing the values through.\naction_controller.logger Sets ActionController::Base.logger \u2014 if it\u2019s not already set \u2014 to Rails.logger.\naction_controller.initialize_framework_caches Sets ActionController::Base.cache_store \u2014 if it\u2019s not already set \u2014 to RAILS_CACHE.\naction_controller.set_configs Sets up Action Controller by using the settings in config.action_controller by send\u2019ing the method names as setters to ActionController::Base and passing the values through.\naction_controller.compile_config_methods Initializes methods for the config settings specified so that they are quicker to access.\nactive_record.initialize_timezone Sets ActiveRecord::Base.time_zone_aware_attributes to true, as well as setting ActiveRecord::Base.default_timezone to UTC. When attributes are read from the database, they will be converted into the time zone specified by Time.zone.\nactive_record.logger Sets ActiveRecord::Base.logger \u2014 if it\u2019s not already set \u2014 to Rails.logger.\nactive_record.set_configs Sets up Active Record by using the settings in config.active_record by send\u2019ing the method names as setters to ActiveRecord::Base and passing the values through.\nactive_record.initialize_database Loads the database configuration (by default) from config/database.yml and establishes a connection for the current environment.\nactive_record.log_runtime Includes ActiveRecord::Railties::ControllerRuntime which is responsible for reporting the time taken by Active Record calls for the request back to the logger.\nactive_record.set_dispatch_hooks Resets all reloadable connections to the database if config.cache_classes is set to false.\naction_mailer.logger Sets ActionMailer::Base.logger \u2014 if it\u2019s not already set \u2014 to Rails.logger.\naction_mailer.set_configs Sets up Action Mailer by using the settings in config.action_mailer by send\u2019ing the method names as setters to ActionMailer::Base and passing the values through.\naction_mailer.compile_config_methods Initializes methods for the config settings specified so that they are quicker to access.\nactive_resource.set_configs Sets up Active Resource by using the settings in config.active_resource by send\u2019ing the method names as setters to ActiveResource::Base and passing the values through.\nset_load_path This initializer runs before bootstrap_hook. Adds the vendor, lib, all directories of app and any paths specified by config.load_paths to $LOAD_PATH.\nset_autoload_paths This initializer runs before bootstrap_hook. Adds all sub-directories of app and paths specified by config.autoload_paths to ActiveSupport::Dependencies.autoload_paths.\nadd_routing_paths Loads (by default) all config/routes.rb files (in the application and railties, including engines) and sets up the routes for the application.\nadd_locales Adds the files in config/locales (from the application, railties and engines) to I18n.load_path, making available the translations in these files.\nadd_view_paths Adds the directory app/views from the application, railties and engines to the lookup path for view files for the application.\nload_environment_config Loads the config/environments file for the current environment.\nappend_asset_paths Finds asset paths for the application and all attached railties and keeps a track of the available directories in config.static_asset_paths.\nprepend_helpers_path Adds the directory app/helpers from the application, railties and engines to the lookup path for helpers for the application.\nload_config_initializers Loads all Ruby files from config/initializers in the application, railties and engines. The files in this directory can be used to hold configuration settings that should be made after all of the frameworks and plugins are loaded.\nengines_blank_point Provides a point-in-initialization to hook into if you wish to do anything before engines are loaded. After this point, all railtie and engine initializers are ran.\nadd_generator_templates Finds templates for generators at lib/templates for the application, railities and engines and adds these to the config.generators.templates setting, which will make the templates available for all generators to reference.\nensure_autoload_once_paths_as_subset Ensures that the config.autoload_once_paths only contains paths from config.autoload_paths. If it contains extra paths, then an exception will be raised.\nadd_to_prepare_blocks The block for every config.to_prepare call in the application, a railtie or engine is added to the to_prepare callbacks for Action Dispatch which will be ran per request in development, or before the first request in production.\nadd_builtin_route If the application is running under the development environment then this will append the route for rails/info/properties to the application routes. This route provides the detailed information such as Rails and Ruby version for public/index.html in a default Rails application.\nbuild_middleware_stack Builds the middleware stack for the application, returning an object which has a call method which takes a Rack environment object for the request.\neager_load! If config.cache_classes is true, runs the config.before_eager_load hooks and then calls eager_load! which will load all the Ruby files from config.eager_load_paths.\nfinisher_hook Provides a hook for after the initialization of process of the application is complete, as well as running all the config.after_initialize blocks for the application, railties and engines.\nset_routes_reloader Configures Action Dispatch to reload the routes file using ActionDispatch::Callbacks.to_prepare.\ndisable_dependency_loading Disables the automatic dependency loading if the config.cache_classes is set to true and config.dependency_loading is set to false.\nYou're encouraged to help improve the quality of this guide.\nIf you see any typos or factual errors you are confident to\nrepository and open a new pull request. You can also ask for commit rights on\nseveral patches. Commits are reviewed, but that happens after you've submitted your\ncontribution. This repository is cross-merged with master periodically.\nYou may also find incomplete content, or stuff that is not up to date.\nPlease do add any missing documentation for master. Check the\nRuby on Rails Guides Guidelines\nIf for whatever reason you spot something to fix but cannot patch it yourself, please\nAnd last but not least, any kind of discussion regarding Ruby on Rails\ndocumentation is very welcome in the rubyonrails-docs mailing list.\nThis work is licensed under a Creative Commons Attribution-Share Alike 3.0 License\n\"Rails\", \"Ruby on Rails\", and the Rails logo are trademarks of David Heinemeier Hansson. All rights reserved.", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How do I deal with installing peer dependencies in Angular CLI? ", "id": 1845, "answers": [{"answer_id": 1831, "document_id": 1416, "question_id": 1845, "text": "You can ignore the peer dependency warnings by using the --force flag with Angular cli when updating dependencies.\nng update @angular/cli @angular/core --force", "answer_start": 422, "answer_category": null}], "is_impossible": false}], "context": "I've found myself in an almost endless cycle of errors when trying to update my Angular CLI and NPM. Every time I update, I am met with WARN messages telling me to install peer dependencies (see below), but each time I install a dependency, I am met with more WARN messages. Is there a better way of handling this situation or does it seriously take hours?\nI know I must be doing something wrong, but I'm new to Angular. \nYou can ignore the peer dependency warnings by using the --force flag with Angular cli when updating dependencies.\nng update @angular/cli @angular/core --force\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to install plugins to Sublime Text 2 editor?", "id": 1573, "answers": [{"answer_id": 1562, "document_id": 1150, "question_id": 1573, "text": "You should have a Data/Packages folder in your Sublime Text 2 install directory. All you need to do is download the plugin and put the plugin folder in the Packages folder.", "answer_start": 114, "answer_category": null}], "is_impossible": false}], "context": "How to install plugins to the Sublime Text editor?\nI would like to install Emmet plugin to Sublime Text 2 editor.\nYou should have a Data/Packages folder in your Sublime Text 2 install directory. All you need to do is download the plugin and put the plugin folder in the Packages folder.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Repository deployment and Composer : what workflow?", "id": 1112, "answers": [{"answer_id": 1104, "document_id": 689, "question_id": 1112, "text": "The best way is to run composer install on the server after updating to the latest code. You should also make sure you commit your composer.lock file, which the server will then use to install (you should not run composer update on the server).", "answer_start": 579, "answer_category": null}], "is_impossible": false}], "context": "As a PHP developer I find myself working with Composer a lot. In the past it was on personal projects and such so I didn't have much problems with it, but now with Laravel 4 it's on project that require deploying and I'm in kind of a struggle to adapt my workflow.\nAll my projects are git repositories, thus per convention and because it's still quite buggy, like most developers I put the vendor directory in my .gitignore. Now the problem is : I also use Git to deploy to the server, and by all logic the vendor directory is not uploaded as it's not tracked by the repository.\nThe best way is to run composer install on the server after updating to the latest code. You should also make sure you commit your composer.lock file, which the server will then use to install (you should not run composer update on the server).\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Enterprise app deployment doesn't work on iOS 7.1", "id": 1647, "answers": [{"answer_id": 1635, "document_id": 1221, "question_id": 1647, "text": " Changing the URL to HTTPS resolved the problem.\nI.e.\nitms-services://?action=download-manifest&url=http://example.com/manifest.plist\nbecomes\nitms-services://?action=download-manifest&url=https://example.com/manifest.plist", "answer_start": 811, "answer_category": null}], "is_impossible": false}], "context": "We distribute apps via an Enterprise account, using an itms-services:// URL. This has always worked fine, but after installing the iOS 7.1 beta on our iPad it refuses to install. Instead we just get the generic Cannot connect to example.com message that iOS unhelpfully displays when there is any sort of problem downloading the app.\nI've been unable to find anything here on SO, on Google or in the 7.1 release notes to suggest what could be causing the problem.\nI found the issue by connecting the iPad to the computer and viewing the console through the XCode Organizer while trying to install the app. The error turns out to be:\nCould not load non-https manifest URL: http://example.com/manifest.plist\nTurns out that in iOS 7.1, the URL for the manifest.plist file has to be HTTPS, where we were using HTTP. Changing the URL to HTTPS resolved the problem.\nI.e.\nitms-services://?action=download-manifest&url=http://example.com/manifest.plist\nbecomes\nitms-services://?action=download-manifest&url=https://example.com/manifest.plist\nI would assume you have to have a valid SSL certificate for the domain in question. We already did but I'd imagine you'll have\n\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "What is Dependency Injection and Inversion of Control in Spring Framework?", "id": 1895, "answers": [{"answer_id": 1882, "document_id": 1466, "question_id": 1895, "text": "For your question, there are two reasons:\nSpring helps in the creation of loosely coupled applications because of Dependency Injection.\nIn Spring, objects define their associations (dependencies) and do not worry about how they will get those dependencies. It is the responsibility of Spring to provide the required dependencies for creating objects.", "answer_start": 238, "answer_category": null}], "is_impossible": false}], "context": "\"Dependency Injection\" and \"Inversion of Control\" are often mentioned as the primary advantages of using the Spring framework for developing Web frameworks\nCould anyone explain what it is in very simple terms with an example if possible?\nFor your question, there are two reasons:\nSpring helps in the creation of loosely coupled applications because of Dependency Injection.\nIn Spring, objects define their associations (dependencies) and do not worry about how they will get those dependencies. It is the responsibility of Spring to provide the required dependencies for creating objects.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Raising minimum iOS Deployment Target Version for App Update", "id": 1707, "answers": [{"answer_id": 1695, "document_id": 1280, "question_id": 1707, "text": "From my experience those updates just won't show up as available.\nWhen I upgraded OS on my device from 3.1 to 4.1 about 10 available updates appeared immediately in App store app - so that should be the actual behavior.", "answer_start": 587, "answer_category": null}], "is_impossible": false}], "context": "Let's say we have an application with a deployment target set to 3.0 and we want to raise the deployment target to 3.2. Normally, the App Store won't let the App be installed on devices with an IOS version less then this, but what about devices which already had the App installed prior to the update? Will they see the update but won't be able to install, will they just not see the update or, heavens forbid, will be able to install and the app just won't start?\nI searched everywhere for this, but I can't find anything about raising the minimum OS version for an app update.\nThanks! From my experience those updates just won't show up as available.\nWhen I upgraded OS on my device from 3.1 to 4.1 about 10 available updates appeared immediately in App store app - so that should be the actual behavior.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to clean old dependencies from maven repositories?", "id": 1860, "answers": [{"answer_id": 1846, "document_id": 1431, "question_id": 1860, "text": "If you are on Unix, you could use the access time of the files in there. Just enable access time for your filesystem, then run a clean build of all your projects you would like to keep dependencies for and then do something like this (UNTESTED!):\nfind ~/.m2 -amin +5 -iname '*.pom' | while read pom; do parent=`dir", "answer_start": 294, "answer_category": null}], "is_impossible": false}], "context": "I have too many files in .m2 folder where maven stores downloaded dependencies. Is there a way to clean all old dependencies? For example, if there is a dependency with 3 different versions: 1, 2 and 3, after cleaning there must be only 3rd. How I can do it for all dependencies in .m2 folder?\nIf you are on Unix, you could use the access time of the files in there. Just enable access time for your filesystem, then run a clean build of all your projects you would like to keep dependencies for and then do something like this (UNTESTED!):\nfind ~/.m2 -amin +5 -iname '*.pom' | while read pom; do parent=`dir\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How do I publish a Asp.NET web application using MSBuild?", "id": 1108, "answers": [{"answer_id": 1100, "document_id": 685, "question_id": 1108, "text": "You should see http://msdn2.microsoft.com/en-us/library/ms164291.aspx for more info on this task. Your \"PublishDir\" would correspond to the TargetPath property of the task.", "answer_start": 379, "answer_category": null}], "is_impossible": false}], "context": "I am trying to publish an Asp.net MVC web application locally using the NAnt and MSBuild. This is what I am using for my NAnt target;\nThe \"Publish\" target you are trying to invoke is for \"OneClick\" deployment, not for publishing a website... This is why you are getting the seemingly bizarre message. You would want to use the AspNetCompiler task, rather than the MSBuild task. \nYou should see http://msdn2.microsoft.com/en-us/library/ms164291.aspx for more info on this task. Your \"PublishDir\" would correspond to the TargetPath property of the task.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Ruby on Rails - unable to convert \"\\x89\" from ASCII-8BIT to UTF-8 for xxx/xxxx/xxxx", "id": 1167, "answers": [{"answer_id": 1160, "document_id": 744, "question_id": 1167, "text": "Set such environment variables before you do install:\nexport LANGUAGE=en_US.UTF-8\nexport LANG=en_US.UTF-8\nexport LC_ALL=en_US.UTF-8", "answer_start": 356, "answer_category": null}], "is_impossible": false}], "context": "I am installing ruby on rails 4.0.0 on my linux system , but i am getting this kind of errors unable to convert \"\\x89\" from ASCII-8BIT to UTF8 for guides/assets/images/getting_started/routing_error_no_route_matches.png, skipping and many similar errors , i read here that it won't cause any effect ,but i do not understand why it is coming . Any pointers?\nSet such environment variables before you do install:\nexport LANGUAGE=en_US.UTF-8\nexport LANG=en_US.UTF-8\nexport LC_ALL=en_US.UTF-8\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How do I ensure that I am able to modify the Apache/MySQL/PHP config on run time? ", "id": 988, "answers": [{"answer_id": 983, "document_id": 608, "question_id": 988, "text": "I modified the configuration of Apache and MySQL so that everything that changes lies in %APPDATA%. The configuration files are passed as command line arguments.", "answer_start": 75, "answer_category": null}], "is_impossible": false}, {"question": "How do I tell the installer to add Apache & MySQL to the firewall exception list, or how do I tell the firewall that applications that listen only locally are not a threat?", "id": 989, "answers": [{"answer_id": 984, "document_id": 608, "question_id": 989, "text": "The current version of Advanced Installer allows Firewall rules to be set on installation", "answer_start": 237, "answer_category": null}], "is_impossible": false}], "context": "2\n\nI realized I do not want a dirty hack here and tried to do it \"right\":\n\nI modified the configuration of Apache and MySQL so that everything that changes lies in %APPDATA%. The configuration files are passed as command line arguments.\nThe current version of Advanced Installer allows Firewall rules to be set on installation.\nActually it wasn't that hard to change the config as I thought it would be and I learned some bits in the process...\n\nShare\nFollow\nedited Mar 12 '13 at 9:16\n\nBogdan Mitrache\n9,7591616 silver badges3131 bronze badges\nanswered Mar 12 '13 at 7:41\n\nAlexander Reifinger\n50044 silver badges1818 bronze badges\nAdd a comment\n\nReport this ad\n\n0\n\nI am unsure how this might be helpful, but take a look at inno setup it's free tool to deploy software, and it allow to make custom installation script using pascal, by putting some effort i hope , you can install wamp, add firewall restrictions etc.\n\nShare\nFollow\nanswered Mar 6 '13 at 8:23\n\nsansknwoledge\n4,10799 gold badges3535 silver badges5757 bronze badges\nThe installation is done already, it installs o.k., but I have to use \"Start as Administrator\" on the tool so it can change files in program files and the firewall asks me whether to allow the 2 servers. I want to get rid of all this. \u2013 \nAlexander Reifinger\n Mar 6 '13 at 10:16\nyou can disable uac in control panel, windows.microsoft.com/en-za/windows-vista/\u2026 , for fire wall exception you can use batch files , or you can use windows powershell file. if you are fluent in .net programming language you can use them as well. \u2013 \nsansknwoledge\n Mar 6 '13 at 15:07", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to create a shared library in Android", "id": 1296, "answers": [{"answer_id": 1288, "document_id": 867, "question_id": 1296, "text": "LIB is bundled with every application. But that would need a strong communication protocol between the libraries using intents, so that one of them acts as master, and shares the data with the other libraries. Effectively, the LIB packaged within an apk starts its engine, only if no other instance is running. One way of accomplishing this is using broadcasts and to maintain security, permissions can be associated with the involved broadcast actions.", "answer_start": 349, "answer_category": null}], "is_impossible": false}], "context": "I have a library, say LIB, which exposes the quite a few APIs and classes for use by application developers.\nIf there are more than one applications that use LIB on the phone, I want only one instance of LIB to be created and running. It is somewhat similar to what Android Platform Services like LocationManager, SensorManager, PackageManager etc.\nLIB is bundled with every application. But that would need a strong communication protocol between the libraries using intents, so that one of them acts as master, and shares the data with the other libraries. Effectively, the LIB packaged within an apk starts its engine, only if no other instance is running. One way of accomplishing this is using broadcasts and to maintain security, permissions can be associated with the involved broadcast actions.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Where do programs save their secret license?", "id": 1809, "answers": [{"answer_id": 1794, "document_id": 1380, "question_id": 1809, "text": "The simplest method is to register a handler for a custom unused file type like .sof (if that is there, it was installed before) Edit 1 You have to register the handle to open a known executable on the system, not to your app. Because cleaners will detect if points to a no longer existing app location. As for storing additional params like date of trial expiry you can include them in the path as a param, like: cmd.exe -o 2010-02-09", "answer_start": 483, "answer_category": null}], "is_impossible": false}], "context": "Where do programs save their secret license or install related information? I notice that often times when you uninstall a program, clear out appdata references, check registries to make sure there is no residue of any relevant information.\nIf you reinstall the trial program again, it seems to know it was installed before. I'm not looking to find a way to crack trial programs but actually need to implement something similar and can't find any good information on how to do this.\nThe simplest method is to register a handler for a custom unused file type like .sof (if that is there, it was installed before) Edit 1 You have to register the handle to open a known executable on the system, not to your app. Because cleaners will detect if points to a no longer existing app location. As for storing additional params like date of trial expiry you can include them in the path as a param, like: cmd.exe -o 2010-02-09\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to get Wix to update a previously installed version of a program", "id": 772, "answers": [{"answer_id": 772, "document_id": 459, "question_id": 772, "text": "You need to use the upgrade table:\n<Upgrade Id='15E2DAFB-35C5-4043-974B-0E342C25D76A'>\n    <UpgradeVersion Property='OLDVERSIONFOUND' IncludeMinimum='no' Minimum='0.0.0.0' />\n</Upgrade>\nYou need also add an action:\n<InstallExecuteSequence>\n    <LaunchConditions After='AppSearch' />\n    <RemoveExistingProducts After='InstallValidate' />\n</InstallExecuteSequence>", "answer_start": 364, "answer_category": null}], "is_impossible": false}], "context": "I wrote an install program with Wix and it worked fine to install my program. Now I need to update it, so I bumped up the version number but when I go to install the new program over the old one it complains that an older version is already installed and tells me to uninstall it first.\nHow do I get it to update or automatically uninstall it before reinstalling?\nYou need to use the upgrade table:\n<Upgrade Id='15E2DAFB-35C5-4043-974B-0E342C25D76A'>\n    <UpgradeVersion Property='OLDVERSIONFOUND' IncludeMinimum='no' Minimum='0.0.0.0' />\n</Upgrade>\nYou need also add an action:\n<InstallExecuteSequence>\n    <LaunchConditions After='AppSearch' />\n    <RemoveExistingProducts After='InstallValidate' />\n</InstallExecuteSequence>\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "how can i install netbeans with jdk 10 on windows?", "id": 1425, "answers": [{"answer_id": 1414, "document_id": 999, "question_id": 1425, "text": "Open cmd as Admin User\n\nnetbeans-XXX-windows.exe --extract\n\nthen run \n\njava -jar bundle.jar", "answer_start": 2187, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI try the command line with JavaHome:\n\nnetbeans-trunk-nightly-201803230002-javase-windows.exe --javahome \"C:\\Program Files\\Java\\jdk-10\"\n\n\nIt should work, but I receive a message: \n\n\n  There is no JRE at the specified location C:\\Program Files\\Java\\jdk-10\n\n\n\n\nThanks!\n    \n\n\n  Updated on July 31, 2018 to strikethrough obsolete information, and add details on the official release of NetBeans 9.\n\n\nYou are downloading NetBeans from Oracle's site, so you should have no expectation that even the nightly dev build will work with Java 10.\n\nNetBeans is currently being handed over to Apache from Oracle, and there is a development build available for download from Apache in the form of a zip file from here:\n\nhttps://builds.apache.org/job/incubator-netbeans-linux/\n\nYou can unzip that file on Linux, Mac or Windows and it will work with Java 10. Just be aware that it has not yet been formally released; that is a few weeks away.\n\nYou can check for bugs and/or report bugs here:\n\nhttps://issues.apache.org/jira/projects/NETBEANS\n\n\n\nUPDATE June 4, 2018\n\nApache NetBeans 9.0 RC1 was released on May 28. The source and the binary can be downloaded as zip files from here:\n\nhttps://netbeans.apache.org/download/nb90/nb90-rc1.html\n\n\n\nUPDATE July 31, 2018\n\nApache NetBeans 9.0 was released on July 29. The source and the binary can be downloaded as zip files from here:\n\nhttps://netbeans.apache.org/download/nb90/nb90.html#_downloading\n\n!!! IMPORTANT NOTE !!!\n\nNetBeans 9 officially only supports Java SE which means:\n\n\nThe Project Wizard is limited compared to NetBeans 8.2. For example, it does not even allow you to create a Java web application.\nMost plugins are not available. The unzipped download of NetBeans does not even allow you to install plugins for Java EE, C/C++, Fortran, Grails, Groovy, PHP etc. \n\n\nHowever, it is a simple process to enable that functionality (at your own risk). See the answer to How to get Netbeans 9 to deploy a valid webapp on Tomcat for details.\n\nFor an explanation on why the plugins are not available by default see What's Happened to My Favorite NetBeans Plugins?.\n    \n\nFor Windows you can install with NetBeans Extracted bundle : \n\nOpen cmd as Admin User\n\nnetbeans-XXX-windows.exe --extract\n\nthen run \n\njava -jar bundle.jar\n    \n\nNetbeans won't work properly with jdk version 10. Install jdk 8u/17l and everything should go well.\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "how to install the JavaFX SDK for JDK 7u2?", "id": 380, "answers": [{"answer_id": 387, "document_id": 169, "question_id": 380, "text": "Installers for JDK 7u2 to 7u5 install the JDK first, then start the JavaFX SDK installer, which installs JavaFX SDK in the default directory C:\\Program Files\\Oracle\\JavaFX 2.0 SDK or C:\\Program Files (x86)\\Oracle\\JavaFX 2.0 SDK on 64-bit operating systems.", "answer_start": 3149, "answer_category": null}], "is_impossible": false}, {"question": "how to run the JDK 8  installer for windows?", "id": 379, "answers": [{"answer_id": 386, "document_id": 169, "question_id": 379, "text": "You must have administrative permissions in order to install the JDK on Microsoft Windows.\n\nThe file jdk-8version-windows-i586-i.exe is the JDK installer for 32-bit systems. The file jdk-8version-windows-x64.exe is the JDK installer for 64-bit systems. If you downloaded either file instead of running it directly from the web site, double-click the installer's icon. Then, follow the instructions the installer provides. When finished with the installation, you can delete the downloaded file to recover disk space.", "answer_start": 2521, "answer_category": null}], "is_impossible": false}, {"question": "how to Install the public JRE in silent mode?", "id": 381, "answers": [{"answer_id": 388, "document_id": 169, "question_id": 381, "text": "jdk.exe /s", "answer_start": 4715, "answer_category": null}], "is_impossible": false}, {"question": "how to Install development tools and source code in silent mode but not the public JRE?", "id": 382, "answers": [{"answer_id": 389, "document_id": 169, "question_id": 382, "text": "jdk.exe /s ADDLOCAL=\"ToolsFeature,SourceFeature\"", "answer_start": 4807, "answer_category": null}], "is_impossible": false}, {"question": "how to Install development tools, source code, and the public JRE in silent mode?", "id": 383, "answers": [{"answer_id": 390, "document_id": 169, "question_id": 383, "text": "jdk.exe /s ADDLOCAL=\"ToolsFeature,SourceFeature,PublicjreFeature\"", "answer_start": 4931, "answer_category": null}], "is_impossible": false}, {"question": "how to Install the public JRE in the specified directory C:\\test\\ in silent mode?", "id": 384, "answers": [{"answer_id": 391, "document_id": 169, "question_id": 384, "text": "jdk.exe /s /INSTALLDIRPUBJRE=C:\\test\\", "answer_start": 5072, "answer_category": null}], "is_impossible": false}], "context": "This page describes how to install and uninstall JDK 8 for Windows.\n\nThe page has these topics:\n\n\"System Requirements\"\n\n\"Installation Instructions Notation\"\n\n\"Installation Instructions\"\n\n\"Uninstalling the JDK\"\n\n\"Installed Directory Tree\"\n\n\"Installation Troubleshooting\"\n\nSee \"JDK 8 and JRE 8 Installation Start Here\" for general information about installing JDK 8 and JRE 8.\n\nSystem Requirements\nSee http://www.oracle.com/technetwork/java/javase/certconfig-2095354.html for information about supported platforms, operating systems, and browsers.\n\nSee \"Windows System Requirements for JDK and JRE\" for minimum processor, disk space, and memory requirements.\n\nNote:\n\nThe JDK and JRE have a version string that enables you to determine the version number. See http://www.oracle.com/technetwork/java/javase/jdk8-naming-2157130.html for information about Java SE 8 version numbers.\n\nThe JDK has the option of installing the public JRE. For more information about JRE installation, see \"JRE Installation for Microsoft Windows\".\n\nIf you have any difficulties, see \"Installation Troubleshooting\" or submit a bug report at http://bugreport.java.com/bugreport/.\n\nInstallation Instructions Notation\nFor any text in this document that contains the following notation, you must substitute the appropriate update version number:\n\nversion\nFor example, if you were downloading the JDK installer for 32-bit systems for update 1.8.0_01, the file name: jdk-8version-windows-i586.exe becomes jdk-8u1-windows-i586.exe.\n\nSimilarly, if you were downloading the JDK installer for 64-bit systems for update 1.8.0_01, the file name jdk-8version-windows-x64.exe becomes jdk-8u1-windows-x64.exe.\n\nInstallation Instructions\nIn these instructions, you run the self-installing executable file to unpack and install the JDK. As part of the JDK, this installation includes an option to include the public Java Runtime Environment. (The JDK also contains a private JRE for use only by its tools; see \"Private Versus Public JRE\" for more information.)\n\nInstall the JDK by doing the following:\n\n\"Downloading the Installer\"\n\n\"Running the JDK Installer\"\n\n\"Installing the JDK Silently\"\n\n\"Updating the PATH Environment Variable\"\n\n\"Starting to Use the JDK\"\n\nDownloading the Installer\nIf you save the self-installing executable file to disk without running it from the download page at the web site, note the file size specified on the download page. After the download has completed, verify that you have downloaded the complete file.\n\nRunning the JDK Installer\nYou must have administrative permissions in order to install the JDK on Microsoft Windows.\n\nThe file jdk-8version-windows-i586-i.exe is the JDK installer for 32-bit systems. The file jdk-8version-windows-x64.exe is the JDK installer for 64-bit systems. If you downloaded either file instead of running it directly from the web site, double-click the installer's icon. Then, follow the instructions the installer provides. When finished with the installation, you can delete the downloaded file to recover disk space.\n\nInstallers for JDK 7u6 and later install the JavaFX SDK and integrate it into the JDK installation directory. Installers for JDK 7u2 to 7u5 install the JDK first, then start the JavaFX SDK installer, which installs JavaFX SDK in the default directory C:\\Program Files\\Oracle\\JavaFX 2.0 SDK or C:\\Program Files (x86)\\Oracle\\JavaFX 2.0 SDK on 64-bit operating systems. If you want to install the JavaFX SDK (version 2.0.2) with JDK 7u1 or earlier, see http://docs.oracle.com/javafx/2/installation/jfxpub-installation.htm for more information.\n\nJava Start Menu\nStarting with JDK 7u40 release, Java menu items are added to the Windows Start Menu to provide easy access to Java resources.\n\nDuring JDK install, a Java Development Kit folder is created in the Windows Start Menu, which contains the following items:\n\nReference Documentation: Opens the Online API documentation web page.\n\nJava Mission Control: Opens the Java Mission Control profiling and diagnostics tools suite.\n\nDuring JDK install and uninstall processes, the appropriate start menu items are updated to be associated with the latest JDK version on the system\n\n\nNote:\n\nJava Mission Control is a commercial feature available to users with a Java SE Advanced license.\nInstalling the JDK Silently\nInstead of double-clicking or opening the installer, you can perform a silent, non-interactive, JDK installation by using the command-line arguments. The following table lists example installation scenarios and the commands required to perform them. The notation jdk stands for the downloaded installer file base name, such as jdk-8u05-windows-i586.\n\nInstallation Scenario\tCommand\nInstall the public JRE in silent mode\t\njdk.exe /s\nInstall development tools and source code in silent mode but not the public JRE\t\njdk.exe /s ADDLOCAL=\"ToolsFeature,SourceFeature\"\nInstall development tools, source code, and the public JRE in silent mode\t\njdk.exe /s ADDLOCAL=\"ToolsFeature,SourceFeature,PublicjreFeature\"\nInstall the public JRE in the specified directory C:\\test\\ in silent mode\t\njdk.exe /s /INSTALLDIRPUBJRE=C:\\test\\", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Repair installation and Service Packs", "id": 986, "answers": [{"answer_id": 981, "document_id": 606, "question_id": 986, "text": "After a repair you must re-apply the service packs and roll-ups.\n\nAfter uninstalling a Service Pack or roll-up you must also repair Visual Studio before re-applying them.", "answer_start": 694, "answer_category": null}], "is_impossible": false}], "context": "What happens if:\n\nI install VS2010.\nI install VS2010 SP1.\nI run VS2010's Repair installation because of some problem in VS2010.\nAt this point, do I need to install SP1 again? In simple words, is the Repair Installation intelligent enough not to overwrite newer versions of files (from SP1) with the old ones that it has? Or is it that the Repair Install will simply bulldoze everything to original state?\n\ninstallation\nvisual-studio-2010\nrepair\nShare\nImprove this question\nFollow\nedited Feb 11 '13 at 8:44\n\ndandan78\n12.5k1212 gold badges6262 silver badges7575 bronze badges\nasked Feb 11 '13 at 8:40\n\ndotNET\n30k1919 gold badges130130 silver badges216216 bronze badges\nAdd a comment\n1 Answer\n\n2\n\nAfter a repair you must re-apply the service packs and roll-ups.\n\nAfter uninstalling a Service Pack or roll-up you must also repair Visual Studio before re-applying them.\n\nShare\nImprove this answer\nFollow", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Is there a way to exclude a Maven dependency globally?", "id": 1869, "answers": [{"answer_id": 1855, "document_id": 1440, "question_id": 1869, "text": "You can see this: http://jlorenzen.blogspot.com/2009/06/maven-global-excludes.html", "answer_start": 208, "answer_category": null}], "is_impossible": false}], "context": "\nI\u2019m trying to find a \u201cgeneric\u201d way of excluding a transitive dependency from being included without having to exclude it from all the dependencies that depend on it. For example, if I want to exclude slf4j.\nYou can see this: http://jlorenzen.blogspot.com/2009/06/maven-global-excludes.html\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to stop ElasticSearch Node?", "id": 200675, "answers": [{"answer_id": 239769, "document_id": 357788, "question_id": 200675, "text": "Run elasticsearch-node remove-settings name-of-setting-to-remove on the node\n", "answer_start": 4048, "answer_category": null}], "is_impossible": false}, {"question": "What does repurpose configuration do?", "id": 200676, "answers": [{"answer_id": 239771, "document_id": 357788, "question_id": 200676, "text": "Delete excess data when a node\u2019s roles are changed.", "answer_start": 14536, "answer_category": null}], "is_impossible": false}, {"question": "How to prevent unsafe-bootstrapping in ElasticSearch?", "id": 200679, "answers": [{"answer_id": 239797, "document_id": 357788, "question_id": 200679, "text": "use the elasticsearch-node\ndetach-cluster tool to migrate any other surviving nodes from the failed\ncluster into this new cluster.", "answer_start": 8631, "answer_category": null}], "is_impossible": false}, {"question": "How to bypass Elasticsearch vesion check?", "id": 200677, "answers": [{"answer_id": 239776, "document_id": 357788, "question_id": 200677, "text": "you can use the elasticsearch-node override-version\ntool to overwrite the version number stored in the data path with the current\nversion, causing Elasticsearch to believe that it is compatible with the on-disk data.", "answer_start": 7670, "answer_category": null}], "is_impossible": false}, {"question": "How does Elasticsearch check its version?", "id": 200678, "answers": [{"answer_id": 239781, "document_id": 357788, "question_id": 200678, "text": "The data\nstored on disk includes the version of the node that wrote it, and Elasticsearch checks\nthat it is compatible with this version when starting up.", "answer_start": 7125, "answer_category": null}], "is_impossible": false}], "context": "\n\nelasticsearch-node\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDocs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nElasticsearch Guide [7.15]\n\u00bb\nCommand line tools\n\u00bb\nelasticsearch-node\n\n\n\n\u00ab elasticsearch-migrate\n\n\nelasticsearch-saml-metadata \u00bb\n\n\n\n\nelasticsearch-node\n\nThe elasticsearch-node command enables you to perform certain unsafe\noperations on a node that are only possible while it is shut down. This command\nallows you to adjust the role of a node, unsafely edit cluster\nsettings and may be able to recover some data after a disaster or start a node\neven if it is incompatible with the data on disk.\nSynopsis\n\nbin/elasticsearch-node repurpose|unsafe-bootstrap|detach-cluster|override-version\n[--ordinal <Integer>] [-E <KeyValuePair>]\n[-h, --help] ([-s, --silent] | [-v, --verbose])\n\nDescription\nThis tool has a number of modes:\n\n\n\nelasticsearch-node repurpose can be used to delete unwanted data from a\nnode if it used to be a data node or a\nmaster-eligible node but has been repurposed not to have one\nor other of these roles.\n\n\nelasticsearch-node remove-settings can be used to remove persistent settings\nfrom the cluster state in case where it contains incompatible settings that\nprevent the cluster from forming.\n\n\nelasticsearch-node remove-customs can be used to remove custom metadata\nfrom the cluster state in case where it contains broken metadata that\nprevents the cluster state from being loaded.\n\n\nelasticsearch-node unsafe-bootstrap can be used to perform unsafe cluster\nbootstrapping. It forces one of the nodes to form a brand-new cluster on\nits own, using its local copy of the cluster metadata.\n\n\nelasticsearch-node detach-cluster enables you to move nodes from one\ncluster to another. This can be used to move nodes into a new cluster\ncreated with the elasticsearch-node unsafe-bootstrap command. If unsafe\ncluster bootstrapping was not possible, it also enables you to move nodes\ninto a brand-new cluster.\n\n\nelasticsearch-node override-version enables you to start up a node\neven if the data in the data path was written by an incompatible version of\nElasticsearch. This may sometimes allow you to downgrade to an earlier version of\nElasticsearch.\n\n\n\nChanging the role of a node\nThere may be situations where you want to repurpose a node without following\nthe proper repurposing processes. The elasticsearch-node\nrepurpose tool allows you to delete any excess on-disk data and start a node\nafter repurposing it.\nThe intended use is:\n\n\n\nStop the node\n\n\nUpdate elasticsearch.yml by setting node.roles as desired.\n\n\nRun elasticsearch-node repurpose on the node\n\n\nStart the node\n\n\n\nIf you run elasticsearch-node repurpose on a node without the data role and\nwith the master role then it will delete any remaining shard data on that\nnode, but it will leave the index and cluster metadata alone. If you run\nelasticsearch-node repurpose on a node without the data and master roles\nthen it will delete any remaining shard data and index metadata, but it will\nleave the cluster metadata alone.\n\n\n\nRunning this command can lead to data loss for the indices mentioned if the\ndata contained is not available on other nodes in the cluster. Only run this\ntool if you understand and accept the possible consequences, and only after\ndetermining that the node cannot be repurposed cleanly.\n\n\nThe tool provides a summary of the data to be deleted and asks for confirmation\nbefore making any changes. You can get detailed information about the affected\nindices and shards by passing the verbose (-v) option.\nRemoving persistent cluster settings\nThere may be situations where a node contains persistent cluster\nsettings that prevent the cluster from forming. Since the cluster cannot form,\nit is not possible to remove these settings using the\nCluster update settings API.\nThe elasticsearch-node remove-settings tool allows you to forcefully remove\nthose persistent settings from the on-disk cluster state. The tool takes a\nlist of settings as parameters that should be removed, and also supports\nwildcard patterns.\nThe intended use is:\n\n\n\nStop the node\n\n\nRun elasticsearch-node remove-settings name-of-setting-to-remove on the node\n\n\nRepeat for all other master-eligible nodes\n\n\nStart the nodes\n\n\n\nRemoving custom metadata from the cluster state\nThere may be situations where a node contains custom metadata, typically\nprovided by plugins, that prevent the node from starting up and loading\nthe cluster from disk.\nThe elasticsearch-node remove-customs tool allows you to forcefully remove\nthe problematic custom metadata. The tool takes a list of custom metadata names\nas parameters that should be removed, and also supports wildcard patterns.\nThe intended use is:\n\n\n\nStop the node\n\n\nRun elasticsearch-node remove-customs name-of-custom-to-remove on the node\n\n\nRepeat for all other master-eligible nodes\n\n\nStart the nodes\n\n\n\nRecovering data after a disaster\nSometimes Elasticsearch nodes are temporarily stopped, perhaps because of the need to\nperform some maintenance activity or perhaps because of a hardware failure.\nAfter you resolve the temporary condition and restart the node,\nit will rejoin the cluster and continue normally. Depending on your\nconfiguration, your cluster may be able to remain completely available even\nwhile one or more of its nodes are stopped.\nSometimes it might not be possible to restart a node after it has stopped. For\nexample, the node\u2019s host may suffer from a hardware problem that cannot be\nrepaired. If the cluster is still available then you can start up a fresh node\non another host and Elasticsearch will bring this node into the cluster in place of the\nfailed node.\nEach node stores its data in the data directories defined by the\npath.data setting. This means that in a disaster you can\nalso restart a node by moving its data directories to another host, presuming\nthat those data directories can be recovered from the faulty host.\nElasticsearch requires a response from a majority of the\nmaster-eligible nodes in order to elect a master and to update the cluster\nstate. This means that if you have three master-eligible nodes then the cluster\nwill remain available even if one of them has failed. However if two of the\nthree master-eligible nodes fail then the cluster will be unavailable until at\nleast one of them is restarted.\nIn very rare circumstances it may not be possible to restart enough nodes to\nrestore the cluster\u2019s availability. If such a disaster occurs, you should\nbuild a new cluster from a recent snapshot and re-import any data that was\ningested since that snapshot was taken.\nHowever, if the disaster is serious enough then it may not be possible to\nrecover from a recent snapshot either. Unfortunately in this case there is no\nway forward that does not risk data loss, but it may be possible to use the\nelasticsearch-node tool to construct a new cluster that contains some of the\ndata from the failed cluster.\nBypassing version checks\nThe data that Elasticsearch writes to disk is designed to be read by the current version\nand a limited set of future versions. It cannot generally be read by older\nversions, nor by versions that are more than one major version newer. The data\nstored on disk includes the version of the node that wrote it, and Elasticsearch checks\nthat it is compatible with this version when starting up.\nIn rare circumstances it may be desirable to bypass this check and start up an\nElasticsearch node using data that was written by an incompatible version. This may not\nwork if the format of the stored data has changed, and it is a risky process\nbecause it is possible for the format to change in ways that Elasticsearch may\nmisinterpret, silently leading to data loss.\nTo bypass this check, you can use the elasticsearch-node override-version\ntool to overwrite the version number stored in the data path with the current\nversion, causing Elasticsearch to believe that it is compatible with the on-disk data.\nUnsafe cluster bootstrapping\nIf there is at least one remaining master-eligible node, but it is not possible\nto restart a majority of them, then the elasticsearch-node unsafe-bootstrap\ncommand will unsafely override the cluster\u2019s voting\nconfiguration as if performing another\ncluster bootstrapping process.\nThe target node can then form a new cluster on its own by using\nthe cluster metadata held locally on the target node.\n\n\n\nThese steps can lead to arbitrary data loss since the target node may not hold the latest cluster\nmetadata, and this out-of-date metadata may make it impossible to use some or\nall of the indices in the cluster.\n\n\nSince unsafe bootstrapping forms a new cluster containing a single node, once\nyou have run it you must use the elasticsearch-node\ndetach-cluster tool to migrate any other surviving nodes from the failed\ncluster into this new cluster.\nWhen you run the elasticsearch-node unsafe-bootstrap tool it will analyse the\nstate of the node and ask for confirmation before taking any action. Before\nasking for confirmation it reports the term and version of the cluster state on\nthe node on which it runs as follows:\n\nCurrent node cluster state (term, version) pair is (4, 12)\n\nIf you have a choice of nodes on which to run this tool then you should choose\none with a term that is as large as possible. If there is more than one\nnode with the same term, pick the one with the largest version.\nThis information identifies the node with the freshest cluster state, which minimizes the\nquantity of data that might be lost. For example, if the first node reports\n(4, 12) and a second node reports (5, 3), then the second node is preferred\nsince its term is larger. However if the second node reports (3, 17) then\nthe first node is preferred since its term is larger. If the second node\nreports (4, 10) then it has the same term as the first node, but has a\nsmaller version, so the first node is preferred.\n\n\n\nRunning this command can lead to arbitrary data loss. Only run this tool if you\nunderstand and accept the possible consequences and have exhausted all other\npossibilities for recovery of your cluster.\n\n\nThe sequence of operations for using this tool are as follows:\n\n\n\nMake sure you have really lost access to at least half of the\nmaster-eligible nodes in the cluster, and they cannot be repaired or recovered\nby moving their data paths to healthy hardware.\n\n\nStop all remaining nodes.\n\n\nChoose one of the remaining master-eligible nodes to become the new elected\nmaster as described above.\n\n\nOn this node, run the elasticsearch-node unsafe-bootstrap command as shown\nbelow. Verify that the tool reported Master node was successfully\nbootstrapped.\n\n\nStart this node and verify that it is elected as the master node.\n\n\nRun the elasticsearch-node detach-cluster\ntool, described below, on every other node in the cluster.\n\n\nStart all other nodes and verify that each one joins the cluster.\n\n\nInvestigate the data in the cluster to discover if any was lost during this\nprocess.\n\n\n\nWhen you run the tool it will make sure that the node that is being used to\nbootstrap the cluster is not running. It is important that all other\nmaster-eligible nodes are also stopped while this tool is running, but the tool\ndoes not check this.\nThe message Master node was successfully bootstrapped does not mean that\nthere has been no data loss, it just means that tool was able to complete its\njob.\nDetaching nodes from their cluster\nIt is unsafe for nodes to move between clusters, because different clusters\nhave completely different cluster metadata. There is no way to safely merge the\nmetadata from two clusters together.\nTo protect against inadvertently joining the wrong cluster, each cluster\ncreates a unique identifier, known as the cluster UUID, when it first starts\nup. Every node records the UUID of its cluster and refuses to join a\ncluster with a different UUID.\nHowever, if a node\u2019s cluster has permanently failed then it may be desirable to\ntry and move it into a new cluster. The elasticsearch-node detach-cluster\ncommand lets you detach a node from its cluster by resetting its cluster UUID.\nIt can then join another cluster with a different UUID.\nFor example, after unsafe cluster bootstrapping you will need to detach all the\nother surviving nodes from their old cluster so they can join the new,\nunsafely-bootstrapped cluster.\nUnsafe cluster bootstrapping is only possible if there is at least one\nsurviving master-eligible node. If there are no remaining master-eligible nodes\nthen the cluster metadata is completely lost. However, the individual data\nnodes also contain a copy of the index metadata corresponding with their\nshards. This sometimes allows a new cluster to import these shards as\ndangling indices. You can sometimes\nrecover some indices after the loss of all master-eligible nodes in a cluster\nby creating a new cluster and then using the elasticsearch-node\ndetach-cluster command to move any surviving nodes into this new cluster.\nThere is a risk of data loss when importing a dangling index because data nodes\nmay not have the most recent copy of the index metadata and do not have any\ninformation about which shard copies are in-sync. This\nmeans that a stale shard copy may be selected to be the primary, and some of\nthe shards may be incompatible with the imported mapping.\n\n\n\nExecution of this command can lead to arbitrary data loss. Only run this tool\nif you understand and accept the possible consequences and have exhausted all\nother possibilities for recovery of your cluster.\n\n\nThe sequence of operations for using this tool are as follows:\n\n\n\nMake sure you have really lost access to every one of the master-eligible\nnodes in the cluster, and they cannot be repaired or recovered by moving their\ndata paths to healthy hardware.\n\n\nStart a new cluster and verify that it is healthy. This cluster may comprise\none or more brand-new master-eligible nodes, or may be an unsafely-bootstrapped\ncluster formed as described above.\n\n\nStop all remaining data nodes.\n\n\nOn each data node, run the elasticsearch-node detach-cluster tool as shown\nbelow. Verify that the tool reported Node was successfully detached from the\ncluster.\n\n\nIf necessary, configure each data node to\ndiscover the new cluster.\n\n\nStart each data node and verify that it has joined the new cluster.\n\n\nWait for all recoveries to have completed, and investigate the data in the\ncluster to discover if any was lost during this process.\n\n\n\nThe message Node was successfully detached from the cluster does not mean\nthat there has been no data loss, it just means that tool was able to complete\nits job.\nParameters\n\n\n\n\nrepurpose\n\n\n\nDelete excess data when a node\u2019s roles are changed.\n\n\n\nunsafe-bootstrap\n\n\n\nSpecifies to unsafely bootstrap this node as a new\none-node cluster.\n\n\n\ndetach-cluster\n\n\n\nSpecifies to unsafely detach this node from its cluster so\nit can join a different cluster.\n\n\n\noverride-version\n\n\n\nOverwrites the version number stored in the data path so\nthat a node can start despite being incompatible with the on-disk data.\n\n\n\nremove-settings\n\n\n\nForcefully removes the provided persistent cluster settings\nfrom the on-disk cluster state.\n\n\n\n--ordinal <Integer>\n\n\n\nIf there is more than one\nnode sharing a data path then this specifies which node to target. Defaults\nto 0, meaning to use the first node in the data path.\n\n\n\n-E <KeyValuePair>\n\n\n\nConfigures a setting.\n\n\n\n-h, --help\n\n\n\nReturns all of the command parameters.\n\n\n\n-s, --silent\n\n\n\nShows minimal output.\n\n\n\n-v, --verbose\n\n\n\nShows verbose output.\n\n\n\nExamples\nRepurposing a node as a dedicated master node\nIn this example, a former data node is repurposed as a dedicated master node.\nFirst update the node\u2019s settings to node.roles: [ \"master\" ] in its\nelasticsearch.yml config file. Then run the elasticsearch-node repurpose\ncommand to find and remove excess shard data:\n\nnode$ ./bin/elasticsearch-node repurpose\n\nWARNING: Elasticsearch MUST be stopped before running this tool.\n\nFound 2 shards in 2 indices to clean up\nUse -v to see list of paths and indices affected\nNode is being re-purposed as master and no-data. Clean-up of shard data will be performed.\nDo you want to proceed?\nConfirm [y/N] y\nNode successfully repurposed to master and no-data.\n\nRepurposing a node as a coordinating-only node\nIn this example, a node that previously held data is repurposed as a\ncoordinating-only node. First update the node\u2019s settings to node.roles: [] in\nits elasticsearch.yml config file. Then run the elasticsearch-node repurpose\ncommand to find and remove excess shard data and index metadata:\n\nnode$./bin/elasticsearch-node repurpose\n\nWARNING: Elasticsearch MUST be stopped before running this tool.\n\nFound 2 indices (2 shards and 2 index meta data) to clean up\nUse -v to see list of paths and indices affected\nNode is being re-purposed as no-master and no-data. Clean-up of index data will be performed.\nDo you want to proceed?\nConfirm [y/N] y\nNode successfully repurposed to no-master and no-data.\n\nRemoving persistent cluster settings\nIf your nodes contain persistent cluster settings that prevent the cluster\nfrom forming, i.e., can\u2019t be removed using the Cluster update settings API,\nyou can run the following commands to remove one or more cluster settings.\n\nnode$ ./bin/elasticsearch-node remove-settings xpack.monitoring.exporters.my_exporter.host\n\nWARNING: Elasticsearch MUST be stopped before running this tool.\n\nThe following settings will be removed:\nxpack.monitoring.exporters.my_exporter.host: \"10.1.2.3\"\n\nYou should only run this tool if you have incompatible settings in the\ncluster state that prevent the cluster from forming.\nThis tool can cause data loss and its use should be your last resort.\n\nDo you want to proceed?\n\nConfirm [y/N] y\n\nSettings were successfully removed from the cluster state\n\nYou can also use wildcards to remove multiple settings, for example using\n\nnode$ ./bin/elasticsearch-node remove-settings xpack.monitoring.*\n\nRemoving custom metadata from the cluster state\nIf the on-disk cluster state contains custom metadata that prevents the node\nfrom starting up and loading the cluster state, you can run the following\ncommands to remove this custom metadata.\n\nnode$ ./bin/elasticsearch-node remove-customs snapshot_lifecycle\n\nWARNING: Elasticsearch MUST be stopped before running this tool.\n\nThe following customs will be removed:\nsnapshot_lifecycle\n\nYou should only run this tool if you have broken custom metadata in the\ncluster state that prevents the cluster state from being loaded.\nThis tool can cause data loss and its use should be your last resort.\n\nDo you want to proceed?\n\nConfirm [y/N] y\n\nCustoms were successfully removed from the cluster state\n\nUnsafe cluster bootstrapping\nSuppose your cluster had five master-eligible nodes and you have permanently\nlost three of them, leaving two nodes remaining.\n\n\n\nRun the tool on the first remaining node, but answer n at the confirmation\nstep.\n\n\n\n\nnode_1$ ./bin/elasticsearch-node unsafe-bootstrap\n\nWARNING: Elasticsearch MUST be stopped before running this tool.\n\nCurrent node cluster state (term, version) pair is (4, 12)\n\nYou should only run this tool if you have permanently lost half or more\nof the master-eligible nodes in this cluster, and you cannot restore the\ncluster from a snapshot. This tool can cause arbitrary data loss and its\nuse should be your last resort. If you have multiple surviving master\neligible nodes, you should run this tool on the node with the highest\ncluster state (term, version) pair.\n\nDo you want to proceed?\n\nConfirm [y/N] n\n\n\n\n\nRun the tool on the second remaining node, and again answer n at the\nconfirmation step.\n\n\n\n\nnode_2$ ./bin/elasticsearch-node unsafe-bootstrap\n\nWARNING: Elasticsearch MUST be stopped before running this tool.\n\nCurrent node cluster state (term, version) pair is (5, 3)\n\nYou should only run this tool if you have permanently lost half or more\nof the master-eligible nodes in this cluster, and you cannot restore the\ncluster from a snapshot. This tool can cause arbitrary data loss and its\nuse should be your last resort. If you have multiple surviving master\neligible nodes, you should run this tool on the node with the highest\ncluster state (term, version) pair.\n\nDo you want to proceed?\n\nConfirm [y/N] n\n\n\n\n\nSince the second node has a greater term it has a fresher cluster state, so\nit is better to unsafely bootstrap the cluster using this node:\n\n\n\n\nnode_2$ ./bin/elasticsearch-node unsafe-bootstrap\n\nWARNING: Elasticsearch MUST be stopped before running this tool.\n\nCurrent node cluster state (term, version) pair is (5, 3)\n\nYou should only run this tool if you have permanently lost half or more\nof the master-eligible nodes in this cluster, and you cannot restore the\ncluster from a snapshot. This tool can cause arbitrary data loss and its\nuse should be your last resort. If you have multiple surviving master\neligible nodes, you should run this tool on the node with the highest\ncluster state (term, version) pair.\n\nDo you want to proceed?\n\nConfirm [y/N] y\nMaster node was successfully bootstrapped\n\nDetaching nodes from their cluster\nAfter unsafely bootstrapping a new cluster, run the elasticsearch-node\ndetach-cluster command to detach all remaining nodes from the failed cluster\nso they can join the new cluster:\n\nnode_3$ ./bin/elasticsearch-node detach-cluster\n\nWARNING: Elasticsearch MUST be stopped before running this tool.\n\nYou should only run this tool if you have permanently lost all of the\nmaster-eligible nodes in this cluster and you cannot restore the cluster\nfrom a snapshot, or you have already unsafely bootstrapped a new cluster\nby running `elasticsearch-node unsafe-bootstrap` on a master-eligible\nnode that belonged to the same cluster as this node. This tool can cause\narbitrary data loss and its use should be your last resort.\n\nDo you want to proceed?\n\nConfirm [y/N] y\nNode was successfully detached from the cluster\n\nBypassing version checks\nRun the elasticsearch-node override-version command to overwrite the version\nstored in the data path so that a node can start despite being incompatible\nwith the data stored in the data path:\n\nnode$ ./bin/elasticsearch-node override-version\n\nWARNING: Elasticsearch MUST be stopped before running this tool.\n\nThis data path was last written by Elasticsearch version [x.x.x] and may no\nlonger be compatible with Elasticsearch version [y.y.y]. This tool will bypass\nthis compatibility check, allowing a version [y.y.y] node to start on this data\npath, but a version [y.y.y] node may not be able to read this data or may read\nit incorrectly leading to data loss.\n\nYou should not use this tool. Instead, continue to use a version [x.x.x] node\non this data path. If necessary, you can use reindex-from-remote to copy the\ndata from here into an older cluster.\n\nDo you want to proceed?\n\nConfirm [y/N] y\nSuccessfully overwrote this node's metadata to bypass its version compatibility checks.\n\n\n\n\n\u00ab elasticsearch-migrate\n\n\nelasticsearch-saml-metadata \u00bb\n\n\n\n\n\n\n\n\nMost Popular\n\nGet Started with Elasticsearch: Video\nIntro to Kibana: Video\nELK for Logs & Metrics: Video\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Deploying a rails app ... how to load first user?", "id": 975, "answers": [{"answer_id": 970, "document_id": 599, "question_id": 975, "text": "db/seeds.rb", "answer_start": 595, "answer_category": null}], "is_impossible": false}, {"question": "What does reails 2.3.4 do?", "id": 976, "answers": [{"answer_id": 971, "document_id": 599, "question_id": 976, "text": "Rails 2.3.4 includes a conventional way to add seed data to your application - no more including it in the migration files.", "answer_start": 68, "answer_category": null}], "is_impossible": false}], "context": "\n#179 Seed Data\nSep 14, 2009 | 7 minutes | Active Record, Rails 2.3\nRails 2.3.4 includes a conventional way to add seed data to your application - no more including it in the migration files.\nClick to Play Video \u25b6\nTweetDownload:source codemp4m4vwebmogv\nShow NotesASCIIcast64 CommentsSimilar EpisodesNext Episode >< Previous Episode\nBrowse_code Browse Source Code\nResources\n\nRails 2.3.4 Release Notes\nEpisode 126: Populating a Database\nSeed-Fu\nBootstrapper\nbash\nscript/generate model operating_system name:string\nscript/generate model country name:string code:string\nrake db:migrate\nrake db:seed\ndb/seeds.rb\nrequire 'open-uri'\nrequire 'active_record/fixtures'\n\n[\"Windows\", \"Linux\", \"Mac OS X\"].each do |os|\n  OperatingSystem.find_or_create_by_name(os)\nend\n\nCountry.delete_all\nopen(\"http://openconcept.ca/sites/openconcept.ca/files/country_code_drupal_0.txt\") do |countries|\n  countries.read.each_line do |country|\n    code, name = country.chomp.split(\"|\")\n    Country.create!(:name => name, :code => code)\n  end\nend\n\nFixtures.create_fixtures(\"#{Rails.root}/test/fixtures\", \"operating_systems\")", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Android Studio - local path doesn't exist", "id": 141, "answers": [{"answer_id": 149, "document_id": 90, "question_id": 141, "text": "In build.gradle make sure gradle is set to 0.9.0. In gradle-wrapper.properties make sure to use gradle 1.11.", "answer_start": 196, "answer_category": null}], "is_impossible": false}], "context": "I originally saw this error after upgrading from 0.2.13 to 0.3. These instructions have been updated for the release of Android Studio 0.5.2. These are the steps I completed to resolve the issue. In build.gradle make sure gradle is set to 0.9.0. In gradle-wrapper.properties make sure to use gradle 1.11.\n\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to disable AppArmor?", "id": 245, "answers": [{"answer_id": 253, "document_id": 121, "question_id": 245, "text": "run:\n$ sudo invoke-rc.d apparmor kill\n$ sudo update-rc.d -f apparmor remove\n\n\nand then check the status:\n$ sudo apparmor_status", "answer_start": 18619, "answer_category": null}], "is_impossible": false}, {"question": "How to set AppArmor  to Complaining mode?", "id": 244, "answers": [{"answer_id": 252, "document_id": 121, "question_id": 244, "text": "sudo aa-complain /etc/apparmor.d/*", "answer_start": 18549, "answer_category": null}], "is_impossible": false}, {"question": "How to install AppArmor?", "id": 243, "answers": [{"answer_id": 251, "document_id": 121, "question_id": 243, "text": "sudo apt-get install apparmor-utils \u2014yes\n", "answer_start": 18299, "answer_category": null}], "is_impossible": false}, {"question": "How to know if AppArmor is installed?", "id": 242, "answers": [{"answer_id": 250, "document_id": 121, "question_id": 242, "text": "sudo apparmor_status", "answer_start": 18226, "answer_category": null}], "is_impossible": false}, {"question": "How to disable Apache Qpid?", "id": 248, "answers": [{"answer_id": 256, "document_id": 121, "question_id": 248, "text": " run:\n$ service qpidd stop\n\n\nand then:\n$ chkconfig --del qpidd", "answer_start": 19238, "answer_category": null}], "is_impossible": false}, {"question": "What is the hardware requirements for Chef Standalone Deployments?", "id": 255, "answers": [{"answer_id": 263, "document_id": 121, "question_id": 255, "text": "4 total cores (physical or virtual)\n8 GB of RAM or more\n5 GB of free disk space in /opt\n5 GB of free disk space in /var", "answer_start": 11257, "answer_category": null}], "is_impossible": false}, {"question": "What is the minimum RAM requirements for Chef deployments?", "id": 259, "answers": [{"answer_id": 268, "document_id": 121, "question_id": 259, "text": "4 GB", "answer_start": 11440, "answer_category": null}], "is_impossible": false}, {"question": "How to know the range of IDs in Chef server?", "id": 260, "answers": [{"answer_id": 269, "document_id": 121, "question_id": 260, "text": " grep -E '(UID|GID)' /etc/login.defs", "answer_start": 15603, "answer_category": null}], "is_impossible": false}, {"question": "How to disable SELinux?", "id": 261, "answers": [{"answer_id": 270, "document_id": 121, "question_id": 261, "text": "setenforce Permissive\n", "answer_start": 17852, "answer_category": null}], "is_impossible": false}, {"question": "What is opscode in Chef?", "id": 263, "answers": [{"answer_id": 272, "document_id": 121, "question_id": 263, "text": "The user name under which services will run.", "answer_start": 26331, "answer_category": null}], "is_impossible": false}], "context": "\n\n\n\n\n\n\nChef Infra Server Prerequisites\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLearn Chef\n\n\n\n\n\n\n\n\n\nTutorials\n\n\nSkills Library\n\n\nDocs\n\n\nTraining\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOverview\n\n\n\n\n\nPlatform Overview\n\n\n\n\n\nCommunity\n\n\n\n\n\nAbout the Community\n\n\n\n\nContributing\n\n\n\n\nGuidelines\n\n\n\n\nDocs Style Guide\n\n\n\n\nSend Feedback\n\n\n\n\n\n\nPackages & Platforms\n\n\n\n\n\nPackages\n\n\n\n\nPlatforms\n\n\n\n\nSupported Versions\n\n\n\n\nOmnitruck API\n\n\n\n\nLicensing\n\n\n\n\n\nAbout Licensing\n\n\n\n\nAccepting License\n\n\n\n\n\n\n\n\n\n\nChef Infra\n\n\n\n\n\nGetting Started\n\n\n\n\n\nChef Infra Overview\n\n\n\n\nInstall ChefDK\n\n\n\n\nConfigure ChefDK\n\n\n\n\nQuick Start\n\n\n\n\nSystem Requirements\n\n\n\n\nChef on Azure Guide\n\n\n\n\n\nInstalling Chef on Windows\n\n\n\n\nMicrosoft Azure\n\n\n\n\nChef Workstation on Azure Cloud Shell\n\n\n\n\nAzure Powershell_Cmdlets\n\n\n\n\nKnife Azure\n\n\n\n\nKnife Azurerm\n\n\n\n\n\n\nChef on Windows Guide\n\n\n\n\n\nChef for Microsoft Windows\n\n\n\n\nChef Workstation and ChefDK on Windows\n\n\n\n\nChef Infra Client on Windows\n\n\n\n\nKnife Windows\n\n\n\n\n\n\nGlossary\n\n\n\n\nUninstall\n\n\n\n\n\n\nConcepts\n\n\n\n\n\nChefDK\n\n\n\n\nChef Infra Client Overview\n\n\n\n\nChef Infra Server Overview\n\n\n\n\nchef-repo\n\n\n\n\nCookbooks\n\n\n\n\nCustom Resources\n\n\n\n\nNodes\n\n\n\n\nPolicy\n\n\n\n\n\nAbout Policy\n\n\n\n\nData Bags\n\n\n\n\nRun-lists\n\n\n\n\nEnvironments\n\n\n\n\nRoles\n\n\n\n\n\n\nSecrets\n\n\n\n\nAuthentication\n\n\n\n\nAuthorization\n\n\n\n\nEnvironment Variables\n\n\n\n\nSupermarket\n\n\n\n\n\nSupermarket\n\n\n\n\nPublic Supermarket\n\n\n\n\nPrivate Supermarket\n\n\n\n\nShare Cookbooks\n\n\n\n\n\n\n\n\nFeatures\n\n\n\n\n\nFIPS\n\n\n\n\nHandlers\n\n\n\n\nManagement Console\n\n\n\n\n\nAbout the Management Console\n\n\n\n\nConfigure SAML\n\n\n\n\nClients\n\n\n\n\nCookbooks\n\n\n\n\nData Bags\n\n\n\n\nEnvironments\n\n\n\n\nNodes\n\n\n\n\nRoles\n\n\n\n\nUsers\n\n\n\n\nmanage.rb\n\n\n\n\nchef-manage-ctl\n\n\n\n\n\n\nPush Jobs\n\n\n\n\nSearch\n\n\n\n\n\n\nTroubleshooting\n\n\n\n\n\nSetup\n\n\n\n\n\nChefDK\n\n\n\n\nNodes\n\n\n\n\n\nInstall via Bootstrap\n\n\n\n\nInstall via Install Script\n\n\n\n\nchef-client (executable)\n\n\n\n\nclient.rb\n\n\n\n\nUpgrades\n\n\n\n\nSecurity\n\n\n\n\n\n\nChef Infra Server\n\n\n\n\n\nHosted Chef Server\n\n\n\n\nInstall Chef Infra Server\n\n\n\n\nInstall Standalone\n\n\n\n\nChef Infra Server Prerequisites\n\n\n\n\nTiered Installation\n\n\n\n\nInstall High Availability\n\n\n\n\n\n\nWorking with Proxies\n\n\n\n\nAir-gapped Installation\n\n\n\n\nFIPS-mode\n\n\n\n\nIntegrations\n\n\n\n\n\nAWS Marketplace\n\n\n\n\nGoogle Cloud Platform\n\n\n\n\nVMware\n\n\n\n\n\n\nSupermarket\n\n\n\n\n\nPublic Supermarket\n\n\n\n\nInstall Private Supermarket\n\n\n\n\nCustomize Supermarket\n\n\n\n\nsupermarket.rb Settings\n\n\n\n\nBackup and Restore\n\n\n\n\nLog Files\n\n\n\n\nMonitoring\n\n\n\n\nknife supermarket\n\n\n\n\nsupermarket-ctl\n\n\n\n\nSupermarket API\n\n\n\n\n\n\nManagement Console\n\n\n\n\nPush Jobs\n\n\n\n\n\n\nCookbook Reference\n\n\n\n\n\nAbout Cookbooks\n\n\n\n\nAttributes\n\n\n\n\nFiles\n\n\n\n\nLibraries\n\n\n\n\nRecipes\n\n\n\n\n\nAbout Recipes\n\n\n\n\nDebug Recipes, Client Runs\n\n\n\n\n\n\nRecipe DSL\n\n\n\n\n\nDSL Overview\n\n\n\n\nattribute?\n\n\n\n\ncookbook_name\n\n\n\n\ndata_bag\n\n\n\n\ndata_bag_item\n\n\n\n\ndeclare_resource\n\n\n\n\ndelete_resource\n\n\n\n\ndelete_resource!\n\n\n\n\nedit_resource\n\n\n\n\nedit_resource!\n\n\n\n\nfind_resource\n\n\n\n\nfind_resource!\n\n\n\n\nplatform?\n\n\n\n\nplatform_family?\n\n\n\n\nreboot_pending?\n\n\n\n\nrecipe_name\n\n\n\n\nresources\n\n\n\n\nsearch\n\n\n\n\nshell_out\n\n\n\n\nshell_out!\n\n\n\n\ntag, tagged?, untag\n\n\n\n\nvalue_for_platform\n\n\n\n\nvalue_for_platform_family\n\n\n\n\nwith_run_context\n\n\n\n\nWindows Platform\n\n\n\n\nregistry_data_exists?\n\n\n\n\nregistry_get_subkeys\n\n\n\n\nregistry_get_values\n\n\n\n\nregistry_has_subkeys?\n\n\n\n\nregistry_key_exists?\n\n\n\n\nregistry_value_exists?\n\n\n\n\nWindows Platform Helpers\n\n\n\n\nLog Entries\n\n\n\n\n\n\nCustom Resources DSL\n\n\n\n\nResources\n\n\n\n\n\nAbout Resources\n\n\n\n\nCommon Functionality\n\n\n\n\nMigrating from Definitions\n\n\n\n\nCustom Resources\n\n\n\n\nAll Resources (Single Page)\n\n\n\n\napt_package\n\n\n\n\napt_preference\n\n\n\n\napt_repository\n\n\n\n\napt_update\n\n\n\n\narchive_file\n\n\n\n\nbash\n\n\n\n\nbatch\n\n\n\n\nbff_package\n\n\n\n\nbreakpoint\n\n\n\n\nbuild_essential\n\n\n\n\ncab_package\n\n\n\n\nchef_gem\n\n\n\n\nchef_handler\n\n\n\n\nchef_sleep\n\n\n\n\nchocolatey_config\n\n\n\n\nchocolatey_feature\n\n\n\n\nchocolatey_package\n\n\n\n\nchocolatey_source\n\n\n\n\ncookbook_file\n\n\n\n\ncron\n\n\n\n\ncron_d\n\n\n\n\ncron_access\n\n\n\n\ncsh\n\n\n\n\ndirectory\n\n\n\n\ndmg_package\n\n\n\n\ndnf_package\n\n\n\n\ndpkg_package\n\n\n\n\ndsc_resource\n\n\n\n\ndsc_script\n\n\n\n\nexecute\n\n\n\n\nfile\n\n\n\n\nfreebsd_package\n\n\n\n\ngem_package\n\n\n\n\ngit\n\n\n\n\ngroup\n\n\n\n\nhomebrew_cask\n\n\n\n\nhomebrew_package\n\n\n\n\nhomebrew_tap\n\n\n\n\nhostname\n\n\n\n\nhttp_request\n\n\n\n\nifconfig\n\n\n\n\nips_package\n\n\n\n\nkernel_module\n\n\n\n\nksh\n\n\n\n\nlaunchd\n\n\n\n\nlink\n\n\n\n\nlocale\n\n\n\n\nlog\n\n\n\n\nmacos_userdefaults\n\n\n\n\nmacports_package\n\n\n\n\nmdadm\n\n\n\n\nmount\n\n\n\n\nmsu_package\n\n\n\n\nohai_hint\n\n\n\n\nohai\n\n\n\n\nopenbsd_package\n\n\n\n\nopenssl_dhparam\n\n\n\n\nopenssl_ec_private_key\n\n\n\n\nopenssl_ec_public_key\n\n\n\n\nopenssl_rsa_private_key\n\n\n\n\nopenssl_rsa_public_key\n\n\n\n\nopenssl_x509_certificate\n\n\n\n\nopenssl_x509_crl\n\n\n\n\nopenssl_x509_request\n\n\n\n\nosx_profile\n\n\n\n\npackage\n\n\n\n\npacman_package\n\n\n\n\npaludis_package\n\n\n\n\nperl\n\n\n\n\nportage_package\n\n\n\n\npowershell_package\n\n\n\n\npowershell_package_source\n\n\n\n\npowershell_script\n\n\n\n\npython\n\n\n\n\nreboot\n\n\n\n\nreference\n\n\n\n\nregistry_key\n\n\n\n\nremote_directory\n\n\n\n\nremote_file\n\n\n\n\nrhsm_errata_level\n\n\n\n\nrhsm_errata\n\n\n\n\nrhsm_register\n\n\n\n\nrhsm_repo\n\n\n\n\nrhsm_subscription\n\n\n\n\nroute\n\n\n\n\nrpm_package\n\n\n\n\nruby\n\n\n\n\nruby_block\n\n\n\n\nscript\n\n\n\n\nservice\n\n\n\n\nsmartos_package\n\n\n\n\nsnap_package\n\n\n\n\nsolaris_package\n\n\n\n\nssh_known_hosts_entry\n\n\n\n\nsubversion\n\n\n\n\nsudo\n\n\n\n\nswap_file\n\n\n\n\nsysctl\n\n\n\n\nsystemd_unit\n\n\n\n\ntemplate\n\n\n\n\ntimezone\n\n\n\n\nuser\n\n\n\n\nwindows_ad_join\n\n\n\n\nwindows_auto_run\n\n\n\n\nwindows_certificate\n\n\n\n\nwindows_dfs_folder\n\n\n\n\nwindows_dfs_namespace\n\n\n\n\nwindows_dfs_server\n\n\n\n\nwindows_dns_record\n\n\n\n\nwindows_dns_zone\n\n\n\n\nwindows_env\n\n\n\n\nwindows_feature\n\n\n\n\nwindows_feature_dism\n\n\n\n\nwindows_feature_powershell\n\n\n\n\nwindows_firewall_rule\n\n\n\n\nwindows_font\n\n\n\n\nwindows_package\n\n\n\n\nwindows_pagefile\n\n\n\n\nwindows_path\n\n\n\n\nwindows_printer\n\n\n\n\nwindows_printer_port\n\n\n\n\nwindows_service\n\n\n\n\nwindows_share\n\n\n\n\nwindows_shortcut\n\n\n\n\nwindows_task\n\n\n\n\nwindows_uac\n\n\n\n\nwindows_workgroup\n\n\n\n\nyum_package\n\n\n\n\nyum_repository\n\n\n\n\nzypper_package\n\n\n\n\nzypper_repository\n\n\n\n\n\n\nTemplates\n\n\n\n\nCookbook Repo\n\n\n\n\nmetadata.rb\n\n\n\n\nCookbook Versioning\n\n\n\n\nRuby Guide\n\n\n\n\n\n\nChefDK\n\n\n\n\n\nAbout ChefDK\n\n\n\n\nBerkshelf\n\n\n\n\nchef-shell (executable)\n\n\n\n\nchef (executable)\n\n\n\n\n\nchef env\n\n\n\n\nchef exec\n\n\n\n\nchef gem\n\n\n\n\nchef generate attribute\n\n\n\n\nchef generate cookbook\n\n\n\n\nchef generate file\n\n\n\n\nchef generate recipe\n\n\n\n\nchef generate repo\n\n\n\n\nchef generate resource\n\n\n\n\nchef generate template\n\n\n\n\nchef shell-init\n\n\n\n\n\n\nchef-apply (executable)\n\n\n\n\nChef Solo\n\n\n\n\n\nAbout Chef Solo\n\n\n\n\nchef-solo (executable)\n\n\n\n\nsolo.rb\n\n\n\n\n\n\nchef-vault\n\n\n\n\nChefSpec\n\n\n\n\nconfig.rb (knife.rb)\n\n\n\n\nOptional config.rb Settings\n\n\n\n\ncookstyle\n\n\n\n\nDelivery CLI\n\n\n\n\nFoodcritic\n\n\n\n\nTest Kitchen\n\n\n\n\n\nAbout Test Kitchen\n\n\n\n\nkitchen (executable)\n\n\n\n\nkitchen.yml\n\n\n\n\nkitchen-vagrant\n\n\n\n\n\n\nKnife\n\n\n\n\n\nAbout Knife\n\n\n\n\nSetting up Knife\n\n\n\n\nKnife Common Options\n\n\n\n\nconfig.rb (knife.rb)\n\n\n\n\nknife bootstrap\n\n\n\n\nknife client\n\n\n\n\nknife configure\n\n\n\n\nknife cookbook\n\n\n\n\nknife cookbook site\n\n\n\n\nknife data bag\n\n\n\n\nknife delete\n\n\n\n\nknife deps\n\n\n\n\nknife diff\n\n\n\n\nknife download\n\n\n\n\nknife edit\n\n\n\n\nknife environment\n\n\n\n\nknife exec\n\n\n\n\nknife list\n\n\n\n\nknife node\n\n\n\n\nknife raw\n\n\n\n\nknife recipe list\n\n\n\n\nknife role\n\n\n\n\nknife search\n\n\n\n\nknife serve\n\n\n\n\nknife show\n\n\n\n\nknife ssh\n\n\n\n\nknife ssl_check\n\n\n\n\nknife ssl_fetch\n\n\n\n\nknife status\n\n\n\n\nknife supermarket\n\n\n\n\nknife tag\n\n\n\n\nknife upload\n\n\n\n\nknife user\n\n\n\n\nknife xargs\n\n\n\n\nknife opc\n\n\n\n\n\n\nOhai\n\n\n\n\n\nAbout Ohai\n\n\n\n\nohai (executable)\n\n\n\n\n\n\nPolicyfile\n\n\n\n\n\nAbout Policyfile\n\n\n\n\nPolicyfile.rb\n\n\n\n\n\n\npush-jobs-client (executable)\n\n\n\n\n\n\nChef Infra Server\n\n\n\n\n\nRunbook (Single Page)\n\n\n\n\nBackup & Restore\n\n\n\n\nBackend Failure Recovery\n\n\n\n\nFirewalls & Ports\n\n\n\n\nActive Directory & LDAP\n\n\n\n\nLog Files\n\n\n\n\nMonitor\n\n\n\n\nOrganizations & Groups\n\n\n\n\nSecurity\n\n\n\n\nServices\n\n\n\n\nTuning\n\n\n\n\nUpgrades\n\n\n\n\nUpgrade HA Cluster\n\n\n\n\nUsers\n\n\n\n\nchef-server-ctl\n\n\n\n\nchef-backend-ctl\n\n\n\n\nchef-server.rb\n\n\n\n\nChef Infra Server Optional Settings\n\n\n\n\nopscode-expander-ctl\n\n\n\n\nChef Infra Server API\n\n\n\n\nPush Jobs\n\n\n\n\n\nknife push jobs\n\n\n\n\npush-jobs-client\n\n\n\n\npush-jobs-client.rb\n\n\n\n\npush-jobs-server.rb\n\n\n\n\nPush Jobs API\n\n\n\n\nChef Infra Server Sent Events\n\n\n\n\n\n\n\n\nRelease Notes\n\n\n\n\n\nChef Infra Client\n\n\n\n\nChefDK\n\n\n\n\nChef Infra Server\n\n\n\n\nChef Push Jobs\n\n\n\n\n\n\nDeprecations\n\n\n\n\n\n\n\nChef Workstation\n\n\n\n\n\nChef Workstation\n\n\n\n\n\nDocumentation\n\n\n\n\n\n\n\nChef Habitat\n\n\n\n\n\nDocumentation\n\n\n\n\n\nTutorials\n\n\n\n\n\n\n\nChef InSpec\n\n\n\n\n\nDocumentation\n\n\n\n\n\nTutorials\n\n\n\n\n\n\n\nChef Automate\n\n\n\n\n\nChef Automate Documentation\n\n\n\n\n\n\n\nLegacy\n\n\n\n\n\nWorkflow\n\n\n\n\n\nWorkflow Basics\n\n\n\n\n\nWorkflow Overview\n\n\n\n\nConfigure a Pipeline\n\n\n\n\nConfigure a Project\n\n\n\n\nConfigure Data Collection\n\n\n\n\nData Collection with Chef HA\n\n\n\n\nData Collection without Chef Infra Server\n\n\n\n\nAudit Cookbook\n\n\n\n\n\n\nManaging Workflow\n\n\n\n\n\nbuild-cookbook (cookbook)\n\n\n\n\ndelivery-truck (cookbook)\n\n\n\n\nManage Dependencies\n\n\n\n\nManage Secrets\n\n\n\n\nPublish to Multiple Chef Infra Servers\n\n\n\n\nRunners\n\n\n\n\nWorkflow w/Bitbucket\n\n\n\n\nWorkflow w/Email (SMTP)\n\n\n\n\nWorkflow w/GitHub\n\n\n\n\nWorkflow w/Slack\n\n\n\n\nUsers and Roles\n\n\n\n\nAuthentication w/LDAP\n\n\n\n\nAuthentication w/SAML\n\n\n\n\nElasticsearch and Kibana Auth\n\n\n\n\nTuning\n\n\n\n\n\n\nReference\n\n\n\n\n\nDelivery CLI\n\n\n\n\ndelivery.rb\n\n\n\n\nWorkflow DSL\n\n\n\n\n\n\nAWS OpsWorks for Chef Automate\n\n\n\n\nChef Automate for Microsoft Azure\n\n\n\n\n\n\n\n\nExtension APIs\n\n\n\n\n\nCompliance DSL\n\n\n\n\n\nHandlers\n\n\n\n\n\nCustom Handlers\n\n\n\n\nHandler DSL\n\n\n\n\nCommunity Handlers\n\n\n\n\n\n\nKnife Plugins\n\n\n\n\n\nCloud Plugins\n\n\n\n\nWriting Custom Plugins\n\n\n\n\n\n\nOhai Plugins\n\n\n\n\n\nCustom Plugins\n\n\n\n\nCommunity Plugins\n\n\n\n\n\n\n\n\n\nAvailable on GitHub\n\n\n\n\n\nGet Chef\n\n\n\n\n\nSend Feedback\n\n\n\n\n\nSupport\n\n\n\n\n\nSite Map\n\n\n\n\n\nArchive\n\n\n\n\n\n\n\n\n\n\n\n\nTable Of Contents\n\nChef Infra Server Prerequisites\nPlatforms\nUntested Platforms\n\n\nCapacity Planning\nHardware Requirements\nSoftware Requirements\nUIDs and GIDs\nFirewalls\niptables\nFirewallD\nUFW\n\n\nSecurity Modules\nSELinux\nAppArmor\n\n\nApache Qpid\ncron\nEnterprise Linux Updates\nIP Addresses\nHostnames\nConfigure Hostnames\n\n\nMail Relay\nNTP\nChef Infra Client\n\n\nRequired Accounts\nGroup Accounts\nUser Accounts\n\n\n\n\n\n\n\n\n\n\n\n\n\nChef Infra Server Prerequisites\u00b6\n[edit on GitHub]\nThe following is a detailed discussion of the prerequisites for every installation of the Chef Infra Server. See Install Chef Infra Server </install_server.html> for installation instructions.\n\nPlatforms\u00b6\nThe following table lists the commercially-supported platforms and versions for the Chef Infra Server:\n\n\n\n\n\n\n\nPlatform\nArchitecture\nVersion\n\n\n\nCentOS\nx86_64\n6.x, 7.x\n\nOracle Enterprise Linux\nx86_64\n6.x, 7.x\n\nRed Hat Enterprise Linux\nx86_64, ppc64le (7.x only), ppc64 (7.x only)\n6.x, 7.x\n\nSUSE Enterprise Linux Server\nx86_64\n12 SP1+, 15\n\nUbuntu\nx86_64\n16.04, 18.04\n\n\n\n\nUntested Platforms\u00b6\nThe following platforms are not tested by Chef Software:\n\nAny Linux or UNIX distribution that is not listed as a Foundational platform.\nMicrosoft Windows\n32-bit architectures\n\n\n\n\nCapacity Planning\u00b6\nRead the guidance around capacity planning for information about how to choose the right topology for the Chef Infra Server.\n\n\nHardware Requirements\u00b6\nAll machines in a Chef Infra Server deployment have the following hardware requirements. Disk space for standalone and backend servers should scale up with the number of nodes that the servers are managing. A good rule to follow is to allocate 2 MB per node. The disk values listed below should be a good default value that you will want to modify later if/when your node count grows. Fast, redundant storage (SSD/RAID-based solution either on-prem or in a cloud environment) is preferred.\nAll Deployments\n\n64-bit architecture\n\nStandalone Deployments\n\n4 total cores (physical or virtual)\n8 GB of RAM or more\n5 GB of free disk space in /opt\n5 GB of free disk space in /var\n\n\nNote\nThe RAM requirement can be lowered down to a minimum of 4 GB of RAM if the number of Chef Infra Client runs (CCRs) per minute are low (i.e. less than 33 CCRs/min). See Capacity Planning for more information on how this metric affects scalability.\n\nFor a high availability deployment:\nGeneral Requirements\n\nThree backend servers; as many frontend servers as required\n1 x GigE NIC interface (if on premises)\n\nFrontend Requirements\n\n4 cores (physical or virtual)\n4GB RAM\n20 GB of free disk space (SSD if on premises, Premium Storage in Microsoft Azure, EBS-Optimized GP2 in AWS)\n\nBackend Requirements\n\n2 cores (physical or virtual)\n8GB RAM\n50 GB/backend server (SSD if on premises, Premium Storage in Microsoft Azure, EBS-Optimized GP2 in AWS)\n\n\nWarning\nThe Chef Infra Server MUST NOT use a network file system of any type\u2014virtual or physical\u2014for backend storage. The Chef Infra Server database operates quickly. The behavior of operations, such as the writing of log files, will be unpredictable when run over a network file system.\n\n\n\nSoftware Requirements\u00b6\nBefore installing the Chef Infra Server, ensure that each machine has the following installed and configured properly:\n\nHostnames \u2014 Ensure that all systems have properly configured hostnames. The hostname for the Chef Infra Server must be a FQDN, have fewer than 64 characters including the domain suffix, be lowercase, and resolvable. See Hostnames, FQDNs for more information\nFQDNs \u2014 Ensure that all systems have a resolvable FQDN\nNTP \u2014 Ensure that every server is connected to NTP; the Chef Infra Server is sensitive to clock drift\nMail Relay \u2014 The Chef Infra Server uses email to send notifications for various events; a local mail transfer agent should be installed and available to the Chef server\ncron \u2014 Periodic maintenance tasks are performed using cron\ngit \u2014 git must be installed so that various internal services can confirm revisions\nlibfreetype and libpng \u2014 These libraries are required\nApache Qpid \u2014 This daemon must be disabled on CentOS and Red Hat systems\nRequired users \u2014 If the environment in which the Chef Infra Server will run has restrictions on the creation of local user and group accounts, ensure that the correct users and groups exist before reconfiguring\nFirewalls and ports \u2014 If host-based firewalls (iptables, ufw, etc.) are being used, ensure that ports 80 and 443 are open. These ports are used by the nginx service\n\nIn addition:\n\nBrowser \u2014 Firefox, Google Chrome, Safari, or Internet Explorer (versions 9 or better)\nChef Infra Client communication with the Chef Infra Server The Chef Infra Server must be able to communicate with every node that will be configured by Chef Infra Client and every workstation that will upload data to the Chef Infra\n\n\nUIDs and GIDs\u00b6\nThe installation process for the Chef Infra Server requires the use of at least two user and group identifiers (UIDs and GIDs). These are used to create the opscode and opscode-pgsql users and their default groups.\n\nNote\nThe creation of required user and group identifiers is done automatically during the installation process for the Chef Infra Server; however, the following user and group accounts may be created in advance of installing the Chef Infra Server if specific UIDs and GIDs are preferred. The user and group must be created as a pair to satisfy reconfiguration requirements.\n\nA local user account named opscode under which services will run\nA local user account named opscode-pgsql that is used by PostgreSQL\nA group account for each user account, one named opscode and the other named opscode-pgsql under which services will run\n\n\n\nWarning\nIf the UID and GID of opscode and opscode-pgsql do not match on both backend Chef Infra Server machines, a high availability configuration will not run correctly.\nThe embedded Chef Infra Server cookbooks can handle two cases:\n\nBoth opscode and opscode-pgsql user and group not found on the new server\nBoth opscode and opscode-pgsql user and group found on the new server\n\nHaving only the group and not the corresponding users present during a chef-server-ctl reconfigure is unsupported and may lead to an error in the reconfiguration run.\nTo determine the current range of IDs, run the following command:\n$ grep -E '(UID|GID)' /etc/login.defs\n\n\nThe defaults for CentOS and Red Hat Enterprise Linux systems look like this:\nUID_MIN             500\nUID_MAX           60000\nGID_MIN             500\nGID_MAX           60000\n\n\nIf the defaults have been changed for any reason, and if that change would result in less than 2 UID/GIDs being available to the useradd program, edit /etc/login.defs with changes to make at least 2 more UIDs and GIDs available for association. The currently used ID ranges for UIDs and GIDs can be found in /etc/passwd and /etc/group, respectively.\nIf the opscode and opscode-pgsql user and group identifiers exist prior to installing the Chef Infra Server, the Chef Infra Server installation process will use the existing identifiers instead of creating them.\n\n\n\nFirewalls\u00b6\n\niptables\u00b6\nTo allow access to your Chef Infra Server on ports 80 and 443 via the iptables firewall, issue the following command with root privileges:\n$ iptables -A INPUT -p tcp -m multiport --destination-ports 80,443 -j ACCEPT\n\n\nNote that you will need to make use of a tool such as iptables-persistent to restore your iptables rules upon reboot.\n\n\nFirewallD\u00b6\nOn RHEL and CentOS versions 7 and above, the FirewallD firewall is enabled by default. Issue the following command with root privileges to open ports 80 and 443:\n$ firewall-cmd --permanent --zone public --add-service http && firewall-cmd --permanent --zone public --add-service https && firewall-cmd --reload\n\n\n\n\nUFW\u00b6\nWhile UFW is installed on Ubuntu, it is not enabled by default. However, if you wish to use a UFW-based firewall on your Chef Infra Server, issue the following command with root privileges to open ports 80 and 443:\n$ ufw allow proto tcp from any to any port 80,443\n\n\n\n\n\nSecurity Modules\u00b6\n\nSELinux\u00b6\nOn CentOS and Red Hat Enterprise Linux systems, SELinux is enabled in enforcing mode by default. The Chef Infra Server does not have a profile available to run under SELinux. In order for the Chef Infra Server to run, SELinux must be disabled or set to Permissive mode.\nTo determine if SELinux is installed, run the following command:\n$ getenforce\n\n\nIf a response other than \"Disabled\" or \"Permissive\" is returned, SELinux must be disabled.\nTo set SELinux to Permissive mode, run:\n$ setenforce Permissive\n\n\nand then check the status:\n$ getenforce\n\n\n\n\nAppArmor\u00b6\nOn Ubuntu systems, AppArmor is enabled in enforcing mode by default. Chef products do not have a profile available to run under AppArmor. In order for the Chef products to run, AppArmor must set to Complaining mode or disabled.\nTo determine if AppArmor is installed, run the following command:\n$ sudo apparmor_status\n\n\nTo install AppArmor, run the following command:\n$ sudo apt-get install apparmor-utils \u2014yes\n\n\nIf a response other than \"0 processes are in enforce mode\" or \"0 profiles are in enforce mode.\" is returned, AppArmor must be set to Complaining mode or disabled.\nTo set AppArmor to Complaining mode, run:\n$ sudo aa-complain /etc/apparmor.d/*\n\n\nOr to disable AppArmor entirely, run:\n$ sudo invoke-rc.d apparmor kill\n$ sudo update-rc.d -f apparmor remove\n\n\nand then check the status:\n$ sudo apparmor_status\n\n\n\n\n\nApache Qpid\u00b6\nOn CentOS and Red Hat Enterprise Linux  systems, the Apache Qpid daemon is installed by default. The Chef Infra Server uses RabbitMQ for messaging. Because both Apache Qpid and RabbitMQ share the same protocol, Apache Qpid must be disabled.\nTo determine if Apache Qpid is installed, run the following command:\n$ rpm -qa | grep qpid\n\n\nIf Apache Qpid is installed, a response similar to the following is displayed:\n$ qpid-cpp-server-0.12-6.el6.x86_64\n\n\nTo disable Apache Qpid run:\n$ service qpidd stop\n\n\nand then:\n$ chkconfig --del qpidd\n\n\n\n\ncron\u00b6\nPeriodic maintenance tasks are performed on the Chef Infra Server servers via cron and the /etc/cron.d directory. With certain CentOS 6 configurations, an additional step is required to install crontab:\n$ yum install crontabs\n\n\n\n\nEnterprise Linux Updates\u00b6\nThe Chef Infra Server requires an x86_64 compatible systems architecture. When the Chef Infra Server is installed on Red Hat Enterprise Linux or CentOS, run yum update prior to installing the Chef Infra Server. This will ensure those platforms are fully compatible with this requirement.\n\n\nIP Addresses\u00b6\nUnless you intend to operate the Chef Infra Server in IPv6 mode, you should disable ipv6 in the system\u2019s /etc/hosts file by commenting out or removing all references to IPv6 addresses like \u201c::1\u201d or \u201cfe80:db8:85a3:8d3:1319:8a2e:370:7348\u201d.\nWithout these changes, a Chef Infra Server install intended to run in ipv4 mode will mistakenly only start the postgres service on the ipv6 loopback address of \u201c::1\u201d rather than the ipv4 loopback address of 127.0.0.1. This will make further progress through an initial reconfiguration impossible.\n\n\nHostnames\u00b6\nThe hostname for the Chef Infra Server may be specified using a FQDN or an IP address. This hostname must be resolvable, be 64 characters or less, and be lowercase. For example, a Chef Infra Server running in a production environment with a resolvable FQDN hostname can be added the DNS system. But when deploying Chef Infra Server into a testing environment, adding the hostname to the /etc/hosts file is enough to ensure that hostname is resolvable.\n\nFQDN Hostnames When the hostname for the Chef Infra Server is a FQDN be sure to include the domain suffix. For example, something like mychefserver.example.com (and not something like mychefserver).\n\nIP Address Hostnames When the Chef Infra Server is run in IPv6 mode, a hostname specified using an IP address must also be bracketed ([ ]) or the Chef Infra Server will not be able to recognize it as an IPv6 address. For example:\nbookshelf['url'] \"https://[2001:db8:85a3:8d3:1319:8a2e:370:7348]\"\n\n\n\n\nThe api_fqdn setting can be added to the private-chef.rb file (it is not there by default). When added, its value should be equal to the FQDN or IP address for the service URI used by the Chef Infra Server. Then configure the same value for the bookshelf['vip'] setting prior to installing the Chef Infra Server. For example: api_fqdn \"chef.example.com\" or api_fqdn 123.45.67.890.\n\nConfigure Hostnames\u00b6\nUse the following sections to verify the hostnames that is used by the Chef Infra Server.\nTo verify if a hostname is a FQDN\nTo verify if a hostname is a FQDN, run the following command:\n$ hostname\n\n\nIf the hostname is a FQDN, it will return something like:\n$ mychefserver.example.com\n\n\nIf the hostname is not a FQDN, it must be configured so that it is one.\nTo verify the FQDN is all lowercase\nTo verify if the alphabetic parts of a FQDN are all lowercase, run the following command:\n$ hostname -f | grep -E '^([[:digit:]]|[[:lower:]]|\\.|-|_)+$' && echo yes\n\n\nIf the hostname is all lowercase, it will return something like:\nmychefserver.example.com\nyes\n\n\nIf the hostname\u2019s alphabetic parts are not all lowercase, it must be configured so that they are.\nTo verify a hostname is resolvable\nTo verify is a hostname is resolvable, run the following command:\n$ hostname -f\n\n\nIf the hostname is resolvable, it will return something like:\n$ mychefserver.example.com\n\n\nTo change a hostname\nIn some cases, the hostname for the Chef Infra Server needs to be updated. The process for updating a hostname varies, depending on the platform on which the Chef Infra Server will run. Refer to the manual for the platform or contact a local systems administrator for specific guidance for a specific platform. The following example shows how a hostname can be changed when running Red Hat or CentOS:\n$ sudo hostname 'mychefserver.example.com'\n\n\nand then:\n$ echo \"mychefserver.example.com\" | sudo tee /etc/hostname\n\n\nTo add a hostname to /etc/hosts\nIf a hostname is not resolvable, refer to a local systems administrator for specific guidance on how to add the hostname to the DNS system. If the Chef Infra Server is being into a testing environment, just add the hostname to /etc/hosts. The following example shows how a hostname can be added to /etc/hosts when running Red Hat or CentOS:\n$ echo -e \"127.0.0.2 `hostname` `hostname -s`\" | sudo tee -a /etc/hosts\n\n\n\nWarning\nThe FQDN for the Chef Infra Server should be resolvable, lowercase, and should not exceed 64 characters when using OpenSSL, as OpenSSL requires the CN in a certificate to be no longer than 64 characters.\n\n\n\n\nMail Relay\u00b6\nThe Chef Infra Server server uses email to send notifications for various events:\n\nPassword resets\nUser invitations\nFailover notifications\nFailed job notifications\n\nConfigure a local mail transfer agent on the Chef Infra Server using the steps appropriate for the platform on which the Chef Infra Server is running.\n\n\nNTP\u00b6\nThe Chef Infra Server requires that the systems on which it is running be connected to Network Time Protocol (NTP), as the Chef Infra Server is particularly sensitive to clock drift. For Red Hat and CentOS 6:\n$ yum install ntp\n\n\nor:\n$ chkconfig ntpd on\n\n\nor:\n$ service ntpd start\n\n\nFor Ubuntu:\n$ apt-get install ntp\n\n\n\nChef Infra Client\u00b6\nThe Chef Infra Server server requires that every node that is under management by Chef also have an accurate clock that is synchronized very closely with the clock on the Chef Infra Server. If the clocks are not synchronized closely, the authentication process may fail when the clocks are out-of-sync by more than 15 minutes. A failure will trigger a 401 Unauthorized response similar to:\n[Tue, 01 Nov 2011 16:55:23 -0700] INFO: *** Chef 11.X.X ***\n[Tue, 01 Nov 2011 16:55:23 -0700] INFO: Client key /etc/chef/client.pem is not present - registering\n[Tue, 01 Nov 2011 16:55:24 -0700] INFO: HTTP Request Returned 401 Unauthorized:\nFailed to authenticate as ORGANIZATION-validator. Synchronize the clock on your host.\n[Tue, 01 Nov 2011 16:55:24 -0700] FATAL: Stacktrace dumped to /var/chef/cache/chef-stacktrace.out\n[Tue, 01 Nov 2011 16:55:24 -0700] FATAL: Net::HTTPServerException: 401 \"Unauthorized\"\n\n\nIn this situation, re-synchronize the system clocks with the Network Time Protocol (NTP) server, and then re-run Chef Infra Client.\n\n\n\nRequired Accounts\u00b6\nBy default, accounts required by the Chef Infra Server are created during setup. If your environment has restrictions on the creation of local user and group accounts that will prevent these accounts from being created automatically during setup, you will need to create these accounts.\n\nNote\nThe Chef Push Jobs feature of the Chef Infra Server use the same user and group accounts as the Chef Infra Server.\n\n\nGroup Accounts\u00b6\nThe following group accounts are required:\n\n\n\n\n\n\nGroup Account\nDescription\n\n\n\nopscode\nThe group name under which services will run.\n\n\n\n\n\nUser Accounts\u00b6\nThe following user accounts are required:\n\n\n\n\n\n\nUser Account\nDescription\n\n\n\nopscode\nThe user name under which services will run.\n\nopscode-pgsql\nThe user name for PostgreSQL. (This is only required on the back end servers in a high availability setup.)\n\n\n\n\n\n\n\n\n\n\u00a9 Copyright: 2019 Chef Software, Inc.\nThis page is about: Current version of Chef.\nProvide feedback on Chef documentation.\nPrivacy policy.\n\n\n\n\n\n\n\n\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "keras installation", "id": 1912, "answers": [{"answer_id": 1899, "document_id": 1484, "question_id": 1912, "text": "pip install tensorflow\npip install keras", "answer_start": 1803, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI create an virtual environment in my conda named 'keras_ev' and install the keras in it \nby \n\nconda install keras\n\n\nafter that when i \n\nactivate keras_ev\njupyter notebook\n\n\nthe notebook does not show my keras_ev environment \n\nand i fail to import the keras in my notebook.\n\nDoes anybody know how to fix this! Thank you\n    \n\nTry conda install ipykernel in your keras_ev environment. Then it should appear in your Jupyter notebook.\n\nYou can also install Python dependencies while using your Jupyter notebook. First, activate the environment keras_ev in another terminal tab. Then install your dependency using conda or pip (conda is recommended). It should be something like the text below. \n\nIn a new terminal:\n\nsource activate keras_ev\nconda install *your_package*\n\n    \n\nIn order for Keras to work you should first install one of its backends. If you don't install any of the backends it won't work, although it seems that it is installed. please refer to here.\n    \n\nIf you have anaconda navigator installed, you can manually go there and click the environments panel (make sure to go to your preferred environment, there is a button in which you can select this)and add Keras. Then, if you run any application off of anaconda, it will automatically have Keras.\n    \n\n\nFirst You need to install Anaconda in your computer. Make sure, you\nneed to install anaconda for your python version.  \nAfter install that setup the anaconda environment and you need to\ncreate new environment using anaconda prompt or command prompt or\nterminal\n\nconda create -n yourEnvName python=3.6\n\n\nNow you can activate your environment using follwing command\n\nactivate yourEnvName\n\n\nNow install keras using PIP. Before install keras you need to install tensorflow because keras run top of the tensorflow\n\npip install tensorflow\npip install keras\n\n    \n\ntry this for activate the enviroment:\n\nconda activate keras_ev\n\n\nnow, try to start the jupyter notebook using\n\njupyter notebook\n\n\nor\n\njupyter lab\n\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to install SWIG?", "id": 1767, "answers": [{"answer_id": 1753, "document_id": 1338, "question_id": 1767, "text": "The page http://www.swig.org/download.html has a specific download for Windows with a pre-built version of swig.exe. You can download it and avoid the hassle of compiling swig by yourself.", "answer_start": 414, "answer_category": null}], "is_impossible": false}], "context": "Noob question ahead...\nI'm trying to install SWIG on Windows. According to the INSTALL document, I have to\ncd to the directory containing the package's source code and type ./configure to configure the package for your system.\nI tried the command in both the root directory and in the /CCache directory (these are the only ones that have the configure and configure.in files), however, the shell reports back that\nThe page http://www.swig.org/download.html has a specific download for Windows with a pre-built version of swig.exe. You can download it and avoid the hassle of compiling swig by yourself.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Installing git on Windows: Git Bash Here or git-cheetah shell extension?", "id": 793, "answers": [{"answer_id": 789, "document_id": 476, "question_id": 793, "text": "git bash here will give you a console window with access to git commands, shell extension will give you right click menu access to git commands creating an environment similar to TortoiseSVN and such", "answer_start": 113, "answer_category": null}], "is_impossible": false}], "context": "More specifically, what is the most commonly used version? Is the Git Bash tool the same as the shell extension? git bash here will give you a console window with access to git commands, shell extension will give you right click menu access to git commands creating an environment similar to TortoiseSVN and such. I'm installing Git on Windows XP. During the setup, the installer asks whether I want:", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to make a .NET Windows Service start right after the installation?", "id": 679, "answers": [{"answer_id": 684, "document_id": 372, "question_id": 679, "text": "You need to add a Custom Action to the end of the 'ExecuteImmediate' sequence in the MSI, using the component name of the EXE or a batch (sc start) as the source. I don't think this can be done with Visual Studio, you may have to use a real MSI authoring tool for that.", "answer_start": 219, "answer_category": null}], "is_impossible": false}], "context": "I don't remember where I found it originally (perhaps Marc Gravell?), but I did find a solution online that allows you to install and start your service by actually running your service itself. Here's the step-by-step: You need to add a Custom Action to the end of the 'ExecuteImmediate' sequence in the MSI, using the component name of the EXE or a batch (sc start) as the source. I don't think this can be done with Visual Studio, you may have to use a real MSI authoring tool for that.\nI've posted a step-by-step procedure for creating a Windows service in C# here. It sounds like you're at least to this point, and now you're wondering how to start the service once it is installed. Setting the StartType property to Automatic will cause the service to start automatically after rebooting your system, but it will not (as you've discovered) automatically start your service after installation.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "how do i use cpack install commands", "id": 1938, "answers": [{"answer_id": 1925, "document_id": 1517, "question_id": 1938, "text": "To override the file, provide your own copy of it in your source tree, perhaps in a ${CMAKE_CURRENT_SOURCE_DIR}/CMake directory, and then in your CMakeLists.txt file, add:\n\nset(CMAKE_MODULE_PATH ${CMAKE_CURRENT_SOURCE_DIR}/CMake ${CMAKE_MODULE_PATH})", "answer_start": 1580, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI'm creating a Linux tgz self-extracting installer using CPack and I'd like the installer to run a script or sequence of commands after all files have been installed. CPack documentation contains the following guidance:\n\nCPACK_INSTALL_COMMANDS  Extra commands to install components.\n\nI set this variable in my CMakeLists.txt file and I see it set in the resulting CPackConfig.cmake file, but the commands I embed in this variable do not appear anywhere in the final .sh install script. What am I missing?\n    \n\nYou're not missing anything, that's simply not how the CPACK_INSTALL_COMMANDS variable works.\n\nOn a typical project, CPack does a \"make install\" into a temporary location, in order to build the final installer based on the \"make install\" tree. The CPACK_INSTALL_COMMANDS variable is meant to be set for projects that would rather run some other command sequence, instead of the typical \"make install\" in order to produce the install tree.\n\nSo, CPack should be running your commands as it generates the package. It will not run your commands on the end user's machine at the end of him/her running the generated installation script...\n\nThere are per-generator ways of running installed executables and/or scripts at the end of the end user installation, but it will require some customization on your part. In this case, I'd recommend attempting to override the CPack.STGZ_Header.sh.in input file that is used when CPack generates the STGZ self-extracting script. Customize that file and add your calls to the bottom of it, above the line:\n\nexit 0\n\n\nTo override the file, provide your own copy of it in your source tree, perhaps in a ${CMAKE_CURRENT_SOURCE_DIR}/CMake directory, and then in your CMakeLists.txt file, add:\n\nset(CMAKE_MODULE_PATH ${CMAKE_CURRENT_SOURCE_DIR}/CMake ${CMAKE_MODULE_PATH})\n\n\n(Actually, as I'm writing this, I'm wondering if that's sufficient, or if the module path also needs to be set at the time that CPack runs... Try this, and let us know if your customization gets used by CPack or not. If not, I'll investigate a bit further and add some more advice here.)\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Slow initial server startup when using Phusion Passenger and Rails", "id": 336, "answers": [{"answer_id": 344, "document_id": 148, "question_id": 336, "text": " What's happening is that your Application and/or ApplicationSpawners are shutting down due to time-out. To process your new request, Passenger has to startup a new copy of your application, which can take several seconds, even on a fast machine. To fix the issue, there are a few Apache configuration options you can use to keep your Application alive.Here's specifically what I've done on my servers. ", "answer_start": 117, "answer_category": null}], "is_impossible": false}], "context": "To jump on the band-wagon of Phusion Passenger we've setup a staging server for a small rails app to test things out. What's happening is that your Application and/or ApplicationSpawners are shutting down due to time-out. To process your new request, Passenger has to startup a new copy of your application, which can take several seconds, even on a fast machine. To fix the issue, there are a few Apache configuration options you can use to keep your Application alive.Here's specifically what I've done on my servers. The PassengerSpawnMethod and PassengerMaxPreloaderIdleTime are the configuration options most important in your situation.", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "how do you install docutils from the terminal so that django admindocs will work", "id": 1973, "answers": [{"answer_id": 1959, "document_id": 1558, "question_id": 1973, "text": "mkdir docutilsetup\ncd docutilsetup\ncurl -o docutils-docutils.tar.gz http://docutils.svn.sourceforge.net/viewvc/docutils/trunk/docutils/?view=tar\ngunzip docutils-docutils.tar.gz \ntar -xf docutils-docutils.tar \ncd docutils\nsudo python setup.py install", "answer_start": 1419, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nDocutils is a great package. If you are using Django the admindocs package needs docutils. Instructions are available for installing with a web browser, but what if you are remote and logging in with a terminal over SSH? How to install in that case? What if you just want a quick recipe to do the job with the terminal?\n    \n\nI know I'm rather late to this question, but the accepted answer doesn't really reflect the common best practices from Python community members (and even less so from the Django community members.)  While the outlined manual installation process does work, is is far more pains taking and error prone than the following:\n\nYou really should be using Pip.  With Pip installing docutils system wide is as simple as:\n\n$ sudo pip install docutils\n\n\nThis not only works for docutils but nearly any package on the 'Cheese Shop' as well as many other code repositories (Github, Bitbucket, etc.)\n\nYou may also want to look into other common Python best practice tools like virtualenv and virtualenvwrapper so that you can avoid global package installation.\n\nTo install Pip on Ubuntu/Debain I generally do the following:\n\n$ sudo apt-get install python-pip\n\n\nBTW: for virtualenv 'sudo apt-get install python-virtualenv' and for virtualenvwrapper 'sudo apt-get install virtualenvwrapper'.\n    \n\nThe key to the install is to use the curl utility.  The following will install docutils:\n\nmkdir docutilsetup\ncd docutilsetup\ncurl -o docutils-docutils.tar.gz http://docutils.svn.sourceforge.net/viewvc/docutils/trunk/docutils/?view=tar\ngunzip docutils-docutils.tar.gz \ntar -xf docutils-docutils.tar \ncd docutils\nsudo python setup.py install\n\n\nThis performs the following steps: Create a directory to download docutils into. cd into the directory just made, and use curl to download the zipped version of docutils. Unzip the file  which creates a subdirectory docutils. cd into that directory and install with root permissions.\n\nIf you are using Django you will have to restart Django for admindocs to start working.\n    \n\nAlthough it is an old thread, I want to share the answer I found. To install type command\nsudo apt install python-docutils\n\nor\nsudo apt install python3-docutils\n\nThis will install the dependencies too. Yesterday, I installed docutils using this command for Geany editor and it is working fine.\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Is there benefit to using Monit instead of a basic Upstart setup?", "id": 1636, "answers": [{"answer_id": 1624, "document_id": 1210, "question_id": 1636, "text": "a tool like Monit that makes an actual request will provide you an answer of app sanity more faithfully", "answer_start": 701, "answer_category": null}], "is_impossible": false}], "context": "I'm configuring my server to run node.js as a daemon. I've setup Upstart to handle startup and shutdown of node, which works wonderfully. The next step is to make sure that node.js is restarted if it dies. A few of the guides have suggested using Monit (or Fugue) to monitor the process (in Monit's case by doing an HTTP request to the server and waiting for a response).\nI'm happy to use something like Monit or Fugue, but I'm not sure why one wouldn't (or couldn't) just use Upstart's respawn feature. I assume Upstart will monitor the PID of the launched process and just kick it off again if it dies. What does Monit or Fugue give you that Upstart doesn't? Given that Upstart just checks the PID, a tool like Monit that makes an actual request will provide you an answer of app sanity more faithfully. A process may happily be running but stuck in some way such that it is not serving requests.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to install Docker Engine on Ubuntu?\n\n", "id": 77, "answers": [{"answer_id": 81, "document_id": 70, "question_id": 77, "text": " the apt package index, and install the latest version of Docker\nEngine and containerd, or go to the next step to install a specific version:\n$ sudo apt-get update\n$ sudo apt-get install docker-ce docker-ce-cli containerd.io\n\n\nGot", "answer_start": 3974, "answer_category": null}], "is_impossible": false}], "context": "Install Docker Engine on Ubuntu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSearch\n\n\n\n\n\n\n\nToggle navigation\n\n\n\n\n\n\n\nHomeGuidesProduct manualsReferenceSamples\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInstall Docker Engine on UbuntuEstimated reading time: 11 minutesTo get started with Docker Engine on Ubuntu, make sure you\nmeet the prerequisites, then\ninstall Docker.\nPrerequisites\ud83d\udd17\nOS requirements\ud83d\udd17\nTo install Docker Engine, you need the 64-bit version of one of these Ubuntu\nversions:\n\nUbuntu Focal 20.04 (LTS)\nUbuntu Bionic 18.04 (LTS)\nUbuntu Xenial 16.04 (LTS)\n\nDocker Engine is supported on x86_64 (or amd64), armhf, and arm64 architectures.\nUninstall old versions\ud83d\udd17\nOlder versions of Docker were called docker, docker.io, or docker-engine.\nIf these are installed, uninstall them:\n$ sudo apt-get remove docker docker-engine docker.io containerd runc\n\nIt\u2019s OK if apt-get reports that none of these packages are installed.\nThe contents of /var/lib/docker/, including images, containers, volumes, and\nnetworks, are preserved. If you do not need to save your existing data, and want to\nstart with a clean installation, refer to the uninstall Docker Engine\nsection at the bottom of this page.\nSupported storage drivers\ud83d\udd17\nDocker Engine on Ubuntu supports overlay2, aufs and btrfs storage drivers.\nDocker Engine uses the overlay2 storage driver by default. If you need to use\naufs instead, you need to configure it manually.\nSee use the AUFS storage driver\nInstallation methods\ud83d\udd17\nYou can install Docker Engine in different ways, depending on your needs:\n\n\nMost users\nset up Docker\u2019s repositories and install\nfrom them, for ease of installation and upgrade tasks. This is the\nrecommended approach.\n\n\nSome users download the DEB package and\ninstall it manually and manage\nupgrades completely manually. This is useful in situations such as installing\nDocker on air-gapped systems with no access to the internet.\n\n\nIn testing and development environments, some users choose to use automated\nconvenience scripts to install Docker.\n\n\nInstall using the repository\ud83d\udd17\nBefore you install Docker Engine for the first time on a new host machine, you need\nto set up the Docker repository. Afterward, you can install and update Docker\nfrom the repository.\nSet up the repository\n\n\nUpdate the apt package index and install packages to allow apt to use a\nrepository over HTTPS:\n$ sudo apt-get update\n\n$ sudo apt-get install \\\napt-transport-https \\\nca-certificates \\\ncurl \\\ngnupg-agent \\\nsoftware-properties-common\n\n\n\nAdd Docker\u2019s official GPG key:\n$ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -\n\nVerify that you now have the key with the fingerprint\n9DC8 5822 9FC7 DD38 854A\u00a0\u00a0E2D8 8D81 803C 0EBF CD88, by searching for the\nlast 8 characters of the fingerprint.\n$ sudo apt-key fingerprint 0EBFCD88\n\npub   rsa4096 2017-02-22 [SCEA]\n9DC8 5822 9FC7 DD38 854A  E2D8 8D81 803C 0EBF CD88\nuid           [ unknown] Docker Release (CE deb) <docker@docker.com>\nsub   rsa4096 2017-02-22 [S]\n\n\n\nUse the following command to set up the stable repository. To add the\nnightly or test repository, add the word nightly or test (or both)\nafter the word stable in the commands below. Learn about nightly and test channels.\n\nNote: The lsb_release -cs sub-command below returns the name of your\nUbuntu distribution, such as xenial. Sometimes, in a distribution\nlike Linux Mint, you might need to change $(lsb_release -cs)\nto your parent Ubuntu distribution. For example, if you are using\nLinux Mint Tessa, you could use bionic. Docker does not offer any guarantees on untested\nand unsupported Ubuntu distributions.\n\n\nx86_64 / amd64\narmhf\narm64\n\n\n\n$ sudo add-apt-repository \\\n\"deb [arch=amd64] https://download.docker.com/linux/ubuntu \\\n$(lsb_release -cs) \\\nstable\"\n\n\n\n$ sudo add-apt-repository \\\n\"deb [arch=armhf] https://download.docker.com/linux/ubuntu \\\n$(lsb_release -cs) \\\nstable\"\n\n\n\n$ sudo add-apt-repository \\\n\"deb [arch=arm64] https://download.docker.com/linux/ubuntu \\\n$(lsb_release -cs) \\\nstable\"\n\n\n\n\n\n\nInstall Docker Engine\n\n\nUpdate the apt package index, and install the latest version of Docker\nEngine and containerd, or go to the next step to install a specific version:\n$ sudo apt-get update\n$ sudo apt-get install docker-ce docker-ce-cli containerd.io\n\n\nGot multiple Docker repositories?\nIf you have multiple Docker repositories enabled, installing\nor updating without specifying a version in the apt-get install or\napt-get update command always installs the highest possible version,\nwhich may not be appropriate for your stability needs.\n\n\n\nTo install a specific version of Docker Engine, list the available versions\nin the repo, then select and install:\na. List the versions available in your repo:\n$ apt-cache madison docker-ce\n\ndocker-ce | 5:18.09.1~3-0~ubuntu-xenial | https://download.docker.com/linux/ubuntu  xenial/stable amd64 Packages\ndocker-ce | 5:18.09.0~3-0~ubuntu-xenial | https://download.docker.com/linux/ubuntu  xenial/stable amd64 Packages\ndocker-ce | 18.06.1~ce~3-0~ubuntu       | https://download.docker.com/linux/ubuntu  xenial/stable amd64 Packages\ndocker-ce | 18.06.0~ce~3-0~ubuntu       | https://download.docker.com/linux/ubuntu  xenial/stable amd64 Packages\n...\n\nb. Install a specific version using the version string from the second column,\nfor example, 5:18.09.1~3-0~ubuntu-xenial.\n$ sudo apt-get install docker-ce=<VERSION_STRING> docker-ce-cli=<VERSION_STRING> containerd.io\n\n\n\nVerify that Docker Engine is installed correctly by running the hello-world\nimage.\n$ sudo docker run hello-world\n\nThis command downloads a test image and runs it in a container. When the\ncontainer runs, it prints an informational message and exits.\n\n\nDocker Engine is installed and running. The docker group is created but no users\nare added to it. You need to use sudo to run Docker commands.\nContinue to Linux postinstall to allow non-privileged\nusers to run Docker commands and for other optional configuration steps.\nUpgrade Docker Engine\nTo upgrade Docker Engine, first run sudo apt-get update, then follow the\ninstallation instructions, choosing the new\nversion you want to install.\nInstall from a package\ud83d\udd17\nIf you cannot use Docker\u2019s repository to install Docker Engine, you can download the\n.deb file for your release and install it manually. You need to download\na new file each time you want to upgrade Docker.\n\n\nGo to https://download.docker.com/linux/ubuntu/dists/,\nchoose your Ubuntu version, then browse to pool/stable/, choose amd64,\narmhf, or arm64, and download the .deb file for the\nDocker Engine version you want to install.\n\nNote: To install a nightly or test (pre-release) package,\nchange the word stable in the above URL to nightly or test.\nLearn about nightly and test channels.\n\n\n\nInstall Docker Engine, changing the path below to the path where you downloaded\nthe Docker package.\n$ sudo dpkg -i /path/to/package.deb\n\nThe Docker daemon starts automatically.\n\n\nVerify that Docker Engine is installed correctly by running the hello-world\nimage.\n$ sudo docker run hello-world\n\nThis command downloads a test image and runs it in a container. When the\ncontainer runs, it prints an informational message and exits.\n\n\nDocker Engine is installed and running. The docker group is created but no users\nare added to it. You need to use sudo to run Docker commands.\nContinue to Post-installation steps for Linux to allow\nnon-privileged users to run Docker commands and for other optional configuration\nsteps.\nUpgrade Docker Engine\nTo upgrade Docker Engine, download the newer package file and repeat the\ninstallation procedure, pointing to the new file.\n\nInstall using the convenience script\ud83d\udd17\nDocker provides convenience scripts at get.docker.com\nand test.docker.com for installing edge and\ntesting versions of Docker Engine - Community into development environments quickly and\nnon-interactively. The source code for the scripts is in the\ndocker-install repository.\nUsing these scripts is not recommended for production\nenvironments, and you should understand the potential risks before you use\nthem:\n\nThe scripts require root or sudo privileges to run. Therefore,\nyou should carefully examine and audit the scripts before running them.\nThe scripts attempt to detect your Linux distribution and version and\nconfigure your package management system for you. In addition, the scripts do\nnot allow you to customize any installation parameters. This may lead to an\nunsupported configuration, either from Docker\u2019s point of view or from your own\norganization\u2019s guidelines and standards.\nThe scripts install all dependencies and recommendations of the package\nmanager without asking for confirmation. This may install a large number of\npackages, depending on the current configuration of your host machine.\nThe script does not provide options to specify which version of Docker to install,\nand installs the latest version that is released in the \u201cedge\u201d channel.\nDo not use the convenience script if Docker has already been installed on the\nhost machine using another mechanism.\n\nThis example uses the script at get.docker.com to\ninstall the latest release of Docker Engine - Community on Linux. To install the latest\ntesting version, use test.docker.com instead. In\neach of the commands below, replace each occurrence of get with test.\n\nWarning:\nAlways examine scripts downloaded from the internet before\nrunning them locally.\n\n$ curl -fsSL https://get.docker.com -o get-docker.sh\n$ sudo sh get-docker.sh\n\n<output truncated>\n\nIf you would like to use Docker as a non-root user, you should now consider\nadding your user to the \u201cdocker\u201d group with something like:\nsudo usermod -aG docker your-user\n\nRemember to log out and back in for this to take effect!\n\nWarning:\nAdding a user to the \u201cdocker\u201d group grants them the ability to run containers\nwhich can be used to obtain root privileges on the Docker host. Refer to\nDocker Daemon Attack Surface\nfor more information.\n\nDocker Engine - Community is installed. It starts automatically on DEB-based distributions. On\nRPM-based distributions, you need to start it manually using the appropriate\nsystemctl or service command. As the message indicates, non-root users can\u2019t\nrun Docker commands by default.\n\nNote:\nTo install Docker without root privileges, see\nRun the Docker daemon as a non-root user (Rootless mode).\nRootless mode is currently available as an experimental feature.\n\nUpgrade Docker after using the convenience script\nIf you installed Docker using the convenience script, you should upgrade Docker\nusing your package manager directly. There is no advantage to re-running the\nconvenience script, and it can cause issues if it attempts to re-add\nrepositories which have already been added to the host machine.\nUninstall Docker Engine\ud83d\udd17\n\n\nUninstall the Docker Engine, CLI, and Containerd packages:\n$ sudo apt-get purge docker-ce docker-ce-cli containerd.io\n\n\n\nImages, containers, volumes, or customized configuration files on your host\nare not automatically removed. To delete all images, containers, and\nvolumes:\n$ sudo rm -rf /var/lib/docker\n\n\n\nYou must delete any edited configuration files manually.\nNext steps\ud83d\udd17\n\nContinue to Post-installation steps for Linux.\nReview the topics in Develop with Docker to learn how to build new applications using Docker.\n\nrequirements, apt, installation, ubuntu, install, uninstall, upgrade, updateRate this page:\u00a0566\u00a0158\u00a0i\n\n\n\n\n\nDocker EngineOverviewInstallInstallation per distroInstall on CentOSInstall on DebianInstall on FedoraInstall on UbuntuInstall binariesOptional post-installation stepsRelease notesPrevious versionsEngine 18.09 release notesEngine 18.06 release notesEngine 18.05 release notesEngine 18.04 release notesEngine 18.03 release notesEngine 18.02 release notesEngine 18.01 release notesEngine 17.12 release notesEngine 17.11 release notesEngine 17.10 release notesEngine 17.09 release notesEngine 17.07 release notesEngine 17.06 release notesEngine 17.05 release notesEngine 17.04 release notesEngine 17.03 release notesEngine 1.13 and earlierDeprecated featuresDocker AppDocker BuildxDocker ContextDocker ScanDocker ComposeOverview of Docker ComposeInstall ComposeRelease notesGetting startedEnvironment variables in ComposeEnvironment fileExtend services in ComposeNetworking in ComposeUsing Compose in productionControl startup orderSample apps with ComposeDocker Compose release notesDocker DesktopOverviewMacInstall Docker Desktop for MacUser manualDeploy on KubernetesLeveraging Multi-CPU architecture supportNetworkingDisk utilizationLogs and troubleshootingFAQsStable release notesEdge release notesWindowsInstall Docker Desktop for WindowsInstall Docker Desktop on Windows HomeUser manualDeploy on KubernetesNetworkingLogs and troubleshootingFAQsStable release notesEdge release notesDocker Desktop WSL 2 backendDashboardOpen source licensingDocker HubQuickstartDocker ID accountsSecurity and authenticationManage access tokensTwo-factor authenticationEnable two-factor authenticationDisable two-factor authenticationRecover your Docker Hub accountGenerate a new recovery codeTeams and organizationsRepositoriesVulnerability scanningOfficial imagesAutomated buildsSet up automated buildsTesting in automated buildsAdvanced automated buildsLink to GitHub and BitBucketWebhooksSlack integrationDownload rate limitAdministrationConvert an account into an organizationDeactivate an account or an organizationBillingOverviewUpgrade your planDowngrade your planAdd seatsRemove seatsPublisher & certified contentOverviewSubmit a product for Docker HubUser FAQsPublisher FAQsCertify images & pluginsCertify logging pluginsTrust chainBring Your Own License (BYOL)Release notesOpen-source projectsNotaryGet started with NotaryUse the Notary clientUnderstand the service architectureRun a Notary serviceNotary changelogConfiguration filesServer configurationSigner configurationClient configurationCommon Server and signer configurationsDocker RegistryRegistry overviewUnderstand the RegistryDeploy a registry serverConfigure a registryWork with notificationsRecipesRecipes overviewAuthenticating proxy with apacheAuthenticating proxy with nginxMirroring Docker HubRunning on macOSGarbage collectionTesting an insecure registryDeprecated featuresCompatibilityGetting helpRelease notesSuperseded products and toolsDocker MachineMachine overviewInstall MachineRelease notesGet started with a local VMProvision hosts in the cloudLearn by exampleProvision Digital Ocean DropletsProvision AWS EC2 instancesMachine concepts and helpMachine (docker-machine) CLIMachine CLI overviewMachine command-line completionactiveconfigcreateenvhelpinspectipkilllsmountprovisionregenerate-certsrestartrmscpsshstartstatusstopupgradeurlMachine driversDrivers overviewDriver options and operating system defaultsAmazon Web ServicesDigital OceanExoscaleGenericGoogle Compute EngineIBM SoftlayerMicrosoft AzureMicrosoft Hyper-VOpenStackOracle VirtualBoxRackspaceVMware FusionVMware vCloud AirVMware vSphereMigrate from Boot2Docker to MachineDocker Toolbox (deprecated)KitematicKitematic user guide: intro & overviewSet up an Nginx web serverSet up a Minecraft ServerCreating a local RethinkDB database for developmentFrequently asked questionsKnown issuesTroubleshooting\n\n\n\n\n\n\n\nEdit this page Request docs changes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn this page:\n\nPrerequisites\n\nOS requirements\nUninstall old versions\nSupported storage drivers\n\n\nInstallation methods\n\nInstall using the repository\n\nSet up the repository\nInstall Docker Engine\nUpgrade Docker Engine\n\n\nInstall from a package\n\nUpgrade Docker Engine\n\n\nInstall using the convenience script\n\nUpgrade Docker after using the convenience script\n\n\n\n\nUninstall Docker Engine\nNext steps\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhy Docker?\nWhat is a Container?\n\n\n\n\nProducts\nDocker Desktop\nDocker Hub\nFeatures\nContainer Runtime\nDeveloper Tools\nKubernetes\n\n\n\n\nDevelopers\nUse Cases\nPlay with Docker\nCommunity\nOpen Source\nDocker Captains\n\n\n\n\nCompany\nAbout Us\nBlog\nCustomers\nPartners\nNewsroom\nCareers\nContact Us\n\n\n\n\n\n\nStatus\nSecurity\nLegal\nContact\n\n\n\n\n\n\n\nCopyright \u00a9 2013-2020 Docker Inc. All rights reserved.\n\n\n\nTwitter\nYoutube\nGitHub\nLinkedin\nFacebook\nSlideshare\nReddit\n\n\n\n\n\n\n\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Forcing browsers to reload Silverlight xap after an update", "id": 520, "answers": [{"answer_id": 522, "document_id": 245, "question_id": 520, "text": "You have to do this for your xap in your html:\n<param name=\"source\" value=\"ClientBin/myApp.xap\"/>\nI would version it so whenever you do a push you change the version number. Example:\n<param name=\"source\" value=\"ClientBin/myApp.xap?ver=1\"/>", "answer_start": 486, "answer_category": null}], "is_impossible": false}], "context": "I have a Silverlight control packaged up and deployed to a SharePoint web part. I'm having trouble with the browser loading new versions of the control after I push an update. I'm updating the assembly and file version of my xap project, but it doesn't seem to matter. The only way to get the browser to load the new xap is to go in and delete temporary Internet files. For me, during development, that's OK, but I'll need to find a solution before it's time for production. Any ideas?\nYou have to do this for your xap in your html:\n<param name=\"source\" value=\"ClientBin/myApp.xap\"/>\nI would version it so whenever you do a push you change the version number. Example:\n<param name=\"source\" value=\"ClientBin/myApp.xap?ver=1\"/>\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "how to remove docker installed using wget", "id": 1998, "answers": [{"answer_id": 1984, "document_id": 1584, "question_id": 1998, "text": "  You may (as I did) also need to remove runtime files such as /var/run/docker.sock.\n  Try find / -name '*docker*' to see all of these files. (This searches your entire filesystem, look in at least /var, /run, /etc separately if you prefer not to do this.)", "answer_start": 2062, "answer_category": null}], "is_impossible": false}], "context": "\n\n                    \n\n            \n\n        \n\n            \n\n                \n\n                    \n\n                        Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.\n\n                        \n\n                    \n\n                \n\n            \n\n        \n\n            \n\n        \n\n                \n\n                    \n\n                \n\n            \n\n                \n\n                    Want to improve this question? Update the question so it's on-topic for Stack Overflow.\n\n                \n\n                    Closed 5 years ago.\n\n\n\n            \n\n        \n\n            \n\n                    \n\n                        Improve this question\n\n                    \n\n            \n\n\n\n\n\n    \n\n\n\nI've installed docker following installation tutorial on docker.com, using wget https://get.docker.com/ | sh command.\nNow i need to remove it entirely. apt-get remove docker, apt-get --auto-remove docker, apt-get remove docker.io, apt-get --auto-remove docker.io or any other combination doesn't work, since I didn't install it using apt-get. Is there any way to remove docker faster than removing it by hand?\n\n(I'm working on Ubuntu 14.04 if it makes any difference).\n    \n\nThe uninstallation step mentions:\n\nsudo apt-get purge -y docker-engine\nsudo apt-get autoremove -y --purge docker-engine\nsudo apt-get autoclean\n\n\nNote: chasmani adds in the comments:\n\n\n  I had to also add docker-ce, docker.io and docker to the first two lines to completely get rid of \n\n\nsudo apt-get purge -y docker-engine docker docker.io docker-ce\nsudo apt-get autoremove -y --purge docker-engine docker docker.io docker-ce\n\n\nIt adds:\n\n\n  The above commands will not remove images, containers, volumes, or user created configuration files on your host. If you wish to delete all images, containers, and volumes run the following command:\n\n\nsudo rm -rf /var/lib/docker\n\n\n\n  Remove docker from apparmor.d:\n\n\nsudo rm /etc/apparmor.d/docker\n\n\n\n  Remove docker group:\n\n\nsudo groupdel docker\n\n\nAs Micah Smith adds in the comments:\n\n\n  You may (as I did) also need to remove runtime files such as /var/run/docker.sock.\n  Try find / -name '*docker*' to see all of these files. (This searches your entire filesystem, look in at least /var, /run, /etc separately if you prefer not to do this.)\n\n\nNow, You have successfully deleted docker.\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Install specific git commit with pip", "id": 1587, "answers": [{"answer_id": 1576, "document_id": 1164, "question_id": 1587, "text": "You can also install a compressed distribution. This is faster and more efficient, as it does not require cloning the entire repository. GitHub creates those bundles automatically.", "answer_start": 262, "answer_category": null}], "is_impossible": false}], "context": "I'm developing a django app and I'm using pip to manage my requirements. How can I do to install a specific git's commit?\nIn my case I need to install this commit: https://github.com/aladagemre/django-notification/commit/2927346f4c513a217ac8ad076e494dd1adbf70e1\nYou can also install a compressed distribution. This is faster and more efficient, as it does not require cloning the entire repository. GitHub creates those bundles automatically.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Should I use git to deploy websites?", "id": 1348, "answers": [{"answer_id": 1338, "document_id": 917, "question_id": 1348, "text": "You can use git to track my website, and deploy it like this:\ngit archive --format=tar --prefix=\"homepage/\" master | gzip | ssh webserver \"ta r xvz -C ~/public_html\"", "answer_start": 483, "answer_category": null}], "is_impossible": false}], "context": "I have a site running on django, (but the question applies to anything, php, etc)\nCurrently I'm using unison to deploy my changes, and I (kinda used to) love it because before that I was doing it manually!!\nNow, as I'm getting my feet wet with git, I'm starting to love it! And I'm thinking if maybe I should use it instead of unison to deploy my changes!\nThis way I'll have the added benefit of being able to revert my changes if somehow deploying them turned out to be a disaster!\nYou can use git to track my website, and deploy it like this:\ngit archive --format=tar --prefix=\"homepage/\" master | gzip | ssh webserver \"ta r xvz -C ~/public_html\"\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "setting up phpstorm with tortoisesvn", "id": 1954, "answers": [{"answer_id": 1940, "document_id": 1535, "question_id": 1954, "text": "Connect PhpStorm to your SVN server with his built-in feature.\nOr use TortoiseSVN", "answer_start": 1283, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI have installed the VisualSVN server on our Windows Server 2008 plus i did connect it with Dreamweaver on other client PC.\n\nso Dreamweaver is ready to go.\nBut i also want to setup the PHPStorm on other Client PC with that visualSVN.\n\nBut i want PHPStorm to use TortoiseSVN to connect to VisualSVN.\n\nI can't find its settings page, i am new to PHPStorm, Especially to this Subversion Control thingy.\n\nI also searched for this over web, but i cant find specific PHPStorm Setup procedure with TortoiseSVN or connecting to VisualSVN Server.\n\nCan anyone Guide me to the Right Path?\n    \n\nThe accepted answer is not entirely accurate.  It is possible to use TortoiseSVN through PHPStorm's External Tools configurations.  This does not integrate into the project navigation directly, but does allow direct file manipulation (and allows for 'blame' support - something PHPStorm's subversion lacks).\n\nSimilar functionality is used in eclipse.\n\nExample, paths/macros's might need to be altered:\n\n\nName: SVN View Log\nProgram: C:\\Program Files\\TortoiseSVN\\bin\\TortoiseProc.exe\nParameters: /command:log /path:\"$FileName$\"\nWorking Directory: $FileDir$\n\n    \n\nIt's impossible to connect PhpStorm to TortoiseSVN as they are doing the same thing.\n\nThen you have two choices : \n\n\nConnect PhpStorm to your SVN server with his built-in feature.\nOr use TortoiseSVN\n\n\nI'll recommande using PhpStorm feature as it is directly in the IDE.\n\nYou can find documentation here : https://www.jetbrains.com/phpstorm/webhelp/using-subversion-integration.html\n    \n\nYou can actually do this very easily and it will work inside PHP Storm 8.  Install Tortoise SVN and make sure to include \"Command Line Tools\" as part of the installation.  Then you can enable External client and select the \"svn.exe\" as the executable.  This will enable SVN 1.8 format and still work within the IDE.\n    \n\nMy solution:\n\n\nInstall TortoiseSVN\nInstall CollabNet Subversion with command-line binaries (32 or 64-bit) \n\n\n\n\nOpen phpStorm\nFile &gt; Settings &gt; Version Control &gt; Subversion\nSet path for your SVN command line client  \n\n\n\ne.g.\n\n\n  C:\\Program Files\\CollabNet\\Subversion Client\\svn.exe\n\n\nTortoise can be used as a GUI tool, whereas CollabNet Subversion command line tool can be used with phpStorm. Enjoy!\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "MSBuildExtensionsPath32 not set correctly?", "id": 550, "answers": [{"answer_id": 552, "document_id": 275, "question_id": 550, "text": "You could override it by setting an environment variable.\nSET MSBuildExtensionsPath=\"C:\\Program Files\\MSBuild\"", "answer_start": 185, "answer_category": null}], "is_impossible": false}], "context": "For the life of me, I cannot find where this value is actually set. It SHOULD be pointing at C:\\Program Files\\MSBuild, but on our build box, it's pointing at C:. How can I change this?\nYou could override it by setting an environment variable.\nSET MSBuildExtensionsPath=\"C:\\Program Files\\MSBuild\"\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How do I install cURL on Windows?", "id": 706, "answers": [{"answer_id": 710, "document_id": 397, "question_id": 706, "text": "you need to edit. first, add a PHPinfo(); to a info.php, and run it from your browser.Enable the extension", "answer_start": 1833, "answer_category": null}], "is_impossible": false}], "context": "I have followed all the instructions here: http://www.tonyspencer.com/2003/10/22/curl-with-php-and-apache-on-windows/\nto install & config apache get the PHP5 packages and get the CURL packages.\nI run the apache and run a PHP script. no problem. but when I run the php script with curl, it fails.\nIt returns: **Call to undefined function curl_version() in C:\\Program Files\\Apache Software Foundation\\Apache2.2\\htdocs\\testing.php on line 5**\nIn which line 5 is a called to curl_init()\nI output the php -i to see whether the right path to extension is called. It is correctly set:\nextension_dir => C:\\PHP\\ext => C:\\PHP\\ext\ncURL support => enabled\ncURL Information => libcurl/7.16.0 OpenSSL/0.9.8g zlib/1.2.3\nI even tried to run curl_version() but still, same kind of error comes up.\nIt looks like the PHP can't find the CURL extension, but the php.ini (and also php -i) shows that it is set.\nany idea? :)\nP.S>  System I m running on:\nWindows XP\nApache 2.2\nPHP 5.2.6\nCURL Win32 Generic Binaries: Win32 2000/XP  metalink    7.19.0  binary  SSL enabled     Daniel Stenberg     249 KB\nI didn't get this:\nWin32 2000/XP   7.19.0  libcurl     SSL enabled     G\u00fcnter Knauf    1.55 MB\nShould I get this one instead?\n________________________________________\nThe reason I need to use CURL is that it is the requirement from my project. So, I can only stick with that. XAMPP... how does it work in Windows? Is there any site that you can recommend? Thanks.\nI have tried a lot of things on installing cURL and check everything, but still, I'm stilling circling around the problem and have no idea what's going on.\nThe Apache server uses the right PHP.ini. and the PHP.ini has the correct extension_dir and extension=php_curl.dll I have no idea why it doesn't work. even I follow every step for setting it up.\nYou're probably mistaking what PHP.ini. you need to edit. first, add a PHPinfo(); to a info.php, and run it from your browser.Enable the extension\nYou're done :-)\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How do I install an R package from source?", "id": 1583, "answers": [{"answer_id": 1572, "document_id": 1160, "question_id": 1583, "text": "If you have the file locally, then use install.packages() and set the repos=NULL:\ninstall.packages(path_to_file, repos = NULL, type=\"source", "answer_start": 287, "answer_category": null}], "is_impossible": false}], "context": "A friend sent me along this great tutorial on webscraping NYtimes with R. I would really love to try it. However, the first step is to installed a package called RJSONIO from source.\nI know R reasonably well, but I have no idea how to install a package from source.\nI'm running Mac OSX.\nIf you have the file locally, then use install.packages() and set the repos=NULL:\ninstall.packages(path_to_file, repos = NULL, type=\"source\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "System.IO.FileNotFoundException: Could not load file or assembly 'X' or one of its dependencies when deploying the application", "id": 1721, "answers": [{"answer_id": 1709, "document_id": 1294, "question_id": 1721, "text": "I resolved this problem by renaming the DLL. The DLL had been manually renamed when it was uploaded to its shared location (a version number was appended to the file name). Removing the version number from the downloaded file resolved the issue", "answer_start": 1029, "answer_category": null}], "is_impossible": false}], "context": "I'm having a strange problem with deploying an application, which references an assembly, written in managed c++.\nI've created an assembly X, compiled it and referenced it in an exe file, called Starter.\nStarter.exe starts normally on local mashine. However, when I copy ALL contents of the starter debug folder to a virtual mashine, and try to start it there, it crashes with following exception:\nUnhandled Exception: System.IO.FileNotFoundException: Could not load file or \nassembly 'X' or one of its dependencies. The specified module could not be found.\nThis does not make any sense to me, because X is right in the same folder as Starter.exe.\nWhat could be causing this problem?\nUPDATE\nI've examined the dependencies in Reflector on the target machine, and it was able to find files for all of those.\nI've also changed the configurations to x86/win32 for all projects.\nUPDATE\nHere are the logs from Fusion Log (location: C:\\FusionLog\\Default\\Starter.exe\\X, Version=1.0.4538.22813, Culture=neutral, PublicKeyToken=null.HTM):\nI resolved this problem by renaming the DLL. The DLL had been manually renamed when it was uploaded to its shared location (a version number was appended to the file name). Removing the version number from the downloaded file resolved the issue.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to modify .NET config files during installation?", "id": 1166, "answers": [{"answer_id": 1159, "document_id": 743, "question_id": 1166, "text": "We use WIX to change the application's configuration file. It works really well, you'll need to add wixUtilExtension.dll in the reference", "answer_start": 348, "answer_category": null}], "is_impossible": false}], "context": "I use the app.config file to store some values (path to a mapping database, data connection selections). These settings differ on the user machines and I would like the installer to set them right. Is there an installer that can work with .NET config files during setup and allow me to create some dialogs that would help me fill in these values ?\nWe use WIX to change the application's configuration file. It works really well, you'll need to add wixUtilExtension.dll in the reference.\nI know this question may be similar to : Initializing user.config or app.exe.config during install, but I am not limited to VS 2008 setup project and I want to change the settings in the config files.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "are there guidelines and or standards for creating desktop shortcuts during inst", "id": 1479, "answers": [{"answer_id": 1468, "document_id": 1055, "question_id": 1479, "text": "From here: https://docs.microsoft.com/en-us/windows/win32/uxguide/winenv-desktop", "answer_start": 397, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nPersonally I hate auto-created desktop shortcut icons, but some folks seem to think that unless your installer clutters up your desktop, it hasn't worked correctly!\n\nAre there definite guidelines for this (for Windows?)\n\n(Having a \"Leave clutter on my desktop?\" checkbox in the installer is one option, but to my mind, that's just put MORE clutter into the installer...)\n    \n\nFrom here: https://docs.microsoft.com/en-us/windows/win32/uxguide/winenv-desktop\n\nIf your users are very likely to use your program frequently, provide an option during setup to put a program shortcut on the desktop. Most programs won't be used frequently enough to warrant offering this option.\nPresent the option unselected by default. Requiring users to select the option is important because once undesired icons are on the desktop, many users are reluctant to remove them. This can lead to unnecessary desktop clutter.\nIf users select the option, provide only a single program shortcut. If your product consists of multiple programs, provide a shortcut only to the main program.\nPut only program shortcuts on the desktop. Don't put the actual program or other types of files.\n\n    \n\nMy take is this: the installer must ask me if I want a desktop icon - to which I can reply yes or no.\n\nAny app that just blindly and without asking installs its icon on my desktop is a bad installation in my opinion.\n\nAsk for permission - if I deem your app important enough to me personally, I might say yes (but most likely I won't). Give your users a choice - don't just assume since it's your app, it's so darn important to everyone that everyone will want to clutter up their desktop with your program icon.\n\nThe same goes for the installation directory - unless you have a very good technical reason why you can't install anywhere, allow me to change the program's installation target directory. Not everyone is a big fan of the \"c:\\program files\" folder hierarchy (I'm not, for one - I like to keep my apps in C:\\bin for instance).\n\nSo in general: any decent installer should ASK the user installing for these things and present sensible defaults - but always give me the option to change the settings to my liking (to my standards).\n    \n\nI don't know of any meaningful guidelines, other than your conscience. As a programmer, I sympathize: I don't want icons on my desktop, either :-) However, having watched non-technical family members struggle with installing software and then trying to run it, I think it's worth noting that\n\n1) There are more non-techies than techies\n2) Techies can cope with checkboxes on installers\n\nBased on that, I usually go for having a checkbox on the installer for creating icons, which defaults to on. I don't mind anything other than the \"always create icons\" approach. (I'm looking at you, Adobe.)\n    \n\nI think that depends on what you see your client doing with the app, the level of the client's expertise with computers and how frequently you see him using it.\n\n\nIf the client is not very well versed with computers he would prefer to have the icon on the desktop where he can access it. If you target market is experienced users you don't need to bother because he can make the icon himself if he wants it.\nIf the application is for daily frequent use like a web browser the client would want it on his desktop for quick access.\n\n\nFinally the decision rests on you. If you're being obnoxious you can create 4 icons on the desktop (I've seen apps that do that).\n\nI don't think asking for permission is a bad idea. After all the installation needs to be done only once and it's just one checkbox to tick.\n    \n\nI've no particular love for desktop (or quick launch) shortcut icons either, but I think that you should still give your users the option in the installer to install neither, one or both of these shortcuts.\n\nDepending on how computer literate your users are (if it's possible to determine this) you can default the two options to either enabled or disabled accordingly.\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Global Node modules not installing correctly. Command not found", "id": 675, "answers": [{"answer_id": 680, "document_id": 368, "question_id": 675, "text": "npm config set prefix /usr/local\nthen try running npm install -g again", "answer_start": 289, "answer_category": null}], "is_impossible": false}], "context": "I am having a problem installing global node modules and everything I find online says the solve is just adding -g. Which is not the problem. I believe it's a linking issue or wrong directory issue. 148\nThis may mean your node install prefix isn't what you expect.\nYou can set it like so:\nnpm config set prefix /usr/local\nthen try running npm install -g again, and it should work out. Worked for me on a mac, and the solution comes from this site:\nhttp://webbb.be/blog/command-not-found-node-npm/\nEDIT: Note that I just came across this again on a new Mac I'm setting up, and had to do the process detailed here on stackoverflow as well.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "unable to download application app could not be downloaded at this time", "id": 1465, "answers": [{"answer_id": 1454, "document_id": 1036, "question_id": 1465, "text": "nswers, you'll need to check the Console t", "answer_start": 3144, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI have a \"PLUS\" version of my app and the regular version.  I archive both of them using different targets.  I upload the ipa's to TestFlight (also to my Hockey server) and the \"PLUS\" version always downloads just fine.  But the regular version always give me the \"Unable to Download Application\" error.\n\nI'm not changing the code at all between the two builds.  I do a \"clean\" in between builds.  The only difference in code is I define a \"PLUS_VER\" macro that changes some of the code.  And there are a few differences in resource files associated with the two targets.  This used to never happen, but this issue has been consistent across every time I've built with this current version.\n\nAny ideas?\n\n&lt;&gt;&lt;\n    \n\nthanks for the tip on looking at the console.  While it was installing I saw that the provisioning profile didn't agree with the iCloud ubiquitous key-value store entitlement.  We're enabling iCloud with this release.\n\nI went in and \"edited\" each provisioning profile (essentially doing nothing....) and re-downloaded them and it fixed the problem.\n\nKind of annoying that Apple doesn't invalidate your provisioning profile or let you know it needs updating at least... :-/\n    \n\nI was battling with this same issue and this question is 2nd on google. The error basically means that there is a reason that the app cannot be installed. There are lots of different reasons this could be the case. The only way to find out is to look at the console while the app installs and a more useful error message will appear there.\n\nIf you have access to the machine you can use the organiser window in Xcode to see the console for an attached device. If you don't have access to the machine get the user to install the iPhone Configuration Utility. There are both Windows and Mac versions available to download from Apple. They can then see the console and email the output to you.\n\nIn my case I was using Testflight to install development builds. I thought there must be some provisioning issue and tried loads of different things. In the end when I saw the console output it was obvious that we were trying to install an iOS 6 app on an iOS 5 device.\n    \n\nI had problems using OTA (useing XCode 4.3.3, i guess its the same in XCode 4.x.x). \n\nIt only let me install Apps on Development Devices when i use the Release Provisioning Profile. When i changed the setting in the Scheme (By klicking on the Schemename and selct \"Edit Scheme...\". On the left row, there are this scheme settings, where i had to click \"Archive\" and change the \"Build Configuration\" to \"Debug\"). Than i was able to distribut Apps using OTA with my Development Provisioning Profile.\n\nHope it helps someone\n    \n\nCheck that the application bundle ID in the Info.plist matches the installation plist on the servers, that the installation URL point to the correct plist and the installation plist points to the correct IPA URL.\n    \n\nI just had just problem, it seems testflight is down. It was not showing in the system status from the beginning but they updated it to reflect the period I experienced the problem.\n\n\n    \n\nAs said in other answers, you'll need to check the Console to be sure.\n\nAs an example, i found the following error which is self-explanatory:\n\nThe bundle being installed with bundle ID com.foo.myapp is authorized by a free provisioning profile, but apps validated by those are not allowed to be installed from this source\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Is it possible to answer dialog questions when installing under docker?", "id": 681, "answers": [{"answer_id": 686, "document_id": 374, "question_id": 681, "text": " You should set DEBIAN_FRONTEND=noninteractive as an envvar", "answer_start": 330, "answer_category": null}], "is_impossible": false}], "context": "From what I understand I just simply can't respond to the dialogs, but is there some way that I can pass a parameter to answer each question in advance? I know that it's just changing some configurations, so I could do it after the fact, but presumably it's better to let the install scripts do it so everything gets set properly. You should set DEBIAN_FRONTEND=noninteractive as an envvar. In most cases this will at least make it so the installation doesn't error out.\nAlso as @Azdle mentioned, using debconf-set-selections will let you set specific items.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to use virtualenvwrapper in Supervisor?", "id": 535, "answers": [{"answer_id": 537, "document_id": 260, "question_id": 535, "text": "One way to use your virtualenv from the command line is to use the python executable located inside of your virtualenv.\nfor me i have my virtual envs in .virtualenvs directory. For example\n/home/ubuntu/.virtualenvs/yourenv/bin/python", "answer_start": 338, "answer_category": null}], "is_impossible": false}], "context": "Now, I want to use Supervisord to manage the same project as it is ready for deployment. The question is what is the proper way to tell Supervisord to activate the right virtualenv before executing the script? Do I need to write a separate bash script that does this, and call that script in the command field of Supervisord config file?\nOne way to use your virtualenv from the command line is to use the python executable located inside of your virtualenv.\nfor me i have my virtual envs in .virtualenvs directory. For example\n/home/ubuntu/.virtualenvs/yourenv/bin/python\n", "document_id": 0}]}, {"paragraphs": [{"qas": [], "context": "When I was developing and testing my project, I used to use virtualenvwrapper to manage the environment and run it:\nworkon myproject\npython myproject.py\nOf course, once I was in the right virtualenv, I was using the right version of Python, and other corresponding libraries for running my project.\nNow, I want to use Supervisord to manage the same project as it is ready for deployment. The question is what is the proper way to tell Supervisord to activate the right virtualenv before executing the script? Do I need to write a separate bash script that does this, and call that script in the command field of Supervisord config file?\nOne way to use your virtualenv from the command line is to use the python executable located inside of your virtualenv.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "how to make apt get accept new config files in a unattended install of debian fr", "id": 1442, "answers": [{"answer_id": 1431, "document_id": 1015, "question_id": 1442, "text": "r is\n\napt-get -o Dpkg::Options::=\"--force-confnew\" install your-p", "answer_start": 1400, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI am writing a script for a unattended install of  a package that is in our Repo, It is a Software Package with one of the Debians marked config. \nIs there any option that I can pass to apt-get/aptitude so that it accepts the new Config Files.\n\nBasically I need a apt/aptitude equivalent of  dpkg --force-confnew \n\nI need to answer the following question posed while apt-get installation with a Y\n\n\n\nConfiguration file `/opt/application/conf/XXX.conf'\n\n==&gt; File on system created by you or by a script.\n\n==&gt; File also in package provided by package maintainer.\n\nWhat would you like to do about it ?  Your options are:\n\nY or I  : install the package maintainer's version\n\nN or O  : keep your currently-installed version\n\n  D     : show the differences between the versions\n\n  Z     : background this process to examine the \n\n\nThe default action is to keep your current version.\n\n\n\nAdditional Info:\n\nAlso,I am passing the sudo password in a pipe to execute the command\n\necho \"mysudopass\"|sudo -S apt-get mypackage\n\nThis is flagging an Error in installation when the installation is at the Config Interactive phase.\n\nI am on Ubuntu 10.04 \napt version:  apt 0.7.25.3\n\nWhy i cannotuse dpkg : These debians are to be installed from Repo and I dont have local debians on my machine\n\nThanks Guys for your Help in advance !!!!\n    \n\nI think the apt-get command line you're looking for is\n\napt-get -o Dpkg::Options::=\"--force-confnew\" install your-package\n\n\nAs for your sudo -S issue, maybe try adding some spaces around the '|' character.\nIs not, the question How to pass the password to su/sudo/ssh without overriding the TTY? may be useful.\n    \n\nI tried the suggestions in the other answers and discovered that it wasn't enough when trying to install the localepurge package.\n\nAfter using debconf-set-selections to set what I needed, I had to use the following:\n\nDEBIAN_FRONTEND=noninteractive apt-get -o Dpkg::Options::=\"--force-confnew\" install localepurge\n\n\nOnly with the DEBIAN_FRONTEND environment variable set to noninteractive could I force localepurge to do what I had asked. This may be true of other packages.\n\nOf course with a script you could export the variable if you wanted to, but be aware that it could mess with other packages. For example:\n\n#!/bin/sh\nexport DEBIAN_FRONTEND=noninteractive\napt-get -o Dpkg::Options::=\"--force-confnew\" install -y localepurge foo bar\n\n\nWhere the -y tells apt-get to stop asking questions (see the man page).\n    \n\nYou can also have apt configured to do that by creating /etc/apt/apt.conf.d/local and adding:\n\nDpkg::Options {\n   \"--force-confdef\";\n   \"--force-confold\";\n}\n\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "database.yml deployment best practice", "id": 1055, "answers": [{"answer_id": 1051, "document_id": 636, "question_id": 1055, "text": "You should do this in your database.yml:\nafter \"deploy:update_code\",\"deploy:config_symlink\"\n\nnamespace :deploy do\n\n  task :config_symlink do\n    run \"cp #{shared_path}/../../shared/database.yml #{release_path}/config/database.yml\"\n  end\nend", "answer_start": 361, "answer_category": null}], "is_impossible": false}], "context": "I don't check my database.yml file into source control and I was wondering what others do/best practice for copying this file over to the server when deploying.\nCurrently, I keep a shared folder called shared that lives outside of my deply_to dirs. I keep my database.yml and other config files there and have a hook in cap to cp those over during deployment. \nYou should do this in your database.yml:\nafter \"deploy:update_code\",\"deploy:config_symlink\"\n\nnamespace :deploy do\n\n  task :config_symlink do\n    run \"cp #{shared_path}/../../shared/database.yml #{release_path}/config/database.yml\"\n  end\nend\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How do I install and use cURL on Windows?", "id": 137, "answers": [{"answer_id": 145, "document_id": 83, "question_id": 137, "text": "It is possible that you won't need to download anything: If you are on Windows 10, version 1803 or later, your OS ships with a copy of curl, already set up and ready to use. If you have Git for Windows installed (if you downloaded Git from git-scm.com, the answer is yes), you have curl.exe under: C:\\Program Files\\Git\\mingw64\\bin\\\u3002", "answer_start": 101, "answer_category": null}], "is_impossible": false}], "context": "I have downloaded a cURL zip file from here, but it seems to contain source code, not an executable. It is possible that you won't need to download anything: If you are on Windows 10, version 1803 or later, your OS ships with a copy of curl, already set up and ready to use. If you have Git for Windows installed (if you downloaded Git from git-scm.com, the answer is yes), you have curl.exe under: C:\\Program Files\\Git\\mingw64\\bin\\\u3002\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Capistrano Staging Deploy Requires Whole New Database Migration", "id": 1320, "answers": [{"answer_id": 1310, "document_id": 889, "question_id": 1320, "text": "you should probably have a different location for production sqlite3 file where it will not get rewritten for every deploy, instead of db/production.sqlite3. Something like /home/user/production.sqlite3", "answer_start": 705, "answer_category": null}], "is_impossible": false}], "context": "I have Capistrano set up such that it allows me to deploy the staging version of my Ruby on Rails website to an amazon server from a git repository successfully. However, after deploying, I must run a migration on the database on the server. If I do not, any page with login forms or database-based content cannot load. When I run the migration on the server, I see every singly migration on the site being migrated, not just the most recent ones.\nI know that Capistrano can maintain the previous migrations, and I thought it did that automatically. My question is, how do stop the apparent database wipe or loss which is occurring so that a migration is only needed if there are actually new migrations?\nyou should probably have a different location for production sqlite3 file where it will not get rewritten for every deploy, instead of db/production.sqlite3. Something like /home/user/production.sqlite3\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Installing Flash Player on an Android Emulator?", "id": 981, "answers": [{"answer_id": 976, "document_id": 604, "question_id": 981, "text": "1 Download and install the Android emulator SDK on your Eclipse installation.\n\n2 Download the APK file of the Adobe Flash player and save it to the \"yourSDK/tools/\" directory in your SDK installation. Replace \"yourSDK\" with the name of your SDK root directory.\n\n3 Run \"adb install yourSDK/tools/flash.apk\" in the shell in the SDK. Replace \"yourSDK\" with the name of your SDK root directory and \"flash.apk\" with the exact name of the Flash APK file.", "answer_start": 1342, "answer_category": null}], "is_impossible": false}], "context": "3\n\n\nI want to know the process(detailed please) of installing flash player on an android emulator. I need to do this to test an app and currently I don't have any physical device.\n\nThings that i have already tried: 1. i have checked out all the stack overflow links regarding this topic. 2. i already know adobe has stopped flash for android. So links of google play store with flash player are all dead. 3. i have tried installing it from a third party such as http://d-h.st/x4v but this link is a pain to open from the browser inside the emulator.\n\nSo please help of any sort is highly appreciated.\n\nThanks in advance.\n\nandroid\ninstallation\nemulation\nShare\nImprove this question\nFollow\nedited Nov 15 '17 at 14:48\n\nsrv_sud\n59577 silver badges2626 bronze badges\nasked Mar 2 '13 at 4:44\n\nD'yer Mak'er\n1,61255 gold badges2424 silver badges4747 bronze badges\nGo through this: stackoverflow.com/questions/4263339/\u2026 \u2013 \nSanket Pandya\n Mar 2 '13 at 5:32\nAdd a comment\n2 Answers\n\n2\n\nThough the Android Emulator only comes with a select few applications installed, you can install others as long as you have access to the basic download file APK file. If you want to develop an application on your emulator that interacts with the Adobe Flash Player, there is a way to work around the limitations without installing the Android Market.\n\nInstructions\n\n1 Download and install the Android emulator SDK on your Eclipse installation.\n\n2 Download the APK file of the Adobe Flash player and save it to the \"yourSDK/tools/\" directory in your SDK installation. Replace \"yourSDK\" with the name of your SDK root directory.\n\n3 Run \"adb install yourSDK/tools/flash.apk\" in the shell in the SDK. Replace \"yourSDK\" with the name of your SDK root directory and \"flash.apk\" with the exact name of the Flash APK file.\n\nRead more: How to Get a Flash Package on an Android Emulator | eHow.com http://www.ehow.com/how_12172392_flash-package-android-emulator.html#ixzz2MM3xgmjX\n\nShare\nImprove this answer\nFollow\nanswered Mar 2 '13 at 4:51\n\nandroidgeek\n3,33711 gold badge1313 silver badges2727 bronze badges\nthanks a lot for the detailed steps @androidgeek . This solved my problem. \u2013 \nD'yer Mak'er\n Mar 20 '13 at 6:54\nAdd a comment\n\n0\n\nJust download the latest archive from Adobe's OFFICIAL page. You can do that from device. If the URL is too unwieldy, just tinyurl it.\n\nI've set this one up a while ago as it has come up more than once: http://tinyurl.com/flash4ics It downloads the very last (truly the last one!) Flash player from Adobe.", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to host multiple .NET Core apps under the same URL?", "id": 1080, "answers": [{"answer_id": 1072, "document_id": 657, "question_id": 1080, "text": "You could use a reverse proxy for serving multiple ASP Net Core application on the same domain.", "answer_start": 250, "answer_category": null}], "is_impossible": false}], "context": "I am building a few web sites in ASP.NET Core (multiple user interface applications and a WebAPI app). They all work together, utilising the WebAPI. For the purpose of my question we'll call them App1, App2 and App3. I am hosting my websites in IIS.\nYou could use a reverse proxy for serving multiple ASP Net Core application on the same domain.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Best deployment strategy for PlayFramework applications?", "id": 1043, "answers": [{"answer_id": 1038, "document_id": 624, "question_id": 1043, "text": "The best approach is to use the included Play server, putting NGinx as reverse proxy in front of it to tackle all the redirection/request managing.", "answer_start": 483, "answer_category": null}], "is_impossible": false}], "context": "This question is server oriented. I have a hosted server (a rather small one, 1,6Ghz atom, 2Go, 200 GO) with a couple (4 or 5) play apps and more coming. Most of these apps have a real small usage, let's say a hundred requests a day each.\nIs it better to deploy each of those applications using the embedded server of Play! and thus use 64mb of memory for each application?\nOr deploy a Tomcat with all the applications inside the tomcat? With a bigger memory shared by all the apps?\nThe best approach is to use the included Play server, putting NGinx as reverse proxy in front of it to tackle all the redirection/request managing.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "python setup script extensions how do you include a h file", "id": 1403, "answers": [{"answer_id": 1392, "document_id": 975, "question_id": 1403, "text": "You should provide the include files via \"include_dirs\"", "answer_start": 1947, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nSo I've got a directory that looks something like this:\n\n home\\\n     setup.py\n     some_python_file.py\n     ext\\\n         __init__.py\n         c_file1.c\n         c_file2.c\n         ext_header.h\n\n\nObviously the header file is necessary to compile the c files, but the problem is that I can't get the setup script to include the header file.\n\nMy extension object is something like this:\n\nExtension('ext.the_extension', ['ext/c_file1.c', 'ext/c_file2.c'])\n\n\nWhich works, but doesn't include the header file.  If I change it to:\n\nExtension('ext.the_extension', ['ext/c_file1.c', 'ext/c_file2.c', 'ext_header.h'])\n\n\nIt includes the '.h' file but then doesn't build when I run install.  Instead it gives and error error: unknown file type '.h' (from 'ext/ext_header.h')\n\nIf I include the header file as a data file like this:\n\ndata_files=[('ext', ['ext/ext_header.h'])]\n\n\nit doesn't work at all, the .h file doesn't even make it into the MANIFEST file.\n\nSo my queustion is, how do you include this extension with the header file so that python setup.py install will build it correctly?\n    \n\nI have a feeling pyfunc is on track for a more standard solution, but I did find another solution on my own.  I have no idea if this is a good solution or just a hack, but all I did is add the header file to the MANIFEST.in.  The documentation doesn't really make it seem like this is what the MANIFEST.in file is for, but it does work.  My MANIFEST.in file now looks like this:\n\ninclude ext/ext_header.h\n\n\nWhich includes the file and sucessfully compiles when I run python setup.py install\n    \n\nFrom the docs,\n\nmodule1 = Extension('demo',\n                define_macros = [('MAJOR_VERSION', '1'),\n                                 ('MINOR_VERSION', '0')],\n                include_dirs = ['/usr/local/include'],\n                libraries = ['tcl83'],\n                library_dirs = ['/usr/local/lib'],\n                sources = ['demo.c'])\n\n\nYou should provide the include files via \"include_dirs\".\n\nWhy does this not work for you?\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Publish is not transforming web.config?", "id": 1327, "answers": [{"answer_id": 1317, "document_id": 896, "question_id": 1327, "text": "You cannot set a namespace on the <configuration> tag (ex: for <location path=\".\" inheritInChildApplications=\"false\">)", "answer_start": 183, "answer_category": null}], "is_impossible": false}], "context": "I made a web.config (full file, it doesn't show XML errors)\nBut when I look at the web.config in the output folder it isn't changed.\nWhat could be the problem? Am I doing this right?\nYou cannot set a namespace on the <configuration> tag (ex: for <location path=\".\" inheritInChildApplications=\"false\">)\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Salesforce - How to Deploy between Environments (Sandboxes, Live etc)", "id": 498, "answers": [{"answer_id": 501, "document_id": 225, "question_id": 498, "text": "I recommend the Force.com Migration Tool.The Migration Tool allows you to use ant targets to move your metadata between salesforce.com organzations.", "answer_start": 439, "answer_category": null}], "is_impossible": false}], "context": "We're looking into setting up a proper deployment process.\nFrom what I've read there seems to be 4 methods of doing this.\nCopy & Paste -- We don't want to do this\nUsing the \"Package\" mechanism built into the Salesforce Web Interface\nEclipse Force IDE \"Deploy to Server\" option\nAnt Script (haven't tried this one yet)\nDoes anyone have advice on the limitation of the various methods .\nCan you include everything in a Web Interface package?\nI recommend the Force.com Migration Tool.The Migration Tool allows you to use ant targets to move your metadata between salesforce.com organzations.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to install Qt on Windows after building?", "id": 1766, "answers": [{"answer_id": 1752, "document_id": 1337, "question_id": 1766, "text": "To summarize: after moving your Qt directory to where you want it, download any one of the official Qt installers and run it with the following commandline arguments:\ncd <path>\ninstaller.exe --runoperation QtPatch windows <path> qt5\nReplace <path> with the full path of your Qt directory after you moved it (the qtbase directory if you are using Qt 5). Omit the final qt5 argument if you are using Qt 4.", "answer_start": 691, "answer_category": null}], "is_impossible": false}], "context": "I can't find any information on how to install Qt built on Windows.\nIn wiki article How to set up shadow builds on Mac and Linux there's description of -prefix option in configure script but this option is not available on Windows.\nI know I can use Qt right from the build folder but it does not seem the right thing not to perform an install step. One problem with this approach is size; Qt's build folder takes about 4GB space whereas after installing using binary installer Qt takes about 1GB space. I guess the difference is due to temporary files created during building. I hope some install procedure would install (copy) only needed files leaving temporary files in the build folder.\nTo summarize: after moving your Qt directory to where you want it, download any one of the official Qt installers and run it with the following commandline arguments:\ncd <path>\ninstaller.exe --runoperation QtPatch windows <path> qt5\nReplace <path> with the full path of your Qt directory after you moved it (the qtbase directory if you are using Qt 5). Omit the final qt5 argument if you are using Qt 4.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "what is the default database location of an android app for an unrooted device", "id": 1970, "answers": [{"answer_id": 1956, "document_id": 1555, "question_id": 1970, "text": " context.getDatabasePath.\n\nWorks for me:\n\nFile dbFile = conte", "answer_start": 1787, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI have quite a specific question.\n\nFirst let me explain what I am trying to do and then the problem.\n\nBasically what I am trying to do is to store an existing .db database file in assets folder and then after installation I want to copy the .db file from assets to the default database location in Android. Something similar to this page answer [link]How to use an existing database with an Android application. \n\nProblem For unrooted device I cannot access /data/data/&lt;&lt;package name folder&gt;&gt; . In this case where is the database file gets stored. In other term I want to copy a file from assets to applications default location for database which is DB_PATH = \"/data/data/\" + context.getPackageName() + \"/databases/\" in case of a rooted device. Would it be the same for the unrooted device. \n    \n\nYeah, For both the cases it will be same path. /data/data/&lt;application_package_name&gt;/databases\n\nNow, on un-rooted device you can not access /data/ directory of device's internal storage. That's why you can not seen the database file.\n\nIf you want to get the file you can copy database file from internal storage  /data/data/&lt;application_package_name&gt;/databases to external storage (sdcard) then using ddms or adb pull get the database file. \n\nAlso just try command adb pull /data/data/&lt;application_package_name&gt;/databases/&lt;database_file_name&gt; from your system to get the database file.\n\nBut by default all the android application store database on internal storage path /data/data/&lt;application_package_name&gt;/databases. And its applicable for all devices rooted or un-rooted.\n    \n\nAccording to Android docs at http://developer.android.com/reference/android/content/Context.html#getDatabasePath(java.lang.String) you should use context.getDatabasePath.\n\nWorks for me:\n\nFile dbFile = context.getDatabasePath(name_of_database_file);\n\n    \n\nBy Default Android Stores its SQLite Database to this below link, where you can access your .db file by using shell from command prompt to get owner access using chmod for read or write permission.\n\nString DATABASE_PATH = \"/data/data/\" + PACKAGE_NAME + \"/databases/\" + DATABASE_NAME;\n\nWhere:\nString DATABASE_NAME = \"your_dbname\";\nString PACKAGE_NAME = \"com.example.your_app_name\";\n\n\nFor more details you can check the following link. Click here\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Installer capabilities, WIX vs InstallShield Express", "id": 1092, "answers": [{"answer_id": 1084, "document_id": 669, "question_id": 1092, "text": "You can also check my favorite AdvancedInstaller. ", "answer_start": 680, "answer_category": null}], "is_impossible": false}], "context": "Programmers that actually promote their products to production need an installer. (pre-emptive \"programming related\" justificaton.)\nFor deploying a new suite of internal corporate apps and services, I'm trying to decide between using WIX and the InstallShield Express edition that comes with Visual Studio 2010.\nI've looked, but haven't found a feature matrix that highlights the features that are not in the express edition. I expect WIX to be generally quite capable, but more difficult to use, and have heard of situations that WIX doesn't support well.\nHas anyone found a feature matrix, or have other recommendations on the long-term best way to manage internal deployments?\nYou can also check my favorite AdvancedInstaller. They have also free express edition but I think both of them will be no use to you, because if you need to do anything with IIS, MS SQL, Active directory, GAC etc, you will need \"enterprise level\" editions. WiX is free but learning curve is so steep, that it's not worth learning. I regret ever learning it.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Using git-flow in a multi-stage deployment", "id": 551, "answers": [{"answer_id": 553, "document_id": 276, "question_id": 551, "text": "Here's what I ended up doing, this is a slight variation of what I proposed above and stems from another question I posted here: Deploy git branches", "answer_start": 312, "answer_category": null}], "is_impossible": false}], "context": "Drawing a blank with finalizing my deploy scheme here. After posting this question: Migrating a production site with no VCS at all to Git, I've got the gist of deploying to a local repo down.\nMy local development server has a git-flow repository on it that I can push to and it will update an external worktree. Here's what I ended up doing, this is a slight variation of what I proposed above and stems from another question I posted here: Deploy git branches\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Where is the Smarty installation files?", "id": 838, "answers": [{"answer_id": 833, "document_id": 520, "question_id": 838, "text": "in the\n/libs/  sub directory of\nthe distribution. ", "answer_start": 156, "answer_category": null}], "is_impossible": false}, {"question": "How to unzip Smarty installationfiles on Windows?", "id": 839, "answers": [{"answer_id": 834, "document_id": 520, "question_id": 839, "text": "/usr/local/lib/Smarty-v.e.r/ for *nix\nmachines\nand\nc:\\webroot\\libs\\Smarty-v.e.r\\ for the", "answer_start": 426, "answer_category": null}], "is_impossible": false}, {"question": "What files do Smarty installation need?", "id": 840, "answers": [{"answer_id": 835, "document_id": 520, "question_id": 840, "text": "Smarty-v.e.r/\nlibs/\nSmarty.class.php\ndebug.tpl\nsysplugins/* (everything)\nplugins/*    (everything)", "answer_start": 585, "answer_category": null}], "is_impossible": false}, {"question": "Why web server user account cannot write to Smarty files?", "id": 841, "answers": [{"answer_id": 836, "document_id": 520, "question_id": 841, "text": "Smarty will need write access\n(windows users please ignore) to the\n\n$compile_dir and\n\n$cache_dir directories\n(templates_c/ and\ncache/), so be sure the web server\nuser account can write  to them.", "answer_start": 4665, "answer_category": null}], "is_impossible": false}], "context": "\n\n\nBasic Installation\n\n\n\n\n\n\n\n\n\nBasic Installation\n\n\nPrev\nChapter\u00a02.\u00a0Installation\nNext\n\n\n\n\n\n\n\nBasic Installation\n\nInstall the Smarty library files which are in the\n/libs/  sub directory of\nthe distribution. These are .php files that you\nSHOULD NOT edit. They are shared among all applications and only get\nchanged when you upgrade to a new version of Smarty.\n\nIn the examples below the Smarty tarball has been  unpacked to:\n\n\n\n/usr/local/lib/Smarty-v.e.r/ for *nix\nmachines\nand\nc:\\webroot\\libs\\Smarty-v.e.r\\ for the\nwindows environment.\n\n\n\n\nExample\u00a02.1.\u00a0Required Smarty library files\n\n\nSmarty-v.e.r/\nlibs/\nSmarty.class.php\ndebug.tpl\nsysplugins/* (everything)\nplugins/*    (everything)\n\n\n\n\nSmarty uses a PHP constant\nnamed SMARTY_DIR\nwhich is the full system file path\nto the Smarty libs/ directory.\nBasically, if your application can find  the\nSmarty.class.php file, you do not need to set the\nSMARTY_DIR\nas Smarty will figure it out on its own.\nTherefore, if\nSmarty.class.php is not in your\ninclude_path,\nor you do not supply an absolute path to it in your application,\nthen you must define SMARTY_DIR manually.\nSMARTY_DIR must include a\ntrailing slash/.\n\n\n\nHere's how you create an instance of Smarty in your PHP scripts:\n\n\n\n<?php\n// NOTE: Smarty has a capital 'S'\nrequire_once('Smarty.class.php');\n$smarty = new Smarty();\n?>\n\n\n\n\nTry running the above script. If you get an error saying the\nSmarty.class.php file could not be found, you need to\ndo one of the following:\n\n\nExample\u00a02.2.\u00a0Set SMARTY_DIR constant manually\n\n\n<?php\n// *nix style (note capital 'S')\ndefine('SMARTY_DIR', '/usr/local/lib/Smarty-v.e.r/libs/');\n\n// windows style\ndefine('SMARTY_DIR', 'c:/webroot/libs/Smarty-v.e.r/libs/');\n\n// hack version example that works on both *nix and windows\n// Smarty is assumend to be in 'includes/' dir under current script\ndefine('SMARTY_DIR',str_replace(\"\\\\\",\"/\",getcwd()).'/includes/Smarty-v.e.r/libs/');\n\nrequire_once(SMARTY_DIR . 'Smarty.class.php');\n$smarty = new Smarty();\n?>\n\n\n\n\nExample\u00a02.3.\u00a0Supply absolute path to library file\n\n\n<?php\n// *nix style (note capital 'S')\nrequire_once('/usr/local/lib/Smarty-v.e.r/libs/Smarty.class.php');\n\n// windows style\nrequire_once('c:/webroot/libs/Smarty-v.e.r/libs/Smarty.class.php');\n\n$smarty = new Smarty();\n?>\n\n\n\n\nExample\u00a02.4.\u00a0Add the library path to the php.ini file\n\n\n;;;;;;;;;;;;;;;;;;;;;;;;;\n; Paths and Directories ;\n;;;;;;;;;;;;;;;;;;;;;;;;;\n\n; *nix: \"/path1:/path2\"\ninclude_path = \".:/usr/share/php:/usr/local/lib/Smarty-v.e.r/libs/\"\n\n; Windows: \"\\path1;\\path2\"\ninclude_path = \".;c:\\php\\includes;c:\\webroot\\libs\\Smarty-v.e.r\\libs\\\"\n\n\n\n\nExample\u00a02.5.\u00a0Appending the include path in a php script with\nini_set()\n\n\n<?php\n// *nix\nini_set('include_path', ini_get('include_path').PATH_SEPARATOR.'/usr/local/lib/Smarty-v.e.r/libs/');\n\n// windows\nini_set('include_path', ini_get('include_path').PATH_SEPARATOR.'c:/webroot/lib/Smarty-v.e.r/libs/');\n?>\n\n\n\n\nNow that the library files are in place, it's time to setup the Smarty\ndirectories for your application:\n\n\nSmarty requires four directories which\nare by default named templates/,\ntemplates_c/, configs/ and cache/\n\nEach of these are definable by the\nSmarty class properties\n\n$template_dir,\n\n$compile_dir,\n\n$config_dir, and\n\n$cache_dir respectively\n\n\nIt is highly recommended\nthat you setup a separate set of these directories for each application\nthat will use Smarty\n\n\nYou can verify if your system has the correct access rights for these directories with\ntestInstall().\n\n\n\nFor our installation example, we will be setting up the Smarty environment\nfor a guest book application. We picked an application only for the purpose\nof a directory naming convention. You can use the same environment for any\napplication, just replace guestbook/ with\nthe name of your application.\n\n\nExample\u00a02.6.\u00a0What the file structure looks like\n\n\n/usr/local/lib/Smarty-v.e.r/libs/\nSmarty.class.php\ndebug.tpl\nsysplugins/*\nplugins/*\n\n/web/www.example.com/\nguestbook/\ntemplates/\nindex.tpl\ntemplates_c/\nconfigs/\ncache/\nhtdocs/\nindex.php\n\n\n\n\nBe sure that you know the location of your web server's document root as a\nfile path. In the following examples, the document root is /web/www.example.com/guestbook/htdocs/.\nThe Smarty\ndirectories are only accessed by the Smarty library and never accessed\ndirectly by the web browser. Therefore to avoid any security concerns, it\nis recommended (but not mandatory) to place these directories\noutside of the web server's document root.\n\n\nYou will need as least one file under your document root, and that is the\nscript accessed by the web browser. We will name our script\nindex.php, and place it in a subdirectory under the\ndocument root /htdocs/.\n\n\nSmarty will need write access\n(windows users please ignore) to the\n\n$compile_dir and\n\n$cache_dir directories\n(templates_c/ and\ncache/), so be sure the web server\nuser account can write  to them.\n\n\n\nNote\nThis is usually user \u201cnobody\u201d and\ngroup \u201cnobody\u201d. For OS X users,\nthe default is user \u201cwww\u201d and group \u201cwww\u201d.\nIf you are using Apache, you can  look in your\nhttpd.conf file to see\nwhat user and group are being used.\n\n\n\n\nExample\u00a02.7.\u00a0Permissions and making directories writable\n\n\nchown nobody:nobody /web/www.example.com/guestbook/templates_c/\nchmod 770 /web/www.example.com/guestbook/templates_c/\n\nchown nobody:nobody /web/www.example.com/guestbook/cache/\nchmod 770 /web/www.example.com/guestbook/cache/\n\n\n\n\nNote\n\nchmod 770 will be fairly tight security, it only allows\nuser \u201cnobody\u201d and group \u201cnobody\u201d read/write access\nto the directories. If you would like to  open up read access to anyone\n(mostly for your own convenience of viewing\nthese files), you can use 775 instead.\n\n\n\nWe need to create the index.tpl file that Smarty will\ndisplay. This needs to be located in the\n$template_dir.\n\n\nExample\u00a02.8.\u00a0/web/www.example.com/guestbook/templates/index.tpl\n\n\n{* Smarty *}\n\nHello {$name}, welcome to Smarty!\n\n\n\n\nTechnical Note\n\n{* Smarty *} is a template\ncomment.\nIt is not required, but it is good\npractice to start all your template files with this comment. It makes\nthe file easy to recognize regardless of the file extension. For\nexample, text editors could recognize the file and turn on special\nsyntax highlighting.\n\n\n\nNow lets edit index.php. We'll create an instance of Smarty,\nassign() a\ntemplate variable and display()\nthe index.tpl file.\n\n\nExample\u00a02.9.\u00a0Editing /web/www.example.com/docs/guestbook/index.php\n\n\n<?php\n\nrequire_once(SMARTY_DIR . 'Smarty.class.php');\n\n$smarty = new Smarty();\n\n$smarty->setTemplateDir('/web/www.example.com/guestbook/templates/');\n$smarty->setCompileDir('/web/www.example.com/guestbook/templates_c/');\n$smarty->setConfigDir('/web/www.example.com/guestbook/configs/');\n$smarty->setCacheDir('/web/www.example.com/guestbook/cache/');\n\n$smarty->assign('name','Ned');\n\n//** un-comment the following line to show the debug console\n//$smarty->debugging = true;\n\n$smarty->display('index.tpl');\n\n?>\n\n\n\n\nNote\n\nIn our example, we are setting absolute paths to all of the Smarty\ndirectories. If /web/www.example.com/guestbook/ is\nwithin your PHP include_path, then these settings are not necessary.\nHowever, it is more efficient and (from experience) less error-prone to\nset them to absolute paths. This ensures that Smarty is getting files\nfrom the directories you intended.\n\n\n\nNow navigate to the index.php file with the web browser.\nYou should see \"Hello Ned, welcome to Smarty!\"\n\n\nYou have completed the basic setup for Smarty!\n\n\n\n\n\n\n\nPrev\nUp\nNext\n\n\n\nChapter\u00a02.\u00a0Installation\nHome\nExtended Setup\n\n\n\n\n\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Installing/uninstalling my module with pip", "id": 1216, "answers": [{"answer_id": 1209, "document_id": 792, "question_id": 1216, "text": "try providing the name of the package this setup.py file is actually part of.", "answer_start": 586, "answer_category": null}], "is_impossible": false}], "context": "pip is able to uninstall most installed packages with pip uninstall package-name.\nKnown exceptions include pure-distutils packages installed with python setup.py install >(such packages leave behind no metadata allowing determination of what files were >installed)\nIs there another way to install my module that pip will recognize?\nBy the way, I'm using a windows computer. Just wanted to mention that in case there are different solutions for Windows, Linux, and Mac.\nYou're giving pip a Python file and not a package name, so it doesn't know what to do. If you want pip to remove it, try providing the name of the package this setup.py file is actually part of.\nThere are some good suggestions in this related thread: python setup.py uninstall\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Difference between Groovy Binary and Source release?", "id": 1546, "answers": [{"answer_id": 1535, "document_id": 1123, "question_id": 1546, "text": "A source release will be compiled on your own machine while a binary release must match your operating system.\nsource releases are more common on linux systems because linux systems can dramatically vary in cpu, installed library versions, kernelversions and nearly every linux system has a compiler installed.", "answer_start": 267, "answer_category": null}], "is_impossible": false}], "context": "I have been seeing the words binary and source release in many websites download sections.\nWhat do they actually mean?\nFor example, I have seen this in Groovy download page.\nMy question is how they differ? Both tend to install Groovy, but what's the main difference?\nA source release will be compiled on your own machine while a binary release must match your operating system.\nsource releases are more common on linux systems because linux systems can dramatically vary in cpu, installed library versions, kernelversions and nearly every linux system has a compiler installed.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to Install gcc 5.3 with yum on CentOS 7.2?", "id": 654, "answers": [{"answer_id": 659, "document_id": 347, "question_id": 654, "text": "sudo yum -y --enablerepo=centos-sclo-rh-testing install devtoolset-7-gcc;\necho \"source /opt/rh/devtoolset-7/enable\" | sudo tee -a /etc/profile;\nsource /opt/rh/devtoolset-7/enable;\ngcc --version;", "answer_start": 401, "answer_category": null}], "is_impossible": false}], "context": "I am using CentOS 7.2\nWhen I use yum groupinstall \"Development Tools\", gcc version is 4.8.5, like this:I would like to install gcc 5.3\nHow to approach this with yum?\nYou can use the centos-sclo-rh-testing repo to install GCC v7 without having to compile it forever, also enable V7 by default and let you switch between different versions if required.\nsudo yum install -y yum-utils centos-release-scl;\nsudo yum -y --enablerepo=centos-sclo-rh-testing install devtoolset-7-gcc;\necho \"source /opt/rh/devtoolset-7/enable\" | sudo tee -a /etc/profile;\nsource /opt/rh/devtoolset-7/enable;\ngcc --version;\nWhile this code may answer the question, it is better to explain how to solve the problem and provide the code as an example or reference. Code-only answers can be confusing and lack context.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Error installing any ruby version with RVM on OSX", "id": 1740, "answers": [{"answer_id": 1727, "document_id": 1312, "question_id": 1740, "text": "You just need to keep running \"rvm requirements\" and reading the log and installing the packages needed until there are no more errors.", "answer_start": 259, "answer_category": null}], "is_impossible": false}], "context": "Guys I'm about to kill myself with this one!\nI had some problems with RVM installing multiple versions of Ruby, and following a thread on Stackoverflow I decided to remove it completely. After reinstalling RVM, I am unable to install any Ruby version at all.\nYou just need to keep running \"rvm requirements\" and reading the log and installing the packages needed until there are no more errors.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "sqlexpress local database administrator password", "id": 1907, "answers": [{"answer_id": 1894, "document_id": 1478, "question_id": 1907, "text": "he below steps to resolve the issue:\n\n\nOpen Sql Server Management Studio as Administartor\nLogin to  .\\SQLEXPRESS using Windows Authentication\nGo to Security Tab -- &gt; Logins --&gt; Change the \"sa\" password and Press Ok\nUse the newly created Pas", "answer_start": 1340, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI'm trying to install some .Net eCommerce projects on my local machine (windows Vista) with Webmatrix. Some of them ask for a Database administrator username and password, but I have no idea what those are.\n\nCan anybody help me on this. See image which is a screenshot of the installation process.\n\n\n    \n\nBy Default the SQLEXPRESS installation uses Windows Authentication. in the management studio on the security-tab of the server properties you can select \"SQL Server and Windows Authentication\"\nONLY THEN The 'sa' account is accepted. You probably have to set/reset the password for this account if you can't remember you entered it.\n    \n\nIn your client (SQL Server Management Studio) you have to:\n\n\nchange password for login user \"sa\"\nenable login for user \"sa\"\nensure SQL Server authentication is enabled\n\n\n1. change password for login user \"sa\"\n\nSecurity &gt; Logins &gt; sa (right-click) &gt; Properties &gt; General &gt; Password and Confirm password\n\n2. enable login for user \"sa\"\n\nSecurity &gt; Logins &gt; sa (right-click) &gt; Properties &gt; Status &gt; Login, click in Enabled\n\n3. ensure SQL Server authentication is enabled\n\nRight click on server &gt; Properties &gt; Security &gt; Server authentication, click in SQL Server and Windows Authentication mode\n    \n\nI had the same problem and i performed the below steps to resolve the issue:\n\n\nOpen Sql Server Management Studio as Administartor\nLogin to  .\\SQLEXPRESS using Windows Authentication\nGo to Security Tab -- &gt; Logins --&gt; Change the \"sa\" password and Press Ok\nUse the newly created Password in WebMatrix and you should be all Set.\n\n    \n\nSince SQL 2005 the Express edition is installed with windows authentication. In the configuration (management studio, connect to the server and choose database properties) you can activate sql internal authentification then (default deactivated, user sa without password).\n\nSo if you can't connect it you might have to activate it and set a password for the user sa then.\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Permission denied (publickey) when deploying heroku code. fatal: The remote end hung up unexpectedly", "id": 1644, "answers": [{"answer_id": 1632, "document_id": 1218, "question_id": 1644, "text": "1) Find out what keys you have in Heroku now.\n$ heroku keys\n=== 1 key for joe@example.com\nssh-dss AAAAB8NzaC...DVj3R4Ww== joe@workstation.local\n2) Build a ~/.ssh/config file:\n$ sudo vim ~/.ssh/config\n3)Edit with this info\nHost heroku.com\nHostname heroku.com \nPort 22 \nIdentitiesOnly yes \nIdentityFile ~/.ssh/ssh-dss # location and name of your private key\nTCPKeepAlive yes \nUser joe@workstation.local", "answer_start": 350, "answer_category": null}], "is_impossible": false}], "context": "I'm attempting to deploy my code to heroku with the following command line:\ngit push heroku master\nbut get the following error:\nPermission denied (publickey).\nfatal: The remote end hung up unexpectedly\nI have already uploaded my public SSH key, but it still comes up with this error.\nThis problem was messing with me for a few days.\nThis might help.\n1) Find out what keys you have in Heroku now.\n$ heroku keys\n=== 1 key for joe@example.com\nssh-dss AAAAB8NzaC...DVj3R4Ww== joe@workstation.local\n2) Build a ~/.ssh/config file:\n$ sudo vim ~/.ssh/config\n3)Edit with this info\nHost heroku.com\nHostname heroku.com \nPort 22 \nIdentitiesOnly yes \nIdentityFile ~/.ssh/ssh-dss # location and name of your private key\nTCPKeepAlive yes \nUser joe@workstation.local\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Tensorflow r1.0 : could not a find a version that satisfies the requirement tensorflow", "id": 714, "answers": [{"answer_id": 717, "document_id": 404, "question_id": 714, "text": "pip3 install --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.0.0-py3-none-any.whl", "answer_start": 124, "answer_category": null}], "is_impossible": false}], "context": "I don't understand what the problem is...\nAnd I tried another way...\nI was in same problem.\nBelow command solved my problem\npip3 install --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.0.0-py3-none-any.whl\nto find the list of all the urls based on the python version and CPU or GPU only refer to: https://www.tensorflow.org/install/pip\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "What is CouchDB?", "id": 200654, "answers": [{"answer_id": 239440, "document_id": 357721, "question_id": 200654, "text": "Apache CouchDB is one of a new breed of database management systems.", "answer_start": 1294, "answer_category": null}], "is_impossible": false}, {"question": "What is the main character of CouchDB?", "id": 200655, "answers": [{"answer_id": 239448, "document_id": 357721, "question_id": 200655, "text": " relax", "answer_start": 1919, "answer_category": null}], "is_impossible": false}, {"question": "What does the idea of CouchDB come from?", "id": 200657, "answers": [{"answer_id": 239476, "document_id": 357721, "question_id": 200657, "text": "CouchDB\u2019s design borrows heavily from web architecture and the concepts of\nresources, methods, and representations.", "answer_start": 5240, "answer_category": null}], "is_impossible": false}, {"question": "What does CouchDB Replication do?", "id": 200658, "answers": [{"answer_id": 239495, "document_id": 357721, "question_id": 200658, "text": "Its fundamental function\nis to synchronize two or more CouchDB databases.", "answer_start": 10866, "answer_category": null}], "is_impossible": false}, {"question": "Why relax is the main character of CouchDB?", "id": 200656, "answers": [{"answer_id": 239455, "document_id": 357721, "question_id": 200656, "text": " learning CouchDB and understanding its core concepts\nshould feel natural to most everybody who has been doing any work on the Web.\nAnd it is still pretty easy to explain to non-technical people.", "answer_start": 2448, "answer_category": null}], "is_impossible": false}], "context": "\n\n\n1.2. Why CouchDB?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApache CouchDB\u00ae\n\n\n\n\n\n\n\nstable\n\n\n\n\n\n\n\n\n\n\nTable of Contents\nUser Guides\n\n1. Introduction\n1.1. Technical Overview\n1.2. Why CouchDB?\n1.2.1. Relax\n1.2.2. A Different Way to Model Your Data\n1.2.3. A Better Fit for Common Applications\n1.2.3.1. Self-Contained Data\n1.2.3.2. Syntax and Semantics\n\n\n1.2.4. Building Blocks for Larger Systems\n1.2.5. CouchDB Replication\n1.2.6. Local Data Is King\n1.2.7. Wrapping Up\n\n\n1.3. Eventual Consistency\n1.4. cURL: Your Command Line Friend\n1.5. Security\n1.6. Getting Started\n1.7. The Core API\n\n\n2. Replication\n3. Design Documents\n4. Best Practices\n\nAdministration Guides\n\n1. Installation\n2. Setup\n3. Configuration\n4. Cluster Management\n5. Maintenance\n6. Fauxton\n7. Experimental Features\n\nReference Guides\n\n1. API Reference\n2. JSON Structure Reference\n3. Query Server\n4. Partitioned Databases\n\nOther\n\n1. Release Notes\n2. Security Issues / CVEs\n3. Reporting New Security Problems with Apache CouchDB\n4. License\n5. Contributing to this Documentation\n\nQuick Reference Guides\n\nAPI Quick Reference\nConfiguration Quick Reference\n\nMore Help\n\nCouchDB Homepage\nMailing Lists\nRealtime Chat\nIssue Tracker\nDownload Docs\n\n\n\n\n\n\n\n\nApache CouchDB\u00ae\n\n\n\n\n\nDocs \u00bb\n1. Introduction \u00bb\n1.2. Why CouchDB?\n\nEdit on GitHub\n\n\n\n\n\n\n\n1.2. Why CouchDB?\u00b6\nApache CouchDB is one of a new breed of database management systems.\nThis topic explains why there\u2019s a need for new systems as well as the\nmotivations behind building CouchDB.\nAs CouchDB developers, we\u2019re naturally very excited to be using CouchDB.\nIn this topic we\u2019ll share with you the reasons for our enthusiasm.\nWe\u2019ll show you how CouchDB\u2019s schema-free document model is a better fit\nfor common applications, how the built-in query engine is a powerful way\nto use and process your data, and how CouchDB\u2019s design lends itself\nto modularization and scalability.\n\n1.2.1. Relax\u00b6\nIf there\u2019s one word to describe CouchDB, it is relax. It is the byline\nto CouchDB\u2019s official logo and when you start CouchDB, you see:\nApache CouchDB has started. Time to relax.\n\n\nWhy is relaxation important? Developer productivity roughly doubled in the\nlast five years. The chief reason for the boost is more powerful tools that\nare easier to use. Take Ruby on Rails as an example. It is an infinitely\ncomplex framework, but it\u2019s easy to get started with. Rails is a success\nstory because of the core design focus on ease of use. This is one reason why\nCouchDB is relaxing: learning CouchDB and understanding its core concepts\nshould feel natural to most everybody who has been doing any work on the Web.\nAnd it is still pretty easy to explain to non-technical people.\nGetting out of the way when creative people try to build specialized\nsolutions is in itself a core feature and one thing that CouchDB aims to get\nright. We found existing tools too cumbersome to work with during development\nor in production, and decided to focus on making CouchDB easy, even a pleasure,\nto use.\nAnother area of relaxation for CouchDB users is the production setting.\nIf you have a live running application, CouchDB again goes out of its way\nto avoid troubling you. Its internal architecture is fault-tolerant,\nand failures occur in a controlled environment and are dealt with gracefully.\nSingle problems do not cascade through an entire server system but stay\nisolated in single requests.\nCouchDB\u2019s core concepts are simple (yet powerful) and well understood.\nOperations teams (if you have a team; otherwise, that\u2019s you) do not have to\nfear random behavior and untraceable errors. If anything should go wrong,\nyou can easily find out what the problem is, but these situations are rare.\nCouchDB is also designed to handle varying traffic gracefully. For instance,\nif a website is experiencing a sudden spike in traffic, CouchDB will generally\nabsorb a lot of concurrent requests without falling over. It may take a little\nmore time for each request, but they all get answered. When the spike is over,\nCouchDB will work with regular speed again.\nThe third area of relaxation is growing and shrinking the underlying hardware\nof your application. This is commonly referred to as scaling. CouchDB enforces\na set of limits on the programmer. On first look, CouchDB might seem\ninflexible, but some features are left out by design for the simple reason\nthat if CouchDB supported them, it would allow a programmer to create\napplications that couldn\u2019t deal with scaling up or down.\n\nNote\nCouchDB doesn\u2019t let you do things that would get you in trouble later on.\nThis sometimes means you\u2019ll have to unlearn best practices you might have\npicked up in your current or past work.\n\n\n\n1.2.2. A Different Way to Model Your Data\u00b6\nWe believe that CouchDB will drastically change the way you build\ndocument-based applications. CouchDB combines an intuitive document storage\nmodel with a powerful query engine in a way that\u2019s so simple you\u2019ll probably\nbe tempted to ask, \u201cWhy has no one built something like this before?\u201d\n\nDjango may be built for the Web, but CouchDB is built of the Web. I\u2019ve\nnever seen software that so completely embraces the philosophies behind\nHTTP. CouchDB makes Django look old-school in the same way that Django\nmakes ASP look outdated.\n\u2014Jacob Kaplan-Moss, Django developer\n\nCouchDB\u2019s design borrows heavily from web architecture and the concepts of\nresources, methods, and representations. It augments this with powerful ways\nto query, map, combine, and filter your data. Add fault tolerance, extreme\nscalability, and incremental replication, and CouchDB defines a sweet spot\nfor document databases.\n\n\n1.2.3. A Better Fit for Common Applications\u00b6\nWe write software to improve our lives and the lives of others. Usually this\ninvolves taking some mundane information such as contacts, invoices,\nor receipts and manipulating it using a computer application. CouchDB is a\ngreat fit for common applications like this because it embraces the natural\nidea of evolving, self-contained documents as the very core of its data model.\n\n1.2.3.1. Self-Contained Data\u00b6\nAn invoice contains all the pertinent information about a single transaction\nthe seller, the buyer, the date, and a list of the items or services sold.\nAs shown in Figure 1. Self-contained documents, there\u2019s no abstract reference on this\npiece of paper that points to some other piece of paper with the seller\u2019s\nname and address. Accountants appreciate the simplicity of having everything\nin one place. And given the choice, programmers appreciate that, too.\n\n\nFigure 1. Self-contained documents\n\nYet using references is exactly how we model our data in a relational\ndatabase! Each invoice is stored in a table as a row that refers to other\nrows in other tables one row for seller information, one for the buyer,\none row for each item billed, and more rows still to describe the item\ndetails, manufacturer details, and so on and so forth.\nThis isn\u2019t meant as a detraction of the relational model, which is widely\napplicable and extremely useful for a number of reasons. Hopefully, though, it\nillustrates the point that sometimes your model may not \u201cfit\u201d your data\nin the way it occurs in the real world.\nLet\u2019s take a look at the humble contact database to illustrate a different\nway of modeling data, one that more closely \u201cfits\u201d its real-world counterpart\n\u2013 a pile of business cards. Much like our invoice example, a business card\ncontains all the important information, right there on the cardstock.\nWe call this \u201cself-contained\u201d data, and it\u2019s an important concept\nin understanding document databases like CouchDB.\n\n\n1.2.3.2. Syntax and Semantics\u00b6\nMost business cards contain roughly the same information \u2013 someone\u2019s identity,\nan affiliation, and some contact information. While the exact form of this\ninformation can vary between business cards, the general information being\nconveyed remains the same, and we\u2019re easily able to recognize it as a\nbusiness card. In this sense, we can describe a business card as a real-world\ndocument.\nJan\u2019s business card might contain a phone number but no fax number,\nwhereas J. Chris\u2019s business card contains both a phone and a fax number. Jan\ndoes not have to make his lack of a fax machine explicit by writing something\nas ridiculous as \u201cFax: None\u201d on the business card. Instead, simply omitting\na fax number implies that he doesn\u2019t have one.\nWe can see that real-world documents of the same type, such as business cards,\ntend to be very similar in semantics \u2013 the sort of information they carry,\nbut can vary hugely in syntax, or how that information is structured. As human\nbeings, we\u2019re naturally comfortable dealing with this kind of variation.\nWhile a traditional relational database requires you to model your data\nup front, CouchDB\u2019s schema-free design unburdens you with a powerful way to\naggregate your data after the fact, just like we do with real-world\ndocuments. We\u2019ll look in depth at how to design applications with this\nunderlying storage paradigm.\n\n\n\n1.2.4. Building Blocks for Larger Systems\u00b6\nCouchDB is a storage system useful on its own. You can build many applications\nwith the tools CouchDB gives you. But CouchDB is designed with a bigger picture\nin mind. Its components can be used as building blocks that solve storage\nproblems in slightly different ways for larger and more complex systems.\nWhether you need a system that\u2019s crazy fast but isn\u2019t too concerned with\nreliability (think logging), or one that guarantees storage in two or more\nphysically separated locations for reliability, but you\u2019re willing to take a\nperformance hit, CouchDB lets you build these systems.\nThere are a multitude of knobs you could turn to make a system work better in\none area, but you\u2019ll affect another area when doing so. One example would be\nthe CAP theorem discussed in Eventual Consistency. To give you an idea of\nother things that affect storage systems, see\nFigure 2 and Figure 3.\nBy reducing latency for a given system (and that is true not only for storage\nsystems), you affect concurrency and throughput capabilities.\n\n\nFigure 2. Throughput, latency, or concurrency\n\n\n\nFigure 3. Scaling: read requests, write requests, or data\n\nWhen you want to scale out, there are three distinct issues to deal with:\nscaling read requests, write requests, and data. Orthogonal to all three and\nto the items shown in Figure 2 and Figure 3 are many more attributes like reliability or simplicity.\nYou can draw many of these graphs that show how different features or attributes\npull into different directions and thus shape the system they describe.\nCouchDB is very flexible and gives you enough building blocks to create a\nsystem shaped to suit your exact problem. That\u2019s not saying that CouchDB can\nbe bent to solve any problem \u2013 CouchDB is no silver bullet \u2013 but in the\narea of data storage, it can get you a long way.\n\n\n1.2.5. CouchDB Replication\u00b6\nCouchDB replication is one of these building blocks. Its fundamental function\nis to synchronize two or more CouchDB databases. This may sound simple,\nbut the simplicity is key to allowing replication to solve a number of\nproblems: reliably synchronize databases between multiple machines for\nredundant data storage; distribute data to a cluster of CouchDB instances\nthat share a subset of the total number of requests that hit the cluster\n(load balancing); and distribute data between physically distant locations,\nsuch as one office in New York and another in Tokyo.\nCouchDB replication uses the same REST API all clients use. HTTP is\nubiquitous and well understood. Replication works incrementally; that is,\nif during replication anything goes wrong, like dropping your network\nconnection, it will pick up where it left off the next time it runs. It also\nonly transfers data that is needed to synchronize databases.\nA core assumption CouchDB makes is that things can go wrong,\nlike network connection troubles, and it is designed for graceful error\nrecovery instead of assuming all will be well. The replication system\u2019s\nincremental design shows that best. The ideas behind \u201cthings that can go\nwrong\u201d are embodied in the Fallacies of Distributed Computing:\n\nThe network is reliable.\nLatency is zero.\nBandwidth is infinite.\nThe network is secure.\nTopology doesn\u2019t change.\nThere is one administrator.\nTransport cost is zero.\nThe network is homogeneous.\n\nExisting tools often try to hide the fact that there is a network and that\nany or all of the previous conditions don\u2019t exist for a particular system.\nThis usually results in fatal error scenarios when something finally goes\nwrong. In contrast, CouchDB doesn\u2019t try to hide the network; it just handles\nerrors gracefully and lets you know when actions on your end are required.\n\n\n1.2.6. Local Data Is King\u00b6\nCouchDB takes quite a few lessons learned from the Web,\nbut there is one thing that could be improved about the Web: latency.\nWhenever you have to wait for an application to respond or a website to\nrender, you almost always wait for a network connection that isn\u2019t as fast as\nyou want it at that point. Waiting a few seconds instead of milliseconds\ngreatly affects user experience and thus user satisfaction.\nWhat do you do when you are offline? This happens all the time \u2013 your DSL or\ncable provider has issues, or your iPhone, G1, or Blackberry has no bars,\nand no connectivity means no way to get to your data.\nCouchDB can solve this scenario as well, and this is where scaling is\nimportant again. This time it is scaling down. Imagine CouchDB installed on\nphones and other mobile devices that can synchronize data with centrally\nhosted CouchDBs when they are on a network. The synchronization is not bound\nby user interface constraints like sub-second response times. It is easier to\ntune for high bandwidth and higher latency than for low bandwidth and very\nlow latency. Mobile applications can then use the local CouchDB to fetch\ndata, and since no remote networking is required for that,\nlatency is low by default.\nCan you really use CouchDB on a phone? Erlang, CouchDB\u2019s implementation\nlanguage has been designed to run on embedded devices magnitudes smaller and\nless powerful than today\u2019s phones.\n\n\n1.2.7. Wrapping Up\u00b6\nThe next document Eventual Consistency further explores the distributed\nnature of CouchDB. We should have given you enough bites to whet your interest.\nLet\u2019s go!\n\n\n\n\n\n\nNext\nPrevious\n\n\n\n\n\u00a9 Copyright 2020, Apache Software Foundation. CouchDB\u00ae is a registered trademark of the Apache Software Foundation.\n\n\nRevision 3f39035f.\n\n\n\nBuilt with Sphinx using a theme provided by Read the Docs.\n\n\n\n\n\n\n\n\nRead the Docs\nv: stable\n\n\n\n\nVersions\nmaster\nlatest\nstable\n3.1.1\n2.3.1\n1.6.1\n\n\nDownloads\npdf\nhtml\nepub\n\n\nOn Read the Docs\n\nProject Home\n\n\nBuilds\n\n\n\nFree document hosting provided by Read the Docs.\n\n\n\n\n\n\n\n\n\n\n\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to find the UpgradeCode and ProductCode of an installed application in Windows 7", "id": 1132, "answers": [{"answer_id": 1125, "document_id": 709, "question_id": 1132, "text": "open system registry and search for HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Uninstall key (if it's a 32-bit installer on a 64-bit machine, it might be under HKEY_LOCAL_MACHINE\\SOFTWARE\\Wow6432Node\\Microsoft\\Windows\\CurrentVersion\\Uninstall instead).\n\u2022\tthe GUIDs listed under that key are the products installed on this machine\n\u2022\tfind the one you're talking about - just step one by one until you see its name on the right pane", "answer_start": 426, "answer_category": null}], "is_impossible": false}], "context": "I have an application installed on my machine. I also have its source code but somehow the ProductCode and UpgradeCode of this application were changed.\nNow I want to get the UpgradeCode and ProductCode of this installed application. I feel there must be some tool for this.\nCan anyone kindly let me know how to get the UpgradeCode and ProductCode of an installed application?\nHere's another way (you don't need any tools):\n\u2022\topen system registry and search for HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Uninstall key (if it's a 32-bit installer on a 64-bit machine, it might be under HKEY_LOCAL_MACHINE\\SOFTWARE\\Wow6432Node\\Microsoft\\Windows\\CurrentVersion\\Uninstall instead).\n\u2022\tthe GUIDs listed under that key are the products installed on this machine\n\u2022\tfind the one you're talking about - just step one by one until you see its name on the right pane\nThis GUID you stopped on is the ProductCode.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Gradle Maven plugin \"install\" task does not work with Android library project", "id": 1213, "answers": [{"answer_id": 1206, "document_id": 789, "question_id": 1213, "text": "just recreate the install task with a different name. I called it installArchives. Add the following code to your build.gradle:\ntask installArchives(type: Upload) {\n    description \"Installs the artifacts to the local Maven repository.\"\n    repositories.mavenInstaller {\n        configuration = configurations.default\n        pom.groupId = 'my.group'\n        pom.artifactId = 'my-artifact'\n        pom.version = '1.0.0'\n    }\n}", "answer_start": 274, "answer_category": null}], "is_impossible": false}], "context": "So first thing I noticed is that it's trying for some odd reason to deploy it on a device as APK.\nAm I doing something wrong or is it just android-library plugin not compatible with maven plugin?\nThere's an easier solution if you don't want to use a custom plugin. Instead, just recreate the install task with a different name. I called it installArchives. Add the following code to your build.gradle:\ntask installArchives(type: Upload) {\n    description \"Installs the artifacts to the local Maven repository.\"\n    repositories.mavenInstaller {\n        configuration = configurations.default\n        pom.groupId = 'my.group'\n        pom.artifactId = 'my-artifact'\n        pom.version = '1.0.0'\n    }\n}\nYou can now run gradle installArchives to install your aar locally.\n\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to implement single installer for 32 & 64 platforms", "id": 1757, "answers": [{"answer_id": 1744, "document_id": 1329, "question_id": 1757, "text": "It can't be done. It's a limitation of Windows Installer, if you want to do this without making it twice as large, then you'll need two MSI's with external CAB files and a bootstrapper to execute the correct installation.", "answer_start": 294, "answer_category": null}], "is_impossible": false}], "context": "I work on an WIX based installer.\nThe installer builds to 32 and 64 platforms separately. The installers versions are very similar in both platforms but few conditional steps like avoid registering x64 native dlls in the 32 bit installer.\nIs there a way to unite both of the installers to one?\nIt can't be done. It's a limitation of Windows Installer, if you want to do this without making it twice as large, then you'll need two MSI's with external CAB files and a bootstrapper to execute the correct installation.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "make sure that the default admin share is enable on servername", "id": 1933, "answers": [{"answer_id": 1920, "document_id": 1509, "question_id": 1933, "text": "You need to add the 'admin$' share which is your C:\\Windows location. \n\n\nGo to C:\\windows and right-click --&gt; Properties\nHit advance sharing\nClick the check box Share this folder\nEnter the name admin$ and hit Permissions\nI would recommend removing 'Everyone' and adding just the users that the PsExec command will use to execute.\n\n\nRun the PsExec command again and this should resolve your issue", "answer_start": 630, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nWhen running the psexec command to remotely install or execute something on a sever on the same network the following error was displayed.\n\nCouldn't access ServerName\n\nThe network name cannot be found\n\nMake sure that the default admin$ share is enable on ServerName\n\nMost references suggested that you add the following to the registry, but in my case this was already added to the server. This did not resolve the issue. \n\nHKEY_LOCAL_MACHINE\\Software\\Microsoft\\Windows\\CurrentVersion\\Policies\\System\nand create or modify a REG_DWORD value LocalAccountTokenFilterPolicy and set its value to 1\n    \n\nSolution:\n\nYou need to add the 'admin$' share which is your C:\\Windows location. \n\n\nGo to C:\\windows and right-click --&gt; Properties\nHit advance sharing\nClick the check box Share this folder\nEnter the name admin$ and hit Permissions\nI would recommend removing 'Everyone' and adding just the users that the PsExec command will use to execute.\n\n\nRun the PsExec command again and this should resolve your issue. \n\nEdit:\n\nYou can also turn on your AutoShareServer in the registry, which will automatically create the admin shares.\n\n\nStart regisry regedit\nSearch for key HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Services\\LanmanServer\\Parameters\\AutoShareServer\nChange the AutoShareServer key to 1\n\n    \n\nYou can also enable amins$ share by enabling File and Printer Sharing (SMB-In) Firewall Rule.\n\nGo to Control Panel &gt; System ans Security &gt; Windows Defender Firewall &gt; Advance Settings &gt; Inbound Rules. Right click on File and Printer Sharing (SMB-In) from the list and select Enable Rule`. Normally, there are two File and Printer Sharing (SMB-In), one is for Domain profile and one is for Public &amp; Private profile. I'm not sure which profile should be applied, Domain or Public or Private. For me, it's Domain profile.\n    \n\nIn my case it was a network problem like mentioned in the error message.\n\nI needed to allow SMB traffic on port 445 on the target machine. PSExec worked straight away after adding the firewall rule to allow that traffic.\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Embed a JRE in a Windows executable?", "id": 1698, "answers": [{"answer_id": 1686, "document_id": 1271, "question_id": 1698, "text": "try to use Avian and ProGuard toolkits. Avian allows to embed lightweight virtual machine in you app. Linux, MacOS, Windows and iOS are supported. And ProGuard allows you to shrink large jar file to prepare to embed.", "answer_start": 235, "answer_category": null}], "is_impossible": false}], "context": "Suppose I want to distribute a Java application.\nSuppose I want to distribute it as a single executable. I could easily build a .jar with both the application and all its external dependencies in a single file (with some Ant hacking).\ntry to use Avian and ProGuard toolkits. Avian allows to embed lightweight virtual machine in you app. Linux, MacOS, Windows and iOS are supported. And ProGuard allows you to shrink large jar file to prepare to embed.\n\nNow suppose I want to distribute it as an .exe file on Windows. That's easy enough, given the nice tools out there (such as Launch4j and the likes).\nBut suppose now that I also don't want to depend on the end user having the right JRE (or any JRE at all for that matter) installed. I want to distribute a JRE with my app, and my app should run on this JRE. It's easy enough to create a Windows installer executable, and embed a folder with all necessary JRE files in it. But then I'm distributing an installer and not a single-file app.\nIs there a way to embed both the application, and a JRE, into an .exe file acting as the application launcher (and not as an installer)?\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to manage local vs production settings in Django?", "id": 1646, "answers": [{"answer_id": 1634, "document_id": 1220, "question_id": 1646, "text": "In settings.py:\ntry:\n    from local_settings import *\nexcept ImportError as e:\n    pass\nYou can override what needed in local_settings.py; it should stay out of your version control then. ", "answer_start": 0, "answer_category": null}], "is_impossible": false}], "context": "In settings.py:\ntry:\n    from local_settings import *\nexcept ImportError as e:\n    pass\nYou can override what needed in local_settings.py; it should stay out of your version control then. But since you mention copying I'm guessing you use none ;)\nWhat is the recommended way of handling settings for local development and the production server? Some of them (like constants, etc) can be changed/accessed in both, but some of them (like paths to static files) need to remain different, and hence should not be overwritten every time the new code is deployed.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to detect syntax errors in Apache configure?", "id": 407, "answers": [{"answer_id": 414, "document_id": 173, "question_id": 407, "text": "You can check your configuration files for syntax errors\nwithout starting the server by using apachectl\nconfigtest or the -t command line\noption.", "answer_start": 3109, "answer_category": null}], "is_impossible": false}, {"question": "What files help httpd decentralize management of configuration?", "id": 408, "answers": [{"answer_id": 415, "document_id": 173, "question_id": 408, "text": " .htaccess", "answer_start": 6056, "answer_category": null}], "is_impossible": false}, {"question": "What is Virtual Host in Apache?", "id": 409, "answers": [{"answer_id": 416, "document_id": 173, "question_id": 409, "text": "httpd has the capability to serve many different websites\nsimultaneously.", "answer_start": 5179, "answer_category": null}], "is_impossible": false}, {"question": "What is the maximum length in Apache normal configure file?", "id": 410, "answers": [{"answer_id": 417, "document_id": 173, "question_id": 410, "text": "16 MiB", "answer_start": 3042, "answer_category": null}], "is_impossible": false}, {"question": "What is the maximum length in Apache configure file?", "id": 411, "answers": [{"answer_id": 418, "document_id": 173, "question_id": 411, "text": "8190", "answer_start": 3092, "answer_category": null}], "is_impossible": false}], "context": "\n\nConfiguration Files\n\n\n\n\n\n\nModules | Directives | FAQ | Glossary | Sitemap\nApache HTTP Server Version 2.4\n\n\n\nApache > HTTP Server > Documentation > Version 2.4Configuration Files\nThis document describes the files used to configure Apache HTTP\nServer.\n\nMain Configuration Files\nSyntax of the Configuration Files\nModules\nScope of Directives\n.htaccess Files\n\n\n\nMain Configuration Files\nRelated ModulesRelated Directivesmod_mime<IfDefine>IncludeTypesConfig\nApache HTTP Server is configured by placing directives in plain text\nconfiguration files. The main configuration file is usually called\nhttpd.conf. The location of this file is set at\ncompile-time, but may be overridden with the -f\ncommand line flag. In addition, other configuration files may be\nadded using the Include\ndirective, and wildcards can be used to include many configuration\nfiles. Any directive may be placed in any of these configuration\nfiles. Changes to the main configuration files are only\nrecognized by httpd when it is started or restarted.\nThe server also reads a file containing mime document types;\nthe filename is set by the TypesConfig directive,\nand is mime.types by default.\n\n\nSyntax of the Configuration Files\nhttpd configuration files contain one directive per line.\nThe backslash \"\\\" may be used as the last character on a line\nto indicate that the directive continues onto the next line.\nThere must be no other characters or white space between the\nbackslash and the end of the line.\nArguments to directives are separated by whitespace. If an\nargument contains spaces, you must enclose that argument in quotes.\nDirectives in the configuration files are case-insensitive,\nbut arguments to directives are often case sensitive. Lines\nthat begin with the hash character \"#\" are considered\ncomments, and are ignored. Comments may not be\nincluded on the same line as a configuration directive.\nWhite space occurring before a directive is ignored, so\nyou may indent directives for clarity. Blank lines are also ignored.\nThe values of variables defined with the Define of or shell environment variables can\nbe used in configuration file lines using the syntax ${VAR}.\nIf \"VAR\" is the name of a valid variable, the value of that variable is\nsubstituted into that spot in the configuration file line, and processing\ncontinues as if that text were found directly in the configuration file.\nVariables defined with Define take\nprecedence over shell environment variables.\nIf the \"VAR\" variable is not found, the characters ${VAR}\nare left unchanged, and a warning is logged.\nVariable names may not contain colon \":\" characters, to avoid clashes with\nRewriteMap's syntax.\nOnly shell environment variables defined before the server is started\ncan be used in expansions. Environment variables defined in the\nconfiguration file itself, for example with SetEnv, take effect too late to be used for\nexpansions in the configuration file.\nThe maximum length of a line in normal configuration files, after\nvariable substitution and joining any continued lines, is approximately\n16 MiB. In .htaccess files, the\nmaximum length is 8190 characters.\nYou can check your configuration files for syntax errors\nwithout starting the server by using apachectl\nconfigtest or the -t command line\noption.\nYou can use mod_info's -DDUMP_CONFIG to\ndump the configuration with all included files and environment\nvariables resolved and all comments and non-matching\n<IfDefine> and\n<IfModule> sections\nremoved. However, the output does not reflect the merging or overriding\nthat may happen for repeated directives.\n\n\nModules\nRelated ModulesRelated Directivesmod_so<IfModule>LoadModule\nhttpd is a modular server. This implies that only the most\nbasic functionality is included in the core server. Extended\nfeatures are available through modules which can be loaded\ninto httpd. By default, a base set of modules is\nincluded in the server at compile-time. If the server is\ncompiled to use dynamically loaded\nmodules, then modules can be compiled separately and added at\nany time using the LoadModule\ndirective.\nOtherwise, httpd must be recompiled to add or remove modules.\nConfiguration directives may be included conditional on a\npresence of a particular module by enclosing them in an <IfModule> block. However,\n<IfModule> blocks are not\nrequired, and in some cases may mask the fact that you're missing an\nimportant module.\nTo see which modules are currently compiled into the server,\nyou can use the -l command line option. You can also\nsee what modules are loaded dynamically using the -M\ncommand line option.\n\n\nScope of Directives\nRelated ModulesRelated Directives<Directory><DirectoryMatch><Files><FilesMatch><Location><LocationMatch><VirtualHost>\nDirectives placed in the main configuration files apply to\nthe entire server. If you wish to change the configuration for\nonly a part of the server, you can scope your directives by\nplacing them in <Directory>, <DirectoryMatch>, <Files>, <FilesMatch>, <Location>, and <LocationMatch>\nsections. These sections limit the application of the\ndirectives which they enclose to particular filesystem\nlocations or URLs. They can also be nested, allowing for very\nfine grained configuration.\nhttpd has the capability to serve many different websites\nsimultaneously. This is called Virtual\nHosting. Directives can also be scoped by placing them\ninside <VirtualHost>\nsections, so that they will only apply to requests for a\nparticular website.\nAlthough most directives can be placed in any of these\nsections, some directives do not make sense in some contexts.\nFor example, directives controlling process creation can only\nbe placed in the main server context. To find which directives\ncan be placed in which sections, check the Context of the\ndirective. For further information, we provide details on How Directory, Location and Files sections\nwork.\n\n\n.htaccess Files\nRelated ModulesRelated DirectivesAccessFileNameAllowOverride\nhttpd allows for decentralized management of configuration\nvia special files placed inside the web tree. The special files\nare usually called .htaccess, but any name can be\nspecified in the AccessFileName\ndirective. Directives placed in .htaccess files\napply to the directory where you place the file, and all\nsub-directories. The .htaccess files follow the\nsame syntax as the main configuration files. Since\n.htaccess files are read on every request, changes\nmade in these files take immediate effect.\nTo find which directives can be placed in\n.htaccess files, check the Context of the\ndirective. The server administrator further controls what\ndirectives may be placed in .htaccess files by\nconfiguring the AllowOverride\ndirective in the main configuration files.\nFor more information on .htaccess files, see\nthe .htaccess tutorial.\n\n\nCopyright 2021 The Apache Software Foundation.Licensed under the Apache License, Version 2.0.\nModules | Directives | FAQ | Glossary | Sitemap\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Downgrade npm to an older version", "id": 1550, "answers": [{"answer_id": 1539, "document_id": 1127, "question_id": 1550, "text": "You just need to replace @latest with the version number you want to downgrade to. I wanted to downgrade to version 3.10.10, so I used this command:\nnpm install -g npm@3.10.10", "answer_start": 209, "answer_category": null}], "is_impossible": false}], "context": "I tried updating npm to see if it would solve some dependency problems we were having, and now I want to downgrade to the version the rest of the development team is using. How can I install an older version?\nYou just need to replace @latest with the version number you want to downgrade to. I wanted to downgrade to version 3.10.10, so I used this command:\nnpm install -g npm@3.10.10\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "ADB Install Fails With INSTALL_FAILED_TEST_ONLY", "id": 1531, "answers": [{"answer_id": 1520, "document_id": 1108, "question_id": 1531, "text": "You can see this site: https://developer.android.com/studio/run/index.html", "answer_start": 356, "answer_category": null}], "is_impossible": false}], "context": "I am having issues installing an apk to my device.\nadb install <.apk>\nUsing the above command returns the following:\n5413 KB/s (99747 bytes in 0.017s)\n        pkg: /data/local/tmp/AppClient.TestOnly.App3.apk\nFailure [INSTALL_FAILED_TEST_ONLY]\nAny idea on what might cause this issue?\nIt definitely recognizes the device. Could it be an issue with the apk?\nYou can see this site: https://developer.android.com/studio/run/index.html\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Can I use Add to home screen in Chrome on an iOS device?", "id": 737, "answers": [{"answer_id": 738, "document_id": 425, "question_id": 737, "text": "You can add a website to an iPhone or iPad Home Screen with Safari (not Chrome but maybe it's helpful):\n\u2022\tLaunch Safari\n\u2022\tNavigate to the desired website\n\u2022\tTap the \"share\" button\n\u2022\tSelect \"Add to Home Screen\" icon", "answer_start": 197, "answer_category": null}], "is_impossible": false}], "context": "I know I can use Chrome on a desktop or Android device to \u201cAdd to Home screen\u201d. I know I can use \u201cAdd to homes screen\u201d in Safari iOS.\nBut, can I use \u201cAdd to home screen\u201d feature in Chrome for iOS? You can add a website to an iPhone or iPad Home Screen with Safari (not Chrome but maybe it's helpful):\n\u2022\tLaunch Safari\n\u2022\tNavigate to the desired website\n\u2022\tTap the \"share\" button\n\u2022\tSelect \"Add to Home Screen\" icon\nThat's all. Hope it helps.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "wix how to hide feature options", "id": 1446, "answers": [{"answer_id": 1435, "document_id": 1019, "question_id": 1446, "text": "To remove \"This feature will be installed when required\" set in your feature AllowAdvertise=\"no\"\nhttps://www.firegiant.com/wix/tutorial/user-interface/custom-settings/ ", "answer_start": 2037, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI'm using Wix 3.5 to build a MSI installer.\nI want to know if there is any way to hide some options from the \"Features Custom Setup dialog\" (in which you select from the Feature tree what to install).\n\nI want to only have the options for \"Will be installed on local hard drive\" and \"Entire feature will be  unavailable\"\n\nCurrently, apart from those two options i have these options:\n\n\n\"Entire feature will be installed on local hard drive\" \n\"Will be installed to run from network\" \n\"Entire feature will be installed to run from network\"\n\n    \n\nFeature selection dialog uses SelectionTree control, a built-in control of the Windows Installer.\n\nYou can control which installation options are displayed for a feature using Attributes column of the Feature Table.\n\nFeature element of WiX has four properties which control how a feature can be installed:\n\n\nAbsent: allow / disallow\nAllowAdvertise: no / system / yes\nInstallDefault: followParent / local / source\nTypicalDefault: advertise / install\n\n\nComponent table also controls whether the component can be run from source or not. The Component element has Location property:\n\n\nlocal\nPrevents the component from running from the source or the network (this is the default behavior if this attribute is not set).\nsource\nEnforces that the component can only be run from the source (it cannot be run from the user's computer).\neither\nAllows the component to run from source or locally.\n\n\nSo to remove the option to run from the network, set Location property of your components to local.\n\nYou cannot remove Entire feature will be installed on local hard drive from the options. It is displayed only when there are subfeatures and enables installation of the subfeatures as well as the feature itself as opposed from Will be installed on local hard drive which installs only the selected features and does not affect subfeatures.\n\nIf the subfeatures are always installed with the parent, you can try to set InstallDefault attribute of the subfeatures to followParent.\n    \n\nTo remove \"This feature will be installed when required\" set in your feature AllowAdvertise=\"no\"\nhttps://www.firegiant.com/wix/tutorial/user-interface/custom-settings/ \n    \n\nIf you are using the WiX UIExtension, then you need to download the WiX source code for that extension and modify it appropriately. The following links should help get you started:\n\nWiX UI Customizations\n\nWix custom UI for SQL Database installation\n\nHow to add a UI to a WiX 3 installer?\n\nUPDATE:\n\nUpon examining the WiX UI source, the FeaturesDlg displays a \"SelectionTree\" control. It appears that the control (along with the other controls displayed by the WiX UI) are Windows Installer controls, not WiX-specific controls. See SelectionTree. So it appears there is no easy way to just \"turn off\" those options.\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Installing lapack for numpy", "id": 1229, "answers": [{"answer_id": 1222, "document_id": 805, "question_id": 1229, "text": "removing the package libopenblas-base did the trick:\nsudo apt-get remove libopenblas-base", "answer_start": 408, "answer_category": null}], "is_impossible": false}], "context": "Running Ubuntu 11.10 + python2.7...built numpy from source and installed it, but when I go to install it, I get\nImportError: /usr/lib/liblapack.so.3gf: undefined symbol: ATL_chemv\nwhen it tries to import lapack_lite from numpy.linalg. I tried to rebuild lapack from scratch, but it seems to just make\nand the .so file. Where does the .so.3gf come from, and how do I fix it?\nI was having the same problem and removing the package libopenblas-base did the trick:\nsudo apt-get remove libopenblas-base\nAs already explained by others, several packages provide incompatible versions of liblapack.so.3gf.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Visual Studio 2012 Install Fails: Program Compatibility Mode is on", "id": 775, "answers": [{"answer_id": 773, "document_id": 460, "question_id": 775, "text": "Right-click the file, select Properties and navigate to the Details tab. There should be an entry labelled \"Original filename\". Simply rename the file accordingly and it should run happily", "answer_start": 687, "answer_category": null}], "is_impossible": false}], "context": "I checked the file properties and compatibility mode was off. Googling found that changing the name to \"vs_premium.exe\" or \"vs_ultimate.exe\" or changing the registry keys might help, but the name changes had no effect, and there were no registry keys to delete. I have restarted my machine several times to no avail.\nChanging to Visual Studio 2013 is not an option for me, as my work computer has Visual Studio 2012 on it which they will not update to 2013, and I need to work on my project on both computers.\nPrevious posts are correct in that compatibility mode appears to be based entirely on file names. There is a simple method for determining precisely which name Windows expects:\nRight-click the file, select Properties and navigate to the Details tab. There should be an entry labelled \"Original filename\". Simply rename the file accordingly and it should run happily.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "install / uninstall APKs programmatically (PackageManager vs Intents)", "id": 1581, "answers": [{"answer_id": 1570, "document_id": 1158, "question_id": 1581, "text": "API level 14 introduced two new actions: ACTION_INSTALL_PACKAGE and ACTION_UNINSTALL_PACKAGE. Those actions allow you to pass EXTRA_RETURN_RESULT boolean extra to get an (un)installation result notification.", "answer_start": 561, "answer_category": null}], "is_impossible": false}], "context": "My application installs other applications, and it needs to keep track of what applications it has installed. Of course, this could be achieved by simply keeping a list of installed applications. But this should not be necessary! It should be the responsibility of the PackageManager to maintain the installedBy(a, b) relationship. In fact, according to the API it is:\npublic abstract String getInstallerPackageName(String packageName) - Retrieve the package name of the application that installed a package. This identifies which market the package came from.\nAPI level 14 introduced two new actions: ACTION_INSTALL_PACKAGE and ACTION_UNINSTALL_PACKAGE. Those actions allow you to pass EXTRA_RETURN_RESULT boolean extra to get an (un)installation result notification.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to get started deploying PHP applications from a subversion repository?", "id": 522, "answers": [{"answer_id": 524, "document_id": 247, "question_id": 522, "text": "For PHP, you might want to look into Xinc or phpUnderControl", "answer_start": 403, "answer_category": null}], "is_impossible": false}], "context": "I've heard the phrase \"deploying applications\" which sounds much better/easier/more reliable than uploading individual changed files to a server, but I don't know where to begin.\nI have a Zend Framework application that is under version control (in a Subversion repository). How do I go about \"deploying\" my application? What should I do if I have an \"uploads\" directory that I don't want to overwrite?\nFor PHP, you might want to look into Xinc or phpUnderControl\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "\"Unable to update dependencies of the project\" after committing to Subversion", "id": 337, "answers": [{"answer_id": 345, "document_id": 149, "question_id": 337, "text": " There is a long discussion thread about this on MSDN. It seems like there are many possible causes. The discussion includes a few links for this problem from Microsoft. Here is a hotfix for VS2005 and here is a workaround for VS2010.", "answer_start": 195, "answer_category": null}], "is_impossible": false}], "context": "I have a setup project in .NET. When I save the project and the other projects to subversion, the setup project no longer compiles. I get the error \"Unable to update dependencies of the project.\" There is a long discussion thread about this on MSDN. It seems like there are many possible causes. The discussion includes a few links for this problem from Microsoft. Here is a hotfix for VS2005 and here is a workaround for VS2010.", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Angular 2 Ahead-of-Time compiler: must I make all class properties public?", "id": 1041, "answers": [{"answer_id": 1036, "document_id": 622, "question_id": 1041, "text": "For a given component all its members (methods, properties) accessed by its template must be public in the ahead-of-time compilation scenario. This is due to the fact that a template is turned into a TS class. A generated class and a component are 2 separate classes now and you can't access private members cross-class.", "answer_start": 495, "answer_category": null}], "is_impossible": false}], "context": "I'm trying to move from Just-in-Time to Ahead-of-Time compilation and I'm relying on these resources:\nAngular 2 - Ahead-of-time compilation how to\nhttps://github.com/angular/angular/tree/master/modules/@angular/compiler-cli#angular-template-compiler\nI understand that I need to run the compiler ngc to generate ngfactory.ts files, and that I need to change main.ts to use platformBrowser instead of platformBrowserDynamic to bootstrap. I've hit a roadblock though and don't know how to proceed.\nFor a given component all its members (methods, properties) accessed by its template must be public in the ahead-of-time compilation scenario. This is due to the fact that a template is turned into a TS class. A generated class and a component are 2 separate classes now and you can't access private members cross-class.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "What is bash?", "id": 55, "answers": [{"answer_id": 58, "document_id": 62, "question_id": 55, "text": "Bash is the shell, or command language interpreter,\nfor the GNU operating system.", "answer_start": 118, "answer_category": null}], "is_impossible": false}, {"question": "What is the default shell for GNU operating systems", "id": 56, "answers": [{"answer_id": 100, "document_id": 62, "question_id": 56, "text": "Bash", "answer_start": 844, "answer_category": null}], "is_impossible": false}], "context": "\n\n\n\n\n\nWhat is Bash?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNext: What is a shell?, Up: Introduction \u00a0 [Contents][Index]\n\n\n1.1 What is Bash?\nBash is the shell, or command language interpreter,\nfor the GNU operating system.\nThe name is an acronym for the \u2018Bourne-Again SHell\u2019,\na pun on Stephen Bourne, the author of the direct ancestor of\nthe current Unix shell sh,\nwhich appeared in the Seventh Edition Bell Labs Research version\nof Unix.\n\nBash is largely compatible with sh and incorporates useful\nfeatures from the Korn shell ksh and the C shell csh.\nIt is intended to be a conformant implementation of the IEEE\nPOSIX Shell and Tools portion of the IEEE POSIX\nspecification (IEEE Standard 1003.1).\nIt offers functional improvements over sh for both interactive and\nprogramming use.\n\nWhile the GNU operating system provides other shells, including\na version of csh, Bash is the default shell.\nLike other GNU software, Bash is quite portable.  It currently runs\non nearly every version of Unix and a few other operating systems -\nindependently-supported ports exist for MS-DOS, OS/2,\nand Windows platforms.\n\n\n\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to deploy Python to Windows users?", "id": 1065, "answers": [{"answer_id": 1058, "document_id": 643, "question_id": 1065, "text": "You should copy a Portable Python folder out of your installer, into the same folder as your Delphi/Lazarus app. Set all paths appropriately for that.", "answer_start": 713, "answer_category": null}], "is_impossible": false}], "context": "I'm soon to launch a beta app and this have the option to create custom integration scripts on Python.\nThe app will target Mac OS X and Windows, and my problem is with Windows where Python normally is not present.\nMy actual aproach is silently run the Python 2.6 install. However I face the problem that is not activated by default and the path is not set when use the command line options. And I fear that if Python is installed before and I upgrade to a new version this could break something else...\nSo, I wonder how this can be done cleanly. Is it OK if I copy the whole Python 2.6 directory, and put it in a sub-directory of my app and install everything there? Or with virtualenv is posible run diferents v\nYou should copy a Portable Python folder out of your installer, into the same folder as your Delphi/Lazarus app. Set all paths appropriately for that.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "libusbmuxd version error during flutter install", "id": 1228, "answers": [{"answer_id": 1221, "document_id": 804, "question_id": 1228, "text": "You can try brew install --HEAD usbmuxd.", "answer_start": 1396, "answer_category": null}], "is_impossible": false}], "context": "I'm having problems installing and configuring flutter\nOn mac OS Sierra 10.12.6\nXcode 9.2\nbrew doctor reported all ok\nflutter doctor reported\n[!] iOS toolchain - develop for iOS devices (Xcode 9.2)\n\u2717 **libimobiledevice and ideviceinstaller are not installed. To install, run:\n    brew install --HEAD libimobiledevice\n    brew install ideviceinstaller**\nprompt$: brew install --HEAD libimobiledevice\n==> Cloning https://git.libimobiledevice.org/libimobiledevice.git Updating /Users/rjoiner/Library/Caches/Homebrew/libimobiledevice--git\n==> Checking out branch master Already on 'master' Your branch is up to date with 'origin/master'. HEAD is now at b34e343 tools: Remove length check on device UDID arguments to support newer devices\n==> ./autogen.sh Last 15 lines from /Users/rjoiner/Library/Logs/Homebrew/libimobiledevice/01.autogen.sh: checking dynamic linker characteristics... darwin16.7.0 dyld checking how to hardcode library paths into programs... immediate checking for pkg-config... /usr/local/opt/pkg-config/bin/pkg-config\nchecking pkg-config is at least version 0.9.0... yes checking for libusbmuxd >= 1.1.0... no configure: error: Package requirements (libusbmuxd >= 1.1.0) were not met:\nRequested 'libusbmuxd >= 1.1.0' but version of libusbmuxd is 1.0.10\nI don't know how to install version 1.1.0 of libusbmuxd.\nIt's a bit odd, but it appears the formula for libusbmuxd is usbmuxd. You can try brew install --HEAD usbmuxd.\n\nCheers\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Including MSMQ as a prerequisite for my application", "id": 1101, "answers": [{"answer_id": 1093, "document_id": 678, "question_id": 1101, "text": "Discovered the answer on my own...the windows component installer is not crippled by the typical inability to install more than one MSI at any given time, so I'm able to use a custom installer action to execute a command line script to install MSMQ.", "answer_start": 288, "answer_category": null}], "is_impossible": false}], "context": "I'm working on an application that uses MSMQ for interprocess communication, and I need the setup project to be able to install the service if it isn't already. I've checked around for information on making it a prerequisite, but so far I've been unsuccessful at finding this. Any ideas?\nDiscovered the answer on my own...the windows component installer is not crippled by the typical inability to install more than one MSI at any given time, so I'm able to use a custom installer action to execute a command line script to install MSMQ.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Installing h5py on an Ubuntu server", "id": 807, "answers": [{"answer_id": 802, "document_id": 489, "question_id": 807, "text": "sudo apt-get install libhdf5-dev", "answer_start": 370, "answer_category": null}], "is_impossible": false}], "context": "I was installing h5py on an Ubuntu server. However it seems to return an error that h5py.h is not found. It gives the same error message when I install it using pip or the setup.py file. What am I missing here?\nI have Numpy version 1.8.1, which higher than the required version of 1.6 or above.\nYou need to install libhdf5-dev to get the required header files. Just run\nsudo apt-get install libhdf5-dev\nand it should install it and its dependencies automatically.\nDon't worry about the NumPy warning, it just means that the package developers are using an old version of the API, but everything will still work.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How do I uninstall a Windows service if the files do not exist anymore?", "id": 136, "answers": [{"answer_id": 144, "document_id": 82, "question_id": 136, "text": "From the command prompt, use the Windows \"sc.exe\" utility. You will run something like this: sc delete <service-name>.", "answer_start": 103, "answer_category": null}], "is_impossible": false}], "context": "I installed a .NET Windows Service using InstallUtil. I have since deleted the files but forgot to run.From the command prompt, use the Windows \"sc.exe\" utility. You will run something like this: sc delete <service-name>.\nIts very easy to remove a service from registry if you know the right path. Here is how I did that:\nRun Regedit or Regedt32 Go to the registry entry \"HKEY_LOCAL_MACHINE/SYSTEM/CurrentControlSet/Services\" Look for the service that you want delete and delete it. You can look at the keys to know what files the service was using and delete them as well (if necessary).\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "What are \"Content Files\" (in Visual Studio : Setup Project : File System", "id": 1806, "answers": [{"answer_id": 1791, "document_id": 1377, "question_id": 1806, "text": "The build action property of the file will be labeled \"content\". Here is a link to more information about File Properties.", "answer_start": 240, "answer_category": null}], "is_impossible": false}], "context": "In the context of a Visual Studio 2008 Setup Project, what are \"Content Files\". In other words, when creating a setup project and defining the File System settings and choosing: Add Project Output > Content Files, what files will be added?\nThe build action property of the file will be labeled \"content\". Here is a link to more information about File Properties.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to deploy a meteor application to my own server?", "id": 313, "answers": [{"answer_id": 323, "document_id": 126, "question_id": 313, "text": "you need to provide Node.js 0.8 and a MongoDB server. You can then run the application by invoking node, specifying the HTTP port for the application to listen on, and the MongoDB endpoint. ", "answer_start": 0, "answer_category": null}], "is_impossible": false}], "context": "you need to provide Node.js 0.8 and a MongoDB server. You can then run the application by invoking node, specifying the HTTP port for the application to listen on, and the MongoDB endpoint. So, among the several ways to install Node.js, I got it up and running following the best advice I found, which is basically unpacking the latest version available directly in the official Node.JS website, already compiled for Linux (64 bits, in my case)", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Deploy WAR or \"fat\" JAR?", "id": 1067, "answers": [{"answer_id": 1060, "document_id": 645, "question_id": 1067, "text": "Distributing an application with an embedded webserver allows for standalone setup and running it by just calling java -jar application.jar.", "answer_start": 316, "answer_category": null}], "is_impossible": false}], "context": "I'm noticing a lot of projects (DropWizard, Grails, etc.) starting to embrace the notion of a \"fat\" JAR (using an embedded web server like Jetty or Tomcat) vs. the traditional WAR deploy. Both methods involve a single JVM process (i.e. no matter how many WARs are deployed to Tomcat, it's all the same JVM process).\nDistributing an application with an embedded webserver allows for standalone setup and running it by just calling java -jar application.jar.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How do I download  32-bit Java for Linux?", "id": 164, "answers": [{"answer_id": 171, "document_id": 99, "question_id": 164, "text": "Go to http://java.com and click on the Download button\nThere are two types of installation packages.\nJava on Linux Platforms\nThis installs the Java Runtime Environment (JRE) for 32-bit Linux, using an archive binary file (.tar.gz) that can be installed by anyone (not only the root users), in any location that you can write to. However, only the root user can install Java into the system location.\nJava on RPM-based Linux Platforms\nThis installs the Java Runtime Environment (JRE) for 32-bit RPM-based Linux platforms, such as Red Hat and SuSE, using an RPM binary file (.rpm) in the system location. You must be root to perform this installation.\n\nDownload the package that best suits your needs. You can download the file to any of the directories on your system.\nBefore the file can be downloaded, you must accept the license agreement.\nDownload and check the download file size to ensure that you have downloaded the full, uncorrupted software bundle. Before you download the file, notice its byte size provided on the download page on the web site. Once the download has completed, compare that file size to the size of the downloaded file to make sure they are equal.", "answer_start": 546, "answer_category": null}], "is_impossible": false}, {"question": "How do I install  32-bit Java for Linux Platforms?", "id": 165, "answers": [{"answer_id": 172, "document_id": 99, "question_id": 165, "text": "Change to the directory in which you want to install. Type:\ncd directory_path_name\nFor example, to install the software in the /usr/java/ directory, Type:\ncd /usr/java/\n\nMove the .tar.gz archive binary to the current directory.\nUnpack the tarball and install Java\ntar zxvf jre-8u73-linux-i586.tar.gz\n\nThe Java files are installed in a directory called jre1.8.0_73 in the current directory.\nIn this example, it is installed in the /usr/java/jre1.8.0_73 directory.\nDelete the .tar.gz file if you want to save disk space.", "answer_start": 2530, "answer_category": null}], "is_impossible": false}, {"question": "how  do I  install 32-bit Java for RPM based Linux Platforms?", "id": 166, "answers": [{"answer_id": 173, "document_id": 99, "question_id": 166, "text": "Become root by running su and entering the super-user password.\nUninstall any earlier installations of the Java packages.\nrpm -e package_name\nChange to the directory in which you want to install. Type:\ncd directory_path_name\nFor example, to install the software in the /usr/java/ directory, Type:\ncd /usr/java\n\nInstall the package.\nrpm -ivh jre-8u73-linux-i586.rpm\n\nTo upgrade a package,\nrpm -Uvh jre-8u73-linux-i586.rpm\n\nExit the root shell. No need to reboot.\nDelete the .rpm file if you want to save disk space", "answer_start": 3085, "answer_category": null}], "is_impossible": false}], "context": "How do I download and install 32-bit Java for Linux?\nThis article applies to:\nPlatform(s): Oracle Enterprise Linux, Oracle Linux, Red Hat Linux, SLES, SUSE Linux\nJava version(s): 7.0, 8.0\nLinux System Requirements\nSee supported System Configurations for information about supported platforms, operating systems, desktop managers, and browsers.\n\nNote: For downloading Java other flavors of Linux see Java for Ubuntu Java for Fedora\n\nFollow these steps to download and install 32- bit Java for Linux.\nDownload\nInstall\nEnable and Configure\nDownload\nGo to http://java.com and click on the Download button\nThere are two types of installation packages.\nJava on Linux Platforms\nThis installs the Java Runtime Environment (JRE) for 32-bit Linux, using an archive binary file (.tar.gz) that can be installed by anyone (not only the root users), in any location that you can write to. However, only the root user can install Java into the system location.\nJava on RPM-based Linux Platforms\nThis installs the Java Runtime Environment (JRE) for 32-bit RPM-based Linux platforms, such as Red Hat and SuSE, using an RPM binary file (.rpm) in the system location. You must be root to perform this installation.\n\nDownload the package that best suits your needs. You can download the file to any of the directories on your system.\nBefore the file can be downloaded, you must accept the license agreement.\nDownload and check the download file size to ensure that you have downloaded the full, uncorrupted software bundle. Before you download the file, notice its byte size provided on the download page on the web site. Once the download has completed, compare that file size to the size of the downloaded file to make sure they are equal.\n\nInstall\nJava for Linux Platforms\nJava for RPM based Linux Platforms\nThe instructions below are for installing version Java 8 Update 73 (8u73). If you are installing another version, make sure you change the version number appropriately when you type the commands at the terminal. Example: For Java 8u79 replace 8u73 with 8u79. Note that, as in the preceding example, the version number is sometimes preceded with the letter u and sometimes it is preceded with an underbar, for example, jre1.8.0_73.\n\nNote about root access: To install Java in a system-wide location such as /usr/local, you must login as the root user to gain the necessary permissions. If you do not have root access, install the Java in your home directory or a sub directory for which you have write permissions\n\nJava for Linux Platforms\nChange to the directory in which you want to install. Type:\ncd directory_path_name\nFor example, to install the software in the /usr/java/ directory, Type:\ncd /usr/java/\n\nMove the .tar.gz archive binary to the current directory.\nUnpack the tarball and install Java\ntar zxvf jre-8u73-linux-i586.tar.gz\n\nThe Java files are installed in a directory called jre1.8.0_73 in the current directory.\nIn this example, it is installed in the /usr/java/jre1.8.0_73 directory.\nDelete the .tar.gz file if you want to save disk space.\n\nJava for RPM based Linux Platforms\nBecome root by running su and entering the super-user password.\nUninstall any earlier installations of the Java packages.\nrpm -e package_name\nChange to the directory in which you want to install. Type:\ncd directory_path_name\nFor example, to install the software in the /usr/java/ directory, Type:\ncd /usr/java\n\nInstall the package.\nrpm -ivh jre-8u73-linux-i586.rpm\n\nTo upgrade a package,\nrpm -Uvh jre-8u73-linux-i586.rpm\n\nExit the root shell. No need to reboot.\nDelete the .rpm file if you want to save disk space.\nThe installation is now complete. Go to the Enable and Configure section.", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "cant find start all sh in hadoop installation", "id": 1989, "answers": [{"answer_id": 1975, "document_id": 1573, "question_id": 1989, "text": "Try to run :\n\nhduser@ubuntu:~$ /usr/local/hadoop/sbin/start-all.sh\n\n\nSince start-all.sh and stop-all.sh located in sbin directory while hadoop binary file is located in bin directory.\n\nAlso updated your .bashrc for:\n\n\n  export PATH=$PATH:$HADOOP_HOME/bin:$HAD", "answer_start": 2041, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI am trying to setup hadoop on my local machine and was following this. I have setup hadoop home also\n\nThis is the command I am trying to run now\n\nhduser@ubuntu:~$ /usr/local/hadoop/bin/start-all.sh\n\n\nAnd this is the error I get \n\n-su: /usr/local/hadoop/bin/start-all.sh: No such file or directory\n\n\nThis is what I added to my $HOME/.bashrc file\n\n# Set Hadoop-related environment variables\nexport HADOOP_HOME=/usr/local/hadoop\n\n# Set JAVA_HOME (we will also configure JAVA_HOME directly for Hadoop later on)\nexport JAVA_HOME=/usr/lib/jvm/java-8-oracle\n\n# Some convenient aliases and functions for running Hadoop-related commands\nunalias fs &amp;&gt; /dev/null\nalias fs=\"hadoop fs\"\nunalias hls &amp;&gt; /dev/null\nalias hls=\"fs -ls\"\n\n# If you have LZO compression enabled in your Hadoop cluster and\n# compress job outputs with LZOP (not covered in this tutorial):\n# Conveniently inspect an LZOP compressed file from the command\n# line; run via:\n#\n# $ lzohead /hdfs/path/to/lzop/compressed/file.lzo\n#\n# Requires installed 'lzop' command.\n#\nlzohead () {\n    hadoop fs -cat $1 | lzop -dc | head -1000 | less\n}\n\n# Add Hadoop bin/ directory to PATH\nexport PATH=$PATH:$HADOOP_HOME/bin\n\n\nEDIT After trying the solution given by mahendra I am getting the following output\n\nThis script is Deprecated. Instead use start-dfs.sh and start-yarn.sh\nStarting namenodes on [localhost]\nlocalhost: starting namenode, logging to /usr/local/hadoop/logs/hadoop-hduser-namenode-mmt-HP-ProBook-430-G3.out\nlocalhost: starting datanode, logging to /usr/local/hadoop/logs/hadoop-hduser-datanode-mmt-HP-ProBook-430-G3.out\nStarting secondary namenodes [0.0.0.0]\n0.0.0.0: starting secondarynamenode, logging to /usr/local/hadoop/logs/hadoop-hduser-secondarynamenode-mmt-HP-ProBook-430-G3.out\nstarting yarn daemons\nstarting resourcemanager, logging to /usr/local/hadoop/logs/yarn-hduser-resourcemanager-mmt-HP-ProBook-430-G3.out\nlocalhost: starting nodemanager, logging to /usr/local/hadoop/logs/yarn-hduser-nodemanager-mmt-HP-ProBook-430-G3.out\n    \n\nTry to run :\n\nhduser@ubuntu:~$ /usr/local/hadoop/sbin/start-all.sh\n\n\nSince start-all.sh and stop-all.sh located in sbin directory while hadoop binary file is located in bin directory.\n\nAlso updated your .bashrc for:\n\n\n  export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin\n\n\nso that you can directly access start-all.sh\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to install a Python package from within IPython?", "id": 791, "answers": [{"answer_id": 787, "document_id": 474, "question_id": 791, "text": "You can use the ! prefix like this:\n!pip install packagename", "answer_start": 88, "answer_category": null}], "is_impossible": false}], "context": "I wonder if it's possible to install python packages without leaving the IPython shell. You can use the ! prefix like this:\n!pip install packagename\nThe ! prefix is a short-hand for the %sc command to run a shell command.\nYou can also use the !! prefix which is a short-hand for the %sx command to execute a shell command and capture its output (saved into the _ variable by default).\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to build a dmg Mac OS X file (on a non-Mac platform)?", "id": 713, "answers": [{"answer_id": 716, "document_id": 403, "question_id": 713, "text": "It does seem possible to create DMG files with some third party tools. A quick google search reveals at least a few commercial tools:\n\u2022\tTransMac\n\u2022\tMagicISO", "answer_start": 102, "answer_category": null}], "is_impossible": false}], "context": "Is it possible to build a .dmg file (for distributing apps) from a non-Mac platform? And if yes, how? It does seem possible to create DMG files with some third party tools. A quick google search reveals at least a few commercial tools:\n\u2022\tTransMac\n\u2022\tMagicISO\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How do I undo 'git add' before commit?", "id": 1855, "answers": [{"answer_id": 1841, "document_id": 1426, "question_id": 1855, "text": "You can undo git add before commit with\ngit reset <file>", "answer_start": 178, "answer_category": null}], "is_impossible": false}], "context": "I mistakenly added files to Git using the command:\ngit add myfile.txt\nI have not yet run git commit. Is there a way to undo this, so these files won't be included in the commit?\nYou can undo git add before commit with\ngit reset <file>\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Xampp MySQL not starting - \"Attempting to start MySQL service...\"", "id": 1185, "answers": [{"answer_id": 1178, "document_id": 761, "question_id": 1185, "text": "go to services.msc file on your windows and right click the MySQL file and stop the service, now open your XAMPP and start MySQL. Now MySQL will start on the port 3306", "answer_start": 513, "answer_category": null}], "is_impossible": false}], "context": "I've just installed XAMPP for Windows - should be the newest version (XAMPP Control Panel v3.2.1).\nApache is running just fine on port 80 and 443, but MySQL is not starting. When I press the start button, I get this message:\nAttempting to start MySQL service...\nThen a window pops up and asks me if I want to allow this, which I want. But nothing happens after that. I can press as many times as I want, but with the same result.\nWhat can I do with MySQL? If you have MySQL already installed on your windows then go to services.msc file on your windows and right click the MySQL file and stop the service, now open your XAMPP and start MySQL. Now MySQL will start on the port 3306.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Android SDK installation doesn't find JDK", "id": 1533, "answers": [{"answer_id": 1522, "document_id": 1110, "question_id": 1533, "text": "Press Back when you get the notification and then Next. This time it will find the JDK", "answer_start": 244, "answer_category": null}], "is_impossible": false}], "context": "I'm trying to install the Android SDK on my Windows 7 x64 System.\njdk-6u23-windows-x64.exe is installed, but the Android SDK setup refuses to proceed because it doesn't find the JDK installation.\nIs this a known issue? And is there a solution?\nPress Back when you get the notification and then Next. This time it will find the JDK\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How do you deploy your SharePoint solutions?", "id": 1084, "answers": [{"answer_id": 1076, "document_id": 661, "question_id": 1084, "text": "You should check out the SharePoint Content Deployment Wizard by Chris O'Brien. http://www.codeplex.com/SPDeploymentWizard", "answer_start": 418, "answer_category": null}], "is_impossible": false}], "context": "I am now in the process of planning the deployment of a SharePoint solution into a production environment.\nI have read about some tools that promise an easy way to automate this process, but nothing that seems to fit my scenario.\nIn the testing phase I have used SharePoint Designer to copy site content between the different development and testing servers, but this process is manual and it seems a bit unnecessary.\nYou should check out the SharePoint Content Deployment Wizard by Chris O'Brien. http://www.codeplex.com/SPDeploymentWizard\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Deploy a project using Git push", "id": 1645, "answers": [{"answer_id": 1633, "document_id": 1219, "question_id": 1645, "text": "1.\tCopy over your .git directory to your web server\n2.\tOn your local copy, modify your .git/config file and add your web server as a remote:\n3.\t[remote \"production\"]\n4.\t    url = username@webserver:/path/to/htdocs/.git\n5.\tOn the server, replace .git/hooks/post-update with this file (in the answer below)\n6.\tAdd execute access to the file (again, on the server):\n7.\tchmod +x .git/hooks/post-update\n8.\tNow, just locally push to your web server and it should automatically update the working copy:\ngit push production", "answer_start": 271, "answer_category": null}], "is_impossible": false}], "context": "Is it possible to deploy a website using git push? I have a hunch it has something to do with using git hooks to perform a git reset --hard on the server side, but how would I go about accomplishing this?\nI found this script on this site and it seems to work quite well.\n1.\tCopy over your .git directory to your web server\n2.\tOn your local copy, modify your .git/config file and add your web server as a remote:\n3.\t[remote \"production\"]\n4.\t    url = username@webserver:/path/to/htdocs/.git\n5.\tOn the server, replace .git/hooks/post-update with this file (in the answer below)\n6.\tAdd execute access to the file (again, on the server):\n7.\tchmod +x .git/hooks/post-update\n8.\tNow, just locally push to your web server and it should automatically update the working copy:\ngit push production\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Why does Nginx return a 403 even though all permissions are set properly?", "id": 780, "answers": [{"answer_id": 777, "document_id": 464, "question_id": 780, "text": "According to SELinux: setsebool -P httpd_read_user_content ", "answer_start": 327, "answer_category": null}], "is_impossible": false}], "context": "2014/03/23 12:45:08 [error] 5490#0: *13 open() \"/var/www/html/index.html\" failed (13: Permission denied), client: XXX.XX.XXX.XXX, server: localhost, request: \"GET /index.html HTTP/1.1\", host: \"ec2-XXX-XX-XXX-XXX.compute-1.amazonaws.com\" I ran into the same problem. If you're using Fedora/RedHat/CentOS, this might help you:\n\u2022\tAccording to SELinux: setsebool -P httpd_read_user_content 1\nHope this help\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "installing cuda 5 samples in ubuntu 12 10", "id": 1480, "answers": [{"answer_id": 1469, "document_id": 1056, "question_id": 1480, "text": " 4.7!!  \n\n\n    \n\nCUDA 5 is not yet suppor", "answer_start": 2890, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI'm trying to install cuda 5 samples:\n\nDriver:   Not Selected\nToolkit:  Not Selected\nSamples:  Installation Failed. Missing required libraries.\n\n\nBut I got this error:\n\nMissing required library libglut.so\n\n\nBut:\n\nfrederico@zeus:~/Downloads$ sudo find / -name libglut.so*\n/usr/lib/libglut.so\n/usr/lib32/nvidia-current/libglut.so\n\n\nWhere nvidia installer is looking for? maybe /usr/lib64? There is no /usr/lib64 on Ubuntu 12.10:\n\nfrederico@zeus:~/Downloads$ ls /usr\nbin  games  include  lib  lib32  local  sbin  share  src\n\nfrederico@zeus:~/Downloads$ uname -a\nLinux zeus 3.5.0-17-generic #28-Ubuntu SMP Tue Oct 9 19:31:23 UTC 2012 x86_64 x86_64 x86_64 GNU/Linux\n\n    \n\nI am also running Ubuntu 12.10 and I found this library in folder /usr/lib/x86_64-linux-gnu/ after installing freeglut3 package.\n\nI also make a softlink and I have been able to install CUDA 5.0 examples:\n\nln -s /usr/lib/x86_64-linux-gnu/libglut.so.3 /usr/lib/libglut.so\n\n\nI have not checked if the samples can be compiled yet.\n    \n\nI got CUDA-5.0 to work very fine on Ubuntu 12.10. It built the samples. I was also able to compile OpenCV-2.4.4 with Cuda support, than ffmpeg with libopencv support speeding the editing and encoding of Videos up to 8x.\n\nI've done this:\n\n\nsudo apt-get install linux-headers-3.5.0-26 freeglut3-dev mpich-dev gcc-4.6 g++-4.6\nDon't install the nvidia-driver from the ubuntu repository!, download the latest Driver from nvidia.com and do: chmod +x (Downloadpath)/*run  than sudo lightdm stop  than cd (Downloadpath) than sudo ./NV*run than sudo restart\ndownload the ubuntu-11X-version of cuda-5.0 from nvidia.com and install it (sudo ./*run) without the offered dev-driver, answer the question about install-Path with \" /opt/cuda-5.0\nthe default version of gcc in ubuntu 12.10 (gcc-4.7) conflicts with the Cuda-Code. so let cuda use the perv. gcc-version: sudo ln -s /usr/bin/gcc-4.6 /opt/cuda-5.0/bin/gcc\nlet ldconf know about the cuda-libs: echo '/opt/cuda-5.0/lib64' &gt; /etc/ld.so.conf.d/nvidia-cuda.conf &amp;&amp; echo '/opt/cuda-5.0/lib' &gt;&gt; /etc/ld.so.conf.d/nvidia-cuda.conf than do: sudo ldconfig\nMany programs search for the (nvidia)-OpenCL-Include-Files in /usr/include. But they are not there, so link to them:   sudo ln -s /opt/cuda-5.0/include/CL /usr/include/CL and sudo ln -s /opt/cuda-5.0/include/CL /usr/include/OpenCL\nCuda can build the sample now. But if you want to compile Program-Sourcecode e.g. OpenCV you need to change temorarily the symbolic links in order to point at gcc-4.6 and g++-4.6: sudo ln -s /usr/bin/gcc-4.6 /usr/bin/gcc - sudo ln -s /usr/bin/g++-4.6 /usr/bin/g++ - sudo ln -s /usr/bin/g++-4.6 /usr/bin/c++\nIn order to avoid version-conflicts try to compile all other dependent Packages with those symlinks (e.g. OpenCV+ffmpeg+frei0r-plugins)\n\nAfter compiling: Don't forget to correct the gcc and g++ symlinks to point at Version 4.7!!  \n\n\n    \n\nCUDA 5 is not yet supported on Ubuntu 12.\nFor reference see CUDA 5.0 Toolkit Release Notes And Errata\n\n**  Distributions Currently Supported  \n\n    Distribution       32 64  Kernel                 GCC         GLIBC        \n    -----------------  -- --  ---------------------  ----------  -------------\n    Fedora 16          X  X   3.1.0-7.fc16           4.6.2       2.14.90      \n    ICC Compiler 12.1     X                                                   \n    OpenSUSE 12.1         X   3.1.0-1.2-desktop      4.6.2       2.14.1       \n    Red Hat RHEL 6.x      X   2.6.32-131.0.15.el6    4.4.5       2.12         \n    Red Hat RHEL 5.5+     X   2.6.18-238.el5         4.1.2       2.5          \n    SUSE SLES 11 SP2      X   3.0.13-0.27-pae        4.3.4       2.11.3       \n    SUSE SLES 11.1     X  X   2.6.32.12-0.7-pae      4.3.4       2.11.1       \n    Ubuntu 11.10       X  X   3.0.0-19-generic-pae   4.6.1       2.13         \n    Ubuntu 10.04       X  X   2.6.35-23-generic      4.4.5       2.12.1    \n\n    \n\nMaybe you need to create a softlink from lib64 to lib using:\n\nln -s /usr/lib /usr/lib64\n\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Deploying a production Node.js server", "id": 324, "answers": [{"answer_id": 333, "document_id": 137, "question_id": 324, "text": "Try using pm2 it is simple and intuitive CLI, installable via NPM. Just start your application with PM2 and your application is ready to handle a ton of traffic", "answer_start": 508, "answer_category": null}], "is_impossible": false}], "context": "I've written a Node.js app, I'm looking to get it running on one of our production machines. This seems like a pretty common request yet I can't find an adequate solution. Is there not established solutions for deploying production Node.js apps?\nThe app is simple (<100 LOC), but needs to be very efficient, reliable and could run continuously for years without restarting. It's going to be run on a large site, with dozens of connections/second. (the app is not used as a webserver, it only has a JSON API)\nTry using pm2 it is simple and intuitive CLI, installable via NPM. Just start your application with PM2 and your application is ready to handle a ton of traffic\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to build & install GLFW 3 and use it in a Linux project", "id": 660, "answers": [{"answer_id": 665, "document_id": 353, "question_id": 660, "text": "A pkg-config file describes all necessary compile-time and link-time flags and dependencies needed to use a library.\npkg-config --static --libs glfw3\nshows me that\n-L/usr/local/lib -lglfw3 -lrt -lXrandr -lXinerama -lXi -lXcursor -lGL -lm -ldl -lXrender -ldrm -lXdamage -lX11-xcb -lxcb-glx -lxcb-dri2 -lxcb-dri3 -lxcb-present -lxcb-sync -lxshmfence -lXxf86vm -lXfixes -lXext -lX11 -lpthread -lxcb -lXau -lXdmcp  ", "answer_start": 1513, "answer_category": null}], "is_impossible": false}], "context": "Last night I was working late trying to build the GLFW 3 packages for Linux from source. This process took me a very long time, about 3 hours in total, partly because I am unfamiliar with CMake, and partly because I am was unfamiliar with GLFW.\nI hope that this post will save you from the difficulty I had yesterday! I thought I should make a short write-up, and hopefully save you several hours of your life...\nThanks to \"urraka\", \"b6\" and \"niklas\" on the #glfw IRC channel, I have been able to get glfw version 3.0.1 working.\nIt turns out this is not a trivial process (certainly not for me, I am no expert) as there is not much documentation on the web about glfw3, particularly about setting it up with CMake.\nI was asked to split this into a question and answer section, and so I have done that, and the answer parts are now below.\nAre you a maintainer of GLFW, or a member of the GLFW team?\nIf any of the maintainers of GLFW3 see this, then my message to them is please add a \"setting up GLFW3 on Windows, Mac OS X and Linux\" section to your website! It is quite easy to write programs with GLFW, since the online documentation is quite good, a quick scan of all the available classes and modules and you'll be ready to go. The example of a test project featured here is also very good. The two main problems I found were, firstly how do I set up GLFW3 on my system, and secondly how to I build a GLFW3 project? These two things perhaps aren't quite clear enough for a non-expert. \nI solved it in this way\nA pkg-config file describes all necessary compile-time and link-time flags and dependencies needed to use a library.\npkg-config --static --libs glfw3\nshows me that\n-L/usr/local/lib -lglfw3 -lrt -lXrandr -lXinerama -lXi -lXcursor -lGL -lm -ldl -lXrender -ldrm -lXdamage -lX11-xcb -lxcb-glx -lxcb-dri2 -lxcb-dri3 -lxcb-present -lxcb-sync -lxshmfence -lXxf86vm -lXfixes -lXext -lX11 -lpthread -lxcb -lXau -lXdmcp  \nI don't know if all these libs are actually necessary for compiling but for me it works...\n.ad a brief look today (Date: 2014-01-14) it looks as if the GLFW website has undergone heavy changes since I last looked and there is now a section on compiling GLFW and buliding programs with GLFW, which I think are new.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "could not get google app id in google services file from build environment err", "id": 1388, "answers": [{"answer_id": 1377, "document_id": 960, "question_id": 1388, "text": "ode\n\nSolution:\nAdd GoogleService-Info.plist from Runner/ onto Xcode(10.3 as of this post) by\nFile -&gt; Add f", "answer_start": 1613, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI am using the below guide to set up a dev and prod environment for my project that uses flutter + firebase\nhttps://www.tengio.com/blog/multiple-firebase-environments-with-flutter/ \n\nI have set up my respective GoogleService-Info.plist under Firebase-&gt;dev and Firebase-&gt;prod folders\n\nI have also added below script under my Xcode Target-&gt;Build Phases (before the compile sources)\n\nif [ \"${CONFIGURATION}\" == \"Debug-prod\" ] || [ \"${CONFIGURATION}\" == \"Release-prod\" ] || [ \"${CONFIGURATION}\" == \"Release\" ]; then\ncp -r \"${PROJECT_DIR}/Runner/Firebase/prod/GoogleService-Info.plist\" \"${PROJECT_DIR}/GoogleService-Info.plist\"\n\necho \"Production plist copied\"\n\nelif [ \"${CONFIGURATION}\" == \"Debug-dev\" ] || [ \"${CONFIGURATION}\" == \"Release-dev\" ] || [ \"${CONFIGURATION}\" == \"Debug\" ]; then\n\ncp -r \"${PROJECT_DIR}/Runner/Firebase/dev/GoogleService-Info.plist\" \"${PROJECT_DIR}/GoogleService-Info.plist\"\n\necho \"Development plist copied\"\nfi\n\n\nI have tried:\n\nflutter run --flavor dev\n\n\nafter\n\n\nFlutter clean and then running\nI went to Xcode Product -&gt; Clean Build folder and then running\nI have also tried to delete the iOS -&gt; Pods folder along with Podfile.lock and then running\n\n\nI get this error:\n\n    error: Could not get GOOGLE_APP_ID in Google Services file from build environment\n\n\nAny help to resolve is deeply appreciated\n\n(Note: I have tried other stackoverflow solutions - still no luck)\n    \n\nFigured out the issue finally. (Due to my Xcode ignorance) I did not know the files are to be manually \"added\" to Xcode. A file present in physical directory does not mean it is on Xcode\n\nSolution:\nAdd GoogleService-Info.plist from Runner/ onto Xcode(10.3 as of this post) by\nFile -&gt; Add files to \"Runner..\"\n\nNote: Uncheck 'Copy if required'\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Send maven output to file", "id": 1887, "answers": [{"answer_id": 1873, "document_id": 1458, "question_id": 1887, "text": "You can try mvn -help\n -l,--log-file <arg>  Log file to where all build output will go.    ", "answer_start": 484, "answer_category": null}], "is_impossible": false}], "context": "On Windows 7, I am trying to send the output of a maven-3 command to a text file.\nI call the command from the root of the project I am trying to analyze.\nThe command is:\nmvn dependency:tree -Dverbose -Dincludes=commons-collections -DoutputFile=C:\\Users\\myname\\Documents\\output.txt\nWhen I run the command without the outputFile parameter, I see the output sent to the console.\nBut when I use it with the outputFile parameter, the output file is empty.\nAny idea what I am missing here?\nYou can try mvn -help\n -l,--log-file <arg>  Log file to where all build output will go.      \n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "distributing a stand alone python web based application to non technical users", "id": 1923, "answers": [{"answer_id": 1910, "document_id": 1496, "question_id": 1923, "text": "Your download is smaller as you're not bundling the whole Python Standard Lib and extra stuff your app wont need and you get an exe file to boot!\n    \n\nYou can try the Bitnami Stack for Django that includes Apache, MySQL,Python, etc in an all-in-one installer. It is free/open source", "answer_start": 1797, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI'm writing a web application in Python, intended for use by teachers and pupils in a classroom. It'll run from a hosted website, but I also want people to be able to download a self-contained application they can install locally if they want more performance or they simply won't have an Internet connection available in the classroom.\n\nThe users aren't going to be able to manage instructions like \"first install Python, then install dependencies, download the .tar.gz archive and type these commands into the command line...\". I need to be able to create an all-in-one type installer that can potentially install Python, dependencies (Python-LDAP), some Python code, and register a Python-based web server as a Windows Service.\n\nI've had a look through previous questions, but none quite seem relevant. I'm not concerned about the security of source code (my application will be open source, I'll sell content to go with it), I just need non-technical Windows users to be able to download and use my application with no fuss.\n\nMy current thoughts are to use NSIS to create an installer that includes Python and Python-LDAP as MSIs, then registers my own simple Python-based web server as a Windows service and puts a shortcut in the start menu / on the desktop linking to http://localhost. Is this doable with NSIS - can NSIS check for currently installed copies of Python, for instance? Is there a better way of doing this - is there a handy framework available that lets me shove my code in a folder and bundle it up to make an installer?\n    \n\nUsing NSIS is great (i use it too) but i would suggest using a \"packager\" like pyinstaller (my personal fav, alternatives bb_freeze, py2exe) to create an exe before the using NSIS\n\nThe primary benefit you get by doing this is;\nYour download is smaller as you're not bundling the whole Python Standard Lib and extra stuff your app wont need and you get an exe file to boot!\n    \n\nYou can try the Bitnami Stack for Django that includes Apache, MySQL,Python, etc in an all-in-one installer. It is free/open source\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Options for deploying R models in production", "id": 509, "answers": [{"answer_id": 510, "document_id": 234, "question_id": 509, "text": "The answer really depends on what your production environment is.If your \"big data\" are on Hadoop, you can try this relatively new open source PMML \"scoring engine\" called Pattern.", "answer_start": 141, "answer_category": null}], "is_impossible": false}], "context": "There doesn't seem to be too many options for deploying predictive models in production which is surprising given the explosion in Big Data. The answer really depends on what your production environment is.If your \"big data\" are on Hadoop, you can try this relatively new open source PMML \"scoring engine\" called Pattern.", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Java, JPA, Glassfish, Invalid resource : jdbc/__default__pm", "id": 542, "answers": [{"answer_id": 544, "document_id": 267, "question_id": 542, "text": "If you have only created a MySQL connection pool, you must also create a JDBC resource. This can be created from the context menu above the one you used to create the connection pool.", "answer_start": 148, "answer_category": null}], "is_impossible": false}], "context": "I use Glassfish 3.1.2.2 (build 5), JPA, EclipseLink, MySQL\nI created MySQL pool via Glassfish admin panel. Ping to MySQL from GF admin panel is ok.\nIf you have only created a MySQL connection pool, you must also create a JDBC resource. This can be created from the context menu above the one you used to create the connection pool.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Programmatically install NLTK corpora / models, i.e. without the GUI downloader?", "id": 716, "answers": [{"answer_id": 719, "document_id": 406, "question_id": 716, "text": "mport nltk\nnltk.download(info_or_id=\"popular\", download_dir=\"/path/to/dir\")\nnltk.data.path.append(\"/path/to/dir\")", "answer_start": 386, "answer_category": null}], "is_impossible": false}], "context": "My project uses the NLTK. How can I list the project's corpus & model requirements so they can be automatically installed? I don't want to click through the nltk.download() GUI, installing packages one by one.\nAlso, any way to freeze that same list of requirements (like pip freeze)?\n3\nI've managed to install the corpora and models inside a custom directory using the following code:\nimport nltk\nnltk.download(info_or_id=\"popular\", download_dir=\"/path/to/dir\")\nnltk.data.path.append(\"/path/to/dir\")\nthis will install \"all\" corpora/models inside /path/to/dir, and will let know NLTK where to look for it (data.path.append).\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Install tkinter for Python", "id": 1590, "answers": [{"answer_id": 1579, "document_id": 1166, "question_id": 1590, "text": "You can use apt-get install python-tk on your machine(s). (Works on Debian-derived distributions like for Ubuntu; refer to your package manager and package list on other distributions.)", "answer_start": 399, "answer_category": null}], "is_impossible": false}], "context": "I am trying to import Tkinter. However, I get an error stating that Tkinter has not been installed:\nImportError: No module named _tkinter, please install the python-tk package\nI could probably install it using synaptic manager (can I?), however, I would have to install it on every machine I program on. Would it be possible to add the Tkinter library into my workspace and reference it from there?\nYou can use apt-get install python-tk on your machine(s). (Works on Debian-derived distributions like for Ubuntu; refer to your package manager and package list on other distributions.)\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "What's the best way to distribute Java applications?", "id": 639, "answers": [{"answer_id": 644, "document_id": 332, "question_id": 639, "text": "advanced installer makes it easy to package java apps as windows executables, and it's quite flexible in the way you can set it up. I've found that for distributing java applications to windows clients, this is the easiest way to go. ", "answer_start": 461, "answer_category": null}], "is_impossible": false}], "context": "Java is one of my programming languages of choice. I always run into the problem though of distributing my application to end-users.Giving a user a JAR is not always as user friendly as I would like and using Java WebStart requires that I maintain a web server.What's the best way to distribute a Java application? What if the Java application needs to install artifacts to the user's computer? Are there any good Java installation/packaging systems out there? advanced installer makes it easy to package java apps as windows executables, and it's quite flexible in the way you can set it up. I've found that for distributing java applications to windows clients, this is the easiest way to go. It depends on how sophisticated your target users are. In most cases you want to isolate them from the fact that you are running a Java-based app. Give them with a native installer that does the right thing (create start menu entries, launchers, register with add/remove programs, etc.) and already bundles a Java runtime (so the user does not need to know or care about it).", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Node.js Cygwin not supported", "id": 825, "answers": [{"answer_id": 820, "document_id": 507, "question_id": 825, "text": " Use Console2, it allows you to run create tabs of CLI shells", "answer_start": 203, "answer_category": null}], "is_impossible": false}], "context": "I am trying to install node.js. I followed this tutorial and i am stuck in the middle.\nWhen I write ./configure in my cygwin terminal it says \"cygwin not supported\". Please help me out Thanks in advance. Use Console2, it allows you to run create tabs of CLI shells. It seems running cygwin inside console2 allows me to use node REPL just fine. I have no idea why :P\nFollow this guide to add cygwin to console2:\nhttp://blog.msbbc.co.uk/2009/11/configuring-console-2-and-bash-with.html\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to set user condition to the action launching the EXE?", "id": 1240, "answers": [{"answer_id": 1233, "document_id": 812, "question_id": 1240, "text": "The LogonUser property is the user name for the currently logged on user. Set by the installer by a system call to GetUserName", "answer_start": 77, "answer_category": null}], "is_impossible": false}], "context": "LogonUser property\n03/23/2021\n2 minutes to read\nD\nV\n\nM\nIs this page helpful?\nThe LogonUser property is the user name for the currently logged on user. Set by the installer by a system call to GetUserName.\n\nRequirements\nREQUIREMENTS\nRequirement\tValue\nVersion\nWindows Installer 5.0 on Windows Server 2012, Windows 8, Windows Server 2008 R2 or Windows 7. Windows Installer 4.0 or Windows Installer 4.5 on Windows Server 2008 or Windows Vista. Windows Installer on Windows Server 2003 or Windows XP. See the Windows Installer Run-Time Requirements for information about the minimum Windows service pack that is required by a Windows Installer version.\nSee also\nProperties\n\n \n\n \n\nRecommended content\nMsiGetProductInfoA function (msi.h) - Win32 apps\nThe MsiGetProductInfo function returns product information for published and installed products.\n\nDeferred Execution Custom Actions - Win32 apps\nThe purpose of a deferred execution custom action is to delay the execution of a system change to the time when the installation script is executed.", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "npm install bower using -g vs --save-dev", "id": 1173, "answers": [{"answer_id": 1166, "document_id": 750, "question_id": 1173, "text": "Using the --save and --save-dev flags when installing will add them to the project's package.json.", "answer_start": 369, "answer_category": null}], "is_impossible": false}], "context": "I'm new to node and using npm to both do some node, angular and Express tutorials. I have used bower before in a tutorial. I'm pretty sure I have installed it using -g already as when i run the bower -v command I get back 1.3.3 I am to understand that installing it using -g means, Install this globally so that on the next project I don't have to install it again. 45\nUsing the --save and --save-dev flags when installing will add them to the project's package.json. This allows anyone who might develop on or use the project to install the dependencies as needed with a simple npm install command. By contrast, the -g flag is global\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Android error: Failed to install *.apk on device *: timeout", "id": 1532, "answers": [{"answer_id": 1521, "document_id": 1109, "question_id": 1532, "text": "You can try to change the ADB connection timeout. I think it defaults that to 5000ms and I changed mine to 10000ms to get rid of that problem.", "answer_start": 399, "answer_category": null}], "is_impossible": false}], "context": "I'm getting this error from time to time and don't know what causing this:\nWhen trying to run/debug an Android app on a real device (Galaxy Samsung S in my case) I'm getting the following error.\nThe app has been debugged in the past on that device many times (app is live on Market), but this problem happens every so often, and is VERY FRUSTRATING...\nAny help would be greatly appreciated! Thanks.\nYou can try to change the ADB connection timeout. I think it defaults that to 5000ms and I changed mine to 10000ms to get rid of that problem.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to deploy MERN stack to a hosting service?", "id": 1269, "answers": [{"answer_id": 1261, "document_id": 840, "question_id": 1269, "text": "That is probably because the MongoDB URL is probably still set to something like 'mongodb://localhost:27017/myapp. There are 2 main options here\nUse a \"database hosting service\" like MongoDB Atlas, you will then need to change your database's URL to point to the one provided by the service\nHost the database on your server, which will work with the localhost URL", "answer_start": 566, "answer_category": null}], "is_impossible": false}], "context": "I am new to web development and I have a hosting service on which I want to deploy my reactjs/node project. To deploy it previously, I had been simply uploading the build folder created from running npm run build. I have since added a connection to a mongodb database using the MERN stack.\nThe project was initially created using create-react-app, then I set up my backend node/mongodb following this tutorial: https://appdividend.com/2018/11/11/react-crud-example-mern-stack-tutorial/. All of my server code is contained within the project folder in an api folder.\nThat is probably because the MongoDB URL is probably still set to something like 'mongodb://localhost:27017/myapp. There are 2 main options here\nUse a \"database hosting service\" like MongoDB Atlas, you will then need to change your database's URL to point to the one provided by the service\nHost the database on your server, which will work with the localhost URL\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Visual Studio 2015 installer hangs during install?", "id": 1620, "answers": [{"answer_id": 1607, "document_id": 1194, "question_id": 1620, "text": "You should restore Windows 7 to earliest restore point as possible, when there was nothing installed, so I was sure that there would be no conflicts between the different tools (Java, Android API, etc.)", "answer_start": 660, "answer_category": null}], "is_impossible": false}], "context": "I downloaded the full ISO for Visual Studio Ultimate CTP 6. The installation program got to about the 90% mark, gauging by the progress bar, and just stuck there. There was frequent activity from Superfetch, Anti-malware protection, and other background processes, but the progress bar was dead still. Eventually the background task activity subsided after 20 minutes, but the progress bar still wouldn't budge.\nCHEAP TRICK: Open a notepad window and position the left edge of it so it perfectly marks the current position of the progress bar. If it the progress bar doesn't move past the left edge of the notepad window in about an hour, it's probably stuck.\nYou should restore Windows 7 to earliest restore point as possible, when there was nothing installed, so I was sure that there would be no conflicts between the different tools (Java, Android API, etc.)\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "how to use  binary CSS file when deploying java applications?", "id": 443, "answers": [{"answer_id": 452, "document_id": 183, "question_id": 443, "text": "scene.getStylesheets().add(this.getClass().getResource\n        (\"mystyles.bss\").toExternalForm());", "answer_start": 325, "answer_category": null}], "is_impossible": false}, {"question": "How to Convert the style sheet using NetBeans IDE when  deploying java applications?", "id": 444, "answers": [{"answer_id": 453, "document_id": 183, "question_id": 444, "text": "by selecting Binary Encode JavaFX CSS Files in the Packaging category of Project Properties.\n", "answer_start": 700, "answer_category": null}], "is_impossible": false}, {"question": "What additional information can I provide when create the main application JAR file?", "id": 445, "answers": [{"answer_id": 454, "document_id": 183, "question_id": 445, "text": "Platform requirements\n\nRequired version of the JRE\n\nRequired Java VM arguments\n\nThe following details about the application:\n\nName of the main application class (Required)\n\nName of the JavaFX preloader class, if a preloader is provided\n\nDetails about application resources, if applicable\n\nSet of class files and other resources included in the JAR file\n\nList of auxiliary JAR files needed by the application, including a custom JavaFX preloader JAR file, if needed", "answer_start": 975, "answer_category": null}], "is_impossible": false}, {"question": "How to create the main application JAR file  using NetBeans IDE?", "id": 446, "answers": [{"answer_id": 455, "document_id": 183, "question_id": 446, "text": "Automatically creates the JAR file when you specify this information in the project's properties.", "answer_start": 2399, "answer_category": null}], "is_impossible": false}], "context": "5.4 Style Sheet Conversion\nConverting style sheets to binary format is optional, but improves application performance. This is especially noticeable on larger CSS files.\n\nTo use a binary CSS file, refer to the binary file instead of the CSS file in your code, as shown in Example 5-1:\n\nExample 5-1 Using a Binary Stylesheet\n\nscene.getStylesheets().add(this.getClass().getResource\n        (\"mystyles.bss\").toExternalForm());\nThe following methods are available for converting style sheets to binary:\n\nAnt task: See Convert CSS Files to Binary\n\nJava Packager tool: See the -createbss command in the javapackager reference for Windows or Solaris, Linux, and OS X.\n\nNetBeans IDE: Convert the style sheet by selecting Binary Encode JavaFX CSS Files in the Packaging category of Project Properties.\n\n5.5 Create the Main Application JAR File\nIn addition to application classes and resources, you can provide the following information when you create the main application JAR file:\n\nPlatform requirements\n\nRequired version of the JRE\n\nRequired Java VM arguments\n\nThe following details about the application:\n\nName of the main application class (Required)\n\nName of the JavaFX preloader class, if a preloader is provided\n\nDetails about application resources, if applicable\n\nSet of class files and other resources included in the JAR file\n\nList of auxiliary JAR files needed by the application, including a custom JavaFX preloader JAR file, if needed\n\nThe use of Java packaging tools to create the main application JAR file is very important for packaging JAR files and self-contained applications that can be started with a double-click. The main application JAR file includes a launcher program that takes care of the bootstrap launch, which improves launch by performing the following tasks:\n\nChecks for the JRE\n\nGuides the user through any necessary installations\n\nSets the system proxy for your application\n\n\nNote:\n\nIf you have a JavaFX preloader, as shown in Figure 5-2, create a separate JAR file with the preloader resources, using any of the packaging methods described. For more information, see Chapter 13, \"Preloaders for JavaFX Applications.\"\nThe following methods are available for creating the main application JAR file:\n\nAnt task: See <fx:jar> Usage Examples.\n\nJava Packager tool: See the -createjar command in the javapackager reference for Windows or Solaris, Linux, and OS X.\n\nNetBeans IDE: Automatically creates the JAR file when you specify this information in the project's properties.", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to install Android SDK on Ubuntu?", "id": 1569, "answers": [{"answer_id": 1558, "document_id": 1146, "question_id": 1569, "text": "All you really needed to do is:\nsudo apt update && sudo apt install android-sdk", "answer_start": 327, "answer_category": null}], "is_impossible": false}], "context": "For my Ubuntu machine, I downloaded the latest version of Android SDK from this page.\nAfter extracting the downloaded .tgz file, I was trying to search for installation instructions and found.\nWhat exactly are we supposed to do?\nThere is no need to download any binaries or files or follow difficult installation instructions.\nAll you really needed to do is:\nsudo apt update && sudo apt install android-sdk\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How can I install a CPAN module into a local directory?", "id": 845, "answers": [{"answer_id": 840, "document_id": 524, "question_id": 845, "text": "local::lib - create and use a local lib/ for perl modules with PERL5LIB\n", "answer_start": 1735, "answer_category": null}], "is_impossible": false}], "context": " local::lib\nOCT 17, 2021\nDistribution: local-lib\nModule version: 2.000028\nSource (raw)\nBrowse (raw)\nChanges\nHow to Contribute\nRepository (git clone)\nIssues (5)\nTesters (751 / 3 / 0)\nKwalitee\nBus factor: 12\n55.16% Coverage\nLicense: perl_5\nPerl: v5.6.0\nQuestions? Chat with us!\nACTIVITY\n\n24 month\nTOOLS\nDownload (62.17KB)\nMetaCPAN Explorer\nPermissions\nSubscribe to distribution\nSearch distribution\ngrep distribution\n\nJump to version\n\nDiff with version\nPERMALINKS\nThis version\nLatest version\nHide Right Panel\n++ed by:\nGORTAN BINGOS DBOEHMER BEROV IONCACHE\n49 PAUSE users\n35 non-PAUSE users\nAuthor image\nHAARG\nGraham Knop\nand 1 contributors\nshow them\nDEPENDENCIES\nCPAN\nExtUtils::Install\nExtUtils::MakeMaker\nModule::Build\nand possibly others\nReverse dependencies\nCPAN Testers List\nDependency graph\nContents [ hide ]\nNAME\nSYNOPSIS\nThe bootstrapping technique\nBootstrapping into an alternate directory\nOther bootstrapping options\nDifferences when using this module under Win32\nPowerShell\nRATIONALE\nDESCRIPTION\nCREATING A SELF-CONTAINED SET OF MODULES\nIMPORT OPTIONS\n--deactivate\n--deactivate-all\n--quiet\n--always\n--shelltype\n--no-create\nCLASS METHODS\nensure_dir_structure_for\nprint_environment_vars_for\nbuild_environment_vars_for\nsetup_env_hash_for\nactive_paths\ninstall_base_perl_path\nlib_paths_for\ninstall_base_bin_path\ninstaller_options_for\nresolve_empty_path\nresolve_home_path\nresolve_relative_path\nresolve_path\nOBJECT INTERFACE\nnew\nATTRIBUTES\nroots\ninc\nlibs\nbins\nextra\nno_create\nOBJECT METHODS\nclone\nactivate\ndeactivate\ndeactivate_all\nenvironment_vars_string\nbuild_environment_vars\nsetup_env_hash\nsetup_local_lib\nA WARNING ABOUT UNINST=1\nLIMITATIONS\nTROUBLESHOOTING\nENVIRONMENT\nSEE ALSO\nSUPPORT\nAUTHOR\nCONTRIBUTORS\nCOPYRIGHT\nLICENSE\nNAME\nlocal::lib - create and use a local lib/ for perl modules with PERL5LIB\n\nSYNOPSIS\nIn code -\n\nuse local::lib; # sets up a local lib at ~/perl5\n \nuse local::lib '~/foo'; # same, but ~/foo\n \n# Or...\nuse FindBin;\nuse local::lib \"$FindBin::Bin/../support\";  # app-local support library\nFrom the shell -\n\n# Install LWP and its missing dependencies to the '~/perl5' directory\nperl -MCPAN -Mlocal::lib -e 'CPAN::install(LWP)'\n \n# Just print out useful shell commands\n$ perl -Mlocal::lib\nPERL_MB_OPT='--install_base /home/username/perl5'; export PERL_MB_OPT;\nPERL_MM_OPT='INSTALL_BASE=/home/username/perl5'; export PERL_MM_OPT;\nPERL5LIB=\"/home/username/perl5/lib/perl5\"; export PERL5LIB;\nPATH=\"/home/username/perl5/bin:$PATH\"; export PATH;\nPERL_LOCAL_LIB_ROOT=\"/home/usename/perl5:$PERL_LOCAL_LIB_ROOT\"; export PERL_LOCAL_LIB_ROOT;\nFrom a .bash_profile or .bashrc file -\n\neval \"$(perl -I$HOME/perl5/lib/perl5 -Mlocal::lib)\"\nThe bootstrapping technique\nA typical way to install local::lib is using what is known as the \"bootstrapping\" technique. You would do this if your system administrator hasn't already installed local::lib. In this case, you'll need to install local::lib in your home directory.\n\nEven if you do have administrative privileges, you will still want to set up your environment variables, as discussed in step 4. Without this, you would still install the modules into the system CPAN installation and also your Perl scripts will not use the lib/ path you bootstrapped with local::lib.\n\nBy default local::lib installs itself and the CPAN modules into ~/perl5.\n\nWindows users must also see \"Differences when using this module under Win32\".\n\nDownload and unpack the local::lib tarball from CPAN (search for \"Download\" on the CPAN page about local::lib). Do this as an ordinary user, not as root or administrator. Unpack the file in your home directory or in any other convenient location.\n\nRun this:\n\nperl Makefile.PL --bootstrap\nIf the system asks you whether it should automatically configure as much as possible, you would typically answer yes.\n\nRun this: (local::lib assumes you have make installed on your system)\n\nmake test && make install\nNow we need to setup the appropriate environment variables, so that Perl starts using our newly generated lib/ directory. If you are using bash or any other Bourne shells, you can add this to your shell startup script this way:\n\necho 'eval \"$(perl -I$HOME/perl5/lib/perl5 -Mlocal::lib)\"' >>~/.bashrc\nIf you are using C shell, you can do this as follows:\n\n% echo $SHELL\n/bin/csh\n$ echo 'eval `perl -I$HOME/perl5/lib/perl5 -Mlocal::lib`' >> ~/.cshrc\nAfter writing your shell configuration file, be sure to re-read it to get the changed settings into your current shell's environment. Bourne shells use . ~/.bashrc for this, whereas C shells use source ~/.cshrc.\n\nBootstrapping into an alternate directory\nIn order to install local::lib into a directory other than the default, you need to specify the name of the directory when you call bootstrap. Then, when setting up the environment variables, both perl and local::lib must be told the location of the bootstrap directory. The setup process would look as follows:\n\nperl Makefile.PL --bootstrap=~/foo\nmake test && make install\necho 'eval \"$(perl -I$HOME/foo/lib/perl5 -Mlocal::lib=$HOME/foo)\"' >>~/.bashrc\n. ~/.bashrc\nOther bootstrapping options\nIf you're on a slower machine, or are operating under draconian disk space limitations, you can disable the automatic generation of manpages from POD when installing modules by using the --no-manpages argument when bootstrapping:\n\nperl Makefile.PL --bootstrap --no-manpages\nTo avoid doing several bootstrap for several Perl module environments on the same account, for example if you use it for several different deployed applications independently, you can use one bootstrapped local::lib installation to install modules in different directories directly this way:\n\ncd ~/mydir1\nperl -Mlocal::lib=./\neval $(perl -Mlocal::lib=./)  ### To set the environment for this shell alone\nprintenv                      ### You will see that ~/mydir1 is in the PERL5LIB\nperl -MCPAN -e install ...    ### whatever modules you want\ncd ../mydir2\n... REPEAT ...\nIf you use .bashrc to activate a local::lib automatically, the local::lib will be re-enabled in any sub-shells used, overriding adjustments you may have made in the parent shell. To avoid this, you can initialize the local::lib in .bash_profile rather than .bashrc, or protect the local::lib invocation with a $SHLVL check:\n\n[ $SHLVL -eq 1 ] && eval \"$(perl -I$HOME/perl5/lib/perl5 -Mlocal::lib)\"\nIf you are working with several local::lib environments, you may want to remove some of them from the current environment without disturbing the others. You can deactivate one environment like this (using bourne sh):\n\neval $(perl -Mlocal::lib=--deactivate,~/path)\nwhich will generate and run the commands needed to remove ~/path from your various search paths. Whichever environment was activated most recently will remain the target for module installations. That is, if you activate ~/path_A and then you activate ~/path_B, new modules you install will go in ~/path_B. If you deactivate ~/path_B then modules will be installed into ~/pathA -- but if you deactivate ~/path_A then they will still be installed in ~/pathB because pathB was activated later.\n\nYou can also ask local::lib to clean itself completely out of the current shell's environment with the --deactivate-all option. For multiple environments for multiple apps you may need to include a modified version of the use FindBin instructions in the \"In code\" sample above. If you did something like the above, you have a set of Perl modules at ~/mydir1/lib. If you have a script at ~/mydir1/scripts/myscript.pl, you need to tell it where to find the modules you installed for it at ~/mydir1/lib.\n\nIn ~/mydir1/scripts/myscript.pl:\n\nuse strict;\nuse warnings;\nuse local::lib \"$FindBin::Bin/..\";  ### points to ~/mydir1 and local::lib finds lib\nuse lib \"$FindBin::Bin/../lib\";     ### points to ~/mydir1/lib\nPut this before any BEGIN { ... } blocks that require the modules you installed.\n\nDifferences when using this module under Win32\nTo set up the proper environment variables for your current session of CMD.exe, you can use this:\n\nC:\\>perl -Mlocal::lib\nset PERL_MB_OPT=--install_base C:\\DOCUME~1\\ADMINI~1\\perl5\nset PERL_MM_OPT=INSTALL_BASE=C:\\DOCUME~1\\ADMINI~1\\perl5\nset PERL5LIB=C:\\DOCUME~1\\ADMINI~1\\perl5\\lib\\perl5\nset PATH=C:\\DOCUME~1\\ADMINI~1\\perl5\\bin;%PATH%\n \n### To set the environment for this shell alone\nC:\\>perl -Mlocal::lib > %TEMP%\\tmp.bat && %TEMP%\\tmp.bat && del %TEMP%\\tmp.bat\n### instead of $(perl -Mlocal::lib=./)\nIf you want the environment entries to persist, you'll need to add them to the Control Panel's System applet yourself or use App::local::lib::Win32Helper.\n\nThe \"~\" is translated to the user's profile directory (the directory named for the user under \"Documents and Settings\" (Windows XP or earlier) or \"Users\" (Windows Vista or later)) unless $ENV{HOME} exists. After that, the home directory is translated to a short name (which means the directory must exist) and the subdirectories are created.\n\nPowerShell\nlocal::lib also supports PowerShell, and can be used with the Invoke-Expression cmdlet.\n\nInvoke-Expression \"$(perl -Mlocal::lib)\"\nRATIONALE\nThe version of a Perl package on your machine is not always the version you need. Obviously, the best thing to do would be to update to the version you need. However, you might be in a situation where you're prevented from doing this. Perhaps you don't have system administrator privileges; or perhaps you are using a package management system such as Debian, and nobody has yet gotten around to packaging up the version you need.\n\nlocal::lib solves this problem by allowing you to create your own directory of Perl packages downloaded from CPAN (in a multi-user system, this would typically be within your own home directory). The existing system Perl installation is not affected; you simply invoke Perl with special options so that Perl uses the packages in your own local package directory rather than the system packages. local::lib arranges things so that your locally installed version of the Perl packages takes precedence over the system installation.\n\nIf you are using a package management system (such as Debian), you don't need to worry about Debian and CPAN stepping on each other's toes. Your local version of the packages will be written to an entirely separate directory from those installed by Debian.\n\nDESCRIPTION\nThis module provides a quick, convenient way of bootstrapping a user-local Perl module library located within the user's home directory. It also constructs and prints out for the user the list of environment variables using the syntax appropriate for the user's current shell (as specified by the SHELL environment variable), suitable for directly adding to one's shell configuration file.\n\nMore generally, local::lib allows for the bootstrapping and usage of a directory containing Perl modules outside of Perl's @INC. This makes it easier to ship an application with an app-specific copy of a Perl module, or collection of modules. Useful in cases like when an upstream maintainer hasn't applied a patch to a module of theirs that you need for your application.\n\nOn import, local::lib sets the following environment variables to appropriate values:\n\nPERL_MB_OPT\nPERL_MM_OPT\nPERL5LIB\nPATH\nPERL_LOCAL_LIB_ROOT\nWhen possible, these will be appended to instead of overwritten entirely.\n\nThese values are then available for reference by any code after import.\n\nCREATING A SELF-CONTAINED SET OF MODULES\nSee lib::core::only for one way to do this - but note that there are a number of caveats, and the best approach is always to perform a build against a clean perl (i.e. site and vendor as close to empty as possible).\n\nIMPORT OPTIONS\nOptions are values that can be passed to the local::lib import besides the directory to use. They are specified as use local::lib '--option'[, path]; or perl -Mlocal::lib=--option[,path].\n\n--deactivate\nRemove the chosen path (or the default path) from the module search paths if it was added by local::lib, instead of adding it.\n\n--deactivate-all\nRemove all directories that were added to search paths by local::lib from the search paths.\n\n--quiet\nDon't output any messages about directories being created.\n\n--always\nAlways add directories to environment variables, ignoring if they are already included.\n\n--shelltype\nSpecify the shell type to use for output. By default, the shell will be detected based on the environment. Should be one of: bourne, csh, cmd, or powershell.\n\n--no-create\nPrevents local::lib from creating directories when activating dirs. This is likely to cause issues on Win32 systems.\n\nCLASS METHODS\nensure_dir_structure_for\nArguments: $path\nReturn value: None\nAttempts to create a local::lib directory, including subdirectories and all required parent directories. Throws an exception on failure.\n\nprint_environment_vars_for\nArguments: $path\nReturn value: None\nPrints to standard output the variables listed above, properly set to use the given path as the base directory.\n\nbuild_environment_vars_for\nArguments: $path\nReturn value: %environment_vars\nReturns a hash with the variables listed above, properly set to use the given path as the base directory.\n\nsetup_env_hash_for\nArguments: $path\nReturn value: None\nConstructs the %ENV keys for the given path, by calling \"build_environment_vars_for\".\n\nactive_paths\nArguments: None\nReturn value: @paths\nReturns a list of active local::lib paths, according to the PERL_LOCAL_LIB_ROOT environment variable and verified against what is really in @INC.\n\ninstall_base_perl_path\nArguments: $path\nReturn value: $install_base_perl_path\nReturns a path describing where to install the Perl modules for this local library installation. Appends the directories lib and perl5 to the given path.\n\nlib_paths_for\nArguments: $path\nReturn value: @lib_paths\nReturns the list of paths perl will search for libraries, given a base path. This includes the base path itself, the architecture specific subdirectory, and perl version specific subdirectories. These paths may not all exist.\n\ninstall_base_bin_path\nArguments: $path\nReturn value: $install_base_bin_path\nReturns a path describing where to install the executable programs for this local library installation. Appends the directory bin to the given path.\n\ninstaller_options_for\nArguments: $path\nReturn value: %installer_env_vars\nReturns a hash of environment variables that should be set to cause installation into the given path.\n\nresolve_empty_path\nArguments: $path\nReturn value: $base_path\nBuilds and returns the base path into which to set up the local module installation. Defaults to ~/perl5.\n\nresolve_home_path\nArguments: $path\nReturn value: $home_path\nAttempts to find the user's home directory. If no definite answer is available, throws an exception.\n\nresolve_relative_path\nArguments: $path\nReturn value: $absolute_path\nTranslates the given path into an absolute path.\n\nresolve_path\nArguments: $path\nReturn value: $absolute_path\nCalls the following in a pipeline, passing the result from the previous to the next, in an attempt to find where to configure the environment for a local library installation: \"resolve_empty_path\", \"resolve_home_path\", \"resolve_relative_path\". Passes the given path argument to \"resolve_empty_path\" which then returns a result that is passed to \"resolve_home_path\", which then has its result passed to \"resolve_relative_path\". The result of this final call is returned from \"resolve_path\".\n\nOBJECT INTERFACE\nnew\nArguments: %attributes\nReturn value: $local_lib\nConstructs a new local::lib object, representing the current state of @INC and the relevant environment variables.\n\nATTRIBUTES\nroots\nAn arrayref representing active local::lib directories.\n\ninc\nAn arrayref representing @INC.\n\nlibs\nAn arrayref representing the PERL5LIB environment variable.\n\nbins\nAn arrayref representing the PATH environment variable.\n\nextra\nA hashref of extra environment variables (e.g. PERL_MM_OPT and PERL_MB_OPT)\n\nno_create\nIf set, local::lib will not try to create directories when activating them.\n\nOBJECT METHODS\nclone\nArguments: %attributes\nReturn value: $local_lib\nConstructs a new local::lib object based on the existing one, overriding the specified attributes.\n\nactivate\nArguments: $path\nReturn value: $new_local_lib\nConstructs a new instance with the specified path active.\n\ndeactivate\nArguments: $path\nReturn value: $new_local_lib\nConstructs a new instance with the specified path deactivated.\n\ndeactivate_all\nArguments: None\nReturn value: $new_local_lib\nConstructs a new instance with all local::lib directories deactivated.\n\nenvironment_vars_string\nArguments: [ $shelltype ]\nReturn value: $shell_env_string\nReturns a string to set up the local::lib, meant to be run by a shell.\n\nbuild_environment_vars\nArguments: None\nReturn value: %environment_vars\nReturns a hash with the variables listed above, properly set to use the given path as the base directory.\n\nsetup_env_hash\nArguments: None\nReturn value: None\nConstructs the %ENV keys for the given path, by calling \"build_environment_vars\".\n\nsetup_local_lib\nConstructs the %ENV hash using \"setup_env_hash\", and set up @INC.\n\nA WARNING ABOUT UNINST=1\nBe careful about using local::lib in combination with \"make install UNINST=1\". The idea of this feature is that will uninstall an old version of a module before installing a new one. However it lacks a safety check that the old version and the new version will go in the same directory. Used in combination with local::lib, you can potentially delete a globally accessible version of a module while installing the new version in a local place. Only combine \"make install UNINST=1\" and local::lib if you understand these possible consequences.\n\nLIMITATIONS\nDirectory names with spaces in them are not well supported by the perl toolchain and the programs it uses. Pure-perl distributions should support spaces, but problems are more likely with dists that require compilation. A workaround you can do is moving your local::lib to a directory with spaces after you installed all modules inside your local::lib bootstrap. But be aware that you can't update or install CPAN modules after the move.\n\nRather basic shell detection. Right now anything with csh in its name is assumed to be a C shell or something compatible, and everything else is assumed to be Bourne, except on Win32 systems. If the SHELL environment variable is not set, a Bourne-compatible shell is assumed.\n\nKills any existing PERL_MM_OPT or PERL_MB_OPT.\n\nShould probably auto-fixup CPAN config if not already done.\n\nOn VMS and MacOS Classic (pre-OS X), local::lib loads File::Spec. This means any File::Spec version installed in the local::lib will be ignored by scripts using local::lib. A workaround for this is using use lib \"$local_lib/lib/perl5\"; instead of using local::lib directly.\n\nConflicts with ExtUtils::MakeMaker's PREFIX option. local::lib uses the INSTALL_BASE option, as it has more predictable and sane behavior. If something attempts to use the PREFIX option when running a Makefile.PL, ExtUtils::MakeMaker will refuse to run, as the two options conflict. This can be worked around by temporarily unsetting the PERL_MM_OPT environment variable.\n\nConflicts with Module::Build's --prefix option. Similar to the previous limitation, but any --prefix option specified will be ignored. This can be worked around by temporarily unsetting the PERL_MB_OPT environment variable.\n\nPatches very much welcome for any of the above.\n\nOn Win32 systems, does not have a way to write the created environment variables to the registry, so that they can persist through a reboot.\n\nTROUBLESHOOTING\nIf you've configured local::lib to install CPAN modules somewhere in to your home directory, and at some point later you try to install a module with cpan -i Foo::Bar, but it fails with an error like: Warning: You do not have permissions to install into /usr/lib64/perl5/site_perl/5.8.8/x86_64-linux at /usr/lib64/perl5/5.8.8/Foo/Bar.pm and buried within the install log is an error saying 'INSTALL_BASE' is not a known MakeMaker parameter name, then you've somehow lost your updated ExtUtils::MakeMaker module.\n\nTo remedy this situation, rerun the bootstrapping procedure documented above.\n\nThen, run rm -r ~/.cpan/build/Foo-Bar*\n\nFinally, re-run cpan -i Foo::Bar and it should install without problems.\n\nENVIRONMENT\nSHELL\nCOMSPEC\nlocal::lib looks at the user's SHELL environment variable when printing out commands to add to the shell configuration file.\n\nOn Win32 systems, COMSPEC is also examined.\n\nSEE ALSO\nPerl Advent article, 2011\n\nSUPPORT\nIRC:\n\nJoin #toolchain on irc.perl.org.\nAUTHOR\nMatt S Trout <mst@shadowcat.co.uk> http://www.shadowcat.co.uk/\n\nauto_install fixes kindly sponsored by http://www.takkle.com/\n\nCONTRIBUTORS\nPatches to correctly output commands for csh style shells, as well as some documentation additions, contributed by Christopher Nehren <apeiron@cpan.org>.\n\nDoc patches for a custom local::lib directory, more cleanups in the english documentation and a german documentation contributed by Torsten Raudssus <torsten@raudssus.de>.\n\nHans Dieter Pearcey <hdp@cpan.org> sent in some additional tests for ensuring things will install properly, submitted a fix for the bug causing problems with writing Makefiles during bootstrapping, contributed an example program, and submitted yet another fix to ensure that local::lib can install and bootstrap properly. Many, many thanks!\n\npattern of Freenode IRC contributed the beginnings of the Troubleshooting section. Many thanks!\n\nPatch to add Win32 support contributed by Curtis Jewell <csjewell@cpan.org>.\n\nWarnings for missing PATH/PERL5LIB (as when not running interactively) silenced by a patch from Marco Emilio Poleggi.\n\nMark Stosberg <mark@summersault.com> provided the code for the now deleted '--self-contained' option.\n\nDocumentation patches to make win32 usage clearer by David Mertens <dcmertens.perl@gmail.com> (run4flat).\n\nBrazilian portuguese translation and minor doc patches contributed by Breno G. de Oliveira <garu@cpan.org>.\n\nImprovements to stacking multiple local::lib dirs and removing them from the environment later on contributed by Andrew Rodland <arodland@cpan.org>.\n\nPatch for Carp version mismatch contributed by Hakim Cassimally <osfameron@cpan.org>.\n\nRewrite of internals and numerous bug fixes and added features contributed by Graham Knop <haarg@haarg.org>.\n\nCOPYRIGHT\nCopyright (c) 2007 - 2013 the local::lib \"AUTHOR\" and \"CONTRIBUTORS\" as listed above.\n\nLICENSE\nThis is free software; you can redistribute it and/or modify it under the same terms as the Perl 5 programming language system itself.", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Create Setup/MSI installer in Visual Studio 2017", "id": 727, "answers": [{"answer_id": 730, "document_id": 417, "question_id": 727, "text": "You need to install this extension to Visual Studio 2017/2019 in order to get access to the Installer Projects", "answer_start": 337, "answer_category": null}], "is_impossible": false}], "context": "I have written an outlook add-in VSTO in Visual Studio Pro 2017 (VB.NET). I have published it which creates a setup.exe which is OK but I would like to create a proper installer that copies the files locally and can be run silently etc.\nHow do I go about doing this? When I go to create new project there is no installer project option.\nYou need to install this extension to Visual Studio 2017/2019 in order to get access to the Installer Projects. This extension provides the same functionality that currently exists in Visual Studio 2015 for Visual Studio Installer projects. To use this extension, you can either open the Extensions and Updates dialog, select the online node, and search for \"Visual Studio Installer Projects Extension,\" or you can download directly from this page.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Parser Error when deploy ASP.NET application", "id": 559, "answers": [{"answer_id": 562, "document_id": 284, "question_id": 559, "text": "Try changing CodeBehind=\"Default.aspx.cs\" to CodeFile=\"Default.aspx.cs\"", "answer_start": 325, "answer_category": null}], "is_impossible": false}], "context": "I've finished simple asp.net web application project, compiled it, and try to test on local IIS. I've create virtual directory, map it with physical directory, then put all necessary files there, including bin folder with all .dll's In the project settings, build section, output path is bin\\ So when i try to browse my app. Try changing CodeBehind=\"Default.aspx.cs\" to CodeFile=\"Default.aspx.cs\"", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "MongoDB 3.6.2 2008R2 Plus Not Installing", "id": 1174, "answers": [{"answer_id": 1167, "document_id": 751, "question_id": 1174, "text": "declining to install Compass together with Mongo in the installer wizard.", "answer_start": 382, "answer_category": null}], "is_impossible": false}], "context": "I'm trying to install MongoDb 3.6.2 2008R2 plus on my 64bit Window 10 (build no 1709) but the setup after some time says \"setup wizard ended prematurely because of an error.your system has not been modified.To. Install this program at a later time run setup wizard again.click finish button to exit the setup.\" Any one know any fix regarding this issue? 86\nI solved this problem by declining to install Compass together with Mongo in the installer wizard.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Is there a way to deploy into a vagrant VM using Capistrano?", "id": 1039, "answers": [{"answer_id": 1034, "document_id": 620, "question_id": 1039, "text": "You can also feed Vagrant's SSH options to Capistrano (most of the :ssh_options go directly to Net::SSH, http://net-ssh.github.com/ssh/v1/chapter-2.html, see \"Options\") so there is no need to mess your real ~/.ssh/config", "answer_start": 176, "answer_category": null}], "is_impossible": false}], "context": "I'd like to setup a vagrant instance outside of my project directory. Is there a way to deploy rails into the vagrant VM with capistrano as I would to my real production host?\nYou can also feed Vagrant's SSH options to Capistrano (most of the :ssh_options go directly to Net::SSH, http://net-ssh.github.com/ssh/v1/chapter-2.html, see \"Options\") so there is no need to mess your real ~/.ssh/config\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Passing parameters to Capistrano", "id": 561, "answers": [{"answer_id": 564, "document_id": 286, "question_id": 561, "text": "To pass in parameters, you can set the -s flag when running cap to give you a key value pair.", "answer_start": 588, "answer_category": null}], "is_impossible": false}], "context": "I'm looking into the possibility of using Capistrano as a generic deploy solution. By \"generic\", I mean not-rails. I'm not happy with the quality of the documentation I'm finding, though, granted, I'm not looking at the ones that presume you are deploying rails. So I'll just try to hack up something based on a few examples, but there are a couple of problems I'm facing right from the start.\nMy problem is that cap deploy doesn't have enough information to do anything. Importantly, it is missing the tag for the version I want to deploy, and this has to be passed on the command line.\nTo pass in parameters, you can set the -s flag when running cap to give you a key value pair.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "scipy.misc module has no attribute imread?", "id": 1612, "answers": [{"answer_id": 1599, "document_id": 1186, "question_id": 1612, "text": "You need to install Pillow (formerly PIL). From the docs on scipy.misc.", "answer_start": 129, "answer_category": null}], "is_impossible": false}], "context": "I am trying to read an image with scipy. However it does not accept the scipy.misc.imread part. What could be the cause of this?\nYou need to install Pillow (formerly PIL). From the docs on scipy.misc.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "MSBuild ReadLinesFromFile all text on one line", "id": 1318, "answers": [{"answer_id": 1308, "document_id": 887, "question_id": 1318, "text": "The problem here is you are using the ReadLinesFromFile task in a manner it wasn't intended.", "answer_start": 174, "answer_category": null}], "is_impossible": false}], "context": "When I do a ReadLinesFromFile on a file in MSBUILD and go to output that file again, I get all the text on one line. All the Carriage returns and LineFeeds are stripped out.\nThe problem here is you are using the ReadLinesFromFile task in a manner it wasn't intended.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Installing WindowBuilder on Eclipse 4.2", "id": 1785, "answers": [{"answer_id": 1771, "document_id": 1357, "question_id": 1785, "text": "You should do the following steps in Eclipse:\n1. Goto - Menu > Help > Install New Software...\n2.Select - Work With: Juno - http://download.eclipse.org/releases/juno\n3.The WindowBuilder items are under \"General Purpose Tools\" (or use the filter)", "answer_start": 352, "answer_category": null}], "is_impossible": false}], "context": "I'm using Eclipse Juno 4.2, downloaded from here.\nOn previous installs, I've been using 3.7, and I've been using WindowBuilder, which I find very useful. I noticed it wasn't included this time, so I used this update site provided on this page (the zip file download gives a \"file unavailable\" error).\nDoes anyone know how I can go about installing it?\nYou should do the following steps in Eclipse:\n1. Goto - Menu > Help > Install New Software...\n2.Select - Work With: Juno - http://download.eclipse.org/releases/juno\n3.The WindowBuilder items are under \"General Purpose Tools\" (or use the filter)\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Run command on the Ansible host", "id": 1649, "answers": [{"answer_id": 1637, "document_id": 1223, "question_id": 1649, "text": "I'd like to share that Ansible can be run on localhost via shell:\nansible all -i \"localhost,\" -c local -m shell -a 'echo hello world'", "answer_start": 0, "answer_category": null}], "is_impossible": false}], "context": "I'd like to share that Ansible can be run on localhost via shell:\nansible all -i \"localhost,\" -c local -m shell -a 'echo hello world'\nThis could be helpful for simple tasks or for some hands-on learning of Ansible.\nThe example of code is taken from this good article:\nRunning ansible playbook in localhost\nIs it possible to run commands on the Ansible host?\nMy scenario is that I want to take a checkout from a git server that is hosted internally (and isn't accessible outside the company firewall). Then I want to upload the checkout (tarballed) to the production server (hosted externally).\nAt the moment, I'm looking at running a script that does the checkout, tarballs it, and then runs the deployment script - but if I could integrate this into Ansible that would be preferable.\n\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to change Android application name in Xamarin.Forms?", "id": 481, "answers": [{"answer_id": 488, "document_id": 212, "question_id": 481, "text": "You may also need to update the reference to launcher_foreground in icon.xml and icon_round.xml in the mipmap-anydpi folder.", "answer_start": 285, "answer_category": null}], "is_impossible": false}], "context": "I replaced all the images everywhere (by this I mean in drawable folders and all Windows Assets folders and iOS Resources folder), but it still shows me the default Xamarin icon for the app. I tried this code, too, but it doesn't seem to work either. Can someone tell me the solution?\nYou may also need to update the reference to launcher_foreground in icon.xml and icon_round.xml in the mipmap-anydpi folder.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Tensorflow installation error: not a supported wheel on this platform", "id": 715, "answers": [{"answer_id": 718, "document_id": 405, "question_id": 715, "text": "Turns out you have to have python 3.5.2. Not 2.7, not 3.6.x-- nothing other than 3.5.2.", "answer_start": 255, "answer_category": null}], "is_impossible": false}], "context": "when I try to install tensorflow by cloning from git, I run into the error \"no module named copyreg,\" so I tried installing using a virtualenv. However, I then run into this error. I was trying to do the windows-based install and kept getting this error.\nTurns out you have to have python 3.5.2. Not 2.7, not 3.6.x-- nothing other than 3.5.2.\nAfter installing python 3.5.2 the pip install worked.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "What will happen if set false?", "id": 200680, "answers": [{"answer_id": 239807, "document_id": 357789, "question_id": 200680, "text": "the meta layer\u2019s feature is the bounding box of the tile. Defaults\nto false.", "answer_start": 4134, "answer_category": null}], "is_impossible": false}, {"question": "What will happen if exact_bounds set true?", "id": 200681, "answers": [{"answer_id": 239809, "document_id": 357789, "question_id": 200681, "text": "the meta layer\u2019s feature is a bounding box resulting from a\ngeo_bounds aggregation.", "answer_start": 4221, "answer_category": null}], "is_impossible": false}, {"question": "What is the default numer of maximum number of features to return in the hits layer.?", "id": 200682, "answers": [{"answer_id": 239816, "document_id": 357789, "question_id": 200682, "text": "10000", "answer_start": 5572, "answer_category": null}], "is_impossible": false}, {"question": "How grid_precision determines the geotile_grid?", "id": 200683, "answers": [{"answer_id": 239825, "document_id": 357789, "question_id": 200683, "text": "(2^grid_precision) x (2^grid_precision)", "answer_start": 4946, "answer_category": null}], "is_impossible": false}, {"question": "What is grid_type?", "id": 200684, "answers": [{"answer_id": 239827, "document_id": 357789, "question_id": 200684, "text": "Determines the geometry type for features in the aggs\nlayer. ", "answer_start": 5169, "answer_category": null}], "is_impossible": false}], "context": "\n\nVector tile search API\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDocs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nElasticsearch Guide [7.15]\n\u00bb\nREST APIs\n\u00bb\nSearch APIs\n\u00bb\nVector tile search API\n\n\n\n\u00ab Ranking evaluation API\n\n\nSearchable snapshots APIs \u00bb\n\n\n\n\nVector tile search API\n\n\n\n\nThis functionality is experimental and may be changed or removed completely in a future release. Elastic will take a best effort approach to fix any issues, but experimental features are not subject to the support SLA of official GA features.\n\n\nSearches a vector tile for geospatial values. Returns results as a binary\nMapbox vector tile.\n\nGET my-index/_mvt/my-geo-field/15/5271/12710\n\n\n\n\nRequest\n\nGET <target>/_mvt/<field>/<zoom>/<x>/<y>\nPOST <target>/_mvt/<field>/<zoom>/<x>/<y>\n\n\n\nPrerequisites\n\n\n\n\nBefore using this API, you should be familiar with the\nMapbox vector tile specification.\n\n\nIf the Elasticsearch security features are enabled, you must have the read\nindex privilege for the target data stream, index,\nor alias.\n\n\n\n\n\n\nPath parameters\n\n\n\n\n\n<target>\n\n\n\n(Required, string) Comma-separated list of data streams, indices, or aliases to\nsearch. Supports wildcards (*). To search all data streams and indices, omit\nthis parameter or use * or _all.\n\n\n\n<field>\n\n\n\n\n(Required, string) Field containing geospatial values to return. Must be a\ngeo_point or geo_shape field. The field must\nhave doc values enabled. Cannot be a nested field.\n\n\n\n\nVector tiles do not natively support geometry collections. For\ngeometrycollection values in a geo_shape field, the API returns a hits\nlayer feature for each element of the collection. This behavior may change may\nchange in a future release.\n\n\n\n\n\n<zoom>\n\n\n\n(Required, integer) Zoom level for the vector tile to search. Accepts 0-29.\n\n\n\n<x>\n\n\n\n(Required, integer) X coordinate for the vector tile to search.\n\n\n\n<y>\n\n\n\n(Required, integer) Y coordinate for the vector tile to search.\n\n\n\n\n\n\nDescription\n\nInternally, Elasticsearch translates a vector tile search API request into a\nsearch containing:\n\n\n\nA geo_bounding_box query on the\n<field>. The query uses the <zoom>/<x>/<y> tile as a bounding box.\n\n\nA geotile_grid\naggregation on the <field>. The aggregation uses the <zoom>/<x>/<y> tile as\na bounding box.\n\n\nOptionally, a\ngeo_bounds aggregation\non the <field>. The search only includes this aggregation if the\nexact_bounds parameter is true.\n\n\n\nFor example, Elasticsearch may translate a vector tile search API request with an\nexact_bounds argument of true into the following search:\n\nGET my-index/_search\n{\n\"size\": 10000,\n\"query\": {\n\"geo_bounding_box\": {\n\"my-geo-field\": {\n\"top_left\": {\n\"lat\": -40.979898069620134,\n\"lon\": -45\n},\n\"bottom_right\": {\n\"lat\": -66.51326044311186,\n\"lon\": 0\n}\n}\n}\n},\n\"aggregations\": {\n\"grid\": {\n\"geotile_grid\": {\n\"field\": \"my-geo-field\",\n\"precision\": 11,\n\"size\": 65536,\n\"bounds\": {\n\"top_left\": {\n\"lat\": -40.979898069620134,\n\"lon\": -45\n},\n\"bottom_right\": {\n\"lat\": -66.51326044311186,\n\"lon\": 0\n}\n}\n}\n},\n\"bounds\": {\n\"geo_bounds\": {\n\"field\": \"my-geo-field\",\n\"wrap_longitude\": false\n}\n}\n}\n}\n\n\nThe API returns results as a binary\nMapbox vector tile. Mapbox vector\ntiles are encoded as Google\nProtobufs (PBF). By default, the tile contains three layers:\n\n\n\nA hits layer containing a feature for each <field> value matching the\ngeo_bounding_box query.\n\n\nAn aggs layer containing a feature for each cell of the geotile_grid. You\ncan use these cells as tiles for lower zoom levels. The layer only contains\nfeatures for cells with matching data.\n\n\nA meta layer containing:\n\n\n\nA feature containing a bounding box. By default, this is the bounding box of\nthe tile.\n\n\nValue ranges for any sub-aggregations on the geotile_grid.\n\n\nMetadata for the search.\n\n\n\n\n\n\nThe API only returns features that can display at its zoom level. For example,\nif a polygon feature has no area at its zoom level, the API omits it.\nThe API returns errors as UTF-8 encoded JSON.\n\n\n\nQuery parameters\n\n\n\n\nYou can specify several options for this API as either a query\nparameter or request body parameter. If you specify both parameters, the query\nparameter takes precedence.\n\n\n\n\n\n\nexact_bounds\n\n\n\n\n(Optional, Boolean)\nIf false, the meta layer\u2019s feature is the bounding box of the tile. Defaults\nto false.\n\nIf true, the meta layer\u2019s feature is a bounding box resulting from a\ngeo_bounds aggregation.\nThe aggregation runs on <field> values that intersect the <zoom>/<x>/<y>\ntile with wrap_longitude set to false. The resulting bounding box may be\nlarger than the vector tile.\n\n\n\n\n\n\n\nextent\n\n\n\n(Optional, integer) Size, in pixels, of a side of the tile. Vector tiles are\nsquare with equal sides. Defaults to 4096.\n\n\n\n\n\n\n\ngrid_precision\n\n\n\n\n(Optional, integer) Additional zoom levels available through the aggs layer.\nFor example, if <zoom> is 7 and grid_precision is 8, you can zoom in up to\nlevel 15. Accepts 0-8. Defaults to 8. If 0, results don\u2019t include the\naggs layer.\n\nThis value determines the grid size of the geotile_grid as follows:\n(2^grid_precision) x (2^grid_precision)\nFor example, a value of 8 divides the tile into a grid of 256 x 256 cells. The\naggs layer only contains features for cells with matching data.\n\n\n\n\n\n\n\ngrid_type\n\n\n\n\n(Optional, string) Determines the geometry type for features in the aggs\nlayer. In the aggs layer, each feature represents a geotile_grid cell.\nAccepts:\n\n\n\n\n\ngrid (Default)\n\n\n\nEach feature is a Polygon of the cell\u2019s bounding box.\n\n\n\npoint\n\n\n\nEach feature is a Point that\u2019s the centroid of the cell.\n\n\n\n\n\n\n\n\n\n\nsize\n\n\n\n(Optional, integer) Maximum number of features to return in the hits layer.\nAccepts 0-10000. Defaults to 10000. If 0, results don\u2019t include the\nhits layer.\n\n\n\n\n\n\nRequest body\n\n\n\n\n\naggs\n\n\n\n\n(Optional, aggregation object)\nSub-aggregations for the geotile_grid. Supports the following\naggregation types:\n\n\n\n\navg\n\n\ncardinality\n\n\nmax\n\n\nmin\n\n\nsum\n\n\n\n\n\n\nexact_bounds\n\n\n\n\n(Optional, Boolean)\nIf false, the meta layer\u2019s feature is the bounding box of the tile. Defaults\nto false.\n\nIf true, the meta layer\u2019s feature is a bounding box resulting from a\ngeo_bounds aggregation.\nThe aggregation runs on <field> values that intersect the <zoom>/<x>/<y>\ntile with wrap_longitude set to false. The resulting bounding box may be\nlarger than the vector tile.\n\n\n\nextent\n\n\n\n(Optional, integer) Size, in pixels, of a side of the tile. Vector tiles are\nsquare with equal sides. Defaults to 4096.\n\n\n\nfields\n\n\n\n\n(Optional, array of strings and objects) Fields to return in the hits layer.\nSupports wildcards (*).\n\nThis parameter does not support fields with array values. Fields with\narray values may return inconsistent results.\nYou can specify fields in the array as a string or object.\n\nProperties of fields objects\n\n\n\n\n\nfield\n\n\n\n(Required, string) Field to return. Supports wildcards (*).\n\n\n\nformat\n\n\n\n\n(Optional, string)\nFormat for date and geospatial fields. Other field data types do not support\nthis parameter.\n\ndate and date_nanos fields accept a\ndate format. geo_point and\ngeo_shape fields accept:\n\n\n\n\ngeojson (default)\n\n\n\nGeoJSON\n\n\n\nwkt\n\n\n\nWell Known Text\n\n\n\nmvt(<zoom>/<x>/<y>@<extent>) or mvt(<zoom>/<x>/<y>)\n\n\n\n\n\n[experimental]\n\nThis functionality is experimental and may be changed or removed completely in a future release. Elastic will take a best effort approach to fix any issues, but experimental features are not subject to the support SLA of official GA features.\n\nBinary\nMapbox vector tile. The API\nreturns the tile as a base64-encoded string.\n\n\nmvt parameters\n\n\n\n\n\n<zoom>\n\n\n\n(Required, integer) Zoom level for the tile. Accepts 0-29.\n\n\n\n<x>\n\n\n\n(Required, integer) X coordinate for the tile.\n\n\n\n<y>\n\n\n\n(Required, integer) Y coordinate for the tile.\n\n\n\n<extent>\n\n\n\n(Optional, integer) Size, in pixels, of a side of the tile. Vector tiles are\nsquare with equal sides. Defaults to 4096.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngrid_precision\n\n\n\n\n(Optional, integer) Additional zoom levels available through the aggs layer.\nFor example, if <zoom> is 7 and grid_precision is 8, you can zoom in up to\nlevel 15. Accepts 0-8. Defaults to 8. If 0, results don\u2019t include the\naggs layer.\n\nThis value determines the grid size of the geotile_grid as follows:\n(2^grid_precision) x (2^grid_precision)\nFor example, a value of 8 divides the tile into a grid of 256 x 256 cells. The\naggs layer only contains features for cells with matching data.\n\n\n\ngrid_type\n\n\n\n\n(Optional, string) Determines the geometry type for features in the aggs\nlayer. In the aggs layer, each feature represents a geotile_grid cell.\nAccepts:\n\n\n\n\n\ngrid (Default)\n\n\n\nEach feature is a Polygon of the cell\u2019s bounding box.\n\n\n\npoint\n\n\n\nEach feature is a Point that\u2019s the centroid of the cell.\n\n\n\n\n\n\nquery\n\n\n\n(Optional, object) Query DSL used to filter documents for the\nsearch.\n\n\n\nruntime_mappings\n\n\n\n\n(Optional, object of objects)\nDefines one or more runtime fields in the search\nrequest. These fields take precedence over mapped fields with the same name.\n\n\nProperties of runtime_mappings objects\n\n\n\n\n\n<field-name>\n\n\n\n\n(Required, object)\nConfiguration for the runtime field. The key is the field name.\n\n\nProperties of <field-name>\n\n\n\n\n\ntype\n\n\n\n\n(Required, string)\nField type, which can be any of the following:\n\n\n\n\nboolean\n\n\ncomposite\n\n\ndate\n\n\ndouble\n\n\ngeo_point\n\n\nip\n\n\nkeyword\n\n\nlong\n\n\n\n\n\n\nscript\n\n\n\n\n(Optional, string)\nPainless script executed at query time. The\nscript has access to the entire context of a document, including the original\n_source and any mapped fields plus their values.\n\nThis script must include emit to return calculated values. For example:\n\n\"script\": \"emit(doc['@timestamp'].value.dayOfWeekEnum.toString())\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsize\n\n\n\n(Optional, integer) Maximum number of features to return in the hits layer.\nAccepts 0-10000. Defaults to 10000. If 0, results don\u2019t include the\nhits layer.\n\n\n\nsort\n\n\n\n\n(Optional, array of sort objects) Sorts features in the\nhits layer.\n\nBy default, the API calculates a bounding box for each feature. It sorts\nfeatures based on this box\u2019s diagonal length, from longest to shortest.\n\n\n\n\n\n\nResponse\n\nReturned vector tiles contain the following data:\n\n\n\n\nhits\n\n\n\n\n(object) Layer containing results for the geo_bounding_box query.\n\n\nProperties of hits\n\n\n\n\n\nextent\n\n\n\n(integer) Size, in pixels, of a side of the tile. Vector tiles are square with\nequal sides.\n\n\n\n\n\n\n\nversion\n\n\n\n(integer) Major version number of the\nMapbox vector tile specification.\n\n\n\nfeatures\n\n\n\n\n(array of objects) Array of features. Contains a feature for each <field>\nvalue that matches the geo_bounding_box query.\n\n\nProperties of features objects\n\n\n\n\n\ngeometry\n\n\n\n\n(object) Geometry for the feature.\n\n\nProperties of geometry\n\n\n\n\n\ntype\n\n\n\n\n(string) Geometry type for the feature. Valid values are:\n\n\n\n\nUNKNOWN\n\n\nPOINT\n\n\nLINESTRING\n\n\nPOLYGON\n\n\n\n\n\n\ncoordinates\n\n\n\n(array of integers or array of arrays) Tile coordinates for the feature.\n\n\n\n\n\n\n\n\nproperties\n\n\n\n\n(object) Properties for the feature.\n\n\nProperties of properties\n\n\n\n\n\n_id\n\n\n\n(string) Document _id for the feature\u2019s document.\n\n\n\n<field>\n\n\n\nField value. Only returned for fields in the fields parameter.\n\n\n\n\n\n\n\n\nid\n\n\n\n(integer) Unique ID for the feature within the layer.\n\n\n\n\n\n\n\ntype\n\n\n\n\n(integer) Identifier for the feature\u2019s geometry type. Values are:\n\n\n\n\n1 (POINT)\n\n\n2 (LINESTRING)\n\n\n3 (POLYGON)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\naggs\n\n\n\n\n(object) Layer containing results for the geotile_grid aggregation and its\nsub-aggregations.\n\n\nProperties of aggs\n\n\n\n\n\nextent\n\n\n\n(integer) Size, in pixels, of a side of the tile. Vector tiles are square with\nequal sides.\n\n\n\nversion\n\n\n\n(integer) Major version number of the\nMapbox vector tile specification.\n\n\n\nfeatures\n\n\n\n\n(array of objects) Array of features. Contains a feature for each cell of the\ngeotile_grid.\n\n\nProperties of features objects\n\n\n\n\n\ngeometry\n\n\n\n\n(object) Geometry for the feature.\n\n\nProperties of geometry\n\n\n\n\n\ntype\n\n\n\n\n(string) Geometry type for the feature. Valid values are:\n\n\n\n\nUNKNOWN\n\n\nPOINT\n\n\nLINESTRING\n\n\nPOLYGON\n\n\n\n\n\n\ncoordinates\n\n\n\n(array of integers or array of arrays) Tile coordinates for the feature.\n\n\n\n\n\n\n\n\nproperties\n\n\n\n\n(object) Properties for the feature.\n\n\nProperties of properties\n\n\n\n\n\n_count\n\n\n\n(string) Count of the cell\u2019s documents.\n\n\n\n<sub-aggregation>.value\n\n\n\nSub-aggregation results for the cell. Only returned for sub-aggregations in the\naggs parameter.\n\n\n\n\n\n\n\n\nid\n\n\n\n(integer) Unique ID for the feature within the layer.\n\n\n\ntype\n\n\n\n\n(integer) Identifier for the feature\u2019s geometry type. Values are:\n\n\n\n\n1 (POINT)\n\n\n2 (LINESTRING)\n\n\n3 (POLYGON)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmeta\n\n\n\n\n(object) Layer containing metadata for the request.\n\n\nProperties of meta\n\n\n\n\n\nextent\n\n\n\n(integer) Size, in pixels, of a side of the tile. Vector tiles are square with\nequal sides.\n\n\n\nversion\n\n\n\n(integer) Major version number of the\nMapbox vector tile specification.\n\n\n\nfeatures\n\n\n\n\n(array of objects) Contains a feature for a bounding box.\n\n\nProperties of features objects\n\n\n\n\n\ngeometry\n\n\n\n\n(object) Geometry for the feature.\n\n\nProperties of geometry\n\n\n\n\n\ntype\n\n\n\n\n(string) Geometry type for the feature. Valid values are:\n\n\n\n\nUNKNOWN\n\n\nPOINT\n\n\nLINESTRING\n\n\nPOLYGON\n\n\n\n\n\n\ncoordinates\n\n\n\n(array of integers or array of arrays) Tile coordinates for the feature.\n\n\n\n\n\n\n\n\nproperties\n\n\n\n\n(object) Properties for the feature.\n\n\nProperties of properties\n\n\n\n\n\n_shards.failed\n\n\n\n(integer) Number of shards that failed to execute the search. See the search\nAPI\u2019s shards response property.\n\n\n\n_shards.skipped\n\n\n\n(integer) Number of shards that skipped the search. See the search\nAPI\u2019s shards response property.\n\n\n\n_shards.successful\n\n\n\n(integer)  Number of shards that executed the search successfully. See the\nsearch API\u2019s shards response property.\n\n\n\n_shards.total\n\n\n\n(integer) Total number of shards that required querying, including unallocated\nshards. See the search API\u2019s shards response property.\n\n\n\naggregations._count.avg\n\n\n\n(float) Average _count value for features in the aggs layer.\n\n\n\naggregations._count.count\n\n\n\n(integer) Number of unique _count values for features in the aggs layer.\n\n\n\naggregations._count.max\n\n\n\n(float) Largest _count value for features in the aggs layer.\n\n\n\naggregations._count.min\n\n\n\n(float) Smallest _count value for features in the aggs layer.\n\n\n\naggregations._count.sum\n\n\n\n(float) Sum of _count values for features in the aggs layer.\n\n\n\naggregations.<sub-aggregation>.avg\n\n\n\n(float) Average value for the sub-aggregation\u2019s results.\n\n\n\naggregations.<agg_name>.count\n\n\n\n(integer) Number of unique values from the sub-aggregation\u2019s results.\n\n\n\naggregations.<agg_name>.max\n\n\n\n(float) Largest value from the sub-aggregation\u2019s results.\n\n\n\naggregations.<agg_name>.min\n\n\n\n(float) Smallest value from the sub-aggregation\u2019s results.\n\n\n\naggregations.<agg_name>.sum\n\n\n\n(float) Sum of values for the sub-aggregation\u2019s results.\n\n\n\nhits.max_score\n\n\n\n(float) Highest document _score for the search\u2019s hits.\n\n\n\nhits.total.relation\n\n\n\n\n(string) Indicates whether hits.total.value is accurate or a lower bound.\nPossible values are:\n\n\n\n\n\neq\n\n\n\nAccurate\n\n\n\ngte\n\n\n\nLower bound\n\n\n\n\n\n\nhits.total.value\n\n\n\n(integer) Total number of hits for the search.\n\n\n\ntimed_out\n\n\n\n(Boolean) If true, the search timed out before completion. Results may be\npartial or empty.\n\n\n\ntook\n\n\n\n(integer) Milliseconds it took Elasticsearch to run the search. See the search API\u2019s\ntook response property.\n\n\n\n\n\n\n\n\nid\n\n\n\n(integer) Unique ID for the feature within the layer.\n\n\n\ntype\n\n\n\n\n(integer) Identifier for the feature\u2019s geometry type. Values are:\n\n\n\n\n1 (POINT)\n\n\n2 (LINESTRING)\n\n\n3 (POLYGON)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExamples\n\nThe following requests create the museum index and add several geospatial\nlocation values.\n\nPUT museums\n{\n\"mappings\": {\n\"properties\": {\n\"location\": {\n\"type\": \"geo_point\"\n},\n\"name\": {\n\"type\": \"keyword\"\n},\n\"price\": {\n\"type\": \"long\"\n},\n\"included\": {\n\"type\": \"boolean\"\n}\n}\n}\n}\n\nPOST museums/_bulk?refresh\n{ \"index\": { \"_id\": \"1\" } }\n{ \"location\": \"52.374081,4.912350\", \"name\": \"NEMO Science Museum\",  \"price\": 1750, \"included\": true }\n{ \"index\": { \"_id\": \"2\" } }\n{ \"location\": \"52.369219,4.901618\", \"name\": \"Museum Het Rembrandthuis\", \"price\": 1500, \"included\": false }\n{ \"index\": { \"_id\": \"3\" } }\n{ \"location\": \"52.371667,4.914722\", \"name\": \"Nederlands Scheepvaartmuseum\", \"price\":1650, \"included\": true }\n{ \"index\": { \"_id\": \"4\" } }\n{ \"location\": \"52.371667,4.914722\", \"name\": \"Amsterdam Centre for Architecture\", \"price\":0, \"included\": true }\n\n\nThe following request searches the index for location values that intersect\nthe 13/4207/2692 vector tile.\n\nGET museums/_mvt/location/13/4207/2692\n{\n\"grid_precision\": 2,\n\"fields\": [\n\"name\",\n\"price\"\n],\n\"query\": {\n\"term\": {\n\"included\": true\n}\n},\n\"aggs\": {\n\"min_price\": {\n\"min\": {\n\"field\": \"price\"\n}\n},\n\"max_price\": {\n\"max\": {\n\"field\": \"price\"\n}\n},\n\"avg_price\": {\n\"avg\": {\n\"field\": \"price\"\n}\n}\n}\n}\n\n\nThe API returns results as a binary vector tile. When decoded into JSON, the\ntile contains the following data:\n\n{\n\"hits\": {\n\"extent\": 4096,\n\"version\": 2,\n\"features\": [\n{\n\"geometry\": {\n\"type\": \"Point\",\n\"coordinates\": [\n3208,\n3864\n]\n},\n\"properties\": {\n\"_id\": \"1\",\n\"name\": \"NEMO Science Museum\",\n\"price\": 1750\n},\n\"id\": 0,\n\"type\": 1\n},\n{\n\"geometry\": {\n\"type\": \"Point\",\n\"coordinates\": [\n3429,\n3496\n]\n},\n\"properties\": {\n\"_id\": \"3\",\n\"name\": \"Nederlands Scheepvaartmuseum\",\n\"price\": 1650\n},\n\"id\": 0,\n\"type\": 1\n},\n{\n\"geometry\": {\n\"type\": \"Point\",\n\"coordinates\": [\n3429,\n3496\n]\n},\n\"properties\": {\n\"_id\": \"4\",\n\"name\": \"Amsterdam Centre for Architecture\",\n\"price\": 0\n},\n\"id\": 0,\n\"type\": 1\n}\n]\n},\n\"aggs\": {\n\"extent\": 4096,\n\"version\": 2,\n\"features\": [\n{\n\"geometry\": {\n\"type\": \"Polygon\",\n\"coordinates\": [\n[\n[\n3072,\n3072\n],\n[\n4096,\n3072\n],\n[\n4096,\n4096\n],\n[\n3072,\n4096\n],\n[\n3072,\n3072\n]\n]\n]\n},\n\"properties\": {\n\"_count\": 3,\n\"max_price.value\": 1750.0,\n\"min_price.value\": 0.0,\n\"avg_price.value\": 1133.3333333333333\n},\n\"id\": 0,\n\"type\": 3\n}\n]\n},\n\"meta\": {\n\"extent\": 4096,\n\"version\": 2,\n\"features\": [\n{\n\"geometry\": {\n\"type\": \"Polygon\",\n\"coordinates\": [\n[\n[\n0,\n0\n],\n[\n4096,\n0\n],\n[\n4096,\n4096\n],\n[\n0,\n4096\n],\n[\n0,\n0\n]\n]\n]\n},\n\"properties\": {\n\"_shards.failed\": 0,\n\"_shards.skipped\": 0,\n\"_shards.successful\": 1,\n\"_shards.total\": 1,\n\"aggregations._count.avg\": 3.0,\n\"aggregations._count.count\": 1,\n\"aggregations._count.max\": 3.0,\n\"aggregations._count.min\": 3.0,\n\"aggregations._count.sum\": 3.0,\n\"aggregations.avg_price.avg\": 1133.3333333333333,\n\"aggregations.avg_price.count\": 1,\n\"aggregations.avg_price.max\": 1133.3333333333333,\n\"aggregations.avg_price.min\": 1133.3333333333333,\n\"aggregations.avg_price.sum\": 1133.3333333333333,\n\"aggregations.max_price.avg\": 1750.0,\n\"aggregations.max_price.count\": 1,\n\"aggregations.max_price.max\": 1750.0,\n\"aggregations.max_price.min\": 1750.0,\n\"aggregations.max_price.sum\": 1750.0,\n\"aggregations.min_price.avg\": 0.0,\n\"aggregations.min_price.count\": 1,\n\"aggregations.min_price.max\": 0.0,\n\"aggregations.min_price.min\": 0.0,\n\"aggregations.min_price.sum\": 0.0,\n\"hits.max_score\": 0.0,\n\"hits.total.relation\": \"eq\",\n\"hits.total.value\": 3,\n\"timed_out\": false,\n\"took\": 2\n},\n\"id\": 0,\n\"type\": 3\n}\n]\n}\n}\n\n\n\n\n\n\u00ab Ranking evaluation API\n\n\nSearchable snapshots APIs \u00bb\n\n\n\n\n\n\n\n\nMost Popular\n\nGet Started with Elasticsearch: Video\nIntro to Kibana: Video\nELK for Logs & Metrics: Video\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Git Post-receive hook with part of work tree", "id": 1293, "answers": [{"answer_id": 1285, "document_id": 864, "question_id": 1293, "text": "I recommend you to use some kind of continuous integration tool for the job. One possible workflow, which I could use myself.", "answer_start": 325, "answer_category": null}], "is_impossible": false}], "context": "I would like to use git to deploy to a website to a testing server. My website is a wordpress theme built with gulp.\nI've followed the steps explained here and here which are- set up a bare repository on the server, configure the location of git work tree and write a post-receive hook to checkout the repo to that location.\nI recommend you to use some kind of continuous integration tool for the job. One possible workflow, which I could use myself.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to automatically update an application without ClickOnce?", "id": 1715, "answers": [{"answer_id": 1703, "document_id": 1288, "question_id": 1715, "text": "A Lay men's way is\non Main() rename the executing assembly file .exe to some thing else check date and time of created. and the updated file date time and copy to the application folder.", "answer_start": 0, "answer_category": null}], "is_impossible": false}], "context": "A Lay men's way is\non Main() rename the executing assembly file .exe to some thing else check date and time of created. and the updated file date time and copy to the application folder.\n\nFor the project I am working on, I am not allowed to use ClickOnce. My boss wants the program to look \"real\" (with an installer, etc).\nI have installed Visual Studio 2012 Professional, and have been playing around with the InstallShield installer, and it definitely makes nice installers, but I can't figure out how to enable the application to \"auto-update\" (that is, when it starts up, checks to make sure that it is using the latest version).\nI have been asked to make a tiny change to the code - switching an addition to a subtraction, and I don't really want people to have to uninstall the old version, and then have to reinstall the new version every time I make a small change like this.\nHow can I make the application check for updates, and install them? Or is this not possible (or not easy)?\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "how to use sc to install a service and specify no action for subsequent failu", "id": 1415, "answers": [{"answer_id": 1404, "document_id": 991, "question_id": 1415, "text": "you must provide 'reset' and 'actions' at the same time\nyou must have a space after each option, so reset= &lt;number&gt;, etc\nyou cannot provide no options to 'action' (despite what the documentation on sc.exe claims), but you can provide empty values separated by a slash. All of these 3 commands will make it so there are no actions for any of the 3 at", "answer_start": 1365, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI created a service in VB.NET and wanted to use the \"sc\" program to install it. (I needed to package it so that someone else in my organization could perform the actual install.) \n\nI wanted the \"Recovery\" options to look as follows:\n\n\nFirst Failure: Restart\nSecond Failure: Restart\nSubsequent Failures: Do Nothing\n\n\nThis is the command I initially attempted (after the actual install):\n\nsc failure MyServiceName reset= 86400 actions= restart/15000/restart/30000\n\n\nBut then looking at the service in the GUI, \"Subsequent Failures\" was also set to restart. I looked on SO and couldn't find anything specific. I eventually figured it out, and I am posting this here in case anyone else is looking for the same \"quick\" answer I was. And of course, if anyone has anything to contribute, I would love to read it.\n    \n\nI eventually figured out to run the command like this:\n\nsc failure MyServiceName reset= 86400 actions= restart/15000/restart/30000//1000\n\n\nOnce I did this, and re-openned the service properties GUI, \"Take no action\" was shown as I wanted it to be.\n\nAfter I started writing the question, I did finally find this SO question:\nhttps://stackoverflow.com/a/12631379/1812688\n\nAlthough, it wasn't in direct response to the question\n    \n\nTo expand on this answer, the sc command is stupidly finnicky, and you need to do a couple things:\n\n\nyou must provide 'reset' and 'actions' at the same time\nyou must have a space after each option, so reset= &lt;number&gt;, etc\nyou cannot provide no options to 'action' (despite what the documentation on sc.exe claims), but you can provide empty values separated by a slash. All of these 3 commands will make it so there are no actions for any of the 3 attempts\n\n\nsc failure EraAgentSvc reset= 86400 actions= //\nsc failure EraAgentSvc reset= 86400 actions= ////\nsc failure EraAgentSvc reset= 86400 actions= //////\n\n\n\nand those commands will end up with the result from 'sc qfailure':\n\nC:\\Users\\Administrator&gt;sc qfailure EraAgentSvc\n[SC] QueryServiceConfig2 SUCCESSSERVICE_NAME: EraAgentSvc\n        RESET_PERIOD (in seconds)    : 86400\n        REBOOT_MESSAGE               :\n        COMMAND_LINE                 :\n\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to deploy to a single specific server using Capistrano", "id": 531, "answers": [{"answer_id": 533, "document_id": 256, "question_id": 531, "text": " I have similar problem and tried the following. It works: cap production ROLES=web HOSTS=machine1 stats.", "answer_start": 426, "answer_category": null}], "is_impossible": false}], "context": "I have a system in production that has several servers in several roles. I would like to test a new app server by deploying to that specific server, without having to redeploy to every server in production. Is there a way to ask Capistrano to deploy to a specific server? Ideally I'd like to be able to run something like: cap SERVER=app2.example.com ROLE=app production deploy. if I just wanted to deploy to app2.example.com. I have similar problem and tried the following. It works: cap production ROLES=web HOSTS=machine1 stats.", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "how to install mongodb on windows", "id": 1530, "answers": [{"answer_id": 1519, "document_id": 1107, "question_id": 1530, "text": "I.   Download the zip file http://www.mongodb.org/downloads\nII.  Extract it and copy the files into your desired location.\nIII. Start the DB engine.\nIV.  Test the installation and use it.", "answer_start": 503, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI am trying to test out mongoDB and see if it is anything for me. I downloaded the 32bit windows version, but have no idea on how to continue from now on.\n\nI normally use the WAMP services for developing on my local computer. Can i run mongoDB on Wamp?\n\nHowever, what's the best (easiest!) way to make it work on windows?\n\nThanks!\n    \n\nMongo Installation Process in Windows\nAre you ready for the installation \u2026 and use \u2026\nTechnically, it\u2019s not an installation it\u2019s just Downloading\u2026\nI.   Download the zip file http://www.mongodb.org/downloads\nII.  Extract it and copy the files into your desired location.\nIII. Start the DB engine.\nIV.  Test the installation and use it.\nThat's it! So simple, right? Ok let\u2019s start\n\n1. Download the zip file\n\nGo to http://www.mongodb.org/downloads\n\nYou will see a screen like this:\n\nI am using a windows 7 32 bit machine - that\u2019s why I downloaded the package marked in red.\n\nClick download (It only takes a few seconds).\nWow... I got that downloaded. It was a zipped file called mongodb-win32-i386-2.4.4.zip (The name of the folder will change according to the version you download, here I got version 2.4.4).\n\n\nOK all set.\n\n2. Extract\n\nExtract the zip\nCopy the files into a desired location in your machine.\n\nI am going to copy the extracted files to my D drive, since I don\u2019t have many files there.\nAlright then where are you planning to paste the mongo files? In C: or in your Desktop itself?\nOk, no matter where you paste... In the snap shot below, you can see that I have navigated to the bin folder inside the Mongo folder. I count fifteen files inside bin. What about you?\n\n\n\n\nFinished! That\u2019s all\nWhat we have to do next?\n\n3. Start the DB engine\nLet\u2019s go and start using our mongo db...\n\nOpen up a command prompt, then navigate to bin in the mongo folder\n\n\nType mongo.exe (which is the command used to start mongo Db Power shell). Then see the below response..\n\nThat was an awesome exception J LOL \u2026 What is that?\n\nCouldn\u2019t connect to server.\n\nWhy did the exception happen? I have no idea... Did I create a server in between?\nNo.\nRight, then how come it connected to a server in between? Silly Machine \u2026Jz.\nI got it! Like all other DBs - we have to start the DB engine before we use it.\nSo, how can we start it?\n\nWe have to start the mongo db by using the command mongod. Execute this from the bin folder of mongo.\nLet\u2019s see what had happened.\n\nAgain a wonderfully formatted exception J we got right? Did you notice what I have highlighted on top? Yeah it is the mongod command. The second one is the exception asking us to create a folder called data. And, inside the data folder, a folder called db.\nSo we have to create these data\\db folders.\nThe next question is where to create these folders?\n\nWe have to create the data\\db folders in the C drive of our BOX in which we are installing mongo. Let\u2019s go and create the folder structure in C drive.\nA question arises here: \"Is it mandatory to create the data\\db directories inside C?\" Nooo, not really. Mongo looks in C by default for this folder, but you can create them wherever you want. However, if it's not in C, you have to tell mongo where it is.\nIn other words, if you don't want the mongo databases to be on C:\\, you have to set the db path for mongo.exe.\nOptional\n\nOk, I will create those folders in some other location besides C for better understanding of this option. I will create then in the D drive root, with the help of cmd.\nWhy? Because it\u2019s an opportunity for us to remember the old dos commands...\n\n\nThe next step is to set the Db path to mongo.exe.\nNavigate back to bin, and enter the command, mongod.exe --dbpath d:\\data.\nI got the response below:\n\nI Hope everything went well... Because I didn\u2019t see any ERROR *** in the console J.\n\n\nNext, we can go and start the db using the command start mongo.exe\n\nI didn't see any error or warning messages. But, we have to supply a command to make sure mongo is up and running, i.e. mongod will get a response:\n\n\n\nHope everything went well.\n\n4. Test the Mongo DB installation\nNow we have to see our DB right? Yea very much, Otherwise how will we know it\u2019s running?\nFor testing purpose MONGO has got a DB called test by default. Lets go query that.\nBut how without any management studios? Unlike SQL, we have to depend on the command prompt. Yes exactly the same command prompt\u2026 our good old command prompt\u2026 Heiiiii..  Don\u2019t get afraid yes it\u2019s our old command prompt only.\nOk let\u2019s go and see how we are going to use it\u2026\nOhhh Nooo\u2026 don\u2019t close the above Command prompt, leave it as it is\u2026\n\nOpen a new cmd window.\n\nNavigate to Bin as usual we do\u2026\nI am sure you people may be remembering the old C programming which we have done on our college day\u2019s right?\n\nIn the command prompt, execute the command mongo or mongo.exe again and see what happens.\nYou will get a screen as shown below:\n\n\nI mentioned before that Mongo has got a test db by default called test, try inserting a record into it.\nThe next question here is \"How will we insert?\" Does mongo have SQL commands? No, mongo has got only commands to help with.\n\nThe basic command to insert is\ndb.test.save( { KodothTestField: \u2018My name is Kodoth\u2019 } )\n\nWhere test is the DB and .save is the insert command. KodothTestField is the column or field name, and My name is Kodoth is the value.\n\nBefore talking more let\u2019s check whether it\u2019s stored or not by performing another command: db.test.find()\n\nOur Data got successfully inserted \u2026 Hurrayyyyyy..\nI know that you are thinking about the number which is displayed with every record right called ObjectId. It\u2019s like a unique id field in SQL that auto-increments and all. Have a closer look you can see that the Object Id ends with 92, so it\u2019s different for each and every record.\nAt last we are successful in installing and verifying the MONGO right. Let\u2019s have a party...\nSo do you agree now MONGO is as Sweet as MANGO?\n\n\nAlso we have 3rd party tools to explore the MONGO. One is called MONGO VUE. Using this tool we can perform operations against the mongo DB like we use Management studio for SQL Server.\nCan you just imagine an SQL server or Oracle Db with entirely different rows in same table? Is it possible in our relational DB table? This is how mongo works. I will show you how we can do that\u2026\n\nFirst I will show you how the data will look in a relational DB.\nFor example consider an Employee table and a Student table in relational way. The schemas would be entirely different right? Yes exactly\u2026\n\nLet us now see how it will look in Mongo DB. The above two tables are combined into single Collection in Mongo\u2026\n\nThis is how Collections are stored in Mongo. I think now you can feel the difference really right?\nEvery thing came under a single umbrella. This is not the right way but I just wanted to show you all how this happens that\u2019s why I combined 2 entirely different tables in to one single Collection.\nIf you want to try out you can use below test scripts\n*********************** \nTEST INSERT SCRIPT\n\n*********EMPLOYEE****** \ndb.test.save( { EmployeId: \"1\", EmployeFirstName: \"Kodoth\", EmployeLastName:\"KodothLast\", EmployeAge:\"14\" } )  \ndb.test.save( { EmployeId: \"2\", EmployeFirstName: \"Kodoth 2\", EmployeLastName:\"Kodoth Last2\", EmployeAge:\"14\" } )  \ndb.test.save( { EmployeId: \"3\", EmployeFirstName: \"Kodoth 3\", EmployeLastName:\"Kodoth Last3\", EmployeAge:\"14\" } ) \n\n******STUDENT****** \ndb.test.save( { StudentId: \"1\", StudentName: \"StudentName\", StudentMark:\"25\" } )  \ndb.test.save( { StudentId: \"2\", StudentName: \"StudentName 2\", StudentMark:\"26\" } )  \ndb.test.save( {StudentId: \"3\", StudentName: \"StudentName 3\", StudentMark:\"27\"} )\n************************\n\nThanks\n    \n\nIt's not like WAMP. You need to start mongoDB database with a command after directory has been created C:/database_mongo\n\nmongod --dbpath=C:/database_mongo/\n\nyou can then connect to mongodb using commands.\n    \n\nPretty good documentation is provided on the MongoDB website\n\n\n  Install MongoDB\n  \n  \n  Determine which MongoDB build you need.\n  \n  There are three builds of MongoDB for Windows:\n  \n  MongoDB for Windows Server 2008 R2 edition (i.e. 2008R2) runs only on Windows Server 2008 R2, Windows 7 64-bit, and newer versions of\n  Windows. This build takes advantage of recent enhancements to the\n  Windows Platform and cannot operate on older versions of Windows.\n  \n  MongoDB for Windows 64-bit runs on any 64-bit version of Windows newer than Windows XP, including Windows Server 2008 R2 and Windows 7\n  64-bit.\n  \n  MongoDB for Windows 32-bit runs on any 32-bit version of Windows newer than Windows XP. 32-bit versions of MongoDB are only intended\n  for older systems and for use in testing and development systems.\n  32-bit versions of MongoDB only support databases smaller than 2GB.\n  \n  To find which version of Windows you are running, enter the following command in the Command Prompt:\n  \n  wmic os get osarchitecture\n\n  Download MongoDB for Windows.\n  \n  Download the latest production release of MongoDB from the MongoDB downloads page. Ensure you download the correct version of MongoDB for\n  your Windows system. The 64-bit versions of MongoDB does not work with\n  32-bit Windows.\n  Install the downloaded file.\n  \n  In Windows Explorer, locate the downloaded MongoDB msi file, which typically is located in the default Downloads folder. Double-click the\n  msi file. A set of screens will appear to guide you through the\n  installation process.\n  Move the MongoDB folder to another location (optional).\n  \n  To move the MongoDB folder, you must issue the move command as an Administrator. For example, to move the folder to C:\\mongodb:\n  \n  Select Start Menu &gt; All Programs &gt; Accessories.\n  \n  Right-click Command Prompt and select Run as Administrator from the popup menu.\n  \n  Issue the following commands:\n  \n  \n\ncd \\\nmove C:\\mongodb-win32-* C:\\mongodb\n\n  \n  MongoDB is self-contained and does not have any other system dependencies. You can run MongoDB from any folder you choose. You may\n  install MongoDB in any folder (e.g. D:\\test\\mongodb)\n  \n  \n  Run MongoDB\n  \n  Warning:\n  \n  Do not make mongod.exe visible on public networks without running in\n  \u201cSecure Mode\u201d with the auth setting. MongoDB is designed to be run in\n  trusted environments, and the database does not enable \u201cSecure Mode\u201d\n  by default.\n  \n  \n  Set up the MongoDB environment.\n  \n  MongoDB requires a data directory to store all data. MongoDB\u2019s default data directory path is \\data\\db. Create this folder using the\n  following commands from a Command Prompt:\n  \n  \n\nmd \\data\\db\n\n  \n  You can specify an alternate path for data files using the --dbpath option to mongod.exe, for example:\n  \n  \n\nC:\\mongodb\\bin\\mongod.exe --dbpath d:\\test\\mongodb\\data\n\n  \n  If your path includes spaces, enclose the entire path in double quotes, for example:\n  \n  \n\nC:\\mongodb\\bin\\mongod.exe --dbpath \"d:\\test\\mongo db data\"\n\n  Start MongoDB.\n  \n  To start MongoDB, run mongod.exe. For example, from the Command Prompt:\n  \n  \n\nC:\\Program Files\\MongoDB\\bin\\mongod.exe\n\n  \n  This starts the main MongoDB database process. The waiting for connections message in the console output indicates that the\n  mongod.exe process is running successfully.\n  \n  Depending on the security level of your system, Windows may pop up a Security Alert dialog box about blocking \u201csome features\u201d of\n  C:\\Program Files\\MongoDB\\bin\\mongod.exe from communicating on\n  networks. All users should select Private Networks, such as my home or\n  work network and click Allow access. For additional information on\n  security and MongoDB, please see the Security Documentation.\n  Connect to MongoDB.\n  \n  To connect to MongoDB through the mongo.exe shell, open another Command Prompt. When connecting, specify the data directory if\n  necessary. This step provides several example connection commands.\n  \n  If your MongoDB installation uses the default data directory, connect without specifying the data directory:\n  \n  \n\nC:\\mongodb\\bin\\mongo.exe\n\n  \n  If you installation uses a different data directory, specify the directory when connecting, as in this example:\n  \n  \n\nC:\\mongodb\\bin\\mongod.exe --dbpath d:\\test\\mongodb\\data\n\n  \n  If your path includes spaces, enclose the entire path in double quotes. For example:\n  \n  \n\nC:\\mongodb\\bin\\mongod.exe --dbpath \"d:\\test\\mongo db data\"\n\n  \n  If you want to develop applications using .NET, see the documentation of C# and MongoDB for more information.\n  Begin using MongoDB.\n  \n  To begin using MongoDB, see Getting Started with MongoDB. Also consider the Production Notes document before deploying MongoDB in a\n  production environment.\n  \n  Later, to stop MongoDB, press Control+C in the terminal where the mongod instance is running.\n  \n  \n  Configure a Windows Service for MongoDB\n  \n  Note:\n  \n  There is a known issue for MongoDB 2.6.0, SERVER-13515, which prevents\n  the use of the instructions in this section. For MongoDB 2.6.0, use\n  Manually Create a Windows Service for MongoDB to create a Windows\n  Service for MongoDB instead.\n  \n  \n  Configure directories and files.\n  \n  Create a configuration file and a directory path for MongoDB log output (logpath):\n  \n  Create a specific directory for MongoDB log files:\n  \n  \n\nmd \"C:\\Program Files\\MongoDB\\log\"\n\n  \n  In the Command Prompt, create a configuration file for the logpath option for MongoDB:\n  \n  \n\necho logpath=C:\\Program Files\\MongoDB\\log\\mongo.log &gt; \"C:\\Program Files\\MongoDB\\mongod.cfg\"\n\n  Run the MongoDB service.\n  \n  Run all of the following commands in Command Prompt with \u201cAdministrative Privileges:\u201d\n  \n  Install the MongoDB service. For --install to succeed, you must specify the logpath run-time option.\n  \n  \n\n\"C:\\Program Files\\MongoDB\\bin\\mongod.exe\" --config \"C:\\Program Files\\MongoDB\\mongod.cfg\" --install\n\n  \n  Modify the path to the mongod.cfg file as needed.\n  \n  To use an alternate dbpath, specify the path in the configuration file (e.g. C:\\Program Files\\MongoDB\\mongod.cfg) or on the command line\n  with the --dbpath option.\n  \n  If the dbpath directory does not exist, mongod.exe will not start. The default value for dbpath is \\data\\db.\n  \n  If needed, you can install services for multiple instances of mongod.exe or mongos.exe. Install each service with a unique\n  --serviceName and --serviceDisplayName. Use multiple instances\n  only when sufficient system resources exist and your system design\n  requires it.\n  Stop or remove the MongoDB service as needed.\n  \n  To stop the MongoDB service use the following command:\n  \n  \n\nnet stop MongoDB\n\n  \n  To remove the MongoDB service use the following command:\n  \n  \n\n\"C:\\Program Files\\MongoDB\\bin\\mongod.exe\" --remove\n\n  \n  \n  Manually Create a Windows Service for MongoDB\n  \n  The following procedure assumes you have installed MongoDB using the\n  MSI installer, with the default path C:\\Program Files\\MongoDB 2.6\n  Standard.\n  \n  If you have installed in an alternative directory, you will need to\n  adjust the paths as appropriate.\n  \n  \n  Open an Administrator command prompt.\n  \n  Windows 7 / Vista / Server 2008 (and R2)\n  \n  Press Win + R, then type cmd, then press Ctrl + Shift + Enter.\n  \n  Windows 8\n  \n  Press Win + X, then press A.\n  \n  Execute the remaining steps from the Administrator command prompt.\n  Create directories.\n  \n  Create directories for your database and log files:\n  \n  \n\nmkdir c:\\data\\db\nmkdir c:\\data\\log\n\n  Create a configuration file.\n  \n  Create a configuration file. This file can include any of the configuration options for mongod, but must include a valid setting for\n  logpath:\n  \n  The following creates a configuration file, specifying both the logpath and the dbpath settings in the configuration file:\n  \n  \n\necho logpath=c:\\data\\log\\mongod.log&gt; \"C:\\Program Files\\MongoDB 2.6 Standard\\mongod.cfg\"\necho dbpath=c:\\data\\db&gt;&gt; \"C:\\Program Files\\MongoDB 2.6 Standard\\mongod.cfg\"\n\n  Create the MongoDB service.\n  \n  Create the MongoDB service.\n  \n  \n\nsc.exe create MongoDB binPath= \"\\\"C:\\Program Files\\MongoDB 2.6 Standard\\bin\\mongod.exe\\\" --service --config=\\\"C:\\Program Files\\MongoDB 2.6 Standard\\mongod.cfg\\\"\" DisplayName= \"MongoDB 2.6 Standard\" start= \"auto\"\n\n  \n  sc.exe requires a space between \u201c=\u201d and the configuration values (eg \u201cbinPath=\u201d), and a \u201c\u201d to escape double quotes.\n  \n  If successfully created, the following log message will display:\n  \n  \n\n[SC] CreateService SUCCESS\n\n  Start the MongoDB service.\n  \n  \n\nnet start MongoDB\n\n  Stop or remove the MongoDB service as needed.\n  \n  To stop the MongoDB service, use the following command:\n  \n  \n\nnet stop MongoDB\n\n  \n  To remove the MongoDB service, first stop the service and then run the following command:\n  \n  \n\nsc.exe delete MongoDB\n\n  \n\n    \n\nI realize you've already accepted an answer for this, but I wrote this short howto article to install mongodb into the c:\\wamp directory and run it as a service.  Here is the gist of it.\n\nCreate these directories\n\nmkdir c:\\wamp\\bin\\mongodb\\mongodb-win32...2.x.x\\data\nmkdir c:\\wamp\\bin\\mongodb\\mongodb-win32...2.x.x\\data\\db\nmkdir c:\\wamp\\bin\\mongodb\\mongodb-win32...2.x.x\\logs\nmkdir c:\\wamp\\bin\\mongodb\\mongodb-win32...2.x.x\\conf\n\n\nDownload and extract win32 binaries into c:\\wamp directory along side mysql, apache.\n\nmongodb download page\n\nCreate a mongo.conf file\n\nc:\\wamp\\bin\\mongodb\\mongodb-win32\u20262.x.x\\conf\\mongodb.conf\n\n# mongodb.conf\n\n# data lives here\ndbpath=C:\\wamp\\bin\\mongodb\\mongodb-win32...2.x.x\\data\\db\n\n# where to log\nlogpath=C:\\wamp\\bin\\mongodb\\mongodb-win32...2.x.x\\logs\\mongodb.log\nlogappend=true\n\n# only run on localhost for development\nbind_ip = 127.0.0.1                                                             \n\nport = 27017\nrest = true\n\n\nInstall as a service\n\nmongod.exe --install --config c:\\wamp\\bin\\mongodb\\mongodb-win32...2.x.x\\conf\\mongodb.conf --logpath c:\\wamp\\bin\\mongodb\\mongodb-win32...2.x.x\\logs\\mongodb.log\n\n\nSet service to automatic and start it using services.msc\n\nAdd path to mongo.exe to your path\n\nNeed more details? Read the full article here...\n\nInstalling MongoDB on Windows the WAMP way\n    \n\n1. Download MongoDB\n\n2. Install MongoDB\n\n3. Create the required folders:\n\n\"C:\\MongoDB_2_6_Standard\\bin\\data\\db\"\n\"C:\\MongoDB_2_6_Standard\\logs\"\n\"C:\\MongoDB_2_6_Standard\\etc\"\n\n\nNOTE: If the directories do not exist, mongod.exe will not start.\n\n4. Create a simple configuration file:\n\nsystemLog:\n    destination: file\n    path: C:\\MongoDB_2_6_Standard\\logs\\mongo.log\n    logAppend: true\nnet:\n    bindIp: 127.0.0.1\n    port: 27017\n\n\nMore info about how to create a configuration file: http://docs.mongodb.org/manual/reference/configuration-options/\n\n5. Install MongoDB as a Windows Service (this way it will start automatically when you reboot your computer)\n\nRun cmd with administrator privilegies, and enter the following commands:\n\n\"C:\\MongoDB_2_6_Standard\\bin\\mongod.exe\" --config \"C:\\MongoDB_2_6_Standard\\etc\\mongodb.conf\" --dbpath c:\\MongoDB_2_6_Standard\\bin\\data\\db --directoryperdb --install\n\n\n6. Start the MongoDB Windows Service\n\nnet start MongoDB\n\n\n7. Connect to MongoDB via shell/cmd for testing\n\nC:\\MongoDB_2_6_Standard\\bin\\mongo.exe\n\n\nNOTE: http://docs.mongodb.org/manual/tutorial/getting-started-with-the-mongo-shell/\n\n8. That's it! You are done. :)\n\n9. Uninstall/remove the MongoDB Windows Service (if you messed up something)\n\n\"C:\\MongoDB_2_6_Standard\\bin\\mongod.exe\" --remove\n\n    \n\nStep by Step Solution for windows 32 bit\n\n\nDownload msi file for windows 32 bit.\nDouble click Install it, choose custom and browse the location where ever you have to install(personally i have create the mongodb folder in E drive and install it there).\nOk,now you have to create the data\\db two folder where ever create\nit, i've created it in installed location root e.g on\nE:\\\nNow link the mongod to these folder for storing data use this\ncommand or modify according to your requirement go to using cmd\nE:\\mongodb\\bin and after that write in console \nmongod --dbpath E:\\data it will link.\nNow navigate to E:\\mongodb\\bin and write mongod using cmd.\nOpen another cmd by right click and run as admin point to your\nmonogodb installed directory  and then to bin just like\nE:\\mongodb\\bin and write this mongo.exe\nNext - write db.test.save({Field:'Hello mongodb'}) this command\nwill insert the a field having name Field and its value Hello\nmongodb.\nNext, check the record db.test.find() and press enter you will find\nthe record that you have recently entered.\n\n    \n\nIts very simple to install Mongo DB on windows 7 ( i used 32 bit win7 OS)\n\n\nInstall the correct version of Mongodb ( according to ur bit 32/64 .. imp :- 64 bit is not compatible with 32 bit and vice versa)\n\n\n2.u can install Mongodb from thius website ( acc to ur OS) http://www.mongodb.org/downloads?_ga=1.79549524.1754732149.1410784175\n\n\nDOWNLOAD THE .MSI OR zip file .. and install with proper privellages     \n\n\n4.copy the mongodb folder from c:programfiles  to d: [optional] \n\n5.After installation open command prompt ( as administrator .. right click on cmd and u will find the option)\n\n\nnavigate to D:\\MongoDB 2.6 Standard\\bin\nrun mongo.exe  ... you might get this error  \nIf you get then no isse you just need to do following steps \n\n\ni)  try the coomand in following image yo will get to know the error \n\n\nii)This means that u neeed to create a directory \\data\\db\n\niii) now you have two options either create above directory in c drive or create any \"xyz\" name directory somewhere else ( doesnot make and diffrence) .. lets create a directory of mongodata in d:\n\n\n\n\nNow lets rerun the command but now like this  :-  mongod --dbpath d:\\mongodata [shown in fig] \nthis time you will not get and error \n\n\n\n\n\nHope everything is fine till this point .. open new command propmt [sufficent privellages (admin)]\n\n\ncolured in orange will be the command u need to run .. it will open the new command propmt which we known as mongo shell (or mongodb shell)\n\n\n\n11.dont close the shell[any of command promt as well] as in this we will create /delete/insert our databse operations\n\n\nLets perform basic operation \n\n\na) show databases \nb) show current databse \nc) creation of collection / inserting data into it (name will be  test)\nd) show data of collection \n\n12.please find scrren shot of results of our operation .. please not :- dont close any command propmt \n\n\n\n\na diffrent structure type of number is object id :- which is created automatically\nHope you get some important info for installing mongodb DB.\n\n    \n\nInstalling MongoDB on Windows is bit tricky compared to other Executable files.. Got a good reference after long search i got Installing MongoDB in Windows\n\nAfter Installing open command prompt and type \"mongod\", then keep the window minimized and open another command prompt window and type \"mongo\" and you will find the success message of connecting to the test database\n    \n\nUpdate Nov -2017\n\n1) Go to Mongo DB download center https://www.mongodb.com/download-center#community and pick a flavor of MongoDB you want to install. You can pick from \n\n\nMongoDB Atlas -  MongoDB database in the cloud \nCommuniy Server - MongoDb for windows (with and without SSL),iOS, Linux\nOpManger- Mongo Db for Data Center\nCompass - UI tool for MongoDB\n\n\nTo know your OS version run this command in cmd prompt \n\nwmic os get caption\n\n\nTo know your CPU architecture(32 or 64 bit) run this command in cmd prompt \n\nwmic os get osarchitecture\n\n\nI am using Community version (150MBs- GNU license)\n\n2) Click on MSI  and go through installation Process. Exe will install MongoDb and SSL required by the DB.\n\n\n\nMongo DB should be installed on your C drive\n\n\n  C:\\Program Files\\MongoDB\n\n\nMongoDB is self-contained, it means and does not have any other system dependencies. If you are low on disk in C drive then you can run MongoDB from any folder you choose.\n\nYou can now run mongodb.exe from bin folder. If you get Visual C++ error for missing dlls then download Visual C++ Redistributable from\n\nhttps://www.microsoft.com/en-in/download/details.aspx?id=48145\n\nAfter installation, try to rerun mongo.exe.\n    \n\nYou might want to check https://github.com/Thor1Khan/mongo.git\nit uses a minimal workaround the 32 bit atomic operations on 64 bits operands\n(could use assembly but it doesn't seem to be mandatory here)\nOnly digital bugs were harmed before committing\n    \n\n\nDownload .msi from   https://www.mongodb.com/download-center#community\nDouble click install  - complete option \nInstallation folder C:\\Program Files\\MongoDB\\Server\\3.6\\bin\nCreate database folder in C://  -  c:/data/db  and  c:/data/log  and  c:/data/log/mongo.log   and set write permission if not \nOpen cmd prompt in Administrator mode , navigate to C:\\Program Files\\MongoDB\\Server\\3.6\\bin\nType the following\n\n\n  \n    C:\\Program Files\\MongoDB\\Server\\3.6\\bin&gt;mongod --dbpath=\"c:/data/db\" --logpath=\"c:/data/log/mongo.log\" \n  \n\nCheck folder c:/data/db   -  there should be many files and folder\nCreate a config file named \"mongo.config\" ** inside C:\\data\\\nType the following to set the config values from newly created config file\n\n\n  \n    C:\\Program Files\\MongoDB\\Server\\3.6\\bin&gt;mongod --config C:\\data\\mongo.config\n  \n\nOpen another new cmd prompt in Administrator mode , navigate to C:\\Program Files\\MongoDB\\Server\\3.6\\bin\nexecute the following lines on the console.\nType the following to create service for MongoDB\n\n\n  \n    C:\\Program Files\\MongoDB\\Server\\3.6\\bin&gt;mongod --install --config C:\\data\\mongo.config --logpath=\"c:/data/log/mongo.log\" \n  \n\n\nIf old MongoDB service exists then need to delete the old service first before mongod --install command, run the following in a new console to delete old mongodb service\n\n\n  \n    SC STOP MongoDB\n  \n\n\n&gt;&gt; SC DELETE MongoDB\n\nType the following to start MongoDB\n\n\n  \n    C:\\Program Files\\MongoDB\\Server\\3.6\\bin&gt;net start MongoDB\n  \n\nType the following to stop MongoDB\n\n\n  \n    C:\\Program Files\\MongoDB\\Server\\3.6\\bin&gt;net stop MongoDB \n  \n\n\n\nNow connect the DB on ip - 127.0.0.1 or 192.168.5.64  on port 27017 .\n\n** File name - \"mongo.config\" ,  paste the following on config file -\n\nbind_ip = 127.0.0.1,192.168.5.64\n\nport = 27017\n\ndbpath=C:\\data\\db\n\nlogpath=C:\\data\\log\\mongo.log\n    \n\nInstall MongoDB Community Edition for Windows.\n\n1.Now go to Download Center.\n\n\nThe Download Center should display the Current Stable Release for Windows\nClick Download (msi) and run it\nWhen you go through You can choose either the Complete or Custom setup type. Select Complete Setup.\n\n\nStarting MongoDB server from the Command Prompt\n\n1.Add this path to environmental variables \n\n\u201cC:\\ProgramFiles\\MongoDB\\Server\\4.0\\bin\u201d For those who struggle to setup environmental variable please follow this steps\n\n\nWindows 10 and Windows 8\nIn Search, search for and then select: System (Control Panel)\nClick the Advanced system settings link.\nClick Environment Variables. In the section System Variables, find\nthe PATH environment variable and select it. Click Edit. \nIf the PATH environment variable does not exist, click New.\nIn the Edit System Variable (or New System Variable) window, specify\nthe value of the PATH as\n\u201cC:\\ProgramFiles\\MongoDB\\Server\\4.0\\bin\u201denvironment variable. \nClick OK. \nClose all remaining windows by clicking OK.\nNow create MongoDB Data Directory MongoDB requires a data directory to store all data. Open Command Prompt and paste below\n\n\n\nmd \"C:\\data\\db\" \"C:\\data\\log\"\n\n\n\n3.Lets point server to your database directory.Type below code\n\n\nmongod --dbpath=\"c:\\data\\db\"\n\n\n\n4.Now lets connect to MongoDB Type below code\n\n\n  mongo\n\n\n5.If it is working properly Command prompt will show\n\n\n  [initandlisten] waiting for connections\n\n\nBingo!! You are done, Apparently it is bit confusing to use CLI, well MongoDB introduced a GUI which you can see the actual data ,\n\nTo use mongoDB GUI version Above steps must have been performed\n\n\nNow go to Download Center. Download MongoDb Compass ,a GUI for MongoDB  \nInstall it and open\n\n\nYou can see the hostname : localhost and port : 27017.\n\nWhenever you wanted to connect to mongoDB Server , You have to open Command prompt\n\nType\n\n\n  mongod\n\n\nthen again start a new Command prompt and type\n\n\n  mongo\n\n\nKeep \u2018em all settings as it is in mongoDB Compass. Now click connect ,\n\nYou are in !! Easy .. isn\u2019t it?\n    \n\n\nDownload from http://www.mongodb.org/downloads \nInstall .msi file in folder C:\\mongodb \nCreate data, data\\db, log directories and mongo.config file under C:\\mongodb. \nAdd the following lines in\n\"mongo.config\" file dbpath=C:\\mongodb\\data\\db\\\nlogpath=C:\\mongodb\\log\\mongo.log \nStart server : \nmongod.exe --config=\"C:\\mongodb\\mongo.config\"\n\n\nThat's it !!!\n    \n\nStep 1: First download the .msi i.e is the installation file from\n\nDownload MonggoDB\n\nStep 2: Perform the installation using the so downloaded .msi file.Automatically it gets stored in program files. You could perform a custom installation and change the directory.\n\nAfter this, you should be able to see a MongoDB folder under program files\n\nstarting MongoDB shell and service is not big a deal I Got a good reference after the long search Installing MongoDB in Windows\n    \n\nWAMP = Windows + Apache + MySQL/MariaDB + PHP/Python/Perl\n\nYou can't use MongoDB in wamp.You need to install MongoDB separately\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How can I install a CPAN module into a local directory?", "id": 687, "answers": [{"answer_id": 692, "document_id": 380, "question_id": 687, "text": "local::lib will help you. It will convince \"make install\" (and \"Build install\") to install to a directory you can write to, and it will tell perl how to get at those modules.", "answer_start": 517, "answer_category": null}], "is_impossible": false}], "context": "I'm using a hosted Linux machine so I don't have permissions to write into the /usr/lib directory.\nWhen I try to install a CPAN module by doing the usual:\nperl Makefile.PL\nmake test\nmake install\nThat module is extracted to a blib/lib/ folder. I have kept use blib/lib/ModuleName but it still the compiler says module can not be found. I have tried copying the .pm file into local directory and kept require ModuleName but still it gives me some error.\nHow can I install a module into some other directory and use it?\nlocal::lib will help you. It will convince \"make install\" (and \"Build install\") to install to a directory you can write to, and it will tell perl how to get at those modules.\nIn general, if you want to use a module that is in a blib/ directory, you want to say perl -Mblib ... where ... is how you would normally invoke your script\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "FAIL - Failed to deploy application at context path /ROOT. Deploy plugin of jenkins failling to deploy", "id": 1288, "answers": [{"answer_id": 1280, "document_id": 859, "question_id": 1288, "text": "Just a wild guess but the ROOT context is not: /ROOT but rather / maybe there is something wrong in your config", "answer_start": 370, "answer_category": null}], "is_impossible": false}], "context": "Attempting to set up jenkins and its container deploy plugin.\nWhich isn't really that helpful, I can deploy manually using tomcat manager, and also copying the war file across using command line options works. But i can not get the automated deploy to work.\nI am using a tomcat url of http://localhost:8080/ and the deploy fails when their is no app currently deployed.\nJust a wild guess but the ROOT context is not: /ROOT but rather / maybe there is something wrong in your config\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Deploying Qt 5 App on Windows", "id": 1727, "answers": [{"answer_id": 1715, "document_id": 1300, "question_id": 1727, "text": "Starting from Qt 5.2, there is windeployqt tool you can use. Just run it from command line to get help. But basic usage is, give it the .exe file, it will copy Qt dependencies to go with it.\nYou will want to use --qmldir option to let the tool know where your QML files are, so it can figure out the needed QML dependencies.", "answer_start": 1289, "answer_category": null}], "is_impossible": false}], "context": "I've written a couple of applications in QML (part of Qt 5). In a question that I've made before (https://softwareengineering.stackexchange.com/questions/213698/deploying-qt-based-app-on-mac-os-x), I found the solution for deploying my app on OS X (using the macdeployqt tool).\nDeploying Qt4 apps on Windows was easy:\n1.\tYou compiled it in release mode.\n2.\tYou copied the necessary libraries (DLLs).\n3.\tYou tested and it worked.\nUnfortunately, this approach did not work in Qt5 (I even included the platforms folder with the qwindows.dll file and it did not work). After some days of trying, I gave up and compiled a static version of Qt5.\nAgain, it did not work. The app works on a PC with Qt installed, but it crashes on \"clean\" PCs. As a side note, Windows 8/8.1 systems don't give a warning or a message notifying me about the app's crash. But in Windows 7 a message notifies me that the application crashed.\nI've tried running Dependency Walker (depends.exe) and all libraries in the static build of my application seemed fine.\nIn Windows 8, I don't get any error. But after profiling the app in depends.exe, I get an access violation originating from QtGui.dll. The exact error is\nSecond chance exception 0xC0000005 (Access Violation) occurred in \"QT5GUI.DLL\" at address 0x61C2C000.\nStarting from Qt 5.2, there is windeployqt tool you can use. Just run it from command line to get help. But basic usage is, give it the .exe file, it will copy Qt dependencies to go with it.\nYou will want to use --qmldir option to let the tool know where your QML files are, so it can figure out the needed QML dependencies.\nNote about testing: to make sure you have everything, test in computer with no Qt SDK, or temporarily rename the Qt directory. Otherwise the application might find missing files from there...\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Python does not see pygraphviz", "id": 817, "answers": [{"answer_id": 812, "document_id": 499, "question_id": 817, "text": "sudo apt-get install graphviz libgraphviz-dev pkg-config\n2.\tCreate and activate virtualenv if needed. The commands looks something like sudo apt-get install python-pip python-virtualenv\n3.\tRun pip install pygraphviz\n4.\tRun terminal and check by importing and see if it works", "answer_start": 154, "answer_category": null}], "is_impossible": false}], "context": "I have installed pygraphviz using easy_install But when i launch python i have an error. Assuming that you're on Ubuntu please look at following steps\n1.\tsudo apt-get install graphviz libgraphviz-dev pkg-config\n2.\tCreate and activate virtualenv if needed. The commands looks something like sudo apt-get install python-pip python-virtualenv\n3.\tRun pip install pygraphviz\n4.\tRun terminal and check by importing and see if it works\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "downloading eclipse plug in update sites for offline installation?", "id": 858, "answers": [{"answer_id": 853, "document_id": 538, "question_id": 858, "text": "\n eclipse -nosplash -verbose\n -application org.eclipse.equinox.p2.metadata.repository.mirrorApplication\n -source Insert Source URL (e.g. http://download.eclipse.org/releases/2020-06)\n -destination Insert Destination URL (e.g. file:/tmp/2020-06-Mirror/)\nThe application will be \"eclipsec.exe\" (instead of \"eclipse\") for Windows installations.", "answer_start": 1374, "answer_category": null}], "is_impossible": false}], "context": "Equinox p2 Repository Mirroring\nContents\n1 Introduction\n2 Running the Mirroring Tools\n2.1 Mirroring Metadata\n2.2 Mirroring Artifacts\nIntroduction\np2 manages all of its data in repositories. There are two types of repos, artifact and metadata.\n\np2 artifact repositories can hold many different forms of the same artifact. For example, a JAR artifact may exist as\n\na canonical (unmolested) JAR\na compressed JAR (assuming the original was not compressed)\na pack200 JAR\na delta relative to another version of the artifact\n...\np2 metadata repositories describe inter-component dependencies and identify artifacts to install.\n\nThe Repository Mirroring applications can be used to mirror artifact and metadata repositories. In addition, users can do selective mirroring of artifacts or metadata either to create a more specific mirror (e.g. only mirror latest code) or merge content into an existing mirror.\n\nThis example demonstrates how to mirror a repository using the repository mirror applications available in the equinox p2 tools.\n\nRunning the Mirroring Tools\nCurrently, there are two different mirror applications: one for artifacts and one for metadata.\n\nMirroring Metadata\nTo make an exact mirror of a metadata repository, use the following arguments. Note that if the target repository does not exist, a new repository is created with the same properties as the source.\n\n eclipse -nosplash -verbose\n -application org.eclipse.equinox.p2.metadata.repository.mirrorApplication\n -source Insert Source URL (e.g. http://download.eclipse.org/releases/2020-06)\n -destination Insert Destination URL (e.g. file:/tmp/2020-06-Mirror/)\nThe application will be \"eclipsec.exe\" (instead of \"eclipse\") for Windows installations.\n\nBy adding the argument -writeMode clean, all installable units in the target destination will be removed before the mirroring is performed.\n\nAdding the argument -destinationName <destination name> will set the destination repository's name to the specified destination name. If this argument is not included the destination will use the source repository's name if no repository exists at the destination.\n\nMirroring Artifacts\nTo make an exact mirror of an artifact repository, use the following arguments. Note that if the target repository does not exist, a new repository is created with the same properties as the source.\n\n eclipse -nosplash -verbose\n -application org.eclipse.equinox.p2.artifact.repository.mirrorApplication\n -source Insert Source URL (e.g. http://download.eclipse.org/releases/2020-06)\n -destination Insert Destination URL (e.g. file:/tmp/2020-06-Mirror/)\nAgain, the application will be \"eclipsec.exe\" (instead of \"eclipse\") for Windows installations.\n\nBy adding the argument -writeMode clean, all artifacts in the target destination will be removed before the mirroring is performed.\n\nAdding the argument -destinationName <destination name> will set the destination repository's name to the specified destination name. If this argument is not included the destination will use the source repository's name if no repository exists at the destination.\n\nAdding the argument -verbose will enable verbose error reporting and logging. This will write errors to the <eclipse workspace>/.metadata/.log file.\n\nAdding the argument -ignoreErrors will ensure the mirror application does not fail in the event of an error. Note: while using this argument the mirror application may complete without errors but the destination repository may not include all artifacts from the source repository.\n\nAdding the argument -raw instructs the mirroring application to copy the exact artifact descriptors from source into the destination instead of initializing new artifact descriptors with properties from the source descriptors.\n\nAdding the argument -compare instructs the mirroring application to perform a comparison when a duplicate artifact descriptor is found.\n\nAdding the argument -comparator <comparator ID> specifies the mirroring application should use an Artifact Comparator with an ID of \"comparator ID\" to compare artifact descriptors. The mirroring application uses the \"MD5 Comparator\" to compare the MD5 hash property of the artifact descriptors if no comparator is defined.\n\nAdding the argument -baseline <baseline location> will compare all artifacts in the source against a known good baseline repository specified by the baseline location. In the event of conflicting artifacts, precedence is given to the baseline repository. Can be combined with -comparator <comparator ID>.", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Trouble installing psycopg2 on CentOS", "id": 1798, "answers": [{"answer_id": 1784, "document_id": 1370, "question_id": 1798, "text": "psycopg2 is a python wrapper around the PostgreSQL libraries, so you need those installed on your system too.\nSince you're using CentOS, try this from the command line to install the postgre libs.\nyum install postgresql-libs", "answer_start": 557, "answer_category": null}], "is_impossible": false}], "context": "I'm trying to install psycopg2 on CentOS, I followed everything on this tutorial from \"On with it: Installing Python 2.6\" all the way to when it imports psycopg2, but when I try to import I get the following error:\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/opt/python2.6/lib/python2.6/site-packages/psycopg2/__init__.py\", line 69, in <module>\n    from _psycopg import BINARY, NUMBER, STRING, DATETIME, ROWID\nImportError: libpq.so.5: cannot open shared object file: No such file or directory\nHow to troubleshoot this?\npsycopg2 is a python wrapper around the PostgreSQL libraries, so you need those installed on your system too.\nSince you're using CentOS, try this from the command line to install the postgre libs.\nyum install postgresql-libs\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "assembly-merge-strategy issues using sbt-assembly", "id": 462, "answers": [{"answer_id": 471, "document_id": 195, "question_id": 462, "text": "You can see the document here: https://github.com/sbt/sbt-assembly.", "answer_start": 144, "answer_category": null}], "is_impossible": false}], "context": "I am trying to convert a scala project into a deployable fat jar using sbt-assembly. When I run my assembly task in sbt I am getting the error.\nYou can see the document here: https://github.com/sbt/sbt-assembly.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to upgrade from VS 2017 to 2019?", "id": 691, "answers": [{"answer_id": 695, "document_id": 383, "question_id": 691, "text": "In smartgit/bin folder, there's a shell script waiting for you: add-menuitem.sh. It does just that.", "answer_start": 377, "answer_category": null}], "is_impossible": false}], "context": "I have downloaded latest SmartGit installation and each time I want to use it I need to run script smartgit.sh from SmartGit bin directory, this process requires the same repository setup every time.\nWhat it correct way of installing SmartGit on Ubuntu? Thus I can have normal icon and run the program from state of previous usage, without configuring repositories every time.\nIn smartgit/bin folder, there's a shell script waiting for you: add-menuitem.sh. It does just that.  What is the default location of this bin folder for ubuntu 14 users? I couldn't find it in /etc/ my ubuntu 14. My smartgit is working perfectly but I would like to try this setup also.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Running java without installing jre?", "id": 572, "answers": [{"answer_id": 576, "document_id": 297, "question_id": 572, "text": " You can use Launch4j for this. Well documented and easy to use. While the resulting program still needs a JRE to run, you don't have to install the JRE on the target system. You can just copy it with your application and tell Launch4j were to find it or just wrap it up with everything else.", "answer_start": 260, "answer_category": null}], "is_impossible": false}], "context": "As asked and answered here, python has a useful way of deployment without installers. Can Java do the same thing?Is there any way to run Java's jar file without installing jre?Is there a tool something like java2exe (win32), java2bin (linux) or java2app (mac)? You can use Launch4j for this. Well documented and easy to use. While the resulting program still needs a JRE to run, you don't have to install the JRE on the target system. You can just copy it with your application and tell Launch4j were to find it or just wrap it up with everything else.", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Izpack fails with 'There is no script engine for file extension \".js\",", "id": 707, "answers": [{"answer_id": 711, "document_id": 398, "question_id": 707, "text": "Errors and bugs sometimes in programs that are supposed to 'seamlessly' handle both 32- and 64-bit; one example is Symantec's SEP definition repair program - it works sometimes but not all. Your comment confirms these bugs, and you've even identified a competing program that does not error in this 32/64 handling: \"Haven't solved this problem but have worked around it by running the installer using launch4j instead of winrun4j\". ", "answer_start": 903, "answer_category": null}], "is_impossible": false}], "context": "On test machine using Izpack 5 Beta 11 if I start install.jar using a 64bit winrun4j exe running bundled 64 bit java then Izpack complains\nThere is no script engine for file extension \".js\", then complains The installer could not launch with administrator permissions, then an attempt to install into the default installation directory fails because you don't have admin permissions, installation to another folder outside of C:/Program Files completes okay\nWhereas if I run it with 32bit winrun4j installer running 32 bit java it works fine.\nif I just run install.jar directly without the exe wrapper\ni.e java -jar install.jar\nit gives these errors using both 32bit JVM and 64bit JVM.\nso my only working solution at the moment is installation with 32bit exe wrapper, but I also need 64 bit wrapper.\nSo questions are\n1.\tWhy is 32bit exe working and 64 bit exe not working\nI will attempt to answer them:\nErrors and bugs sometimes in programs that are supposed to 'seamlessly' handle both 32- and 64-bit; one example is Symantec's SEP definition repair program - it works sometimes but not all. Your comment confirms these bugs, and you've even identified a competing program that does not error in this 32/64 handling: \"Haven't solved this problem but have worked around it by running the installer using launch4j instead of winrun4j\". Congrats! :)\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "From virtualenv, pip freeze > requirements.txt give TONES of garbage! How to trim it out?", "id": 583, "answers": [{"answer_id": 589, "document_id": 308, "question_id": 583, "text": "That is one thing that has bugged me too quite a bit. This happens when you create a virtualenv without the --no-site-packages flag.", "answer_start": 388, "answer_category": null}], "is_impossible": false}], "context": "I see lots of stuff from my main python installation which is on ubuntu. Which is BAD since Ubuntu use python for quite a few thing itself (ubuntu-one, usb-creator, etc..).\nI do not need them on heroku! I need only Django, psycopg2, and their dependencies. I do not even know if its fault of pip, or virutalenv. (If you want to know my setup look at link above I copied it into terminal)\nThat is one thing that has bugged me too quite a bit. This happens when you create a virtualenv without the --no-site-packages flag.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "What is SubInACL?", "id": 1003, "answers": [{"answer_id": 998, "document_id": 614, "question_id": 1003, "text": "SubInACL is a little-known command line tool from Microsoft, yet it is one of the best tools to work with security permissions in Windows. This tool is capable of changing permissions of files, folders, registry keys, services, printers, cluster shares and various other types of objects", "answer_start": 1300, "answer_category": null}], "is_impossible": false}, {"question": "What is the advantages of SubInACL?", "id": 1004, "answers": [{"answer_id": 999, "document_id": 614, "question_id": 1004, "text": "If used properly, it does not tamper any existing permission.\nCan change permission/ owner to those subfolders where inheritance is disabled.\nCan change permission even if we do not have access to a folder/subfolder.\nOutput log and error log can be enabled. ", "answer_start": 1639, "answer_category": null}], "is_impossible": false}, {"question": "How to install SubInACL?", "id": 1005, "answers": [{"answer_id": 1000, "document_id": 614, "question_id": 1005, "text": "SubInACL is not an inbuilt windows tool, we need to download it from the Microsoft website", "answer_start": 1913, "answer_category": null}], "is_impossible": false}, {"question": "How to make sure that SubInACL is installed?", "id": 1006, "answers": [{"answer_id": 1001, "document_id": 614, "question_id": 1006, "text": "Once we go to that directory and run \u201csubinacl /help\u201d, it should display command syntax and arguments", "answer_start": 2130, "answer_category": null}], "is_impossible": false}, {"question": "How to  configure audit permission on Folders and Files using SubInACL?", "id": 1007, "answers": [{"answer_id": 1002, "document_id": 614, "question_id": 1007, "text": "\nsubinacl /errorlog=\"c:\\temp\\errorlog.txt\" /outputlog=\"c:\\temp\\outputlog.txt\" /file E:\\scripts /sallowdeny=subhro\\awsadmins=F\n \nsubinacl /errorlog=\"c:\\temp\\errorlog.txt\" /outputlog=\"c:\\temp\\outputlog.txt\" /subdirectories \"E:\\scripts\\*.*\" /sallowdeny=subhro\\awsadmins=F\n", "answer_start": 9678, "answer_category": null}], "is_impossible": false}, {"question": "Completely remove an existing ACE from an ACL?", "id": 1008, "answers": [{"answer_id": 1003, "document_id": 614, "question_id": 1008, "text": "subinacl /errorlog=\"c:\\temp\\errorlog.txt\" /outputlog=\"c:\\temp\\outputlog.txt\" /file \"E:\\scripts\" /revoke=subhro\\awsadmins\n \nsubinacl /errorlog=\"c:\\temp\\errorlog.txt\" /outputlog=\"c:\\temp\\outputlog.txt\" /subdirectories \"E:\\scripts\\*.*\" /revoke=subhro\\awsadmins", "answer_start": 7326, "answer_category": null}], "is_impossible": false}], "context": "Introduction\nManaging security permission in a Windows environment is a challenging task.\n\nThe task becomes more complex if any / all of the below scenarios are true:\n\nThere are large folders with size in TBs or GBs, along with numerous sub-folders with a deep folder hierarchy.\nPermission inheritance is disabled in many subfolders and exclusive permissions are defined on those subfolders.\nThe owners of many subfolders have been changed and our account is removed from ACL, so we are not able to see the existing ACL. We are also not able to see that subfolder size (Might be in TBs ! )\nThere are a few thousand users, who are using those folders. \n\nIn this scenario, the first thing to remember is: We will NOT try to change the ACL of the parent folder (or any big subfolder) through GUI. Things will go worse if we try to do so in a complex folder structure, and there will be a huge impact. We have faced this scenario in production for a big folder, and it took 1 week to restore the permission and resolve the issue! For this kind of scenario, we should always go for a command line utility rather than GUI.\n\nAlso, we should enable logging; so that we can refer the log file and know what ACEs got changed. The log file would help us to troubleshoot and mitigate any possible issue.\n\n  Note\nSubInACL is a little-known command line tool from Microsoft, yet it is one of the best tools to work with security permissions in Windows. This tool is capable of changing permissions of files, folders, registry keys, services, printers, cluster shares and various other types of objects. \nSome of the notable advantages of SubInACL are:\n\nIf used properly, it does not tamper any existing permission.\nCan change permission/ owner to those subfolders where inheritance is disabled.\nCan change permission even if we do not have access to a folder/subfolder.\nOutput log and error log can be enabled. \n\nInstallation\n\nSubInACL is not an inbuilt windows tool, we need to download it from the Microsoft website. It is a free tool from Microsoft.\n\nInstallation is simple, and in our lab, we have installed it in \u201cC:\\SubInACL\u201d folder.\n\n \n\nOnce we go to that directory and run \u201csubinacl /help\u201d, it should display command syntax and arguments. This ensures that installation is successful.\n\n \n\nUnderstanding the Syntax\n\nSubInACL is not difficult to use. However, we strongly recommend testing it before executing it in production.\n\n If we see the help menu of the command, we will notice there are three major sections in this command:\n\nSubInAcl <options>  <object_type>  <action>\n\n\u2022 <options> : This is an optional parameter. Here we can enable different kind of logging and verbose mode.\n\n\u2022 <object_type> : This is a mandatory parameter. As mentioned before, SubInAcl can change ACL of folder, file, registry key, service, process etc. Here we are specifying the object type.  \n  \n /file: Stands for a single folder or file. We generally use it to specify the parent folder. \n\n/subdirectories : Stands for subfolders. When applied with <Parent_foldername>\\*.*, it stands for all subfolders and files within the parent folder. \n         \n/service : Stands for a Windows Service      \n/keyreg : Stands for a Registry Key\n\n\n\u2022 <action> : This is a mandatory parameter. This defines the required action on the specified object. \n\n     Ex:  /setowner= domain/user1 : Will set the owner to user1.\n\nSimilarly, there are other actions like Grant, Revoke, Deny etc.\n\nThe type of action depends upon the type of object which we have specified before. Not all actions are valid for all object type. \n\n       \n\n\n\u2191 Back To Top\nCase Studies\nNow that we have the basic understanding of the SubInACL tool, we will do some hands-on and will configure permission based on multiple scenarios.\n\nScenario 1 : Change owner of Folder, Subfolders, and Files\n\nWe have created a folder \u201cE:\\scripts\u201d, which contains other subfolders and files.\n\nPermission inheritance is disabled for one of the subfolder \u201cPowerGroup\u201d. For other subfolders , permission inheritance is enabled.\n\nAt present, the owner of the scripts folder (and all subfolders) is \u201csubhro\\administrator\u201d.\n\n \n\nWe will change the owner to the AD group \u2018AWSAdmins\u201d, for the parent folder as well as for all subfolders. It should also change the owner of the subfolder PowerGroup, where inheritance is disabled.\n\nSo we will execute below command first, which will change the owner to \u201csubhro/awsadmins\u201d on the folder \u201cE:\\scripts\u201d. It will also create output and error log files at the specified location.\n\nsubinacl /errorlog=\"c:\\temp\\errorlog.txt\" /outputlog=\"c:\\temp\\outputlog.txt\" /file \"E:\\scripts\" /setowner=subhro\\awsadmins\n\nPlease note, that above command has not changed the owner of subfolders and files, it has only changed the owner of the parent folder \u201cE:\\Scripts\u201d.\n\nNow, we want to set the owner to subfolders of \u201cE:\\Scripts\u201d. To do that, we need to execute below command:\n\nsubinacl /errorlog=\"c:\\temp\\errorlog.txt\" /outputlog=\"c:\\temp\\outputlog.txt\" /subdirectories \"E:\\scripts\\*.*\" /setowner=subhro\\awsadmins\n\nThis command will change the owner of all subfolders and files within \u201cE:\\scripts\u201d, including the folder \u201cPowerGroup\u201d where permission inheritance is blocked.\n\n \n\n\nScenario 2 : Add permission of Folder, Subfolders, and Files\n\nNow we are going to add an ACE (Access Control Entry) in the ACL (Access Control List) of the folder \u201cE:\\Scripts\u201d, and all its subfolders.\nWe will grant Full Control to the group \u201csubhro\\AwsAdmins\u201d. We will execute below two commands one by one:\n\nsubinacl /errorlog=\"c:\\temp\\errorlog.txt\" /outputlog=\"c:\\temp\\outputlog.txt\" /file \"E:\\scripts\" /grant=subhro\\awsadmins=F\n \nsubinacl /errorlog=\"c:\\temp\\errorlog.txt\" /outputlog=\"c:\\temp\\outputlog.txt\" /subdirectories \"E:\\scripts\\*.*\" /grant=subhro\\awsadmins=F\n\nAs we can see, it has changed ACL for the parent folders, subfolders, and files; including the subfolder where inheritance is disabled.\n \n\nScenario 3 : Change existing permission of Folder, Subfolders, and Files\n\nIf the previous example, we have granted the group \u201csubhro\\AwsAdmins\u201d full control access on the folder \u201cE:\\Scripts\u201d, including subfolders and files.\n\nNow, if we want to change the permission from \u201cFull Control\u201d to \u201cModify\u201d, what should we do?\n\nFirst, let\u2019s run a command subinacl /grant /help. This is a useful command, which lists all permission options for action \u201cGrant\u201d, for each type of object.\n\nUsing this command, we can see that the syntax for \u201cModify\u201d option is \u201cC\u201d.\n\n \n\nSo we will execute below two commands:\n\nsubinacl /errorlog=\"c:\\temp\\errorlog.txt\" /outputlog=\"c:\\temp\\outputlog.txt\" /file \"E:\\scripts\" /grant=subhro\\awsadmins=C\n \nsubinacl /errorlog=\"c:\\temp\\errorlog.txt\" /outputlog=\"c:\\temp\\outputlog.txt\" /subdirectories \"E:\\scripts\\*.*\" /grant=subhro\\awsadmins=C\n\nAs we can see, now the permission \u201cFull Control\u201d has been changed to \u201cModify\u201d for parent folder and all subfolders.\n \n\nIf we examine the log file carefully, we will notice that the command has actually deleted the old permission and then added new permission.\n\n \n\nScenario 4 : Completely remove an existing ACE from an ACL\n\nNow, there is a requirement to remove the entry \u201csubhro\\AWSAdmins\u201d from the permission list of \u201cE:\\Scripts\u201d, including all its subfolders and files.\nSo, instead of the \u201c/grant\u201d switch, we need to use \u201c/revoke\u201d switch here.\n\nsubinacl /errorlog=\"c:\\temp\\errorlog.txt\" /outputlog=\"c:\\temp\\outputlog.txt\" /file \"E:\\scripts\" /revoke=subhro\\awsadmins\n \nsubinacl /errorlog=\"c:\\temp\\errorlog.txt\" /outputlog=\"c:\\temp\\outputlog.txt\" /subdirectories \"E:\\scripts\\*.*\" /revoke=subhro\\awsadmins\n\nThis would revoke the ACE of \u201csubhro\\awsadmins\u201d from the permission list.\n\n \n\n\nScenario 5 : Set Windows Service Permission\n\nSo far, we have changed owner and configured permissions on files and folders. Although file and folder permission is the most common scenario, there are various other scenarios where the SubInACL utility can be used.\n\nSometimes, we need to control who can (or cannot) start or stop specific services. In this example, we are going to configure that.\n\nFirst, we will check the current ACL of the \u201cspooler\u201d service using the below command:\n\nsubinacl /service \"spooler\" /display=dacl > c:\\spool.txt\n\n(We have redirected the output to a text file so that it would be easy to read the output).\n\nHere is the result:\n\n \n\nNow, we are going to add \u201csubhro\\awsadmins\u201d group to this ACL, and this group member should be able to start and stop this service.\n\nSo we will execute below command:\n\nsubinacl /errorlog=\"c:\\temp\\errorlog.txt\" /outputlog=\"c:\\temp\\outputlog.txt\" /service \"spooler\" /GRANT=subhro\\awsadmins=TO\n\nHere, T= Start Service and O=Stop Service. Please use subinacl /grant /help for the complete list.\n\nFrom the output log, we can see that ACL has been updated with the new ACE.\n\n \n\nNow, if we see the ACL once again, we will find that the new ACE has been added with \u201cStart Service\u201d and \u201cStop Service\u201d access.\n\n \n\nIf our requirement is to \u201cdeny\u201d this group to start and stop the spooler services, then we will run below command:\n\nsubinacl /errorlog=\"c:\\temp\\errorlog.txt\" /outputlog=\"c:\\temp\\outputlog.txt\" /service \"spooler\" /deny=subhro\\awsadmins=TO\n\nWe can see the ACL and verify that it has been updated with the \u201cdeny\u201d option.\n\n \n\nScenario 6 : Configure Audit Permission on Folders and Files\n\nThis is another common requirement, to enable \u201cSuccess\u201d or \u201cFailure\u201d audit on a folder, including all subfolders and files.\n\nWe are now going to enable success and failure auditing on the folder \u201cE\\scripts\u201d (with all subfolders) for the group \u201csubhro\\awsadmins\u201d.\n\nAs we can see, at present no auditing is enabled for this folder.\n\n \n\nWe will execute below commands:\n\nsubinacl /errorlog=\"c:\\temp\\errorlog.txt\" /outputlog=\"c:\\temp\\outputlog.txt\" /file E:\\scripts /sallowdeny=subhro\\awsadmins=F\n \nsubinacl /errorlog=\"c:\\temp\\errorlog.txt\" /outputlog=\"c:\\temp\\outputlog.txt\" /subdirectories \"E:\\scripts\\*.*\" /sallowdeny=subhro\\awsadmins=F\n\n\nAs we can see, the Auditing ACL has been updated.\n\n \n\nPlease remember below four switches in this context:\n/salllowdeny will remove all existing ACEs from the Auditing tab, and add an ACE for both Success and Failure events for the mentioned user/group.\n/sgrant will remove all existing ACEs from the Auditing tab, and add an ACE for only \u201cSuccess\u201d events for the mentioned user/group.\n/sdeny will remove all existing ACEs from the Auditing tab, and add an ACE for only \u201cFailure\u201d events for the mentioned user/group.\n/audit will remove all existing ACEs from the Auditing tab.\n\u2191 Back To Top\nSummary\n\nIn this article, we have covered the utility SubInAcl and multiple usage scenarios. There are many other options available in this utility, which can be useful in various scenarios. We will add those scenarios and options in the forthcoming versions of this article.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "What accounts to use when installing SQL Server 2008 Developer", "id": 790, "answers": [{"answer_id": 786, "document_id": 473, "question_id": 790, "text": "If you want to play it safe, make a local account like \"SQLServices\" on your machine, with no special rights or access. Then install SQL and choose that account, and the SQL installer will add only those rights that the service account requires.", "answer_start": 0, "answer_category": null}], "is_impossible": false}], "context": "If you want to play it safe, make a local account like \"SQLServices\" on your machine, with no special rights or access. Then install SQL and choose that account, and the SQL installer will add only those rights that the service account requires. I do it this way so that the service account will have only minimal OS rights.I am installing SQL Server 2008 Developer here, and on the Server Configuration step of the installation it asks me about Service Accounts. What do I choose here?\nI can see the available ones in the screen shot, although on most of them I can only select two or three of those. When I click the Use the same account for all SQL Server 2008 services button I can choose between NT AUTHORITY\\NETWORK SERVICE and NT AUTHORITY\\SYSTEM.\nWhat do I choose here, and why?\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Nodejs application Error: bind EADDRINUSE when use pm2 deploy", "id": 555, "answers": [{"answer_id": 557, "document_id": 280, "question_id": 555, "text": "You need to verify if the port is already took on your system. To do that:\nOn linux: sudo netstat -nltp | grep 3000\nOn OSX: sudo lsof -i -P | grep 3000", "answer_start": 56, "answer_category": null}], "is_impossible": false}], "context": "Express application deploy with pm2\ndatabase is mongodb\nYou need to verify if the port is already took on your system. To do that:\nOn linux: sudo netstat -nltp | grep 3000\nOn OSX: sudo lsof -i -P | grep 3000\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to download java for 64-bit Linux?", "id": 160, "answers": [{"answer_id": 167, "document_id": 98, "question_id": 160, "text": "Go to http://java.com and click on the Download button.\nDownload and check the download file size to ensure that you have downloaded the full, uncorrupted software bundle. Before you download the file, notice its byte size provided on the download page on the web site. Once the download has completed, compare that file size to the size of the downloaded file to make sure they are equal.", "answer_start": 600, "answer_category": null}], "is_impossible": false}, {"question": "How to Change to the directory in which you want to install when installing java for 64-bit Linux?", "id": 161, "answers": [{"answer_id": 168, "document_id": 98, "question_id": 161, "text": "cd directory_path_name\nFor example, to install the software in the /usr/java/ directory, Type:\ncd /usr/java/", "answer_start": 1770, "answer_category": null}], "is_impossible": false}, {"question": "How to Unpack the tarball and install Java when installing java for 64-bit Linux?", "id": 162, "answers": [{"answer_id": 169, "document_id": 98, "question_id": 162, "text": "tar zxvf jre-8u73-linux-x64.tar.gz", "answer_start": 1974, "answer_category": null}], "is_impossible": false}, {"question": "Do I need root to install Java for Linux 64-bit", "id": 163, "answers": [{"answer_id": 170, "document_id": 98, "question_id": 163, "text": "To install Java in a system-wide location such as /usr/local, you must login as the root user to gain the necessary permissions. If you do not have root access, install Java in your home directory or a subdirectory for which you have write permissions.", "answer_start": 1456, "answer_category": null}], "is_impossible": false}], "context": "This article applies to:\nPlatform(s): Oracle Enterprise Linux, Oracle Linux, Red Hat Linux, SLES, SUSE Linux, Ubuntu Linux\nJava version(s): 7.0, 8.0\nLinux System Requirements\nSee supported System Configurations for information about supported platforms, operating systems, desktop managers, and browsers.\n\nNote: For downloading Java other flavors of Linux see Java for Ubuntu Java for Fedora\n\nFollow these steps to download and install Java for Linux.Download and Install\n\nDownload\nThis procedure installs the Java Runtime Environment (JRE) for 64-bit Linux, using an archive binary file (.tar.gz).\n\nGo to http://java.com and click on the Download button.\nDownload and check the download file size to ensure that you have downloaded the full, uncorrupted software bundle. Before you download the file, notice its byte size provided on the download page on the web site. Once the download has completed, compare that file size to the size of the downloaded file to make sure they are equal.\n\n\nInstall\nThe instructions below are for installing version Java 8 Update 73 (8u73). If you are installing another version, make sure you change the version number appropriately when you type the commands at the terminal. Example: For Java 8u79 replace 8u73 with 8u79. Note that, as in the preceding example, the version number is sometimes preceded with the letter u and sometimes it is preceded with an underbar, for example, jre1.8.0_73.\n\nNote about root access: To install Java in a system-wide location such as /usr/local, you must login as the root user to gain the necessary permissions. If you do not have root access, install Java in your home directory or a subdirectory for which you have write permissions.\n\nChange to the directory in which you want to install. Type:\ncd directory_path_name\nFor example, to install the software in the /usr/java/ directory, Type:\ncd /usr/java/\n\nMove the .tar.gz archive binary to the current directory.\nUnpack the tarball and install Java\ntar zxvf jre-8u73-linux-x64.tar.gz\n\nThe Java files are installed in a directory called jre1.8.0_73 in the current directory. In this example, it is installed in the /usr/java/jre1.8.0_73 directory. When the installation has completed, you will see the word Done.\nDelete the .tar.gz file if you want to save disk space.", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Ruby Gemspec Dependency: Is possible have a git branch dependency?", "id": 1885, "answers": [{"answer_id": 1871, "document_id": 1456, "question_id": 1885, "text": "If you want to do this for your own internal projects, my suggestion would be to use Bundler which supports this quite well.", "answer_start": 215, "answer_category": null}], "is_impossible": false}], "context": "Is possible have a git branch dependency, inside mygem.gemspec?\nI'm thinking something similar to the following:\ngem.add_runtime_dependency 'oauth2', :git => 'git@github.com:lgs/oauth2.git'\n... but it doesn't work.\nIf you want to do this for your own internal projects, my suggestion would be to use Bundler which supports this quite well.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "git clone from local to remote.", "id": 523, "answers": [{"answer_id": 525, "document_id": 248, "question_id": 523, "text": "To answer your first question, yes, you can. Suppose the remote directory is ssh://user@host/home/user/repo. This must be a git repository, create that with git init --bare or scp your local repo.git (can be created with git clone) directory to remote.\nTo get to your next question, you should be able to do the same thing by using a different set of commands.", "answer_start": 294, "answer_category": null}], "is_impossible": false}], "context": "We're in the process of migrating from Mercurial to Git for our workflow and I have two minor issues.\nFirst, is it possible to \"clone\" a local repository directly into an empty remote (ssh) dir?\nIs it possible to have basically the same setup without separating the working dir from the repos?\nTo answer your first question, yes, you can. Suppose the remote directory is ssh://user@host/home/user/repo. This must be a git repository, create that with git init --bare or scp your local repo.git (can be created with git clone) directory to remote.\nTo get to your next question, you should be able to do the same thing by using a different set of commands.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How can I make the SourcePath property of a file in a Visual Studio Setup and Deployment project (Windows Installer) relative rather than absolute?", "id": 1312, "answers": [{"answer_id": 1302, "document_id": 881, "question_id": 1312, "text": "It might be easier now but when you start bumping into the limitations of the tool it's going to get real hard. Let's not even talk about the bad practices it will encourage which could end up being real hard for the poor end user installing your product. You've got Visual Studio 2010 so InstallShield LE ( free ) would be a better choice.", "answer_start": 813, "answer_category": null}], "is_impossible": false}], "context": "I've got a relatively simple project that is under source control (svn), and I wanted to create an installer. I know that I could (should) use WiX, but as I'm new to creating installers I thought it'd be easier to just use the built-in Visual Studio (2010) Setup and Deployment Wizard.\nUnfortunately, it seems that files including external (non-project maintained) documentation, configuration files, and \"Content\" files are added with absolute paths. This, of course, is suboptimal. I searched the web, but found only the same question, without an answer. Another stackoverflow user seems to have asked a similar question, but the only answer, which suggests ClickOnce, seems off-base (I'd like to have an MSI that I distribute not a web-based installation).\nDoes anyone know how (or whether) this can be fixed?\nIt might be easier now but when you start bumping into the limitations of the tool it's going to get real hard. Let's not even talk about the bad practices it will encourage which could end up being real hard for the poor end user installing your product. You've got Visual Studio 2010 so InstallShield LE ( free ) would be a better choice.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "inno setup use program files directory on both 32bit 64bit systems with pf", "id": 1917, "answers": [{"answer_id": 1904, "document_id": 1488, "question_id": 1917, "text": "Source: \"MyDll32.dll\"; DestDir: \"{pf32}\\My Program\"; Check: not IsWin64\nSource: \"MyDll64.dll\"; DestDir: \"{pf64}\\My Program\"; Check: IsWin64", "answer_start": 994, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nThe constant {pf} is the directory of \n\n\n  C:\\Program Files\n\n\nfor 32bit systems and\n\n\n  C:\\Program Files (x86)\n\n\nfor 64bit systems.\n\nHowever I want to use the directory\n\n\n  C:\\Program Files\n\n\nfor both, 32 and 64bit systems. How can I achieve this?\n    \n\nUse a scripted constant like:\n[Setup]\nDefaultDirName={code:GetProgramFiles}\\My Program\n\n[Code]\n\nfunction GetProgramFiles(Param: string): string;\nbegin\n  if IsWin64 then Result := ExpandConstant('{commonpf64}')\n    else Result := ExpandConstant('{commonpf32}')\nend;\n\nThough this approach should only be used, if you generate binaries for the respective platform on the fly. Like in your case, if understand correctly, you compile the Java binaries for the respective architecture.\n\nYou can also use 64-bit install mode.\nThen you can simply use {autopf} constant (previously {pf}):\n[Setup]\nDefaultDirName={autopf}\\My Program\n\n\nIf you have a separate 32-bit and 64-bit binaries in the installer, use a script like:\n[Files]\nSource: \"MyDll32.dll\"; DestDir: \"{pf32}\\My Program\"; Check: not IsWin64\nSource: \"MyDll64.dll\"; DestDir: \"{pf64}\\My Program\"; Check: IsWin64\n\nSee also:\n\nIs it possible to set the install mode in Inno Setup (32 or 64 bit)?\nInno Setup 32bit and 64bit dll installation\n\n    \n\nIf you are using a single installer for both 64 and 32 bit installs then you should be using the ArchitecturesInstallIn64BitMode Setup Directive. This will change {pf} and other scripted constants to be their 64 bit version when installing on a 64 bit system, and their 32 bit versions when installing on a 32 bit system.\n\nYou will obviously also want to use a Check as in Martin's example to make sure you are only installing the correct binaries.\n\nEx:\n\n#define MyAppName \"MyAwesomeApp\"\n[Setup]\nArchitecturesInstallIn64BitMode=x64\nAppName={#MyAppName}\nDefaultDirname={pf}\\{#MyAppName}\n\n[Files]\nSource: \"MyApp_32bit.exe\"; DestDir: \"{app}\"; Check not Is64BitinstallMode;\nSource: \"MyApp_64bit.exe\"; DestDir: \"{app}\"; Check Is64BitinstallMode;\n\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Restarting Unicorn with USR2 doesn't seem to reload production.rb settings", "id": 1114, "answers": [{"answer_id": 1106, "document_id": 691, "question_id": 1114, "text": "My guess is that your unicorns are being restarted in the old production directory rather than the new production directory -- in other words, if your working directory in unicorn.rb is <capistrano_directory>/current, you need to make sure the symlink happens before you attempt to restart the unicorns.", "answer_start": 412, "answer_category": null}], "is_impossible": false}], "context": "I'm running unicorn and am trying to get zero downtime restarts working.\nSo far it is all awesome sauce, the master process forks and starts 4 new workers, then kills the old one, everyone is happy.\nAfter restarting unicorn in this way the asset host is still pointing to the old release. Doing a real restart (ie: stop the master process, then start unicorn again from scratch) picks up the new config changes.\nMy guess is that your unicorns are being restarted in the old production directory rather than the new production directory -- in other words, if your working directory in unicorn.rb is <capistrano_directory>/current, you need to make sure the symlink happens before you attempt to restart the unicorns.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "LocalDB deployment on client PC", "id": 811, "answers": [{"answer_id": 806, "document_id": 493, "question_id": 811, "text": "You don't need to install SQL Server Express to use LocalDB, as LocalDB is SQL Server Express, just easier to install.\nOnce LocalDB is installed you can use AttachDbFileName property of the connection string to \"open\" an MDF file. Keep in mind that the same file can only be opened by a single LocalDB instance (single Windows login) at any given time, so this is not a data-sharing feature.", "answer_start": 358, "answer_category": null}], "is_impossible": false}], "context": "I am very intrigued by this new version of SQL Server Express.\nIt's not clear (to me) what a setup program should do to deploy an application that use a LocalDB.\nIs it required to install SQL Server Express on the client PC and then attach the MDF file?\nOr it's only required to run the LocalDB.msi and it works as a standalone file like SQL Server Compact? You don't need to install SQL Server Express to use LocalDB, as LocalDB is SQL Server Express, just easier to install.\nOnce LocalDB is installed you can use AttachDbFileName property of the connection string to \"open\" an MDF file. Keep in mind that the same file can only be opened by a single LocalDB instance (single Windows login) at any given time, so this is not a data-sharing feature.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "What language does capistrano use?", "id": 971, "answers": [{"answer_id": 966, "document_id": 597, "question_id": 971, "text": "Capistrano is written in Ruby", "answer_start": 382, "answer_category": null}], "is_impossible": false}], "context": "A remote server automation and deployment tool written in Ruby.\nA Simple Task\nrole :demo, %w{example.com example.org example.net}\ntask :uptime do\n  on roles(:demo), in: :parallel do |host|\n    uptime = capture(:uptime)\n    puts \"#{host.hostname} reports: #{uptime}\"\n  end\nend\nCapistrano extends the Rake DSL with methods specific to running commands on() servers.\n\nFor Any Language\nCapistrano is written in Ruby, but it can easily be used to deploy any language.\n\nIf your language or framework has special deployment requirements, Capistrano can easily be extended to support them.\n\nSource Code\ncapistrano / capistrano\n12010 1779\nRemote multi-server automation tool \u2014 Read More\n\nhttp://www.capistranorb.com\n\nLatest commit to the master branch on 6-20-2021", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "installing opencv with brew never finishes", "id": 1445, "answers": [{"answer_id": 1434, "document_id": 1018, "question_id": 1445, "text": " got annoyed and killed it.\n\nSolution:\nUse the brew bottle:\n    brew install gcc --force-bottle.\nIt works a tr", "answer_start": 2173, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nSo I'm trying to install opencv using Homebrew but it isn't working. I used brew tap homebrew/science and then brew install opencv\nWhat happens is:\n\n==&gt; Installing opencv from homebrew/homebrew-science\n==&gt; Installing dependencies for homebrew/science/opencv: gcc, eigen, jpeg, libpng, libtiff, ilmbase, openexr, homebrew/python/numpy\n==&gt; Installing homebrew/science/opencv dependency: gcc\n==&gt; Downloading http://ftpmirror.gnu.org/gcc/gcc-5.1.0/gcc-5.1.0.tar.bz2\nAlready downloaded: /Library/Caches/Homebrew/gcc-5.1.0.tar.bz2\n==&gt; Patching\npatching file gcc/jit/Make-lang.in\n==&gt; ../configure --build=x86_64-apple-darwin13.4.0 --prefix=/usr/local/Cellar/gcc/5.1.0 --libdir=/usr/local/Cellar/gcc/5.1.0/lib/gcc/5 --enable-langua\n==&gt; make bootstrap\n\n\nAnd then it just doesn't stop, I've run it for close to an hour. The task on the top of the terminal window (you know, where it says bash generally) keeps rapidly changing, often to things like \"ruby\" but nothing gets outputted after this point.\n\nAny ideas? Thanks.\n    \n\nNote that it's actually compiling GCC at that point, which is expected to take a long time. Homebrew does provide pre-built binary bottles by default, so it's curious those aren't being used. Is your environment set to build everything from source? You could try brew install gcc --force-bottle\n    \n\nI came across this question with the same problem - \n\nbrew tap homebrew/science\nbrew install opencv\n\n\nstarted installing a bunch of dependencies, which worked great until gcc started, where I got:\n\n==&gt; Installing homebrew/science/opencv dependency: gcc\n==&gt; Downloading http://ftpmirror.gnu.org/gcc/gcc-5.2.0/gcc-5.2.0.tar.bz2\n==&gt; Downloading from http://gnu.mirror.iweb.com/gcc/gcc-5.2.0/gcc-5.2.0.tar.bz2\n######################################################################## 100.0%\n==&gt; Patching\npatching file gcc/jit/Make-lang.in\npatching file gcc/jit/jit-playback.c\nHunk #1 succeeded at 2459 with fuzz 2 (offset 43 lines).\n==&gt; ../configure --build=x86_64-apple-darwin15.0.0 --prefix=/usr/local/Cellar/gcc/5.2.0 --libdir=/usr/loc\n==&gt; make bootstrap\n\n\nThis process went on for 3 hours before I got annoyed and killed it.\n\nSolution:\nUse the brew bottle:\n    brew install gcc --force-bottle.\nIt works a treat but does give the following caveat:\n\nGCC has been built with multilib support. Notably, OpenMP may not work:\n  https://gcc.gnu.org/bugzilla/show_bug.cgi?id=60670\nIf you need OpenMP support you may want to\n  brew reinstall gcc --without-multilib\n\n\nI haven't run into any problems with OpenMP yet. Hope this helps somebody else. \nRunning MBP 13\" (Late 2011) with OSX El Capitain. (It's old, which probably explains the lengthy makes). Credit to @IanLancaster for getting the solution first, but I thought I would elaborate with the caveats.\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "start application after installation using wix burn", "id": 1498, "answers": [{"answer_id": 1487, "document_id": 1077, "question_id": 1498, "text": "strapper. --&gt;\n    &lt;MsiPackage SourceFile=\"..\\PostInstall\\bin\\Release\\PostInstall.msi\"\n                InstallCondition=\"(ProductAInstalled) OR (ProductBInstalled)\" /&gt;\n&lt;/Chain&gt;\n\n    \n\nUse the advice given in the WiX manual, How To: Run the Installed Application After Setup. There is a built-in WiX extension that will", "answer_start": 4553, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI'm aware of similar questions in WiX MSI, but I'm having issues starting an application within a bootstrapper EXE file created with Burn after the installation. My full bundle is below.\n\nIf it makes any difference to the scenario, the bootstrapper is started in passive mode, so the user shouldn't need to press anything.\n\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;Wix xmlns=\"http://schemas.microsoft.com/wix/2006/wi\"\n     xmlns:bal=\"http://schemas.microsoft.com/wix/BalExtension\"\n     xmlns:util=\"http://schemas.microsoft.com/wix/UtilExtension\"&gt;\n\n    &lt;Bundle Name=\"My Company AutoUpdater\"\n            Version=\"1.0.11\"\n            Manufacturer=\"My Company\"\n            UpgradeCode=\"--GUID--\"&gt;\n\n        &lt;BootstrapperApplicationRef Id=\"WixStandardBootstrapperApplication.HyperlinkLicense\"&gt;\n\n            &lt;bal:WixStandardBootstrapperApplication SuppressOptionsUI=\"yes\"\n                                                    LicenseUrl=\"\"\n                                                    LogoFile=\"logo.png\" /&gt;\n        &lt;/BootstrapperApplicationRef&gt;\n\n        &lt;Chain&gt;\n            &lt;MsiPackage SourceFile=\"..\\App1\\bin\\Release\\App1.msi\" /&gt;\n            &lt;MsiPackage SourceFile=\"..\\App2\\bin\\Release\\App2.msi\" /&gt;\n        &lt;/Chain&gt;\n    &lt;/Bundle&gt;\n\n    &lt;Fragment&gt;\n        &lt;Property Id=\"WixShellExecTarget\" \n                  Value=\"[#C:\\Program Files (x86)\\My Company\\App1.exe]\" /&gt;\n\n        &lt;Binary Id=\"MyCA\"\n                SourceFile=\"[#C:\\Program Files (x86)\\My Company\\App1.exe]\"/&gt;\n\n            &lt;CustomAction Id=\"LaunchApplication\"\n                          BinaryKey=\"MyCA\"\n                          ExeCommand=\"-switch\"\n                          Execute=\"deferred\"\n                          Return=\"check\"\n                          HideTarget=\"no\"\n                          Impersonate=\"no\" /&gt;\n\n            &lt;InstallExecuteSequence&gt;\n                &lt;Custom Action=\"LaunchApplication\" \n                        After=\"InstallFiles\" /&gt;\n            &lt;/InstallExecuteSequence&gt;\n    &lt;/Fragment&gt;\n&lt;/Wix&gt;\n\n    \n\nYou can add a variable to your Bundle called \"LaunchTarget\" with a path to the executable you want to run:\n\n&lt;Variable Name=\"LaunchTarget\" Value=\"[InstallFolder]\\path\\to\\file.exe\"/&gt;\n\n\nAfter the install, the Setup Successful screen will display a \"Launch\" button that will start your app.\n    \n\nIt had a number of steps. Remember I was running this from a bootstrapper, not an MSI file, whereby levarius's answer would have sufficed.\n\nBasically, I removed any of the launch logic that was posted in the original question, and created a new package, whose sole functionality was to kick off an application (using a custom action), and whose location had previously been saved in the registry - that is, the application running when it found that an update was available, set this item in the registry.\n\nThe package (called PostInstall below) is then run ONLY if one of the other packages has been installed previously - found by the existence of a key in the registry (set in each product's MSI). This means that no application will be started automatically after a new install has completed.\n\nThe following is from the bootstrapper bundle (WiX 3.6 in my case)\n\n&lt;!-- Determine what items are installed in the event of an upgrade--&gt;\n&lt;util:RegistrySearch Root=\"HKLM\"\n                     Key=\"SOFTWARE\\CompanyName\"\n                     Value=\"ProductAInstalled\"\n                     Variable=\"ProductAInstalled\"\n                     Result=\"exists\"\n                     Format=\"raw\" /&gt;\n&lt;util:RegistrySearch Root=\"HKLM\"\n                     Key=\"SOFTWARE\\CompanyName\"\n                     Value=\"ProductBInstalled\"\n                     Variable=\"ProductBInstalled\"\n                     Result=\"exists\"\n                     Format=\"raw\" /&gt;\n\n&lt;Chain&gt;\n    &lt;!-- Package for .NET prerequisite. References a Package that is\n         actually in the referenced WiX file WixNetFxExtension. --&gt;\n    &lt;PackageGroupRef Id=\"NetFx40Web\"/&gt;\n\n    &lt;MsiPackage SourceFile=\"..\\SetupProductA\\bin\\Release\\SetupProductA.msi\"\n                InstallCondition=\"(chkProductA) OR (ProductAInstalled)\" /&gt;\n\n    &lt;MsiPackage SourceFile=\"..\\SetupProductB\\bin\\Release\\SetupProductB.msi\"\n                InstallCondition=\"(chkProductB) OR (ProductBInstalled)\" /&gt;\n\n    &lt;!-- Run PostInstall only if this was run as part of an upgrade. --&gt;\n    &lt;!-- NB: This is the portion that kicks off the downloaded bootstrapper. --&gt;\n    &lt;MsiPackage SourceFile=\"..\\PostInstall\\bin\\Release\\PostInstall.msi\"\n                InstallCondition=\"(ProductAInstalled) OR (ProductBInstalled)\" /&gt;\n&lt;/Chain&gt;\n\n    \n\nUse the advice given in the WiX manual, How To: Run the Installed Application After Setup. There is a built-in WiX extension that will handle this for you. You should be able to reference the WiX Util extension, add the following code to your project (replacing the value of the property of course), then schedule the action to run:\n\n&lt;Property Id=\"WixShellExecTarget\" \n          Value=\"[#myapplication.exe]\" /&gt;\n&lt;CustomAction Id=\"LaunchApplication\" \n              BinaryKey=\"WixCA\" \n              DllEntry=\"WixShellExec\" \n              Impersonate=\"yes\" /&gt;\n\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "I want to install JDK 8 on Oracle Solaris 11 using IPS packages, how can I know whether the package is available from your IPS publisher?", "id": 185, "answers": [{"answer_id": 192, "document_id": 107, "question_id": 185, "text": "$ pkg list -a jdk-8\n\nNAME (PUBLISHER)        VERSION                    IFO\ndeveloper/java/jdk-8    1.8.0.0-0.183.0.0.0.0.0    ---\nIf you see an \"i\" in the I column, then the package is already installed.", "answer_start": 1524, "answer_category": null}], "is_impossible": false}, {"question": "I want to install JDK 8 on Oracle Solaris 11 using IPS packages, If I prefer to execute a privileged command., what shall I do?", "id": 187, "answers": [{"answer_id": 194, "document_id": 107, "question_id": 187, "text": "$ sudo pkg install jdk-8", "answer_start": 2558, "answer_category": null}], "is_impossible": false}, {"question": "I want to install JDK 8 on Oracle Solaris 11 using IPS packages, If I have the Software Installation rights profile, what shall I do?", "id": 186, "answers": [{"answer_id": 193, "document_id": 107, "question_id": 186, "text": "$ pfexec pkg install jdk-8", "answer_start": 2279, "answer_category": null}], "is_impossible": false}, {"question": "I want to install JDK 8 on Oracle Solaris 11 using IPS packages, If I have the root role, what shall I do ?", "id": 188, "answers": [{"answer_id": 195, "document_id": 107, "question_id": 188, "text": "# pkg install jdk-8", "answer_start": 2752, "answer_category": null}], "is_impossible": false}, {"question": "how to install IPS packages when install JDK8 on solaris 11?", "id": 189, "answers": [{"answer_id": 196, "document_id": 107, "question_id": 189, "text": "Use the profiles command to list the rights profiles that are assigned to you. If you have the Software Installation rights profile, you can use the pfexec command to install and update packages.\n\n$ pfexec pkg install jdk-8\nOther rights profiles also provide installation privilege, such as System Administrator rights profile.\n\nDepending on the security policy at your site, you might be able to use the sudo command with your user password to execute a privileged command.\n\n$ sudo pkg install jdk-8\nUse the roles command to list the roles that are assigned to you. If you have the root role, you can use the su command with the root password to assume the root role.\n\n# pkg install jdk-8", "answer_start": 2082, "answer_category": null}], "is_impossible": false}, {"question": "which file shall I download for installing JDK 8 release on the Oracle Solaris platform?", "id": 190, "answers": [{"answer_id": 197, "document_id": 107, "question_id": 190, "text": "jdk-8uversion-solaris-sparcv9.tar.gz\t64-bit SPARC\tanyone\njdk-8uversion-solaris-x64.tar.gz\t64-bit x64, EM64T\tanyone\njdk-8uversion-solaris-sparcv9.tar.Z\t64-bit SPARC\troot\njdk-8uversion-solaris-x64.tar.Z\t64-bit x64, EM64T\troot", "answer_start": 2972, "answer_category": null}], "is_impossible": false}, {"question": "how to install JDK 8 Manually using  Oracle Solaris Archive Binaries (.tar.gz)?", "id": 191, "answers": [{"answer_id": 198, "document_id": 107, "question_id": 191, "text": "Download the bundle.\n\nBefore the file can be downloaded, you must accept the license agreement. The archive binaries can be installed by anyone in any location that you can write to.\n\nThe .tar.gz archive file (also called a tarball) is a file that can be simultaneously uncompressed and extracted in one step.\n\nChange directory to the location where you want the JDK to be installed.\n\nMove the .tar.gz archive binaries to the current directory.\n\nUnpack the tarball and install the JDK:\n\nOn SPARC processors:\n\n% gzip -dc jdk-8uversion-solaris-sparcv9.tar.gz | tar xf -\nOn x64/EM64T processors:\n\n% gzip -dc jdk-8uversion-solaris-x64.tar.gz | tar xf -", "answer_start": 4410, "answer_category": null}], "is_impossible": false}, {"question": "how to install JDK 8 Manually using  Oracle Solaris Archive Binaries (.tar.z)?", "id": 192, "answers": [{"answer_id": 199, "document_id": 107, "question_id": 192, "text": "Create a new directory to save the download bundle in, and change to that directory.\n\nDownload the bundle.\n\nBefore the file can be downloaded, you must accept the license agreement.\n\nExtract the contents of the compressed tar files:\n\nOn SPARC processors:\n\n% zcat jdk-8uversion-solaris-sparcv9.tar.Z | tar xf -\nOn x64/EM64T processors:\n\n% zcat jdk-8uversion-solaris-x64.tar.Z | tar xf -\nThe first command creates a number of directories (SUNWj8rt, SUNWj8dev, SUNWj8cfg, SUNWj8man, and SUNWj8jmp) plus a few files in the current directory.\n\nAssume the root role.\n\nYou can use the roles(1) command to determine whether you are able to assume the root role.\n\nUninstall any earlier installation of the JDK packages.\n\nIf your machine has an earlier 32-bit or 64-bit version of the JDK installed in the default location (/usr/jdk/jdk1.<major version>.0_<minor version>), you must uninstall it before installing a later version at that location.\n\nYou can skip this step if you intend to install the JDK in a non-default location. For details, see \"Selecting the Default Java Platform\".\n\nRun the pkgadd command to install the packages.\n\n# pkgadd -d . SUNWj8rt SUNWj8dev SUNWj8cfg SUNWj8man \nThe command installs the JDK into /usr/jdk/jdk1.8.0_version.\n\nSee the pkgadd(1) and admin(4) man pages for information on installing the JDK in a non-default location.\n\nJapanese users: Install man pages.\n\nIf your machine has an earlier version of the Japanese man pages already installed in usr/jdk/jdk1.8.0_version, you must uninstall that package before installing this version of the Japanese man pages at that location. Remove that package by running:\n\n# pkgrm SUNWj8jmp\nThen run the pkgadd command to install the new Japanese man page package.\n\n# pkgadd -d . SUNWj8jmp\nTo save space, delete the tar files and extracted SUNW* directories.\n\nExit the root role.\n\nNo need to reboot.", "answer_start": 5769, "answer_category": null}], "is_impossible": false}, {"question": " how to determine the default version of the java executable on a Oracle Solaris system", "id": 193, "answers": [{"answer_id": 200, "document_id": 107, "question_id": 193, "text": "% /usr/java/bin/java -fullversion", "answer_start": 8216, "answer_category": null}], "is_impossible": false}], "context": "\nThis page has these topics:\n\n\"System Requirements\"\n\n\"Installation Instructions Notation\"\n\n\"JDK 8 Installation Instructions for Oracle Solaris 11 using IPS packages\"\n\n\"Manual JDK 8 Installation Instructions\"\n\n\"Selecting the Default Java Platform\"\n\nSee \"JDK 8 and JRE 8 Installation Start Here\" for general information about installing JDK 8 and JRE 8.\n\nFor information on enhancements to JDK 8 that relate to the installer, see \"Installer Enhancements in JDK 8\".\n\nSystem Requirements\nThis version of the JDK is supported on the Oracle Solaris 10 Update 9 or later OS, Oracle Solaris 11 Express OS, and Oracle Solaris 11 OS. For supported processors and browsers, see http://www.oracle.com/technetwork/java/javase/certconfig-2095354.html.\n\nInstallation Instructions Notation\nFor any text on this page containing the following notation, you must substitute the appropriate JDK update version number for the notation.\n\nversion\nFor example, if you are installing update JDK 8 update release 1, the following string representing the name of the bundle:\n\njdk-8uversion-solaris-sparc.tar.gz\nbecomes:\n\njdk-8u1-solaris-sparc.tar.gz\nNote that, as in the preceding example, the version number is sometimes preceded with the letter u, for example, 8u1, and sometimes it is preceded with an underscore, for example, jdk1.8.0_01.\n\nJDK 8 Installation Instructions for Oracle Solaris 11 using IPS packages\nTo install JDK 8 on Oracle Solaris 11, install the jdk-8 package:\n\nMake sure the jdk-8 package is available from your IPS publisher.\n\n$ pkg list -a jdk-8\n\nNAME (PUBLISHER)        VERSION                    IFO\ndeveloper/java/jdk-8    1.8.0.0-0.183.0.0.0.0.0    ---\nIf you see an \"i\" in the I column, then the package is already installed.\n\nThis package is available from the solaris publisher at pkg.oracle.com and also from other publisher origins. If you see a message that no such package is found, use the pkg publisher command to check your publisher origin and contact your system administrator or Oracle Support representative.\n\nMake sure you have permission to install IPS packages.\n\nUse the profiles command to list the rights profiles that are assigned to you. If you have the Software Installation rights profile, you can use the pfexec command to install and update packages.\n\n$ pfexec pkg install jdk-8\nOther rights profiles also provide installation privilege, such as System Administrator rights profile.\n\nDepending on the security policy at your site, you might be able to use the sudo command with your user password to execute a privileged command.\n\n$ sudo pkg install jdk-8\nUse the roles command to list the roles that are assigned to you. If you have the root role, you can use the su command with the root password to assume the root role.\n\n# pkg install jdk-8\nManual JDK 8 Installation Instructions\nThe following table lists the options available for downloading the JDK 8 release on the Oracle Solaris platform.\n\nDownload File(s)\tArchitecture\tWho Can Install\njdk-8uversion-solaris-sparcv9.tar.gz\t64-bit SPARC\tanyone\njdk-8uversion-solaris-x64.tar.gz\t64-bit x64, EM64T\tanyone\njdk-8uversion-solaris-sparcv9.tar.Z\t64-bit SPARC\troot\njdk-8uversion-solaris-x64.tar.Z\t64-bit x64, EM64T\troot\nInstallation instructions are by file type:\n\n.tar.gz files: See \"Installation of Oracle Solaris Archive Binaries (.tar.gz)\". This technique allows you to install a private version of the JDK for the current user into any location, without affecting other JDK installations. However, it may involve manual steps to get some of the features to work (for example, the -version:release option of the java command which allows you to specify the release to be used to run the specified class requires the correct path to the JDK release under /usr/jdk).\n\n.tar.Z files: See \"Installation of Oracle Solaris SVR4 Packages (.tar.Z)\". This technique allows you to perform a system-wide installation of the JDK for all users, and requires root access. Note that this is a legacy install option. See \"JDK 8 Installation Instructions for Oracle Solaris 11 using IPS packages\" for the recommended approach.\n\nInstallation of Oracle Solaris Archive Binaries (.tar.gz)\nYou can install a JDK archive binary in any location that you can write to. It will not displace the system version of the Java platform provided by the Oracle Solaris OS. These instructions install a private version of the JDK.\n\nFollow these steps to install:\n\nDownload the bundle.\n\nBefore the file can be downloaded, you must accept the license agreement. The archive binaries can be installed by anyone in any location that you can write to.\n\nThe .tar.gz archive file (also called a tarball) is a file that can be simultaneously uncompressed and extracted in one step.\n\nChange directory to the location where you want the JDK to be installed.\n\nMove the .tar.gz archive binaries to the current directory.\n\nUnpack the tarball and install the JDK:\n\nOn SPARC processors:\n\n% gzip -dc jdk-8uversion-solaris-sparcv9.tar.gz | tar xf -\nOn x64/EM64T processors:\n\n% gzip -dc jdk-8uversion-solaris-x64.tar.gz | tar xf -\nThe JDK is installed in a directory called jdk1.8.0_version in the current directory. For example, for the JDK 8 update 1 release, the directory would be named: jdk1.8.0_01.\n\nThe JDK documentation is a separate download. See http://www.oracle.com/technetwork/java/javase/downloads/index.html#docs.\n\nInstallation of Oracle Solaris SVR4 Packages (.tar.Z)\nUse these instructions if you want to use the pkgadd utility to install the JDK. This technique allows all users on your system to access Java.\n\nIf you do not have root access to your Oracle Solaris system, see \"JDK 8 Installation Instructions for Oracle Solaris 11 using IPS packages\" to install a private copy of the JDK.\n\nFollow these steps to install:\n\nCreate a new directory to save the download bundle in, and change to that directory.\n\nDownload the bundle.\n\nBefore the file can be downloaded, you must accept the license agreement.\n\nExtract the contents of the compressed tar files:\n\nOn SPARC processors:\n\n% zcat jdk-8uversion-solaris-sparcv9.tar.Z | tar xf -\nOn x64/EM64T processors:\n\n% zcat jdk-8uversion-solaris-x64.tar.Z | tar xf -\nThe first command creates a number of directories (SUNWj8rt, SUNWj8dev, SUNWj8cfg, SUNWj8man, and SUNWj8jmp) plus a few files in the current directory.\n\nAssume the root role.\n\nYou can use the roles(1) command to determine whether you are able to assume the root role.\n\nUninstall any earlier installation of the JDK packages.\n\nIf your machine has an earlier 32-bit or 64-bit version of the JDK installed in the default location (/usr/jdk/jdk1.<major version>.0_<minor version>), you must uninstall it before installing a later version at that location.\n\nYou can skip this step if you intend to install the JDK in a non-default location. For details, see \"Selecting the Default Java Platform\".\n\nRun the pkgadd command to install the packages.\n\n# pkgadd -d . SUNWj8rt SUNWj8dev SUNWj8cfg SUNWj8man \nThe command installs the JDK into /usr/jdk/jdk1.8.0_version.\n\nSee the pkgadd(1) and admin(4) man pages for information on installing the JDK in a non-default location.\n\nJapanese users: Install man pages.\n\nIf your machine has an earlier version of the Japanese man pages already installed in usr/jdk/jdk1.8.0_version, you must uninstall that package before installing this version of the Japanese man pages at that location. Remove that package by running:\n\n# pkgrm SUNWj8jmp\nThen run the pkgadd command to install the new Japanese man page package.\n\n# pkgadd -d . SUNWj8jmp\nTo save space, delete the tar files and extracted SUNW* directories.\n\nExit the root role.\n\nNo need to reboot.\n\nSelecting the Default Java Platform\nThis topic describes how the default Java platform is selected when running the Oracle Solaris SVR4 package installation (via the pkgadd command) of the JDK.\n\nDefault Java Platform\nSeveral versions of the Java platform can be present simultaneously on a Oracle Solaris system (using the default Oracle Solaris package installations), but only one can be the \"default\" Java platform. The default Java platform is defined by the directory that the /usr/java symbolic link points to. To determine the default version of the java executable, run:\n\n% /usr/java/bin/java -fullversion\nThe /usr/java symbolic link can change the default Java platform because there are symbolic links in /usr/bin (also known as /bin) that use it. (For example, the /usr/bin/java link refers to /usr/java/bin/java, which is the Java Runtime Environment). Many Java applications are compatible with later versions of the Java platform, but some applications might be less compatible.\n\nPATH Setting\nThe default Java is linked through /usr/bin, such as /usr/bin/java. If this is in the path before another version of Java is in the path, then that will be the version of Java run from the command line or from any other tool that uses the PATH environment variable to locate Java.\n\nContents    Previous    Next\nCopyright \u00a9 1993, 2021, Oracle and/or its affiliates. All rights reserved. | Cookie \u559c\u597d\u8bbe\u7f6e | Ad Choices.Contact Us", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "installation of oracle database 11g express edition on ubuntu 12 04 1 lts", "id": 2002, "answers": [{"answer_id": 1988, "document_id": 1588, "question_id": 2002, "text": ".\n\nsudo rm -rf /dev/shm\nsudo mkdir /dev/shm\nsudo mount -t tmpfs shmfs -o size=2048m /dev/s", "answer_start": 2204, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI have been struggling to install Oracle 11g Express Edition on Ubuntu 12.04.1 LTS version.\n\nI have followed these steps:\n\n\nDownloaded 11g express edition from Oracle's site\nCreated a new user 'oracle' under the group dba. Following steps are executed using this.\nunzip oracle-xe-11.2.0-1.0.x86_64.rpm.zip and then Converted the rpm to the Ubuntu package\n\nsudo alien --scripts -d oracle-xe-11.2.0-1.0.x86_64.rpm\n\nCreated /sbin/chkconfig file and added the entries as specified here\nCreated /etc/sysctl.d/60-oracle.conf and added the entries as specified in same link as above.\nBelow steps:\n\n\nln -s /usr/bin/awk /bin/awk\nmkdir /var/lock/subsys \ntouch /var/lock/subsys/listener\n\nsudo dpkg --install oracle-xe_11.2.0-2_amd64.deb (.deb generated in step 3)\nsudo /etc/init.d/oracle-xe configure (left the default values as it is)\nSet the following env variables in ~/.bashrc file\n\nexport ORACLE_HOME=/u01/app/oracle/product/11.2.0/xe\nexport ORACLE_SID=XE\nexport NLS_LANG=`$ORACLE_HOME/bin/nls_lang.sh`\nexport ORACLE_BASE=/u01/app/oracle\nexport LD_LIBRARY_PATH=$ORACLE_HOME/lib:$LD_LIBRARY_PATH\nexport PATH=$ORACLE_HOME/bin:$PATH\n\nBelow  steps:\n\nchown -R oracle:dba /var/tmp/.oracle\nchmod -R 755 /var/tmp/.oracle\nchown -R oracle:dba /tmp/.oracle\nchmod -R 755 /tmp/.oracle\n\nsudo service oracle-xe start (I didn't see any issues in this step)\n\n\n12 . sqlplus / as sysdba and got the following\n\n\n  SQL*Plus: Release 11.2.0.2.0 Production on Thu Jan 3 09:41:58 2013\n  \n  Copyright (c) 1982, 2011, Oracle.  All rights reserved.\n  \n  Connected to an idle instance.\n\n\nNow when exectute any SQL statements on SQLplus, i end up with the following error\n\nSQL&gt; select * from dual;\nselect * from dual\n*\nERROR at line 1:\nORA-01034: ORACLE not available\nProcess ID: 0\nSession ID: 0 Serial number: 0\n\n\nI have increased the swap memory as specified in here\n\n\n\n free -m\n\n\n\n\n             total       used       free     shared    buffers     cached\nMem:          1652       1596         56          0         53       1356\n-/+ buffers/cache:        186       1466\nSwap:         2943          0       2943\n\n\nCan you guide me here? I am clueless.\n    \n\nFinally, after a day long struggle\n\nThese did the trick.\n\nsudo rm -rf /dev/shm\nsudo mkdir /dev/shm\nsudo mount -t tmpfs shmfs -o size=2048m /dev/shm\n\n\nMake sure that these are executed before the database is configured.\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How can I deploy my Angular 2 + Typescript + Webpack app.", "id": 1061, "answers": [{"answer_id": 1057, "document_id": 642, "question_id": 1061, "text": "You just need to install the nginx on your EC2. In my case I had a linux Ubuntu 14.04 installed on \"Digital Ocean\".", "answer_start": 379, "answer_category": null}], "is_impossible": false}], "context": "I am actually learning Angular 2 with Typescript and developed a little app by based on the angular-seed project (angular-seed). I have built the app for production.\nHowever, as a fresher, I have no idea how to deploy it now on my EC2 server. I read that I have to config Nginx server to serve my static file but do I have to config it particularly to work with my bundle files?\nYou just need to install the nginx on your EC2. In my case I had a linux Ubuntu 14.04 installed on \"Digital Ocean\".\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "how should i bundle java with my windows application", "id": 1466, "answers": [{"answer_id": 1455, "document_id": 1037, "question_id": 1466, "text": "Find a program which can generate an exe file from a jar file(Google is your friend, I can't remember the name). Note that this .exe file will still need a jre. It is just a smart way to make an exe which contain your .jar file, and which start the java process automatic. (This also allows you to set a custom icon on your .exe file).\n", "answer_start": 1766, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI have a Windows application based on Java, that I should like to install with Java bundled. The installation framework is NSIS. The application executable should be guaranteed to invoke the bundled Java, so there's no clash with other Javas installed in the system.\n\nWhat's the best way to achieve my goal? I haven't tried to solve this kind of problem before, and have little experience with Java, so I don't know which solutions are out there. I think I'd prefer Java to be embedded in the application's executable, if feasible, otherwise I guess Java could be installed along with it (with the executable pointing to said Java).\n\nEdit:\nThis project already generates an executable (.exe), via NSIS. The executable will by default use the system Java, but apparently it'll prefer a JRE in the same directory (i.e. bundled) if present.\n\nEdit 2:\nThe scenario that made me want to bundle Java with this application was that I received an executable for it built with 32-bit Java, which failed (silently) on my system which has 64-bit Java.\n    \n\nAre you absolutely sure you don't want to use the computer JRE? In most cases it's preferable.\nYou can see here (and the included link) some examples with installers that check JRE number and install it (globally) if necessary.\n\nIf you really prefer to include your own JRE in the installer and always use it - what prevents you from doing it? It's just a matter of your main program point having some way of konwing the JRE location and forcing to use it. That depends on how you pack/invoke your Java program. Normally, it would be feasible, perhaps with a simple .bat file - perhaps to be created at installation time.\n    \n\nThe solution we used(A few years ago, but still I think the best way).\n\nFind a program which can generate an exe file from a jar file(Google is your friend, I can't remember the name). Note that this .exe file will still need a jre. It is just a smart way to make an exe which contain your .jar file, and which start the java process automatic. (This also allows you to set a custom icon on your .exe file).\n\nThe java sdk which you use to develop/compile your java application, also contains a folder called jre, which contain a copy of the jre. \n\nInclude this folder with the installer, so the jre folder is located in the same folder as the .exe file. Then the .exe file will use the included jre, and the jre will not be installed on the computer, so you prevent any problems.\n    \n\nWell one extremely simple solution that works actually quite nice if you don't have to get an executable, is just using a simple Windows Batch file that starts the jar and having a shortcut to it so you get your preferred icon on it. For the average user there's no real difference - after all the default is to suppress known extensions on Windows (horrible default but well) - and for people who do know what an exe is, the solution should be quite apparent anyways.\n\nThat way you can also easily start the included java or better yet offer both versions and just change a variable in an ini file, which really is much nicer - nobody wants or needs X different JRE versions that are outdated and are nothing more than security problems. Still can't harm to offer a version that includes the JRE for people without a java install to make it as simple as possible for them.\n\nOne example for this behavior would be weka..\n    \n\nlaunch4j seems to offer to bundle an embedded JRE.\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How do I uninstall Java on my Windows 10  computer?", "id": 147, "answers": [{"answer_id": 155, "document_id": 92, "question_id": 147, "text": "Click Start\nSelect Settings\nSelect System\nSelect Apps & features\nSelect the program to uninstall and then click its Uninstall button.\nRespond to the prompts to complete the uninstall", "answer_start": 761, "answer_category": null}], "is_impossible": false}, {"question": "How do I uninstall Java on my Windows 8  computer?", "id": 148, "answers": [{"answer_id": 156, "document_id": 92, "question_id": 148, "text": "From the Start screen, enter Control Panel in the Search box. Select the Control Panel.\nWhen the Control Panel appears, choose Uninstall a Program from the Programs category.\nSelect the program to uninstall, and then right-click and select Uninstall or you can click the Uninstall option located at the top of the programs list.\nClick Yes to confirm the program uninstall.", "answer_start": 975, "answer_category": null}], "is_impossible": false}, {"question": "How do I uninstall Java on my Windows 7 computer?", "id": 149, "answers": [{"answer_id": 157, "document_id": 92, "question_id": 149, "text": "Click Start\nSelect Control Panel\nSelect Programs\nClick Programs and Features\nSelect the program you want to uninstall by clicking on it, and then click the Uninstall button.\nYou may need administrator privileges to remove programs.\n", "answer_start": 1389, "answer_category": null}], "is_impossible": false}, {"question": "How do I uninstall Java on my Windows xp  computer?", "id": 150, "answers": [{"answer_id": 158, "document_id": 92, "question_id": 150, "text": "Click Start\nSelect Control Panel\nClick the Add/Remove Programs control panel icon\nThe Add/Remove control panel displays a list of software on your system, including any Java software products that are on your computer. Select any that you want to uninstall by clicking on it, and then click the Remove button.", "answer_start": 1654, "answer_category": null}], "is_impossible": false}], "context": "How do I uninstall Java on my Windows computer?\nThis article applies to:\nPlatform(s): Windows 10, Windows 2008 Server, Windows 7, Windows 8, Windows Server 2012, Windows Vista, Windows XP\nWindows Users: Improve the security of your computer by checking for old versions of Java and removing them when you install Java 8 (8u20 and later versions) or by using the Java Uninstall Tool.\n\n\u00bb Learn more about the Java Uninstall tool\n\u00bb Troubleshooting - Uninstaller tool\n\nManual Uninstall\nYou can uninstall older versions of Java manually in the same way as you would uninstall any other software from your Windows computer.\n\nOlder versions of Java may appear in the program list as J2SE, Java 2, Java SE or Java Runtime Environment.\n\n\nWindows 10 - Uninstall Programs\nClick Start\nSelect Settings\nSelect System\nSelect Apps & features\nSelect the program to uninstall and then click its Uninstall button.\nRespond to the prompts to complete the uninstall\nWindows 8 - Uninstall Programs\nFrom the Start screen, enter Control Panel in the Search box. Select the Control Panel.\nWhen the Control Panel appears, choose Uninstall a Program from the Programs category.\nSelect the program to uninstall, and then right-click and select Uninstall or you can click the Uninstall option located at the top of the programs list.\nClick Yes to confirm the program uninstall.\nWindows 7 and Vista - Uninstall Programs\nClick Start\nSelect Control Panel\nSelect Programs\nClick Programs and Features\nSelect the program you want to uninstall by clicking on it, and then click the Uninstall button.\nYou may need administrator privileges to remove programs.\n\nWindows XP - Uninstall Programs\nClick Start\nSelect Control Panel\nClick the Add/Remove Programs control panel icon\nThe Add/Remove control panel displays a list of software on your system, including any Java software products that are on your computer. Select any that you want to uninstall by clicking on it, and then click the Remove button.\nTroubleshooting\nIf you run into issues removing Java, run the Microsoft utility to repair corrupted files and registry keys that prevents programs from being completely uninstalled or blocking new installations and updates.\n\nRELATED INFORMATION\n\nWindows\nWindows 10 Uninstall Programs (Microsoft)\nWindows 8, Windows 7 Uninstall Programs (Microsoft)\nNon-Windows\nMac OS X Uninstall instructions\nLinux Uninstall instructions\nSolaris Uninstall instructions (OTN", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "match element name in web.config transformation", "id": 1097, "answers": [{"answer_id": 1089, "document_id": 674, "question_id": 1097, "text": "this should work:\n<sessionState timeout=\"2000\" \n xdt:Transform=\"SetAttributes(timeout)\">\n</sessionState>", "answer_start": 150, "answer_category": null}], "is_impossible": false}], "context": "I am trying to match element by element name, I have to change sessionState element timeout in release, but I cannot match it by any attribute value.\nthis should work:\n<sessionState timeout=\"2000\" \n xdt:Transform=\"SetAttributes(timeout)\">\n</sessionState>\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Overwrite file in copying IF content to of them not the same", "id": 1323, "answers": [{"answer_id": 1313, "document_id": 892, "question_id": 1323, "text": "Have you tried the command line:\ncp -ru A/* B/\nShould copy recursively all changed files (more recent timestamp) from directory A to directory B.", "answer_start": 546, "answer_category": null}], "is_impossible": false}], "context": "I have a lot of files from one side (A) and a lot of other files in other place (B)\nI'm copying A to B, there are a lot of files are the same, but content could be different!\nUsually I used mc (Midnight Commander) to do it, and selected \"Overwrite if different size\". But there is a situation when size are the same, but content is different. In this case mc keeps file in B place and not overwrite it.\nIn mc overwrite dialog there is a work \"Update\" I don't know what it is doing? In help there is no such information, maybe this is a solution?\nHave you tried the command line:\ncp -ru A/* B/\nShould copy recursively all changed files (more recent timestamp) from directory A to directory B.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How do I rename a local Git branch?", "id": 1852, "answers": [{"answer_id": 1838, "document_id": 1423, "question_id": 1852, "text": "If you want to rename a branch while pointed to any branch, do:\ngit branch -m <oldname> <newname>\nIf you want to rename the current branch, you can do:\ngit branch -m <newname>", "answer_start": 311, "answer_category": null}], "is_impossible": false}], "context": "I don't want to rename a remote branch, as described in Rename master branch for both local and remote Git repositories.\nHow can I rename a local branch which hasn't been pushed to a remote repository?\nIn case you need to rename a remote branch as well:\nHow do I rename both a Git local and remote branch name?\nIf you want to rename a branch while pointed to any branch, do:\ngit branch -m <oldname> <newname>\nIf you want to rename the current branch, you can do:\ngit branch -m <newname>\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Export Perl script with modules", "id": 1286, "answers": [{"answer_id": 1278, "document_id": 857, "question_id": 1286, "text": "Distribute as one program containing everything:fatpack,pp,staticperl", "answer_start": 448, "answer_category": null}], "is_impossible": false}], "context": "I have a perl script which is deleting Logs files for me. Now I want to use this script on multiple other computers and on one server. The problem is, that I use some manual downloaded modules.\nMy question is, how can I export a perl script into a .zip with all used modules in the script?\nWhat I'm trying to reach is, that I can put the .zip file on a different computer and run the script without any error caused by the manual imported modules.\nDistribute as one program containing everything:fatpack,pp,staticperl\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How do you migrate an IIS 7 site to another server?", "id": 342, "answers": [{"answer_id": 349, "document_id": 154, "question_id": 342, "text": "Microsoft Web Deploy v3 can export and import all your files, the configuration settings, etc. It puts it all into a zip archive ready to import on the new server. It can even upgrade to newer versions of IIS (v7-v8).", "answer_start": 323, "answer_category": null}], "is_impossible": false}], "context": "I'm wondering what is the best practice for moving a website to another server. Manually recreate the site on the new server (not maintainable for obvious reasons).Copy the applicationHost.config settings file.Use appcmd to make a backup and restore.Use MSDeploy to publish the site on the new machine.Use a 3rd party tool.Microsoft Web Deploy v3 can export and import all your files, the configuration settings, etc. It puts it all into a zip archive ready to import on the new server. It can even upgrade to newer versions of IIS (v7-v8).", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Installing PostgreSQL on OSX for Rails development", "id": 1783, "answers": [{"answer_id": 1769, "document_id": 1354, "question_id": 1783, "text": "On a 64-bit Mac (Snow Leopard with Core 2 Duo or newer) you should compile PostgreSQL from source, as rails kept complaining that:\n*** Your PostgreSQL installation doesn't seem to have an architecture in common\nwith the running ruby interpreter ([\"ppc\", \"i386\", \"x86_64\"] ", "answer_start": 624, "answer_category": null}], "is_impossible": false}], "context": "Installing PostgreSQL on OSX for Rails development\nI've spent several hours over the past few days trying to get PostgreSQL to play nice with RoR on my Mac.\nI've followed several tutorials using several different methods such as installing PostgreSQL manually and installing from various 1-click installers\nHowever the all the different methods I tried failed on the last step of installing the pg gem. Very frustrating!\nDoes anyone here have a tried and tested tutorial for getting this done? (Or would you like to write some instructions here...?)\nMy environment is this: Macbook running OSX 10.6, PostgreSQL 8.4.1 server\nOn a 64-bit Mac (Snow Leopard with Core 2 Duo or newer) you should compile PostgreSQL from source, as rails kept complaining that:\n*** Your PostgreSQL installation doesn't seem to have an architecture in common\nwith the running ruby interpreter ([\"ppc\", \"i386\", \"x86_64\"] \n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "fxcop 1 36 is gone", "id": 1380, "answers": [{"answer_id": 1369, "document_id": 950, "question_id": 1380, "text": "You can download from here:\n\nhttp://archive.msdn.microsoft.com/codeanalysis/Release/ProjectReleases.aspx?ReleaseId=553\n\nClick on FxCopInstall.exe link and it will download", "answer_start": 1348, "answer_category": null}], "is_impossible": false}], "context": "\n\n                    \n\n            \n\n        \n\n            \n\n                \n\n                    \n\n                        Closed. This question is off-topic. It is not currently accepting answers.\n\n                        \n\n                    \n\n                \n\n            \n\n        \n\n            \n\n        \n\n                \n\n                    \n\n                \n\n            \n\n                \n\n                    Want to improve this question? Update the question so it's on-topic for Stack Overflow.\n\n                \n\n                    Closed 9 years ago.\n\n\n\n            \n\n        \n\n            \n\n                    \n\n                        Improve this question\n\n                    \n\n            \n\n\n\n\n\n    \n\n\n\nI just wanted to download FxCop 1.36 but I can't find it in the internet.\nA few days ago FxCop 10 came out for Windows 7 but I need the old Version.\nThe file is gone on the MS server and everybody linked it.\n\nAnyone has the old install routine?\n    \n\nI have the 1.36 installer...If you still need it, I could put it up on my Dropbox for you...\n\nEdit: here's the link to the 1.36 installer. Enjoy!\n    \n\nGo Here\n\nC:\\Program Files\\Microsoft SDKs\\Windows\\v7.0A\\FXCop\n    \n\nYou seem to be correct. 1.35 is still availble though http://code.msdn.microsoft.com/codeanalysis/Release/ProjectReleases.aspx?ReleaseId=553\n    \n\nYou can download from here:\n\nhttp://archive.msdn.microsoft.com/codeanalysis/Release/ProjectReleases.aspx?ReleaseId=553\n\nClick on FxCopInstall.exe link and it will download.\n\nRegards,\n\nSushil Kumar\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to install SQL Server Management Studio 2012 (SSMS) Express?", "id": 673, "answers": [{"answer_id": 678, "document_id": 366, "question_id": 673, "text": "You need to install ENU\\x64\\SQLEXPRWT_x64_ENU.exe which is Express with Tools (RTM release. SP1 release can be found here).", "answer_start": 454, "answer_category": null}], "is_impossible": false}], "context": "I just installed SQL Server 2012 Express, I can connect with database from VS2012RC.\nDatabase is working :) I use Win7 SP1 64bit.\nI download program from page I choose ENU\\x64\\SQLManagementStudio_x64_ENU.exe\nI want to install Management Studio 2012, but after unpack installer has stopped. I see only for moment some console application.\n1.\tWhat could be cause?\n2.\tWhere can I find any log file?\nWhen I installed: ENU\\x64\\SQLManagementStudio_x64_ENU.exe\nYou need to install ENU\\x64\\SQLEXPRWT_x64_ENU.exe which is Express with Tools (RTM release. SP1 release can be found here).\nAs the page states\nExpress with Tools (with LocalDB) Includes the database engine and SQL Server Management Studio Express) This package contains everything needed to install and configure SQL Server as a database server. Choose either LocalDB or Express depending on your needs above.\nSo install this and use the management studio included with it.\nSearching for \"Management\" pulled it up faster within the Start Menu.\n\nhe previous clues to get SQLManagementStudio_x64_ENU.exe runing didn't work as stated for me. After a while of searching, trying, retrying again and again, I finally figured it out. When executing SQLManagementStudio_x64_ENU.exe on my Windows seven system, I kept runing into compatibility issues. The trick is to run SQLManagementStudio_x64_ENU.exe in compatibility mode with Windows XP SP2. Edit the installer properties and enable compatibility mode with XP (service pack 2), then you'll be able to access Mr Doug (answered Mar 4 at 15:09) resolution.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Installing RubyGems in Windows", "id": 662, "answers": [{"answer_id": 667, "document_id": 355, "question_id": 662, "text": "Use chocolatey in PowerShell\nchoco install ruby -y\nrefreshenv\ngem install bundler", "answer_start": 504, "answer_category": null}], "is_impossible": false}], "context": "I'm new to ruby. I tried to install Ruby Gems on my PC by following the steps given in the site http://rubygems.org/pages/download.\nI downloaded the package from the mentioned site, changed the directory to the directory in which the setup resides, and tried to run setup using the command setup.rb in command prompt.\nBut I get a window pop up that says \"Windows can't open this file\" and prompts me to select a program to open this file.\nWhat should I do now? Let me know if I am doing something wrong.\nUse chocolatey in PowerShell\nchoco install ruby -y\nrefreshenv\ngem install bundler\nIn my case refreshenv did not work: I had to close and reopen PowerShell and then it worked\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to install Docker Desktop on Windows?\n", "id": 78, "answers": [{"answer_id": 82, "document_id": 71, "question_id": 78, "text": "e-click Docker Desktop Installer.exe to run the installer.\nIf you haven\u2019t already downloaded the installer (Docker Desktop Installer.exe), you can get it from\nDocker Hub.\nIt typically downloads to your Downloads folder, or you can run it from\nthe recent downloads bar at the bottom of your web browser.\n\n\nWhen prompted, ensure the Enable Hyper-V Windows Features option is selected on the Configuration page.\n\n\nFollow the instructions on the installation wizard to authorize the installer and proceed with the install.\n\n\nWhen the installation is successful, click Close to complete the installation process.\n\n\nIf", "answer_start": 2856, "answer_category": null}], "is_impossible": false}, {"question": "What is the hardware requirements for installing Docker Desktop for Windows?", "id": 79, "answers": [{"answer_id": 83, "document_id": 71, "question_id": 79, "text": "per-V on Windows 10:\n\n64 bit processor with Second Level Address Translation (SLAT)\n4GB system RAM\nBIOS-level hardware virtualization support must be enabled in the\nBIOS settings.  For more information, see\nVirtualization.\n\n\n", "answer_start": 1097, "answer_category": null}], "is_impossible": false}, {"question": "How to start Docker Desktop?", "id": 80, "answers": [{"answer_id": 84, "document_id": 71, "question_id": 80, "text": " Desktop does not start automatically after installation. To start Docker Desktop, search for Docker, and select Docker Desktop in the search results.\n\nWhen the whale icon in the status bar stays steady, Docker Desktop is up-and-running, and is accessible from any terminal window.\n\nIf the whale icon is hidden in the Notifications area, click the up arrow on the\ntaskbar to show it. To learn more, see Docker Settings.\nWhen the initialization is complete, Docker Desktop launches the onboarding tutorial. The tutorial includes a simple exercise to build an example Docker image, run it as a container, push and save the image to Docker Hub.\n\nCongr", "answer_start": 3808, "answer_category": null}], "is_impossible": false}, {"question": "How to uninstall Docker Desktop for Windows?", "id": 81, "answers": [{"answer_id": 85, "document_id": 71, "question_id": 81, "text": "e Windows Start menu, select Settings > Apps > Apps & features.\nSelect Docker Desktop from the Apps & features list and then select Uninstall.\nClick Uninstall to confirm your selection.\n\n\nNote:", "answer_start": 4706, "answer_category": null}], "is_impossible": false}, {"question": "How to save images in Docker Desktop?", "id": 82, "answers": [{"answer_id": 86, "document_id": 71, "question_id": 82, "text": "r save -o images.tar image1 [image2 ...] to save any images you\nwant to keep. See save in the Docker\nEngine command line reference.\n\n\nUse doc", "answer_start": 6601, "answer_category": null}], "is_impossible": false}, {"question": "How to export docker container in Docker Desktop?", "id": 83, "answers": [{"answer_id": 87, "document_id": 71, "question_id": 83, "text": "r export -o myContainner1.tar container1 to export containers you\nwant to keep. See export in the\nDocker Engine command line reference.\n\n\nUninst", "answer_start": 6744, "answer_category": null}], "is_impossible": false}], "context": "Install Docker Desktop on Windows\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSearch\n\n\n\n\n\n\n\nToggle navigation\n\n\n\n\n\n\n\nHomeGuidesProduct manualsReferenceSamples\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInstall Docker Desktop on WindowsEstimated reading time: 7 minutesDocker Desktop for Windows is the Community version of Docker for Microsoft Windows.\nYou can download Docker Desktop for Windows from Docker Hub.\nThis page contains information on installing Docker Desktop on Windows 10 Pro, Enterprise, and Education. If you are looking for information about installing Docker Desktop on Windows 10 Home, see Install Docker Desktop on Windows Home.\nDownload from Docker\nHub\nBy downloading Docker Desktop, you agree to the terms of the Docker Software End User License Agreement and the Docker Data Processing Agreement.\nWhat to know before you install\ud83d\udd17\nSystem Requirements\ud83d\udd17\n\n\nWindows 10 64-bit: Pro, Enterprise, or Education (Build 16299 or later).\nFor Windows 10 Home, see Install Docker Desktop on Windows Home.\n\nHyper-V and Containers Windows features must be enabled.\n\nThe following hardware prerequisites are required to successfully run Client\nHyper-V on Windows 10:\n\n64 bit processor with Second Level Address Translation (SLAT)\n4GB system RAM\nBIOS-level hardware virtualization support must be enabled in the\nBIOS settings.  For more information, see\nVirtualization.\n\n\n\n\nNote: Docker supports Docker Desktop on Windows based on Microsoft\u2019s support lifecycle for Windows 10 operating system. For more information, see the Windows lifecycle fact sheet.\n\nWhat\u2019s included in the installer\ud83d\udd17\nThe Docker Desktop installation includes Docker Engine,\nDocker CLI client, Docker Compose,\nNotary,\nKubernetes,\nand Credential Helper.\nContainers and images created with Docker Desktop are shared between all\nuser accounts on machines where it is installed. This is because all Windows\naccounts use the same VM to build and run containers. Note that it is not possible to share containers and images between user accounts when using the Docker Desktop WSL 2 backend.\nNested virtualization scenarios, such as running Docker Desktop on a\nVMWare or Parallels instance might work, but there are no guarantees. For\nmore information, see Running Docker Desktop in nested virtualization scenarios.\nAbout Windows containers\ud83d\udd17\nLooking for information on using Windows containers?\n\nSwitch between Windows and Linux containers\ndescribes how you can toggle between Linux and Windows containers in Docker Desktop and points you to the tutorial mentioned above.\nGetting Started with Windows Containers (Lab)\nprovides a tutorial on how to set up and run Windows containers on Windows 10, Windows Server 2016 and Windows Server 2019. It shows you how to use a MusicStore application\nwith Windows containers.\nDocker Container Platform for Windows articles and blog\nposts on the Docker website.\n\nInstall Docker Desktop on Windows\ud83d\udd17\n\n\nDouble-click Docker Desktop Installer.exe to run the installer.\nIf you haven\u2019t already downloaded the installer (Docker Desktop Installer.exe), you can get it from\nDocker Hub.\nIt typically downloads to your Downloads folder, or you can run it from\nthe recent downloads bar at the bottom of your web browser.\n\n\nWhen prompted, ensure the Enable Hyper-V Windows Features option is selected on the Configuration page.\n\n\nFollow the instructions on the installation wizard to authorize the installer and proceed with the install.\n\n\nWhen the installation is successful, click Close to complete the installation process.\n\n\nIf your admin account is different to your user account, you must add the user to\nthe docker-users group. Run\u00a0Computer Management\u00a0as an administrator and navigate to\nLocal Users and Groups > Groups\u00a0>\u00a0docker-users.\u00a0Right-click to add the user to the group.\nLog out and log back in for the changes to take effect.\n\n\nStart Docker Desktop\ud83d\udd17\nDocker Desktop does not start automatically after installation. To start Docker Desktop, search for Docker, and select Docker Desktop in the search results.\n\nWhen the whale icon in the status bar stays steady, Docker Desktop is up-and-running, and is accessible from any terminal window.\n\nIf the whale icon is hidden in the Notifications area, click the up arrow on the\ntaskbar to show it. To learn more, see Docker Settings.\nWhen the initialization is complete, Docker Desktop launches the onboarding tutorial. The tutorial includes a simple exercise to build an example Docker image, run it as a container, push and save the image to Docker Hub.\n\nCongratulations! You are now successfully running Docker Desktop on Windows.\nIf you would like to rerun the tutorial, go to the Docker Desktop menu\nand select Learn.\nUninstall Docker Desktop\ud83d\udd17\nTo uninstall Docker Desktop from your Windows machine:\n\nFrom the Windows Start menu, select Settings > Apps > Apps & features.\nSelect Docker Desktop from the Apps & features list and then select Uninstall.\nClick Uninstall to confirm your selection.\n\n\nNote: Uninstalling Docker Desktop will destroy Docker containers and images local to the machine and remove the files generated by the application.\n\nSwitch between Stable and Edge versions\ud83d\udd17\nDocker Desktop allows you to switch between Stable and Edge releases. However, you can only have one version of Docker Desktop installed at a time. Switching between Stable and Edge versions can destabilize your development environment, particularly in cases where you switch from a newer (Edge) channel to an older (Stable) channel.\nFor example, containers created with a newer Edge version of Docker Desktop may\nnot work after you switch back to Stable because they may have been created\nusing Edge features that aren\u2019t in Stable yet. Keep this in mind as\nyou create and work with Edge containers, perhaps in the spirit of a playground\nspace where you are prepared to troubleshoot or start over.\nExperimental features are turned on by default on Edge releases. However, when you switch from a Stable to an Edge release, you must turn on the experimental features flag to access experimental features. From the Docker Desktop menu, click Settings > Command Line and then turn on the Enable experimental features toggle. Click Apply & Restart for the changes to take effect.\nTo safely switch between Edge and Stable versions, ensure you save images and export the containers you need, then uninstall the current version before installing another. For more information, see the section Save and Restore data below.\nSave and restore data\ud83d\udd17\nYou can use the following procedure to save and restore images and container data. For example, if you want to switch between Edge and Stable, or to reset your VM disk:\n\n\nUse docker save -o images.tar image1 [image2 ...] to save any images you\nwant to keep. See save in the Docker\nEngine command line reference.\n\n\nUse docker export -o myContainner1.tar container1 to export containers you\nwant to keep. See export in the\nDocker Engine command line reference.\n\n\nUninstall the current version of Docker Desktop and install a different version (Stable or Edge), or reset your VM disk.\n\n\nUse docker load -i images.tar to reload previously saved images. See\nload in the Docker Engine.\n\n\nUse docker import -i myContainer1.tar to create a file system image\ncorresponding to the previously exported containers. See\nimport in the Docker Engine.\n\n\nFor information on how to back up and restore data volumes, see Backup, restore, or migrate data volumes.\nWhere to go next\ud83d\udd17\n\nGetting started introduces Docker Desktop for Windows.\nGet started with Docker is a tutorial that teaches you how to\ndeploy a multi-service stack.\nTroubleshooting describes common problems, workarounds, and\nhow to get support.\nFAQs provides answers to frequently asked questions.\nStable Release Notes or Edge Release Notes.\n\nwindows, install, download, run, docker, localRate this page:\u00a01226\u00a0785\u00a0i\n\n\n\n\n\nDocker EngineOverviewInstallInstallation per distroInstall on CentOSInstall on DebianInstall on FedoraInstall on UbuntuInstall binariesOptional post-installation stepsRelease notesPrevious versionsEngine 18.09 release notesEngine 18.06 release notesEngine 18.05 release notesEngine 18.04 release notesEngine 18.03 release notesEngine 18.02 release notesEngine 18.01 release notesEngine 17.12 release notesEngine 17.11 release notesEngine 17.10 release notesEngine 17.09 release notesEngine 17.07 release notesEngine 17.06 release notesEngine 17.05 release notesEngine 17.04 release notesEngine 17.03 release notesEngine 1.13 and earlierDeprecated featuresDocker AppDocker BuildxDocker ContextDocker ScanDocker ComposeOverview of Docker ComposeInstall ComposeRelease notesGetting startedEnvironment variables in ComposeEnvironment fileExtend services in ComposeNetworking in ComposeUsing Compose in productionControl startup orderSample apps with ComposeDocker Compose release notesDocker DesktopOverviewMacInstall Docker Desktop for MacUser manualDeploy on KubernetesLeveraging Multi-CPU architecture supportNetworkingDisk utilizationLogs and troubleshootingFAQsStable release notesEdge release notesWindowsInstall Docker Desktop for WindowsInstall Docker Desktop on Windows HomeUser manualDeploy on KubernetesNetworkingLogs and troubleshootingFAQsStable release notesEdge release notesDocker Desktop WSL 2 backendDashboardOpen source licensingDocker HubQuickstartDocker ID accountsSecurity and authenticationManage access tokensTwo-factor authenticationEnable two-factor authenticationDisable two-factor authenticationRecover your Docker Hub accountGenerate a new recovery codeTeams and organizationsRepositoriesVulnerability scanningOfficial imagesAutomated buildsSet up automated buildsTesting in automated buildsAdvanced automated buildsLink to GitHub and BitBucketWebhooksSlack integrationDownload rate limitAdministrationConvert an account into an organizationDeactivate an account or an organizationBillingOverviewUpgrade your planDowngrade your planAdd seatsRemove seatsPublisher & certified contentOverviewSubmit a product for Docker HubUser FAQsPublisher FAQsCertify images & pluginsCertify logging pluginsTrust chainBring Your Own License (BYOL)Release notesOpen-source projectsNotaryGet started with NotaryUse the Notary clientUnderstand the service architectureRun a Notary serviceNotary changelogConfiguration filesServer configurationSigner configurationClient configurationCommon Server and signer configurationsDocker RegistryRegistry overviewUnderstand the RegistryDeploy a registry serverConfigure a registryWork with notificationsRecipesRecipes overviewAuthenticating proxy with apacheAuthenticating proxy with nginxMirroring Docker HubRunning on macOSGarbage collectionTesting an insecure registryDeprecated featuresCompatibilityGetting helpRelease notesSuperseded products and toolsDocker MachineMachine overviewInstall MachineRelease notesGet started with a local VMProvision hosts in the cloudLearn by exampleProvision Digital Ocean DropletsProvision AWS EC2 instancesMachine concepts and helpMachine (docker-machine) CLIMachine CLI overviewMachine command-line completionactiveconfigcreateenvhelpinspectipkilllsmountprovisionregenerate-certsrestartrmscpsshstartstatusstopupgradeurlMachine driversDrivers overviewDriver options and operating system defaultsAmazon Web ServicesDigital OceanExoscaleGenericGoogle Compute EngineIBM SoftlayerMicrosoft AzureMicrosoft Hyper-VOpenStackOracle VirtualBoxRackspaceVMware FusionVMware vCloud AirVMware vSphereMigrate from Boot2Docker to MachineDocker Toolbox (deprecated)KitematicKitematic user guide: intro & overviewSet up an Nginx web serverSet up a Minecraft ServerCreating a local RethinkDB database for developmentFrequently asked questionsKnown issuesTroubleshooting\n\n\n\n\n\n\n\nEdit this page Request docs changes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn this page:\n\nWhat to know before you install\n\nSystem Requirements\nWhat\u2019s included in the installer\nAbout Windows containers\n\n\nInstall Docker Desktop on Windows\nStart Docker Desktop\nUninstall Docker Desktop\nSwitch between Stable and Edge versions\n\nSave and restore data\n\n\nWhere to go next\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhy Docker?\nWhat is a Container?\n\n\n\n\nProducts\nDocker Desktop\nDocker Hub\nFeatures\nContainer Runtime\nDeveloper Tools\nKubernetes\n\n\n\n\nDevelopers\nUse Cases\nPlay with Docker\nCommunity\nOpen Source\nDocker Captains\n\n\n\n\nCompany\nAbout Us\nBlog\nCustomers\nPartners\nNewsroom\nCareers\nContact Us\n\n\n\n\n\n\nStatus\nSecurity\nLegal\nContact\n\n\n\n\n\n\n\nCopyright \u00a9 2013-2020 Docker Inc. All rights reserved.\n\n\n\nTwitter\nYoutube\nGitHub\nLinkedin\nFacebook\nSlideshare\nReddit\n\n\n\n\n\n\n\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "apc for windows alternative download", "id": 2003, "answers": [{"answer_id": 1989, "document_id": 1590, "question_id": 2003, "text": "I just download xampp-win32-1.7.4-VC6-installer.exe and I download php_apc-3.1.5-5.3-vc6-x86.zip (2010-11-03 17:04 -0800) and it works.", "answer_start": 2271, "answer_category": null}], "is_impossible": false}], "context": "\n\n                    \n\n            \n\n        \n\n            \n\n                    \n\n                        \n\n                    \n\n                \n\n                    \n\n                        This question is unlikely to help any future visitors; it is only relevant to a small geographic area, a specific moment in time,  or an extraordinarily narrow situation that is not generally applicable to the worldwide audience of the internet. For help making  this question more broadly applicable, visit the help center.\n\n                        \n\n                    \n\n                \n\n            \n\n                Closed 8 years ago.\n\n        \n\n\n\n\n\n    \n\n\n\nI am trying to install APC on windows but the site http://pecl4win.php.net/ is down for a while now with the message:\n\n\n  The pecl4win build box is temporarily out of service. We're preparing a new build system.\n\n\nIs there an alternative download for this ?\nOr can anyone share theirs ?\n    \n\nUpdate:\n\nMy answer below is outdated, and the pierre folder seems to be offline. You can now use PECL :: Package Browser, for example: PECL :: Package :: APC, which has DLLs listed.\n\nOld Answer:\n\nI got APC (and some other PECL packages') dll's for Win32 from http://downloads.php.net/pierre/\n\nNot sure how permanent this source is, found it on some forum a while back and bookmarked it.\n\nThis is an old question, but I found it through a google search, so others might too!\n    \n\nVersions up to 3.1.14 are available at Win-Web-Dev. Though they don't have the exact configuration I need of \"APC 3.1.9/PHP 5.3/Apache, thread-safe, VC6\". But they have several other configurations that work.\n    \n\nUp-to-date builds could be found here.\n    \n\nThe link of smautf is down, but you can find the same content of it (APC and another builds for Win) in http://downloads.php.net/pierre/. I neither know how permanent it is, but as july 25th (2009) it's working fine.\n    \n\nI got the latest 3.1.9  nts (non thread safe for iis) from here (as of 2011-08-23) and am running it with IIS 7 on windows 2008: http://dev.freshsite.pl/download/file/details/apc-319-for-php-53-vc9-nts-win72008.html\n    \n\nYou can download the php extenstion at http://downloads.php.net/pierre/, and download the extension. There are a lot of APC files.\n\nI just download xampp-win32-1.7.4-VC6-installer.exe and I download php_apc-3.1.5-5.3-vc6-x86.zip (2010-11-03 17:04 -0800) and it works.\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How can I have a Makefile automatically rebuild source files that include a modified header file? (In C/C++)", "id": 1842, "answers": [{"answer_id": 1828, "document_id": 1413, "question_id": 1842, "text": "You could add a 'make depend' command as others have stated but why not get gcc to create dependencies and compile at the same time.", "answer_start": 965, "answer_category": null}], "is_impossible": false}], "context": "I have the following makefile that I use to build a program (a kernel, actually) that I'm working on. Its from scratch and I'm learning about the process, so its not perfect, but I think its powerful enough at this point for my level of experience writing makefiles.\nMy main issue with this makefile is that when I modify a header file that one or more C files include, the C files aren't rebuilt. I can fix this quite easily by having all of my header files be dependencies for all of my C files, but that would effectively cause a complete rebuild of the project any time I changed/added a header file, which would not be very graceful.\nWhat I want is for only the C files that include the header file I change to be rebuilt, and for the entire project to be linked again. I can do the linking by causing all header files to be dependencies of the target, but I cannot figure out how to make the C files be invalidated when their included header files are newer.\nYou could add a 'make depend' command as others have stated but why not get gcc to create dependencies and compile at the same time.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Install Mono on Centos 5.5 using YUM", "id": 1144, "answers": [{"answer_id": 1137, "document_id": 721, "question_id": 1144, "text": " you need to ensure you install the correct version of mod_mono by running the following, and it will get the right one:\nyum install mod_mono-addon", "answer_start": 749, "answer_category": null}], "is_impossible": false}], "context": "I know how to build Mono from the source. However, according to the page Getting Started With Mono Tools it is possible to install the binaries directly. I'd prefer to install the binaries to avoid having to install all the development pre-requisites on a server with little disk space.\nAm I supposed to add a new repository description to YUM? I tried doing that, but I must have done it wrong, because \"yum list mono-core\" still says the old version (1.2.4-2.el5.centos).\nAnd, why are the .rpm's called \"mono-addon-\" on the release server? It's a bit confusing. It sounds like the .rpm's are an add-on to Mono. I guess they mean they are an \"add-on\" to the server(?).\n5\nIn addition to octonion's post, if, like me, you want to use Apache mod_mono, you need to ensure you install the correct version of mod_mono by running the following, and it will get the right one:\nyum install mod_mono-addon\nDon't just issue yum install mod_mono. It may install mod_mono 1.2 version from the CentOS extras repository and not what you're actually after.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to compile/install node.js(could not configure a cxx compiler!) (Ubuntu).", "id": 765, "answers": [{"answer_id": 765, "document_id": 452, "question_id": 765, "text": "Install base tools:\nyum groupinstall \"Development Tools\"\nNow install openssl-devel:\nyum install openssl-devel", "answer_start": 227, "answer_category": null}], "is_impossible": false}], "context": "How can I compile/install node.js on Ubuntu? It failed with an error about cxx compiler. If like me, you are attempting to install this on an AWS instance running Amazon Linux AMI (which looks to be a cut down version CentOS):\nInstall base tools:\nyum groupinstall \"Development Tools\"\nNow install openssl-devel:\nyum install openssl-devel\nNode should compile fine now.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "troubles installing programmatically an app with install packages permission fro", "id": 1519, "answers": [{"answer_id": 1508, "document_id": 1095, "question_id": 1519, "text": "s on custom ROM\n\nBasically,\n\n\nadd the required &lt;uses-permiss", "answer_start": 6756, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI'm trying to install programmatically an app without user interaction and i'm getting this error\n\n SecurityException: Neither user 10057 nor current process has android.permission.INSTALL_PACKAGES\n\n\nMy installer is located in /system/app on rooted phone, \"Install non market apps\" is checked , the installer has permission\n\n&lt;uses-permission android:name=\"android.permission.INSTALL_PACKAGES\"/&gt;\n\n\nThis is my call function\n\n private void puk(Context context) throws IllegalArgumentException, IllegalAccessException, InvocationTargetException{\n\n    String fileName = PAKAGE_FILE_NAME ;\n    String dir_type = Environment.DIRECTORY_DOWNLOADS;\n\n    File dir= Environment.getExternalStoragePublicDirectory(dir_type);\n    java.io.File file = new java.io.File(dir ,fileName);\n    Uri packageUri = Uri.fromFile(file);\n\n    PackageManager pm = context.getPackageManager();\n\n    Class&lt;? extends PackageManager&gt; o = pm.getClass();\n    Method[] allMethods=o.getMethods();\n\n    for (Method m : allMethods) {\n        if (m.getName().equals(\"installPackage\")) { \n            Log.e(TAG, \"installing the app..\" );\n            m.invoke(pm,new Object[] { packageUri, null, 1, \"com.mic.zapp\"});\n            break;\n        }\n    }\n\n}  \n\n\nI'm getting this error\n\n 11-15 02:46:23.320: W/System.err(10848): java.lang.reflect.InvocationTargetException\n 11-15 02:46:23.330: W/System.err(10848):   at java.lang.reflect.Method.invokeNative(Native Method)\n 11-15 02:46:23.330: W/System.err(10848):   at java.lang.reflect.Method.invoke(Method.java:507)\n 11-15 02:46:23.330: W/System.err(10848):   at com.mic.pvtapi.PvtApiReflectActivity.puk(PvtApiReflectActivity.java:56)\n 11-15 02:46:23.330: W/System.err(10848):   at com.mic.pvtapi.PvtApiReflectActivity.onCreate(PvtApiReflectActivity.java:28)\n 11-15 02:46:23.330: W/System.err(10848):   at android.app.Instrumentation.callActivityOnCreate(Instrumentation.java:1047)\n 11-15 02:46:23.330: W/System.err(10848):   at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:1722)\n 11-15 02:46:23.330: W/System.err(10848):   at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:1784)\n 11-15 02:46:23.330: W/System.err(10848):   at android.app.ActivityThread.access$1500(ActivityThread.java:123)\n 11-15 02:46:23.330: W/System.err(10848):   at android.app.ActivityThread$H.handleMessage(ActivityThread.java:939)\n 11-15 02:46:23.330: W/System.err(10848):   at android.os.Handler.dispatchMessage(Handler.java:99)\n 11-15 02:46:23.330: W/System.err(10848):   at android.os.Looper.loop(Looper.java:130)\n 11-15 02:46:23.330: W/System.err(10848):   at android.app.ActivityThread.main(ActivityThread.java:3835)\n 11-15 02:46:23.330: W/System.err(10848):   at java.lang.reflect.Method.invokeNative(Native Method)\n 11-15 02:46:23.330: W/System.err(10848):   at java.lang.reflect.Method.invoke(Method.java:507)\n 11-15 02:46:23.330: W/System.err(10848):   at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:847)\n 11-15 02:46:23.330: W/System.err(10848):   at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:605)\n 11-15 02:46:23.330: W/System.err(10848):   at dalvik.system.NativeStart.main(Native Method)\n 11-15 02:46:23.330: W/System.err(10848): Caused by: java.lang.SecurityException: Neither user 10057 nor current process has android.permission.INSTALL_PACKAGES.\n 11-15 02:46:23.340: W/System.err(10848):   at android.os.Parcel.readException(Parcel.java:1322)\n 11-15 02:46:23.340: W/System.err(10848):   at android.os.Parcel.readException(Parcel.java:1276)\n 11-15 02:46:23.340: W/System.err(10848):   at android.content.pm.IPackageManager$Stub$Proxy.installPackage(IPackageManager.java:2037)\n 11-15 02:46:23.340: W/System.err(10848):   at android.app.ContextImpl$ApplicationPackageManager.installPackage(ContextImpl.java:2613)\n\n\nI have readed that ti is possible to gain INSTALL_PAKAGES permissions in two ways: signing the app with firmware's key or putting the app in the firmware. My app is runng from /system/app so it has to gain privileges. \n\nSome one know what is wrong and give me some hints? Thanks\n\nEDIT:\n\nOne step forward, two steps backward\n\ni'we added new permission to the app\n\n &lt;permission \n        android:name=\"com.mic.pvtapi.permission.INS_AP\"\n    android:label=\"etichetta_perm\"\n    android:protectionLevel=\"signatureOrSystem\" /&gt;\n\n&lt;uses-permission android:name=\"com.mic.pvtapi.permission.INS_AP\"/&gt;\n&lt;uses-permission android:name=\"android.permission.WRITE_EXTERNAL_STORAGE\"/&gt;\n\n\nno more error\n\nNeither user 10057 nor current process has android.permission.INSTALL_PACKAGES\n\n\nbut the new one \n\n     11-15 19:43:58.895: I/ActivityManager(1838): Displayed com.mic.pvtapi/.PvtApiReflectActivity: +421ms\n 11-15 19:43:58.955: D/dalvikvm(4008): GC_EXPLICIT freed 3K, 51% free 2681K/5379K, external 0K/0K, paused 132ms\n 11-15 19:43:58.955: W/ActivityManager(1838): No content provider found for: \n 11-15 19:43:58.955: E/PackageManager(1838): Couldn't create temp file for downloaded package file.\n 11-15 19:43:58.955: W/dalvikvm(1838): threadid=13: thread exiting with uncaught exception (group=0x40018560)\n 11-15 19:43:58.965: E/AndroidRuntime(1838): *** FATAL EXCEPTION IN SYSTEM PROCESS: PackageManager\n 11-15 19:43:58.965: E/AndroidRuntime(1838): java.lang.NullPointerException\n 11-15 19:43:58.965: E/AndroidRuntime(1838):    at com.android.server.PackageManagerService$FileInstallArgs.createCopyFile(PackageManagerService.java:5247)\n 11-15 19:43:58.965: E/AndroidRuntime(1838):    at com.android.server.PackageManagerService$FileInstallArgs.copyApk(PackageManagerService.java:5255)\n 11-15 19:43:58.965: E/AndroidRuntime(1838):    at com.android.server.PackageManagerService$InstallParams.handleStartCopy(PackageManagerService.java:5051)\n 11-15 19:43:58.965: E/AndroidRuntime(1838):    at com.android.server.PackageManagerService$HandlerParams.startCopy(PackageManagerService.java:4902)\n 11-15 19:43:58.965: E/AndroidRuntime(1838):    at com.android.server.PackageManagerService$PackageHandler.doHandleMessage(PackageManagerService.java:516)\n 11-15 19:43:58.965: E/AndroidRuntime(1838):    at com.android.server.PackageManagerService$PackageHandler.handleMessage(PackageManagerService.java:461)\n 11-15 19:43:58.965: E/AndroidRuntime(1838):    at android.os.Handler.dispatchMessage(Handler.java:99)\n 11-15 19:43:58.965: E/AndroidRuntime(1838):    at android.os.Looper.loop(Looper.java:130)\n 11-15 19:43:58.965: E/AndroidRuntime(1838):    at android.os.HandlerThread.run(HandlerThread.java:60)\n 11-15 19:43:58.975: I/Process(1838): Sending signal. PID: 1838 SIG: 9\n 11-15 19:43:59.005: I/ServiceManager(1637): service 'SurfaceFlinger' died   \n\n\nmake my phone crash, freez and reboot\n    \n\nRefer to signatureOrSystem permissions on custom ROM\n\nBasically,\n\n\nadd the required &lt;uses-permission&gt;\npush apk to /system/priv-app\n\n\nDone (well, at least works for me).\n\nYou do not need to add android:protectionLevel=\"signatureOrSystem\" or android:sharedUserId=\"android.uid.system\". You could sign with any certification.\n    \n\nI assume that putting it in the /system/app folder is not equivalent to it being loaded in the firmware.\n    \n\nAbout the error:\n\nSecurityException: Neither user 10057 nor current process has android.permission.INSTALL_PACKAGES\n\n\nTo get INSTALL_PACKAGES permission:\n\n\nPut your app in /system/app\nSign the app with the system's key\nIn the manifest.xml file, put this attribute inside the tag manifest:\n\n&lt;manifest android:sharedUserId=\"android.uid.system\" ...\n\n\n    \n\nRunning these two lines fixes the issue with PackageManager permission error: \"Couldn't create temp file for downloaded package file.\"\n\nadb shell chown system.shell /data/app\nadb shell chown system.shell /data/local\n\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to Set Node.js to $PATH", "id": 1088, "answers": [{"answer_id": 1080, "document_id": 665, "question_id": 1088, "text": "You don't have to worry about that, the apt-get install command will do that for you. It adds the path to the nodejs process (usually /usr/bin/node) to the global $PATH variable. This ensures that when you type node in your terminal it will start the nodejs process.", "answer_start": 231, "answer_category": null}], "is_impossible": false}], "context": "I am studying RoR and I am setting this virtual machine to \"deploy\" RoR and I got stuck in the Node.js installation.\nI am using Ubuntu 12.04 and I followed this step of this guide:\nhttp://railsapps.github.com/installing-rails.html\nYou don't have to worry about that, the apt-get install command will do that for you. It adds the path to the nodejs process (usually /usr/bin/node) to the global $PATH variable. This ensures that when you type node in your terminal it will start the nodejs process.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Unable to set default python version to python3 in ubuntu", "id": 1618, "answers": [{"answer_id": 1605, "document_id": 1192, "question_id": 1618, "text": "Open your .bashrc file nano ~/.bashrc. Type alias python=python3 on to a new line at the top of the file then save the file with ctrl+o and close the file with ctrl+x. Then, back at your command line type source ~/.bashrc. Now your alias should be permanent.", "answer_start": 160, "answer_category": null}], "is_impossible": false}], "context": "I was trying to set default python version to python3 in Ubuntu 16.04. By default it is python2 (2.7).\nI'm new to Ubuntu and Idon't know what I'm doing wrong.\t\nOpen your .bashrc file nano ~/.bashrc. Type alias python=python3 on to a new line at the top of the file then save the file with ctrl+o and close the file with ctrl+x. Then, back at your command line type source ~/.bashrc. Now your alias should be permanent.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to install Rust on windows?", "id": 110, "answers": [{"answer_id": 118, "document_id": 76, "question_id": 110, "text": "ARM provides .exe installers for Windows. Grab one from here, and follow the instructions.\nJust before the installation process finishes tick/select the \"Add path to environment variable\"\noption. Then verify that the tools are in your %PATH%:\n$ arm-none-eabi-gdb -v\nGNU gdb (GNU Tools for Arm Embedded Processors 7-2018-q2-update) 8.1.0.20180315-git", "answer_start": 1021, "answer_category": null}], "is_impossible": false}], "context": "\n\n\nWindows - The Embedded Rust Book\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1. Introduction1.1. Hardware1.2. no_std1.3. Tooling1.4. Installation1.4.1. Linux1.4.2. MacOS1.4.3. Windows1.4.4. Verify Installation2. Getting started2.1. QEMU2.2. Hardware2.3. Memory-mapped Registers2.4. Semihosting2.5. Panicking2.6. Exceptions2.7. Interrupts2.8. IO3. Peripherals3.1. A first attempt in Rust3.2. The Borrow Checker3.3. Singletons4. Static Guarantees4.1. Typestate Programming4.2. Peripherals as State Machines4.3. Design Contracts4.4. Zero Cost Abstractions5. Portability6. Concurrency7. Collections8. Design Patterns8.1. HALs8.1.1. Checklist8.1.2. Naming8.1.3. Interoperability8.1.4. Predictability8.1.5. GPIO9. Tips for embedded C developers10. Interoperability10.1. A little C with your Rust10.2. A little Rust with your C11. Unsorted topics11.1. Optimizations: The speed size tradeoffAppendix A: Glossary\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLight (default)\nRust\nCoal\nNavy\nAyu\n\n\n\n\n\nThe Embedded Rust Book\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWindows\narm-none-eabi-gdb\nARM provides .exe installers for Windows. Grab one from here, and follow the instructions.\nJust before the installation process finishes tick/select the \"Add path to environment variable\"\noption. Then verify that the tools are in your %PATH%:\n$ arm-none-eabi-gdb -v\nGNU gdb (GNU Tools for Arm Embedded Processors 7-2018-q2-update) 8.1.0.20180315-git\n(..)\n\nOpenOCD\nThere's no official binary release of OpenOCD for Windows but if you're not in the mood to compile\nit yourself, the xPack project provides a binary distribution, here. Follow the\nprovided installation instructions. Then update your %PATH% environment variable to\ninclude the path where the binaries were installed. (C:\\Users\\USERNAME\\AppData\\Roaming\\xPacks\\@xpack-dev-tools\\openocd\\0.10.0-13.1\\.content\\bin\\,\nif you've been using the easy install)\nVerify that OpenOCD is in your %PATH% with:\n$ openocd -v\nOpen On-Chip Debugger 0.10.0\n(..)\n\nQEMU\nGrab QEMU from the official website.\nST-LINK USB driver\nYou'll also need to install this USB driver or OpenOCD won't work. Follow the installer\ninstructions and make sure you install the right version (32-bit or 64-bit) of the driver.\nThat's all! Go to the next section.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to execute custom action only in install (not uninstall)", "id": 764, "answers": [{"answer_id": 764, "document_id": 451, "question_id": 764, "text": "Please be careful with REMOVE=ALL. It is not available before installvalidate sequence.\nAnd check below links for more details:\nhttp://msdn.microsoft.com/en-us/library/aa371194(v=vs.85).aspx\nhttp://msdn.microsoft.com/en-us/library/aa368013(v=vs.85).aspx", "answer_start": 485, "answer_category": null}], "is_impossible": false}], "context": "I'm sure this is fairly easy, but I've kind of had a hard time with it. I've got a custom action that executes a different (non-msi) installer on installation. Unfortunately, I've noticed that it also executes the installer on UNinstallation!\nI've looked through the options but I cant' seem to find out how to stop this. If anybody could help me I would be incredibly grateful.\nAlso, how do I set a custom action to go off only during UNinstall? Any help is greatly appreciated guys! Please be careful with REMOVE=ALL. It is not available before installvalidate sequence.\nAnd check below links for more details:\nhttp://msdn.microsoft.com/en-us/library/aa371194(v=vs.85).aspx\nhttp://msdn.microsoft.com/en-us/library/aa368013(v=vs.85).aspx\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "wix alternatives", "id": 1982, "answers": [{"answer_id": 1968, "document_id": 1567, "question_id": 1982, "text": "Wix generate single component id for entire tree\nWiX Quick Start Tips", "answer_start": 2275, "answer_category": null}], "is_impossible": false}], "context": "\n\n                    \n\n            \n\n        \n\n            \n\n                \n\n                    \n\n                        Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.\n\n                        \n\n                    \n\n                \n\n            \n\n        \n\n            \n\n        \n\n                \n\n                    \n\n                \n\n            \n\n                \n\n                    Want to improve this question? Update the question so it's on-topic for Stack Overflow.\n\n                \n\n                    Closed 6 years ago.\n\n\n\n            \n\n        \n\n            \n\n                    \n\n                        Improve this question\n\n                    \n\n            \n\n\n\n\n\n    \n\n\n\nI have tried to build a MSI package for my Windows Service with WiX for a couple of days but have big problems to get it the way I need.\n\nThe documentation is very bad and then XML structure just dont feels structured. I really don't see how they could have been doing a worse job at version 3.6.\n\nI dont want to invest all my time in the MSI package, instead I need my time in dev of the main application.\n\nSo the questions is, what MSI builder should I use If I need:\n\n\nSet app.config settings during installation\nInstall Windows Service\nStart Windows Service\nSimple install/uninstall\n\n\nThats really all I need\n    \n\nWiX can do all those things fairly easily (I have applications that do that and some that I use WiX to install). Problem is you have to have a pretty good understanding of Windows Installer to use WiX, as it does not really hide any of the detail from you.\n\nThe obviously alternative is InstallShield (They have a LE version that comes with VS2012 apparently, using VS2012 but not that) and Express is relatively reasonably priced. \n\nYou also may want to check out InstallAware, but I have not had good experiences with them personally when I have looked at them. \n\nI have also been told that Advanced Installer is usable, though I have not used it myself personally.\n    \n\nA list and description of other installation products as well as Wix: What installation product to use? InstallShield, WiX, Wise, Advanced Installer, etc\n\nAnd a link to some Wix quick-start suggestions (similar articles):\n\n\nWix generate single component id for entire tree\nWiX Quick Start Tips\n\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Deploying assembly package with maven-release-plugin", "id": 504, "answers": [{"answer_id": 506, "document_id": 230, "question_id": 504, "text": "Deploying files is not part of the release plugin but of the deploy plugin (release doesn't deploy stuff anywhere by itself but you can configure the deploy plugin to be called during a release).\nNormally, the deploy plugin will deploy all artifacts to the remote repository but assemblies aren't artifacts; Maven can't use .tar.gz archives in its repository in any way, so it doesn't make sense to deploy them in the first place.", "answer_start": 615, "answer_category": null}], "is_impossible": false}], "context": "We use Hudson and the maven-release-plugin to do the release builds. Now I have a project which contains an assembly that puts together all needed components and then packages them into a .tar.gz package with the desired directory structure.\nNow I'm trying to get the release-plugin to deploy this package to our Maven repository during the release:perform goal, but only the standard stuff (sources, javadoc, POM) are deployed.\nI've already bound the assembly goal to the maven package phase, and the .tar.gz gets build during the release, but not uploaded to the repository. Any hints what I'm doing wrong here ?\nDeploying files is not part of the release plugin but of the deploy plugin (release doesn't deploy stuff anywhere by itself but you can configure the deploy plugin to be called during a release).\nNormally, the deploy plugin will deploy all artifacts to the remote repository but assemblies aren't artifacts; Maven can't use .tar.gz archives in its repository in any way, so it doesn't make sense to deploy them in the first place.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How can I convert my Java program to an .exe file?", "id": 847, "answers": [{"answer_id": 842, "document_id": 527, "question_id": 847, "text": "You use the javapackager command to perform tasks related to packaging Java and JavaFX applications.", "answer_start": 13, "answer_category": null}], "is_impossible": false}], "context": "javapackager\nYou use the javapackager command to perform tasks related to packaging Java and JavaFX applications.\n\nSynopsis\n\njavapackager command [options]\ncommand\nThe task that you want to perform. See Commands for the javapackager Command.\n\noptions\nOne or more options for the command, separated by spaces. See Options for the createbss Command, Options for the createjar Command, Options for the deploy Command, Options for the makeall Command, and Options for the signjar Command.\n\nNote:\n\nThe javapackager command isn\u2019t available on Oracle Solaris.\n\nDescription\n\nThe Java Packager tool compiles, packages, and prepares Java and JavaFX applications for distribution. The javapackager command is the command-line version. For available Ant tasks, see JavaFX Ant Tasks in Java Platform, Standard Edition Deployment Guide.\n\nFor self-contained applications, the Java Packager for JDK 9 packages applications with a JDK 9 runtime image generated by the jlink tool. To package a JDK 8 or JDK 7 JRE with your application, use the JDK 8 Java Packager.\n\nCommands for the javapackager Command\n\nYou can run the following commands from the command line, followed by the options for the command.\n\n-createbss\nConverts CSS files into binary form. See Options for the createbss Command for the options used with this command.\n\n-createjar\nProduces a JAR according to other parameters. See Options for the createjar Command for the options used with this command.\n\n-deploy\nAssembles the application package for distribution. Modular and nonmodular applications are supported. By default, the deploy task generates the base application package. It can also generate a self-contained application package, if requested. See Options for the deploy Command for the options used with this command.\n\nThe bundle for a self-contained application includes a custom runtime created by calling jlink. The Java Packager for JDK 9 packages applications with a JDK 9 runtime image. To package a JDK 8 or JDK 7 JRE with your application, use the JDK 8 Java Packager.\n\n-makeall\nNote:\n\nThe -makeall command for the Java Packager tool is deprecated in JDK 9 in preparation for removal in a future release.\nPerforms compilation, createjar, and deploy steps as one call, with most arguments predefined, and attempts to generate all applicable self-contained application packages. The source files must be located in a folder called src, and the resulting files (JAR, JNLP, HTML, and self-contained application packages) are put in a folder called dist. This command can be configured only in a minimal way and is as automated as possible. See Options for the makeall Command for the options used with this command.\n\n-signjar\nNote:\n\nThe -signjar command for the Java Packager tool is deprecated in JDK 9 in preparation for removal in a future release. It also doesn\u2019t work with multirelease JAR file. Instead, use the jarsigner tool to sign the JAR file.\nSigns JAR files with a provided certificate. See Options for the signjar Command for the options used with this command.\n\nOptions for the createbss Command\n\n-outdir dir\nName of the directory that receives the generated output files.\n\n-srcdir dir\nBase directory of the files to pack.\n\n-srcfiles files\nList of files in srcdir. If omitted, all files in srcdir (which is a mandatory argument in this case) will be used.\n\nOptions for the createjar Command\n\n-appclass app-class\nQualified name of the application class to be executed.\n\n-argument arg\nAn unnamed argument to be inserted into the JNLP file as an <fx:argument> element.\n\n-classpath files\nList of dependent JAR file names.\n\n-manifestAttrs manifest-attributes\nList of names and values for additional manifest attributes. Syntax:\n\n\"name1=value1,name2=value2,...\"\n-nocss2bin\nThe packager doesn\u2019t convert CSS files to binary form before copying to JAR file.\n\n-noembedlauncher\nIf present, the packager will not add the JavaFX launcher classes to the jarfile.\n\n-outdir dir\nName of the directory that receives the generated output files.\n\n-outfile filename\nName (without the extension) of the file that\u2019s generated.\n\n-paramfile file\nProperties file with named parameters and their default values to pass to the application.\n\n-preloader preloader-class\nQualified name of the JavaFX preloader class to be executed. Use this option only for JavaFX applications. Don\u2019t use for Java applications, including headless applications.\n\n-runtimeversion version\nSpecifies the version of the required JavaFX Runtime.\n\n-srcdir dir\nBase directory of the files to pack.\n\n-srcfiles files\nList of files in srcdir. If omitted, all files in srcdir (which is a mandatory argument in this case) will be packed.\n\nOptions for the deploy Command\n\n--add-modules modulename[,modulename...]\nSpecifies the root modules to resolve in addition to the initial module.\n\n-allpermissions\nIf present, the application requires all security permissions in the JNLP file.\n\n-appclass app-class\nQualified name of the application class to be executed.\n\n-argument arg\nAn unnamed argument to be inserted into an <fx:argument> element in the JNLP file.\n\n-Bbundler-argument=value\nProvides information to the bundler that\u2019s used to package a self-contained application. See Arguments for Self-Contained Application Bundles for information about the arguments for each bundler.\n\n-callbacks callback-methods\nSpecifies one or more user callback methods in generated HTML. The format is the following:\n\n\"name1:value1,name2:value2,...\"\n-description description\nDescription of the application.\n\n-embedjnlp\nIf present, the JNLP file embedded in the HTML document.\n\n-embedCertificates\nIf present, the certificates will be embedded in the jnlp file.\n\n-height height\nHeight of the application.\n\n-htmlparamfile file\nProperties file with parameters for the resulting application when it is run in the browser.\n\n-isExtension\nIf present, the srcfiles as extensions.\n\n--limit-modules modulename[,modulename...]\nLimits the universe of observable modules.\n\n-m modulename [/mainclass] or --module modulename [/mainclass]\nSpecifies the initial module to resolve, and the name of the main class to execute if not specified by the module.\n\n-p module path or --module-path module path\nA : separated list of directories, each directory is a directory of modules.\n\n-name name\nName of the application.\n\n-native type\nGenerate the files needed for a Java Web Start application when type is set to jnlp. Otherwise, generate self-contained application bundles, if possible. Use the -B option to provide arguments to the bundlers being used. If type is specified, then only a bundle of this type is created. If no type is specified, then all is used.\n\nThe following values are valid for type:\n\njnlp: Generates the .jnlp and .html files for a Java Web Start application.\n\nall: Runs all of the installers for the platform on which it\u2019s running, and creates a disk image for the application. This value is used if type isn\u2019t specified.\n\ninstaller: Runs all of the installers for the platform on which it\u2019s running.\n\nimage: Creates a disk image for the application.\n\nLinux and Windows: The image is the directory that gets installed.\n\nmacOS: The image is the .app file.\n\nexe: Generates a Windows .exe package.\n\nmsi: Generates a Windows Installer package.\n\ndmg: Generates a DMG file for macOS.\n\npkg: Generates a .pkg package for macOS.\n\nmac.appStore: Generates a package for the Mac App Store.\n\nrpm: Generates an RPM package for Linux.\n\ndeb: Generates a Debian package for Linux.\n\n-nosign\nLinux and macOS: If present, the bundle generated for self-contained applications isn\u2019t signed by the bundler. The default for bundlers that support signing is to sign the bundle if signing keys are properly configured. This attribute is ignored by bundlers that don\u2019t support signing.\n\n-outdir dir\nName of the directory that receives the generated output files.\n\n-outfile filename\nName (without the extension) of the file that is generated.\n\n-paramfile file\nProperties file with named parameters and their default values to pass to the application.\n\n-preloader preloader-class\nQualified name of the JavaFX preloader class to be executed. Use this option only for JavaFX applications. Don\u2019t use for Java applications, including headless applications.\n\n-srcdir dir\nBase directory of the files to pack.\n\n-srcfiles files\nList of files in srcdir. If omitted, all files in srcdir (which is a mandatory argument in this case) will be used.\n\n--strip-native-commands [true|false]\nRemove command-line tools such as java.exe from the Java runtime that\u2019s generated for packaging with self-contained applications. The default is true. To keep the tools in the runtime, specify false.\n\n-templateId\nApplication ID of the application for template processing.\n\n-templateInFilename\nName of the HTML template file. Placeholders are in the following form:\n\n#XXXX.YYYY(APPID)#\nAPPID is the identifier of an application and XXXX is one of following:\n\nDT.SCRIPT.URL\n\nLocation of dtjava.js in the Deployment Toolkit. By default, the location is\n\nhttp://java.com/js/dtjava.js.\n\nDT.SCRIPT.CODE\n\nScript element to include dtjava.js of the Deployment Toolkit.\n\nDT.EMBED.CODE.DYNAMIC\n\nCode to embed the application into a given placeholder. It is expected that the code is wrapped in the function() method.\n\nDT.EMBED.CODE.ONLOAD\n\nAll of the code needed to embed the application into a web page using the onload hook (except inclusion of dtjava.js).\n\nDT.LAUNCH.CODE\n\nCode needed to launch the application. It\u2019s expected that the code is wrapped in the function() method.\n\n-templateOutFilename\nName of the HTML file generated from the template.\n\n-title title\nTitle of the application.\n\n-updatemode update-mode\nSets the update mode for the JNLP file.\n\n-vendor vendor\nVendor of the application.\n\n-width width\nWidth of the application.\n\nOptions for the makeall Command\n\nNote:\n\nThe -makeall command for the Java Packager tool is deprecated in JDK 9 in preparation for removal in a future release.\n-appclass app-class\nQualified name of the application class to be executed.\n\n-classpath files\nList of dependent JAR file names.\n\n-height height\nHeight of the application.\n\n-name name\nName of the application.\n\n-preloader preloader-class\nQualified name of the JavaFX preloader class to be executed. Use this option only for JavaFX applications. Don\u2019t use for Java applications, including headless applications.\n\n-v\nEnables verbose output.\n\n-width width\nWidth of the application.\n\nOptions for the signjar Command\n\nNote:\n\nThe -signjar command for the Java Packager tool is deprecated in JDK 9 in preparation for removal in a future release. It also doesn\u2019t work with multirelease JAR files. Use the jarsigner tool to sign the JAR file.\n-alias key-alias\nAlias for the key.\n\n-keyPass password\nPassword for recovering the key.\n\n-keyStore file\nKeystore file name.\n\n-outdir dir\nName of the directory that receives the generated output files.\n\n-storePass password\nPassword to check the integrity of the keystore or unlock the keystore.\n\n-storeType type\nKeystore type. The default value is jks.\n\n-srcdir dir\nBase directory of the files to pack.\n\n-srcfiles files\nList of files in srcdir. If omitted, all files in srcdir (which is a mandatory argument in this case) will be packed.\n\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to fix npm throwing error without sudo?", "id": 961, "answers": [{"answer_id": 956, "document_id": 592, "question_id": 961, "text": "sudo chown -R $(whoami) ~/.npm", "answer_start": 2441, "answer_category": null}], "is_impossible": false}, {"question": "How to fix npm throwing error without sudo on MAC OS?", "id": 962, "answers": [{"answer_id": 957, "document_id": 592, "question_id": 962, "text": "System Preference > Users & Groups > (unlock) > press + :\n\nNew Account > \"Group\"\nAccount Name : nodegrp\n\nAfter creating the group, tick the user to be included in this group\n\nsudo chgrp -R nodegrp /usr/local/lib/node_modules/\nsudo chgrp nodegrp /usr/bin/node\nsudo chgrp nodegrp /usr/bin/npm\nsudo chown -R $(whoami):nodegrp ~/.npm\n", "answer_start": 20020, "answer_category": null}], "is_impossible": false}, {"question": "How to fix npm throwing error without sudo with security?", "id": 963, "answers": [{"answer_id": 958, "document_id": 592, "question_id": 963, "text": "First check, where npm point to, if you call:\n\nnpm config get prefix\nIf /usr is returned, you can do the following:\n\nmkdir ~/.npm-global\nexport NPM_CONFIG_PREFIX=~/.npm-global\nexport PATH=$PATH:~/.npm-global/bin\nThis create a npm-Directory in your Home-Directory and point npm to it.\n\nTo got this changes permanent, you have to add the export-command to your .bashrc:\n\necho -e \"export NPM_CONFIG_PREFIX=~/.npm-global\\nexport PATH=\\$PATH:~/.npm-global/bin\" >> ~/.bashrc", "answer_start": 6750, "answer_category": null}], "is_impossible": false}], "context": "How to fix npm throwing error without sudo\nAsked 8 years, 7 months ago\nActive 22 days ago\nViewed 552k times\n\n1397\n\n\n591\nI just installed node and npm through the package on nodejs.org, and whenever I try to search or install something with npm, it throws the following error unless I sudo the command. I have a feeling this is a permissions issue? I am already the admin.\n\nnpm ERR! Error: EACCES, open '/Users/chietala/.npm/-/all/.cache.json'\nnpm ERR!  { [Error: EACCES, open '/Users/chietala/.npm/-/all/.cache.json']\nnpm ERR!   errno: 3,\nnpm ERR!   code: 'EACCES',\nnpm ERR!   path: '/Users/chietala/.npm/-/all/.cache.json' }\nnpm ERR! \nnpm ERR! Please try running this command again as root/Administrator.\n\nnpm ERR! System Darwin 12.2.0\nnpm ERR! command \"node\" \"/usr/local/bin/npm\" \"search\" \"bower\"\nnpm ERR! cwd /Users/chietala\nnpm ERR! node -v v0.10.4\nnpm ERR! npm -v 1.2.18\nnpm ERR! path /Users/chietala/.npm/-/all/.cache.json\nnpm ERR! code EACCES\nnpm ERR! errno 3\nnpm ERR! stack Error: EACCES, open '/Users/chietala/.npm/-/all/.cache.json'\nnpm ERR! \nnpm ERR! Additional logging details can be found in:\nnpm ERR!     /Users/chietala/npm-debug.log\nnpm ERR! not ok code 0\nnode.js\nunix\npermissions\nnpm\nsudo\nShare\nImprove this question\nFollow\nedited May 4 at 16:11\n\nKarl Hill\n8,35033 gold badges4444 silver badges7373 bronze badges\nasked Apr 22 '13 at 15:35\n\nChad\n17.2k88 gold badges2929 silver badges4141 bronze badges\n40\nPlease consider the solution using NVM: stackoverflow.com/a/24404451/1480391 (instead of hacking with permissions) \u2013 \nYves M.\n Jun 30 '14 at 15:57 \n2\n@janaspage You can not install node or NVM (Node Version Manager) via NPM (Node Package Manager), it's non sense. NPM comes within node (it is installed at the same time). Have a look at the Wikipedia page: en.wikipedia.org/wiki/Npm_(software) \u2013 \nYves M.\n Aug 19 '14 at 12:06\n5\nFinally a solution better than sudo chown: github.com/sindresorhus/guides/blob/master/\u2026 \u2013 \nDmitri Zaitsev\n Jan 19 '16 at 10:06\nUnder OSX and installing node with the 0official pkg intaller this solution did not work. I used this one instead: stackoverflow.com/a/34968008/675565 \u2013 \nfmquaglia\n Feb 4 '16 at 13:01 \n3\nIt explains the issue and the fix: docs.npmjs.com/getting-started/fixing-npm-permissions \u2013 \nn00b\n Oct 18 '16 at 5:54\nShow 2 more comments\n38 Answers\n1\n2\nNext\n\n2382\n\nThis looks like a permissions issue in your home directory. To reclaim ownership of the .npm directory execute:\n\nsudo chown -R $(whoami) ~/.npm\nShare\nImprove this answer\nFollow\nedited Mar 27 '16 at 7:54\n\nZeeshan Hassan Memon\n7,60033 gold badges3737 silver badges5252 bronze badges\nanswered Apr 22 '13 at 16:11\n\nNoah\n32.7k44 gold badges3434 silver badges3131 bronze badges\n111\nI thought that whoami was a placeholder, but it works typed literally as-is, so it must be a variable I don't understand. \u2013 \nSimplGy\n May 14 '13 at 5:18\n133\nwhoami is an actual shell command en.wikipedia.org/wiki/Whoami. The backticks around whoami ensure that it gets executed correctly and then placed into the chown command \u2013 \nNoah\n May 14 '13 at 15:21\nAdd a comment\n\n676\n\nPermissions you used when installing Node will be required when doing things like writing in your npm directory (npm link, npm install -g, etc.).\n\nYou probably ran Node.js installation with root permissions, that's why the global package installation is asking you to be root.\n\nSolution 1: NVM\nDon't hack with permissions, install Node.js the right way.\n\nOn a development machine, you should not install and run Node.js with root permissions, otherwise things like npm link, npm install -g will need the same permissions.\n\nNVM (Node Version Manager) allows you to install Node.js without root permissions and also allows you to install many versions of Node to play easily with them.. Perfect for development.\n\nUninstall Node (root permission will probably be required). This might help you.\nThen install NVM following instructions on this page.\nInstall Node via NVM: nvm install node\nNow npm link, npm install -g will no longer require you to be root.\n\nEdit: See also https://docs.npmjs.com/getting-started/fixing-npm-permissions\n\nSolution 2: Install packages globally for a given user\nDon't hack with permissions, install npm packages globally the right way.\n\nIf you are on OSX or Linux, you can create a user dedicated directory for your global package and setup npm and node to know how to find globally installed packages.\n\nCheck out this great article for step by step instructions on installing npm modules globally without sudo.\n\nSee also: npm's documentation on Fixing npm permissions.\n\nShare\nImprove this answer\nFollow\nedited Oct 20 at 18:03\nanswered Jun 25 '14 at 9:05\n\nYves M.\n27.2k2020 gold badges9797 silver badges133133 bronze badges\n42\nOf all the solutions posted the NVM solution here provided the best results for me. Highly recommend using NVM rather than toying with permissions. \u2013 \nwenincode\n Jul 23 '14 at 2:05\nNone of these solutions worked for me. \u2013 \nMr. Panda\n Oct 20 at 12:20\n1\nI installed node from the snap store but I could not run commands like npm install -g. But after trying the NVM solution everything worked smoothly. Thank you very much for your answer. \u2013 \nKevin Infante\n Oct 26 at 4:45\nAdd a comment\n\n401\n\nAlso you will need the write permission in node_modules directory:\n\nsudo chown -R $USER /usr/local/lib/node_modules\nShare\nImprove this answer\nFollow\nedited May 12 '15 at 15:40\n\nKevin Nagurski\n1,87999 silver badges2323 bronze badges\nanswered Aug 24 '13 at 3:23\n\nXilo\n4,81111 gold badge1010 silver badges22 bronze badges\n133\nI don't know why this still gets upvotes. It is a very bad practice to change the ownership of system directories to a particular user! Please see answer below for other solutions (like creating a separate group for node users). \u2013 \nChristopher Will\n May 28 '14 at 11:06\n32\nWhatever you do -- ABSOLUTELY do not run 'sudo chmod -R whoami' on /usr/local/lib or /usr/lib/ you will ruin your sudoers file and you will hate yourself. \u2013 \nqodeninja\n Mar 10 '15 at 1:02\nAdd a comment\n\n99\n\nChanging the owner on \"system-global\" folders is a hack. On a fresh install, I would configure NPM to use an already writable location for \"user-global\" programs:\n\nnpm config set prefix ~/npm\nThen make sure you add that folder to your path:\n\nexport PATH=\"$PATH:$HOME/npm/bin\"\nSee @ErikAndreas' answer to NPM modules won't install globally without sudo and longer step-by-step guide by @sindresorhus with also sets $MANPATH.\n\nShare\nImprove this answer\nFollow\nedited Apr 8 '20 at 7:28\nanswered May 27 '14 at 12:36\n\nJoel Purra\n21.6k77 gold badges5555 silver badges5959 bronze badges\nAdd a comment\n\n69\n\nWatch OUT!!! Watch OUT!!! Watch OUT!!!\n\nchown or chmod is NOT the solution, because of security-risk.\n\nInstead do this, do:\n\nFirst check, where npm point to, if you call:\n\nnpm config get prefix\nIf /usr is returned, you can do the following:\n\nmkdir ~/.npm-global\nexport NPM_CONFIG_PREFIX=~/.npm-global\nexport PATH=$PATH:~/.npm-global/bin\nThis create a npm-Directory in your Home-Directory and point npm to it.\n\nTo got this changes permanent, you have to add the export-command to your .bashrc:\n\necho -e \"export NPM_CONFIG_PREFIX=~/.npm-global\\nexport PATH=\\$PATH:~/.npm-global/bin\" >> ~/.bashrc\nShare\nImprove this answer\nFollow\nedited Oct 19 '19 at 18:29\n\nJo\u00e3o Portela\n5,75255 gold badges3333 silver badges4848 bronze badges\nanswered Dec 30 '16 at 11:18\n\nsuther\n9,61822 gold badges4848 silver badges8484 bronze badges\nAdd a comment\n\n60\n\nI encountered this when installing Recess (https://github.com/twitter/recess) to compile my CSS for Bootstrap 3.\n\nWhen installing recess:\n\n-npm install recess -g\nYou need to unlock permissions in your home directory, like Noah says:\n\nsudo chown -R `whoami` ~/.npm\nYou also need write permissions to the node_modules directory, like Xilo says, so if it still isn't working, try:\n\nsudo chown -R `whoami` /usr/local/lib/node_modules\nIf you are still seeing errors, you may also need to correct /usr/local permissions:\n\nsudo chown -R `whoami` /usr/local\nPlease note that as indicated in this post /usr/local/ isn't actually a system dir if you are on a Mac, so, this answer is actually perfectly \"safe\" for Mac users. However, if you are on Linux, see Christopher Will's answer below for a multi-user friendly, system dir safe (but more complex) solution.\n\nShare\nImprove this answer\nFollow\nedited May 23 '17 at 12:18\n\nCommunityBot\n111 silver badge\nanswered Oct 3 '13 at 23:16\n\ndanomarr\n73755 silver badges33 bronze badges\n37\nThis is a bad idea. You probably do not want system directories to be owned by a particular user. Beside serious security concerns this is also not multiuser compatible. \u2013 \nChristopher Will\n Jan 9 '14 at 11:10\nAdd a comment\n\n40\n\nOther answers are suggesting to change ownerships or permissions of system directories to a specific user. I highly disadvise from doing so, this can become very awkward and might mess up the entire system!\n\nHere is a more generic and safer approach that supports multi-user as well.\n\nCreate a new group for node-users and add the required users to this group. Then set the ownership of node-dependant files/directories to this group.\n\n# Create new group\nsudo groupadd nodegrp \n\n# Add user to group (logname is a variable and gets replaced by the currently logged in user)\nsudo usermod -a -G nodegrp `logname`\n\n# Instant access to group without re-login\nnewgrp nodegrp\n\n# Check group - nodegrp should be listed as well now\ngroups\n\n# Change group of node_modules, node, npm to new group \nsudo chgrp -R nodegrp /usr/lib/node_modules/\nsudo chgrp nodegrp /usr/bin/node\nsudo chgrp nodegrp /usr/bin/npm\n\n# (You may want to change a couple of more files (like grunt etc) in your /usr/bin/ directory.)\nNow you can easily install your modules as user\n\nnpm install -g generator-angular\nSome modules (grunt, bower, yo etc.) will still need to be installed as root. This is because they create symlinks in /user/bin/.\n\nEdit\n\n3 years later I'd recommend to use Node Version Manager. It safes you a lot of time and trouble.\n\nShare\nImprove this answer\nFollow\nedited Sep 21 '17 at 8:06\nanswered Jan 9 '14 at 11:58\n\nChristopher Will\n2,80133 gold badges2525 silver badges3939 bronze badges\n2\nIf node is installed by sources, although multiuser would be a problem, all modules would work perfectly without the use of sudo. This is also very important because in the case of the yeoman module, people can't update generators through sudoing the yeoman application as it doesn't allow sudo execution :( \u2013 \nHeberLZ\n May 8 '14 at 4:23\n1\nOn Linux, I typically use the built-in staff group to give permissions to my dev folders. Also, it's a good idea to run chmod g+ws node_modules to make sure that your group has read/write permission. \u2013 \njackvsworld\n Jul 20 '15 at 22:09 \nAdd a comment\n\n26\n\nThe official documentation on how to fix npm install permissions with an EACCES error is located at https://docs.npmjs.com/getting-started/fixing-npm-permissions.\n\nI encountered this problem after a fresh install of node using the .pkg installer on OSX. There are some great answers here, but I didn't see a link to npmjs.com yet.\n\nOption 1: Change the permission to npm's default directory\n\nFind the path to npm's directory:\n\nnpm config get prefix\nFor many systems, this will be /usr/local.\n\nWARNING: If the displayed path is just /usr, switch to Option 2.\n\nChange the owner of npm's directories to the name of the current user (your username!):\n\nsudo chown -R $(whoami) $(npm config get prefix)/{lib/node_modules,bin,share}\nThis changes the permissions of the sub-folders used by npm and some other tools (lib/node_modules, bin, and share).\n\nOption 2: Change npm's default directory to another directory\n\nThere are times when you do not want to change ownership of the default directory that npm uses (i.e. /usr) as this could cause some problems, for example if you are sharing the system with other users.\n\nInstead, you can configure npm to use a different directory altogether. In our case, this will be a hidden directory in our home folder.\n\nMake a directory for global installations:\n\nmkdir ~/.npm-global\nConfigure npm to use the new directory path:\n\nnpm config set prefix '~/.npm-global'\nOpen or create a ~/.profile file and add this line:\n\nexport PATH=~/.npm-global/bin:$PATH\nBack on the command line, update your system variables:\n\nsource ~/.profile\nShare\nImprove this answer\nFollow\nedited Jan 23 '16 at 20:46\nanswered Jan 23 '16 at 19:46\n\nHoppyKamper\n90888 silver badges88 bronze badges\n1\nThanks, this one worked for me as the chown soludion did not. You saved me. \u2013 \nfmquaglia\n Feb 4 '16 at 13:00\nI love this answer's option 2. (1) It doesn't have the security vulnerabilities of using sudo, (2) it preserves architecture for other users, (3) it references the documentation, AND (4) it has the needed command line entries included. Thank you for adding! \u2013 \nTom Rose\n Jan 15 '19 at 14:16 \nSo what happens when you install something that's symlinked into /usr/bin/? Now only your user can access it. You log in as someone else, still no access, since the files are actually in another user's home directory! \u2013 \nFrans\n Jan 25 '19 at 16:25\nAdd a comment\n\n16\n\nI ran into this issue, and while it's true that ~/.npm should be owned by your user, npm was not installing the modules there.\n\nWhat actually solved my issue is these commands:\n\nnpm config set prefix ~/.npm\nexport PATH=\"$PATH:$HOME/.npm/bin\"\nIt will make sure that all your global installation will go under this prefix. And it's important that your user owns this directory.\n\nShare\nImprove this answer\nFollow\nedited Sep 17 at 14:02\n\nmiloshavlicek\n42411 gold badge77 silver badges2121 bronze badges\nanswered Dec 2 '20 at 14:54\n\nsandor\n59355 silver badges1616 bronze badges\n3\nI had to do the same thing on Ubuntu 20.04. \u2013 \nStonecraft\n Jan 1 at 13:23\n1\nThis helped on WSL with Ubuntu 20.04. Thanks! \u2013 \ndvdblk\n May 12 at 20:17 \nAdd a comment\n\n12\n\nAs if we need more answers here, but anyway..\n\nSindre Sorus has a guide Install npm packages globally without sudo on OS X and Linux outlining how to cleanly install without messing with permissions:\n\nHere is a way to install packages globally for a given user.\n\nCreate a directory for your global packages\n\nmkdir \"${HOME}/.npm-packages\"\nReference this directory for future usage in your .bashrc/.zshrc:\n\nNPM_PACKAGES=\"${HOME}/.npm-packages\"\nIndicate to npm where to store your globally installed package. In your $HOME/.npmrc file add:\n\nprefix=${HOME}/.npm-packages\nEnsure node will find them. Add the following to your .bashrc/.zshrc:\n\nNODE_PATH=\"$NPM_PACKAGES/lib/node_modules:$NODE_PATH\"\nEnsure you'll find installed binaries and man pages. Add the following to your .bashrc/.zshrc:\n\nPATH=\"$NPM_PACKAGES/bin:$PATH\"\n# Unset manpath so we can inherit from /etc/manpath via the `manpath`\n# command\nunset MANPATH # delete if you already modified MANPATH elsewhere in your config\nMANPATH=\"$NPM_PACKAGES/share/man:$(manpath)\"\nCheck out npm-g_nosudo for doing the above steps automagically\n\nCheckout the source of this guide for the latest updates.\n\nShare\nImprove this answer\nFollow\nedited Feb 25 '15 at 4:06\n\nAndy Hayden\n309k8686 gold badges580580 silver badges506506 bronze badges\nanswered Jan 20 '15 at 9:35\n\nptim\n13k99 gold badges7474 silver badges9191 bronze badges\n2\ntx for the edit @AndyHayden :) My preferred method is suggested in comments above: use NVM! stackoverflow.com/a/24404451/1480391 \u2013 \nptim\n Feb 25 '15 at 4:41\nThe only solution that worked for me that doesn't involve mucking about with permissions. I hate NPM and its idiotic permissions stupidity. Thanks for the solution! \u2013 \nRyanNerd\n Feb 1 '18 at 18:22\nAdd a comment\n\n11\n\nTL;DR\n\nalways use sudo -i or sudo -H when running npm install to install global packages.\n\nWhen you use npm, it downloads packages to your user home directory. When you run as sudo, npm installs files to the same directory, but now they are owned by root.\n\nSo this is what happens to absolutely every single person who has ever used npm:\n\ninstall some local packages without issue using npm install foo\ninstall global package using sudo install -g foo-cli without issue\nattempt to install local package with npm install bar\nget frustrated at the npm designers now that you have to go chmod a directory again\nWhen you use the -i or -H option with sudo, your home directory will be root's home directory. Any global installs will cache packages to /root/.npm instead of root-owned files at /home/me/.npm.\n\nJust always use sudo -i or sudo -H when running npm install to install global packages and your npm permissions problems will melt away.\n\nFor good.\n\n-- q.v. the accepted answer for fixing an already messed up npm.\n\nShare\nImprove this answer\nFollow\nedited Oct 20 at 18:58\nanswered May 12 '16 at 6:13\n\nrich remer\n3,06511 gold badge3030 silver badges4343 bronze badges\nbest and simple fix \u2013 \nManjeet Singh\n Dec 9 '18 at 16:47\nFYI, Last link is seems to now be broken. \u2013 \nstudgeek\n Oct 17 at 16:57\nAdd a comment\n\n7\n\nWhen you run npm install -g somepackage, you may get an EACCES error asking you to run the command again as root/Administrator. It's a permissions issue.\n\nIt's easy to fix, open your terminal (Applications > Utilities > Terminal)\n\nsudo chown -R $USER /usr/local/lib/node_modules\n** I strongly recommend you to not use the package management with sudo (sudo npm -g install something), because you can get some issues later **\n\nReference: http://foohack.com/2010/08/intro-to-npm/\n\nShare\nImprove this answer\nFollow\nedited Jun 12 '14 at 6:53\nanswered Oct 20 '13 at 7:22\n\nJuancarlos Rodr\u00edguez\n1,57422 gold badges1212 silver badges1111 bronze badges\nYay! this one did it to me! after doing the other other ones above: sudo chown -R `whoami` ~/.npm, sudo chown -R `whoami` /usr/local/lib and \u2013 \nRegis Zaleman\n Dec 6 '13 at 21:05 \n8\nThis can cause permissions problems with lots of other apps, so I'd suggest not doing this. Why trade one can of worms for another? \u2013 \nBrad Parks\n Jun 5 '14 at 13:34\n1\nOr at least refine it to /usr/local/lib/node_modules. \u2013 \nKen\n Jun 11 '14 at 18:31\nThis, this and more this. After banging my head against a wall, this did the trick. +1 \u2013 \ndashard\n Mar 21 '15 at 5:52\nThis was my issue. On a new macbook (os x 10.10.3), permissions on /usr/local/lib/node_modules were: $ ll /usr/local/lib/node_modules total 0 drwxr-xr-x 3 24561 wheel 102B Mar 31 18:19 . drwxrwxr-x 4 24561 admin 136B Mar 31 18:19 .. drwxr-xr-x 26 65534 staff 884B Apr 13 10:53 npm \u2013 \ntkane2000\n Apr 17 '15 at 0:53\nShow 2 more comments\n\n6\n\nI had a similar problem at NPM modules won't install globally without sudo, the issue was that when i installed node i did it with sudo via chris/lea ppa repo.\n\nMy solution was to uninstall node and then install it this way:\n\nDownload latest stable node sources from nodejs.org #in my case node-v0.10.20.tar.gz\n\ntar -zxf node-v0.10.20.tar.gz #uncompress sources\n\ncd node-v0.10.20 #enter uncompressed folder\n\nsudo chown $USER -R /usr/local\n\n./configure --prefix=/usr/local && make && make install\n\nPD: If you don't want to change ownership of the /usr/local folder, you can install it somewhere you already own. The problem of this approach is that you will have to bind the installation folder with the bash command line so that we can use the node command later on\n\nmkdir ~/opt\n\n./configure --prefix=~/opt && make && make install\n\necho 'export PATH=~/opt/bin:${PATH}' >> ~/.bashrc #or ~/.profile or ~/.bash_profile or ~/.zshenv depending on the current Operative System\n\nWith either of those approaches, you will be able to do the following without using sudo\n\nnpm install -g module_to_install\n\nShare\nImprove this answer\nFollow\nedited Jun 20 '20 at 9:12\n\nCommunityBot\n111 silver badge\nanswered Oct 14 '13 at 6:28\n\nHeberLZ\n9,71944 gold badges2020 silver badges2424 bronze badges\n1\nI ended up using this method. Did use sudo chown $USER /use/local before building. Looks good so far, time to try to build atom! Thanks! \u2013 \nprasanthv\n May 7 '14 at 3:32 \nAdd a comment\n\n5\n\nFor Mac (adopted from Christoper Will's answer)\n\nMac OS X 10.9.4\n\nSystem Preference > Users & Groups > (unlock) > press + :\n\nNew Account > \"Group\"\nAccount Name : nodegrp\n\nAfter creating the group, tick the user to be included in this group\n\nsudo chgrp -R nodegrp /usr/local/lib/node_modules/\nsudo chgrp nodegrp /usr/bin/node\nsudo chgrp nodegrp /usr/bin/npm\nsudo chown -R $(whoami):nodegrp ~/.npm\n\nShare\nImprove this answer\nFollow", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to restart puma after deploy?", "id": 570, "answers": [{"answer_id": 573, "document_id": 295, "question_id": 570, "text": "You can restart manually using the following command\nbundle exec pumactl -P /home/deploy/.pids/puma.pid restart\nMake sure you point to the correct pid path.", "answer_start": 162, "answer_category": null}], "is_impossible": false}], "context": "I'm using Rails, Puma, Capistrano3. I have installed the gem capistrano3-puma as well. I started Puma with Puma Jungle. How do I restart Puma during deployment? \nYou can restart manually using the following command\nbundle exec pumactl -P /home/deploy/.pids/puma.pid restart\nMake sure you point to the correct pid path.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "I already created a simple GUI application that display will \"hello world\" label.\n", "id": 597, "answers": [{"answer_id": 603, "document_id": 322, "question_id": 597, "text": "Deploy the app. from a web site using Java Web Start. Ensure the user has the minimum Java using deployJava.js (linked from the JWS info page).", "answer_start": 326, "answer_category": null}], "is_impossible": false}], "context": "I already created a simple GUI application that display will \"hello world\" label.\nBut, how can I create an installer from .java or .jar for windows. Let's say that I have created a useful application and want to share it with my friends to install it in their PC without they need to know what is JRE, or how to download JRE. Deploy the app. from a web site using Java Web Start. Ensure the user has the minimum Java using deployJava.js (linked from the JWS info page).\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "chef install and update programs from source", "id": 484, "answers": [{"answer_id": 490, "document_id": 214, "question_id": 484, "text": "First and foremost, if you have the means to host an internal package repository, I generally recommend that you build native packages for your target platform(s), and use the package resource to manage them, rather than building from source. I know that is not always available or feasible", "answer_start": 239, "answer_category": null}], "is_impossible": false}], "context": "I have a program that I build from source. For this I'm using the script resource. What is a good way to implement the logic for installation and update? Right now I just have installation implemented with the built-in not_if conditional.\nFirst and foremost, if you have the means to host an internal package repository, I generally recommend that you build native packages for your target platform(s), and use the package resource to manage them, rather than building from source. I know that is not always available or feasible\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "session app error installing apk?", "id": 851, "answers": [{"answer_id": 846, "document_id": 531, "question_id": 851, "text": "Go to the settings menu, and scroll down to \"About phone.\" Tap it.\nScroll down to the bottom again, where you see \"Build number.\" (Your build number may vary from ours here.)\nTap it seven (7) times. After the third tap, you'll see a playful dialog that says you're four taps away from being a developer. (If only it were that simple, eh?) Keep on tapping, and *poof*, you've got the developer settings back.", "answer_start": 824, "answer_category": null}], "is_impossible": false}], "context": "How to enable developer settings on Android 4.2\nGoogle has hidden the developer settings in the latest version of Jelly Bean - here's how to get them back\nPHIL NICKINSON\n2 Nov 2012\n 23\n\nA few months from now, this will seem funny. But for a little while, for a few scary hours, we had no developer settings on the Nexus 4. Ponder that for a moment. A Nexus device with no developer settings. Actually, it wasn't quite that bad. A little hackery, and we had a direct shortcut to the dev settings. \n\nBut there's an easier way to enable the developer settings on Android 4.2. Oh, they're still on the phone, so nobody freak out. Google hasn't taken the \"developer\" out of its Nexus line, and it's not going to anytime soon. But the settings have been hidden from casual view in the settings menu. Here's how to get them back:\n\nGo to the settings menu, and scroll down to \"About phone.\" Tap it.\nScroll down to the bottom again, where you see \"Build number.\" (Your build number may vary from ours here.)\nTap it seven (7) times. After the third tap, you'll see a playful dialog that says you're four taps away from being a developer. (If only it were that simple, eh?) Keep on tapping, and *poof*, you've got the developer settings back.\nSo why would Google hide the developer settings on a Nexus? It likely has nothing at all to do with the device in this case. Think bigger. It's just a change in Android 4.2. If you're reading this blog, chances are you'll want to poke around in them -- or, in most cases, get to the USB debugging settings. There's not a whole lot of danger here. But ever since the dev settings were consolidated into a single menu in Android 4.0, it's seemed odd that they remained in plain sight on more consumer-friendly phones. Does your mom need dev settings? Nah. So, Google's hidden them in Android 4.2.\n\nWe're fine with that move -- and we expect it to be documented in the Android dev portal.", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "installshield silent uninstall not working at command line", "id": 1490, "answers": [{"answer_id": 1479, "document_id": 1067, "question_id": 1490, "text": "    \n\nTry\nFormat: Setup.exe M{Your Product GUID} /s /f1[Full path]\\*.iss for creating the ISS file for uninstallation.\nFrom Stephanie's sample, I think it's missing the", "answer_start": 2707, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nWe have an older app from 2006 we'd like to uninstall at the command line using group policy, but I can't get a silent uninstall to work.\n\nThis works.  Of course I need to click Next to uninstall:\n\n \"C:\\App\\Setup.exe\" /uninst\n\n\nBut this does not.  I see an hourglass for a couple seconds but the app is not uninstalled.\n\n \"C:\\App\\Setup.exe\" /uninst /s\n\n\nI also unsuccessfully tried some VBScripts.  They find the app listed but the uninstall fails.  I'm not too familiar with how this process is supposed to work.\n    \n\nYou need to create first an ISS response file to silently remove your application, \n\n\nCreate response file : \nC:\\App\\Setup.exe /r /f1c:\\app\\uninstall1.iss\nyou will be asked to uninstall, .... and perhaps reply the others windows.\nThen your application would be uninstalled and you get a new response file c:\\app\\uninstall1.iss\nNext, if you want to remove silently this application on another computer :\nlaunch : C:\\App\\Setup.exe\" /s /f1c:\\app\\uninstall1.iss\n\n\nFor more information see:\n\nhttp://www.itninja.com/blog/view/installshield-setup-silent-installation-switches\n\nBest Regards,\nSt\u00e9phane\n    \n\nTry this, with the original setup.exe version that was used to install\n\n\"C:\\App\\Setup.exe\" /x /s /v/qn\n\n    \n\nI've been struggling with the silent uninstaller for a while, and finally came to a solution that works for me in most cases, both for InstallShield v6 and v7.\n\n1. First (as it was mentioned above), you have to generate an InstallShield Response file (e.g. uninstall.iss). In order to do that you have to launch your setup.exe with parameters: \n\n&gt; setup.exe -x -r -f1\"C:\\Your\\Installer\\Location\\uninstall.iss\"\n\n\nThis will go through the normal uninstall wizard and generate a Response file for you: uninstall.iss\n\n2. Then, before trying your silent uninstaller, I guess, you should re-install the software.\n\n3. And finally, run your silent uninstaller playing back the recently generated Response file:\n\n&gt; setup.exe -x -s -l0x9 -ARP -f1\"C:\\Your\\Installer\\Location\\uninstall.iss\"\n\n\nThat's it. \n\nNow, a few important notes:\n\nNote 1: I'm working with a 3-rd party installation package that I didn't build myself.\n\nNote 2:  I use dashes (-) instead of slashes (/) to define parameters. For some reason it doesn't work with slashes for me. Weird but true. \n\nNote 3: The -ARP and -l switches are required for some installation packages to manage the software removal from the Add/Remove Programs list and to preset the default input language accordingly.\n\nSuccessful silent uninstallation is all about the correct parameters!\nSo keep exploring, the correct parameters vary depending on a specific package and installer version.\n\nI hope my input was helpful.\n    \n\nTry\nFormat: Setup.exe M{Your Product GUID} /s /f1[Full path]\\*.iss for creating the ISS file for uninstallation.\nFrom Stephanie's sample, I think it's missing the GUID.\n\nThere's a good link at the developer's site @ Creating the Response File.\n\nTry it out n tell us,\n\nTommy Kwee\n    \n\nI struggled with this for a long time so posting it here in case anybody else stumbles upon it.\nIf you happen to have an installer which uses the legacy Package-For-The-Web format then you need to use the parameter -a to pass additional parameters to the extracted setup file.\nRecord (un)installation files (click through the installer manually):\n.\\DWG2PDF2019.exe -a /r /f1\"c:\\app\\dwg2019_install.iss\"\n.\\DWG2PDF2019.exe -a /r /f1\"c:\\app\\dwg2019_uninstall.iss\"\n\nSilently (un)install:\n.\\DWG2PDF2019.exe -s -a /s /f1\"c:\\app\\dwg2019_install.iss\"\n.\\DWG2PDF2019.exe -s -a /s /f1\"c:\\app\\dwg2019_uninstall.iss\"\n\nSource: https://help.hcltechsw.com/caa/3.0/topics/appacc_silent_install_t.html\n    \n\nThere's another way to uninstall an app by searching for it in the Registry, and using the UninstallString (this sample code uses powershell on windows 10):\n# Get all installed apps from Registry\n$Apps = @()\n$Apps += Get-ItemProperty \"HKLM:\\SOFTWARE\\Wow6432Node\\Microsoft\\Windows\\CurrentVersion\\Uninstall\\*\" # 32 Bit\n$Apps += Get-ItemProperty \"HKLM:\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Uninstall\\*\"             # 64 Bit\n\n# Uninstall My App\n$my_app = $Apps | Where-Object{$_.DisplayName -eq \"The Name Of My App\"}\n$uninstall_string = \" /C \" + $my_app.UninstallString+' /S'\nStart-Process -FilePath \"cmd.exe\" -ArgumentList $uninstall_string -Wait\n\nif you don't know the exact full display name of your app, you can print the \"Apps\" array to a file, and search there:\n$Apps | Out-File C:\\filename.txt\n\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Deploying Django with gunicorn and nginx", "id": 333, "answers": [{"answer_id": 341, "document_id": 145, "question_id": 333, "text": "People use nginx for #1 because its a very fast proxy and it doesn't come with the overhead of a comprehensive server like Apache. You are free to use Apache if you are comfortable with it. There is no requirement that \"for mulitple sites, use nginx\"; you just need a service that is listening on that port, knows how to redirect (proxy) to your processes running the actual django code", "answer_start": 356, "answer_category": null}], "is_impossible": false}], "context": "This is a broad question but I'd like to get a canonical answer. I have been trying to deploy a site using gunicorn and nginx in Django. After reading tons of tutorials I have been successful but I can't be sure that the steps I followed are good enough to run a site without problems or maybe there are better ways to do it. That uncertainty is annoying. People use nginx for #1 because its a very fast proxy and it doesn't come with the overhead of a comprehensive server like Apache. You are free to use Apache if you are comfortable with it. There is no requirement that \"for mulitple sites, use nginx\"; you just need a service that is listening on that port, knows how to redirect (proxy) to your processes running the actual django code. ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "keep files after uninstallation of android app", "id": 1393, "answers": [{"answer_id": 1382, "document_id": 965, "question_id": 1393, "text": "You can put files in a directory derived from Environment.getExternalStorageDirectory(). These files will persist after an uninstall. However, a user can delete those files any time they like, regardless of whether your app is installed or not.\n\nOther than that, there is no place on the device that you can place files that will survive an uninstall.", "answer_start": 1592, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI want to use a file to store purchased data. Since the app is going to be free and a certain amount of credits are given at the installation.\nWhat happens now is that if I uninstall the installation, also the files on the SD card are deleted. So at reinstallation you have your free credits again.\n\nI use the following code:\n\nFile file = new File(mContext.getExternalFilesDir(null), FILENAME);\n\n                try {\n\n                    FileOutputStream os = new FileOutputStream(file); \n                    DataOutputStream out = new DataOutputStream(os);\n\n\n                    out.writeInt(5); //5 CREDITS TO START\n                    out.close();\n\n                    if(CreditFileExists()) {\n                        Log.w(\"ExternalStorageFileCreation\", \"First Time so give 5 credits\");\n                    } else {\n                        Log.e(\"ExternalStorageFileCreation\", \"Error in creating CreditFile\");\n                        Toast.makeText(mContext, R.string.sdnotavailable, Toast.LENGTH_SHORT).show();\n                    }\n\n                } catch (IOException e) {\n                    // Unable to create file, likely because external storage is\n                    // not currently mounted.\n                    Log.e(\"ExternalStorage\", \"Error writing \" + file, e);\n                    Toast.makeText(mContext, R.string.sdnotavailable, Toast.LENGTH_SHORT).show();\n                }\n\n\nSo the file is saved in this directory getExternalFilesDir(null), apparently that directory is cleaned at uninstallation, anybody any tips for this matter?\n\nThanks!\n    \n\nYou can put files in a directory derived from Environment.getExternalStorageDirectory(). These files will persist after an uninstall. However, a user can delete those files any time they like, regardless of whether your app is installed or not.\n\nOther than that, there is no place on the device that you can place files that will survive an uninstall.\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Difference between build and deploy\uff1f", "id": 476, "answers": [{"answer_id": 484, "document_id": 208, "question_id": 476, "text": "Build means to Compile the project. Deploy means to Compile the project & Publish the output", "answer_start": 200, "answer_category": null}], "is_impossible": false}], "context": "What is the difference between a build and deploy and re-deploy? What should be done when you just have some HTML changes and no Java code changes? Should I do a build and deploy or just a re-deploy? Build means to Compile the project. Deploy means to Compile the project & Publish the output", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "wix specify licence shows nothing", "id": 815, "answers": [{"answer_id": 810, "document_id": 497, "question_id": 815, "text": "1.\tOpen WordPad\n2.\tWrite your text\n3.\tSave by default(rtf)\n4.\tRebuild your msi\n5.\tProfit.", "answer_start": 386, "answer_category": null}], "is_impossible": false}], "context": "I'm trying to specify the licence for my wix setup project.\nI have created a rtf with a few dummy lines in wordpad/notepad/vs tried a few different ways as I read there was an issue with ones created in word but I dont think that should apply here, in any case I also opened it up in notepad++ and verified there is no funky characters hidden in it.\nThanks to @Daniel Powell's decision\n1.\tOpen WordPad\n2.\tWrite your text\n3.\tSave by default(rtf)\n4.\tRebuild your msi\n5.\tProfit.\nWhen I run the installer all that is shown in the licence area is a blank text box with no scroll bars etc.\nIs there something else I should be doing?\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "how to build install travis build?", "id": 979, "answers": [{"answer_id": 974, "document_id": 602, "question_id": 979, "text": "git clone https://github.com/travis-ci/travis-build\ncd travis-build\nmkdir -p ~/.travis\nln -s $PWD ~/.travis/travis-build\ngem install bundler\nbundle install --gemfile ~/.travis/travis-build/Gemfile\nbundler binstubs travis", "answer_start": 544, "answer_category": null}], "is_impossible": false}], "context": "Travis Build Build Status\nTravis Build exposes an API that Travis Workers and Job Board use to generate a bash script which is then copied to the job execution environment and executed, with the resulting output streamed back to Travis.\n\nThis code base has gone through several iterations of development, and was originally extracted from the legacy Travis Worker, before taking its current form.\n\nRunning test suites\nRun\n\nbundle exec rake spec\n\nUse as addon for Travis CLI\nYou can set travis-build up as a plugin for the command line client:\n\ngit clone https://github.com/travis-ci/travis-build\ncd travis-build\nmkdir -p ~/.travis\nln -s $PWD ~/.travis/travis-build\ngem install bundler\nbundle install --gemfile ~/.travis/travis-build/Gemfile\nbundler binstubs travis\nYou will now be able to run travis compile, which produces the bash script that runs the specified job, except that the secure environment variables are not defined, and that the build matrix expansion is not considered, e.g:\n\n~/.travis/travis-build/bin/travis compile\nImportant\nThe bash script generated by the compile command contains commands that make changes to the system on which it is executed (e.g., edit /etc/resolv.conf, install software). Some require sudo privileges and they are not easily undone.\n\nIt is highly recommended that you run this in a container or other virtualized environment.\n\nInvocation\nThe command can be invoked in 3 ways:\n\nWithout an argument, it produces and prints a bash script from the actions in the local .travis.yml without considering env and matrix values (travis-build is unable to expand these keys correctly).\n\n~/.travis/travis-build/bin/travis compile\nWith a single integer, it produces the script for the given build (or the first job of that build matrix).\n\n~/.travis/travis-build/bin/travis compile 8\nWith an argument of the form M.N, it produces the bash script for the job M.N.\n\n~/.travis/travis-build/bin/travis compile 351.2\nThe generated script can be used in a container or virtualized environment that closely mimics Travis CI's build environment to aid you in debugging the build failures. Instructions for running such a container are available in the Travis CI docs.\n\nRaw CLI script\nIn addition to the travis CLI plugin you can also run the standalone CLI script:\n\nbundle exec script/compile < payload.json > build.sh\nDocker container\nIf you want to run travis-build locally on your machine (e.g. to interact with worker), you can also run it as a docker container with docker-compose:\n\nFirst, build the image:\n\ndocker-compose build web\nSecond, run the image:\n\ndocker-compose run web\nYou may wish to run with a different setup for local development. The following shows running travis-build in the development environment, forwarding the Docker image's port 4000 to the host's port 4000:\n\ndocker-compose run -e RACK_ENV=development -p 4000:4000 web\nto build and run it. This will create a container with the contents of the travis-build repository in the /usr/src/app directory, and start you off in that directory. From there, you can run the commands listed in the Use as addon for Travis CLI section to make the compile command available to Travis CLI within the container.\n\nLicense & copyright information\nSee LICENSE file.\n\nCopyright (c) 2011-2016 Travis CI development team.", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Add a dependency in Maven", "id": 1817, "answers": [{"answer_id": 1802, "document_id": 1388, "question_id": 1817, "text": "You'll have to do this in two steps:\n1. Give your JAR a groupId, artifactId and version and add it to your repository.\n2. Update dependent projects to reference this JAR.", "answer_start": 199, "answer_category": null}], "is_impossible": false}], "context": "How do I take a jar file that I have and add it to the dependency system in maven 2? I will be the maintainer of this dependency and my code needs this jar in the class path so that it will compile.\nYou'll have to do this in two steps:\n1. Give your JAR a groupId, artifactId and version and add it to your repository.\n2. Update dependent projects to reference this JAR.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Install apps silently, with granted INSTALL_PACKAGES permission", "id": 670, "answers": [{"answer_id": 675, "document_id": 363, "question_id": 670, "text": "You should define\n<uses-permission\n    android:name=\"android.permission.INSTALL_PACKAGES\" />\nin your manifest, then if whether you are in system partition (/system/app) or you have your application signed by the manufacturer, you are going to have INSTALL_PACKAGES permission.", "answer_start": 446, "answer_category": null}], "is_impossible": false}], "context": "I am trying to silently install apk into the system. My app is located in /system/app and successfully granted permission \"android.permission.INSTALL_PACKAGES\"\nHowever I can't find anywhere how to use this permission. I tried to copy files to /data/app and had no success. Also I tried using this code\nBut this code opens standard installation dialog. How can I install app silently without root with granted android.permission.INSTALL_PACKAGES?\nYou should define\n<uses-permission\n    android:name=\"android.permission.INSTALL_PACKAGES\" />\nin your manifest, then if whether you are in system partition (/system/app) or you have your application signed by the manufacturer, you are going to have INSTALL_PACKAGES permission.\nPS I am writing an app that will install many apks from folder into the system on the first start (replace Setup Wizard). I need it to make firmware lighter.\nIf you think that I am writing a virus: All programs are installed into /data/app. Permission Install_packages can only be granted to system-level programs located in /system/app or signed with the system key. So virus can't get there.\nAs said http://www.mail-archive.com/android-porting@googlegroups.com/msg06281.html apps CAN be silent installed if they have install_packages permission. Moreover you don't need Install_packages permission to install packages not silently. Plus http://www.androidzoom.com/android_applications/tools/silent-installer_wgqi.html\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Including a directory using Pyinstaller", "id": 1186, "answers": [{"answer_id": 1179, "document_id": 762, "question_id": 1186, "text": "try this: --add-data=\"path/to/folder/*;.\"", "answer_start": 262, "answer_category": null}], "is_impossible": false}], "context": "All of the documentation for Pyinstaller talks about including individual files. Is it possible to include a directory, or should I write a function to create the include array by traversing my include directory?\nYes, \nThe problem is easier than you can imagine\ntry this: --add-data=\"path/to/folder/*;.\"\nhope it helps !!!\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "installing zombie js error referenceerror set is not defined what am i doing", "id": 1387, "answers": [{"answer_id": 1376, "document_id": 959, "question_id": 1387, "text": " to downgrade Zombie 4.0.7 to 3.1.x so that the mocha test command could work with Node 0.1", "answer_start": 1245, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nBackground: I'm currently reading \"Web Development with Node and Express\" by Ethan Brown (great book by the way for those learning node and express) and I got stuck on Chapter 5 - Quality Insurance.\n\nEverything was running smooth. First I ran the following:\n\n\n  npm install --save-dev mocha\n  \n  npm install -g mocha\n  \n  npm install --save-dev chai\n  \n  npm install --save-dev zombie\n\n\nProblem: Then I ran (as the book instructed):\n\n\n  mocha -u tdd -R spec qa/tests-crosspage.js 2&gt;/dev/null\n\n\nBut this wasn't doing anything. So then I ran:\n\n\n  mocha -u tdd -R spec qa/tests-crosspage.js\n\n\nAnd then this error appeared:\n\n\n  /Users/esanz91/Desktop/CodingNotes/Node/MySite/node_modules/zombie/node_modules/jsdom/lib/jsdom/level2/html.js:405\n  var nonInheritedTags = new Set([                      \n  \n  ReferenceError: Set is not defined\n\n\nVersions:\nJust to give you guys an idea, I have the following versions installed.\n\n\n  cmd:\n  \n  npm list --depth=0\n  \n  results:\n  \n  \u251c\u2500\u2500 chai@2.2.0\n  \n  \u251c\u2500\u2500 express@4.12.3\n  \n  \u251c\u2500\u2500 express3-handlebars@0.5.2\n  \n  \u251c\u2500\u2500 mocha@2.2.4\n  \n  \u2514\u2500\u2500 zombie@4.0.7\n\n\nAlso, I'm running on Node 0.10.\n\nHow can I fix this issue? \n\nThanks!\n    \n\nI found the answer to my own question. I actually needed to downgrade Zombie 4.0.7 to 3.1.x so that the mocha test command could work with Node 0.10. \n\nEdit1: I decided to upgrade my Node version to 0.12 and my Zombie version back up to 4.0.7 and that worked too. (seems like Node 0.10 and Zombie 4.0.7 not compatible...)\n\nEdit2: For anyone reading the book, I suggest looking at the github repo by the author himself. Take a look at the package.json file to see which version of the modules he uses.  \n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "node.js TypeError: path must be absolute or specify root to res.sendFile [failed to parse JSON]", "id": 1882, "answers": [{"answer_id": 1868, "document_id": 1453, "question_id": 1882, "text": "The error is pretty clear, you need to specify an absolute (instead of relative) path and/or set root in the config object for res.sendFile(). Examples:\n// assuming index.html is in the same directory as this script\nres.sendFile(__dirname + '/index.html');", "answer_start": 337, "answer_category": null}], "is_impossible": false}], "context": "So my next problem is that when i try adding a new dependence (npm install --save socket.io). The JSON file is also valid. I get this error: Failed to parse json\nSo I've been trying to figure out why this error has been returning. All of the files (HTML,JSON,JS) are inside the same folder on my desktop. I'm using node.js and socket.io\nThe error is pretty clear, you need to specify an absolute (instead of relative) path and/or set root in the config object for res.sendFile(). Examples:\n// assuming index.html is in the same directory as this script\nres.sendFile(__dirname + '/index.html');\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How do I update a Tomcat webapp without restarting the entire service?", "id": 1674, "answers": [{"answer_id": 1661, "document_id": 1247, "question_id": 1674, "text": "1.\tJust touch web.xml of any webapp.\ntouch /usr/share/tomcat/webapps/<WEBAPP-NAME>/WEB-INF/web.xml\n2.\tDelete webapps/YOUR_WEB_APP directory, Tomcat will start deploying war within 5 seconds (assuming your war file still exists in webapps folder).\n3.\tGenerally overwriting war file with new version gets redeployed by tomcat automatically. If not, you can touch web.xml as explained above.\n4.\tCopy over an already exploded \"directory\" to your webapps folder", "answer_start": 635, "answer_category": null}], "is_impossible": false}], "context": "I'm new to Tomcat. We have a dev machine with about 5 apps running. Even though it's dev, it's used by our clients pretty heavily during testing.\nSo say we need to make one small change on one class file. Right now, we have to shutdown Tomcat (affecting the other four apps), delete the WAR file (and web app directory), redeploy the new WAR file and restart Tomcat.\nOf course, this upsets a few people because it destroys all logged in sessions for all apps.\nIs there a better way to do this? I mean, is there a way to only reload the CLASS that changed instead of everything on the dev machine?\nThanks.\nThere are multiple easy ways.\n1.\tJust touch web.xml of any webapp.\ntouch /usr/share/tomcat/webapps/<WEBAPP-NAME>/WEB-INF/web.xml\n2.\tDelete webapps/YOUR_WEB_APP directory, Tomcat will start deploying war within 5 seconds (assuming your war file still exists in webapps folder).\n3.\tGenerally overwriting war file with new version gets redeployed by tomcat automatically. If not, you can touch web.xml as explained above.\n4.\tCopy over an already exploded \"directory\" to your webapps folder\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Eclipse on-click deploy to remote Tomcat", "id": 1075, "answers": [{"answer_id": 1068, "document_id": 653, "question_id": 1075, "text": "Yes, you can use Tomcat7 Maven Plugin. Here is the steps:\nPlease refer to the following documents for details:\nhttp://tomcat.apache.org/tomcat-7.0-doc/manager-howto.html#Configuring_Manager_Application_Access\nhttp://tomcat.apache.org/maven-plugin-2.1/index.html\nhttp://tomcat.apache.org/maven-plugin-2.0/tomcat7-maven-plugin/plugin-info.html", "answer_start": 418, "answer_category": null}], "is_impossible": false}], "context": "What I need is really simple and I believe that many of you probably do it already: - I develop Java Web Apps in Eclipse and so does my team; - we have a tomcat7 server running on a Ubuntu machine which works as a centralized Dev environment; - I would like to click a deploy button and send the new data to the server and deploy it (reload it), instead of exporting a war every time and manually upload it to server.\nYes, you can use Tomcat7 Maven Plugin. Here is the steps:\nPlease refer to the following documents for details:\nhttp://tomcat.apache.org/tomcat-7.0-doc/manager-howto.html#Configuring_Manager_Application_Access\nhttp://tomcat.apache.org/maven-plugin-2.1/index.html\nhttp://tomcat.apache.org/maven-plugin-2.0/tomcat7-maven-plugin/plugin-info.html\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "closing an application using wix", "id": 1956, "answers": [{"answer_id": 1942, "document_id": 1536, "question_id": 1956, "text": " but it then gives me an error code 2613.\n    \n\nYou could try logging the installation and see what you can track down from there. Try running the installer from the command console like so:\n\nmsiexec.exe /", "answer_start": 1025, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nIn creating my WiX installer I have run into an issue when trying to close an application before installing the upgrade. Below is an example of how I am attempting to do this.  \n\n&lt;util:CloseApplication Id=\"CloseServe\" CloseMessage=\"yes\" Target=\"server.exe\" ElevatedCloseMessage=\"yes\" RebootPrompt=\"no\"/&gt;\n&lt;InstallExecuteSequence&gt;      \n  &lt;Custom Action=\"WixCloseApplications\" After=\"RemoveExistingProducts\" /&gt;\n  &lt;RemoveExistingProducts After=\"InstallInitialize\"/&gt;\n  &lt;Custom Action='LaunchApplication' After='InstallFinalize'/&gt;\n  &lt;!--&lt;Custom Action='StopServer' Before='RemoveExistingProducts'/&gt;--&gt;\n&lt;/InstallExecuteSequence&gt;\n\n\nUsing this example the application does end up closing but the installation gets stalled at that point and then performs a rollback. Could this be due to the fact that the exe is removed prior to trying to close it? I have tried changing the sequence around so that RemoveExistingProducts is performed after the WixCloseApplications but it then gives me an error code 2613.\n    \n\nYou could try logging the installation and see what you can track down from there. Try running the installer from the command console like so:\n\nmsiexec.exe /i [msi filename] /log [filepath\\logfilename.log]\n\n    \n\nI would assume that you need to close the running application as early as possible in the InstallExecuteSequence, and certainly before InstallInitialize which starts the Windows Installer transaction that makes changes to the system.\n\nIn addition you should run the same ApplicationClose operation in the InstallUISequence as well so the application is closed while you cycle through the installation menus interactively. The whole InstallUISequence is skipped when you install silently, so you need it in the InstallExecuteSequence as well.\n\nBe aware that you can accidentially trigger rollback in a custom action by returning an exit code / error code that msiexec.exe interprets as an error. If the completion of the custom action isn't crucial, I turn off error checking to allow the install to continue. In this case that would just cause a reboot to be required.\n\nIf what you are trying to close or shut down is a service, MSI has built-in features to deal with this via the ServiceControl and ServiceInstall elements (and others).\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to install full Xcode Package?", "id": 905, "answers": [{"answer_id": 900, "document_id": 568, "question_id": 905, "text": "Open the App Store on your Mac and search for Xcode.\nDownload Xcode from the App Store.\nAfter your download is complete, launch Xcode.\nEnter your administrator account credentials.", "answer_start": 824, "answer_category": null}], "is_impossible": false}, {"question": "How can I find out if I have Xcode commandline tools installed?", "id": 906, "answers": [{"answer_id": 901, "document_id": 568, "question_id": 906, "text": "You can check the location with the command xcode-select -p.\n", "answer_start": 1118, "answer_category": null}], "is_impossible": false}, {"question": "How to uninstall code package?", "id": 907, "answers": [{"answer_id": 902, "document_id": 568, "question_id": 907, "text": "Find the Xcode application in the Applications folder and drag it to the Trash.\n\nRemove Xcode residual files", "answer_start": 1361, "answer_category": null}], "is_impossible": false}], "context": "Install the Full Xcode Package\nApple provides a complete development environment for programmers named Xcode. It is not pre-installed; it must be downloaded. The full Xcode application is huge, requiring over 40GB of disk space, and supports development for all Apple operating systems.\n\nApple also provides a separate and much smaller download, the Xcode Command Line Tools, that installs utilities for software developers who are not developing for Apple devices. If you are not developing for Apple devices, I suggest to Install Xcode Command Line Tools with Homebrew (recommended) or Install Xcode Command Line Tools Directly (an alternative).\n\nIf you are developing software for macOS, iOS, tvOS, and watchOS, you should install the full Xcode application.\n\nSteps\nHere are steps to install the full Xcode application.\n\nOpen the App Store on your Mac and search for Xcode.\nDownload Xcode from the App Store.\nAfter your download is complete, launch Xcode.\nEnter your administrator account credentials.\nVerify installation\nThe full Xcode application will be installed in /Applications/Xcode.app/Contents/Developer.\n\nYou can check the location with the command xcode-select -p.\n\n$ xcode-select -p\n/Applications/Xcode.app/Contents/Developer\nUninstall the Xcode application\nIf you've decided you no longer need the full Xcode application, you can uninstall it.\n\nFind the Xcode application in the Applications folder and drag it to the Trash.\n\nRemove Xcode residual files\nXcode leaves leftover files in two locations:\n\n~/Library/Developer/\n~/Library/Caches/", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Cache entry deserialization failed, entry ignored", "id": 1204, "answers": [{"answer_id": 1197, "document_id": 780, "question_id": 1204, "text": "rm -rf ~/.cache/pip", "answer_start": 169, "answer_category": null}], "is_impossible": false}], "context": "Regarding the error/warning message in the question's title:\nCache entry deserialization failed, entry ignored\nYou can fix it by removing the pip cache, e.g. on Ubuntu:\nrm -rf ~/.cache/pip\nAnother reason might be that you have a lower python version. For instance, you install tensorflow on python 3.6.0 and you'll get Cache entry deserialization failed, entry ignored for many libraries that come with tensorflow.\nBut if you use python 3.8.0, all is good.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to create a robust, minimal installer for Windows?", "id": 802, "answers": [{"answer_id": 797, "document_id": 484, "question_id": 802, "text": "Take a look at NSIS it is quite simple and it is used to create installers for Windows", "answer_start": 236, "answer_category": null}], "is_impossible": false}], "context": "Hmm... If it's only copying a single file around, and that file is a program anyway (scn screensavers are just renamed exe's), why not make that one program have three different modes? \"run without parameters\" == copy to %systemroot%,. Take a look at NSIS it is quite simple and it is used to create installers for Windows.", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "IIS Deployed ASP.NET 5 BETA 8 site to IIS gives HTTP Error 500.19 - Internal Server Error", "id": 1302, "answers": [{"answer_id": 1293, "document_id": 872, "question_id": 1302, "text": "So you need to install it,\nhttps://azure.microsoft.com/en-us/blog/announcing-the-release-of-the-httpplatformhandler-module-for-iis-8/", "answer_start": 277, "answer_category": null}], "is_impossible": false}], "context": "I created a new ASP.NET5 Beta 8 Web Application.\nI publish it to my local file system and copy those files to my server which is a Windows Server 2012 R2\nIn IIS 8.5 on the server I create an application that uses an app pool that uses Process Model -> Identity as LocalSystem.\nSo you need to install it,\nhttps://azure.microsoft.com/en-us/blog/announcing-the-release-of-the-httpplatformhandler-module-for-iis-8/\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Differences between utf8 and latin1", "id": 1547, "answers": [{"answer_id": 1536, "document_id": 1124, "question_id": 1547, "text": "If you're trying to store non-Latin characters like Chinese, Japanese, Hebrew, Russian, etc using Latin1 encoding, then they will end up as mojibake. You may find the introductory text of this article useful (and even more if you know a bit Java).", "answer_start": 48, "answer_category": null}], "is_impossible": false}], "context": "what is the difference between utf8 and latin1?\nIf you're trying to store non-Latin characters like Chinese, Japanese, Hebrew, Russian, etc using Latin1 encoding, then they will end up as mojibake. You may find the introductory text of this article useful (and even more if you know a bit Java).\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to deploy an Electron app as an executable or installable in Windows?", "id": 528, "answers": [{"answer_id": 530, "document_id": 253, "question_id": 528, "text": "You can package your program using electron-packager and then build a single setup EXE file using InnoSetup.", "answer_start": 111, "answer_category": null}], "is_impossible": false}], "context": "I want to generate a unique .exe file to execute the app or a .msi to install the application. How to do that? You can package your program using electron-packager and then build a single setup EXE file using InnoSetup.", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "how do i create a mac installer for my java application", "id": 1409, "answers": [{"answer_id": 1398, "document_id": 983, "question_id": 1409, "text": "if this is what you looking for, How to Create a Mac Installer for Java Application\ngive a try, many could help.\n    \n\nThis works for me: https://github.com/Jorl17/jar2app\n", "answer_start": 1862, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI have created an executable JAR file for my Java application. If I double-click then it works fine. But I want to create installer for Mac OS, because I cannot give a JAR file to my users. Any suggestions?\n    \n\nWell, all you have to do here is to create a beautiful icon for your app, bundle it to your jar file to make it look more sophisticated, instead of using the default coffee cup icon.\n\nHere is how you can do it:\n\nhttp://www.centerkey.com/mac/java/\n\nPlease read the sessions starting from session 7.\n\nMac has java by default. And if your users are not technical, it makes no different for them if they are using a mac .exe or not, right?\n    \n\nJust a comment to clear some of this up. Mac applications normally dont have installers. At all. They dont have a registry and normally you just drag the icon (which is actually a folder with the executables in a specific folder structure) into the applications folder. Thats it. Thats why if you have an executable with a nice icon and you put it in a .dmg image file nobody is gonna know the difference.\n    \n\nI'd agree that a jar should be sufficent; but maybe you want to check this (ClickInstall MacOSX 1.0.2) Installer Build Tool for OSX.\n    \n\nThe very first hit on Google for \"Mac Installer\" is the Wikipedia article about the Mac Installer.\n\nYou can click through from there to read Apple's Software Delivery Guide. It tells you in exhaustive detail everything you could possibly want to know about this.\n\nPlease, for your own good, read up on How To Ask Questions The Smart Way. You'll get much better results that way.\n    \n\nTry jarbundler from http://informagen.com/JarBundler/.\nYou can create a nice OSX app including icon with it.\nJust ship that. The user can drag this app to Application. No explicit installation step \nnecessary.\n\nI use this for my projects.\n    \n\nWell, if this is what you looking for, How to Create a Mac Installer for Java Application\ngive a try, many could help.\n    \n\nThis works for me: https://github.com/Jorl17/jar2app\n\nFor any changes made in code, I just need to move updated jar file in the folder application.app/Content/Java\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "incompatible jvm when trying to install eclipse on windows 10 pc", "id": 1948, "answers": [{"answer_id": 1935, "document_id": 1528, "question_id": 1948, "text": "k: You have two types of system variables, user and the system. You need to remove all the paths (relative and absolute) that point to any version of java from the system variable p", "answer_start": 2089, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI am trying to run the installer for Eclipse 64-bit and I get an error message:\n\nVersion 1.7.0_10 of the JVM is not suitable for this product. Version: 1.8 or greater is required.\n\n\nWhen i run java -version on the command line, I get:\n\njava version \"1.8.0_121\"\nJava(TM) SE Runtime Environment (build 1.8.0_121-b13)\nJava HotSpot(TM) Client VM (build 25.121-b13, mixed mode, sharing)\n\n\nPlease help, i just want to get up and running with Eclipse.\nThanks!\n    \n\nfollow this steps\nstep1 : just go and find the location of eclipse in your system\nstep2 : open the \"eclipse configuration file\" and search \"-vmargs\" in\nthis file\njust go one line above of \"-vmargs\" and hit enter for blank space\nnow type here \"-vm\" and hit enter\nstep3 : go and copy the bin folder of jdk11\nand paste it below the \"-vm\"\nthen add this \"\\javaw.exe\", see the demo below\nthen save it and now run the eclipse ide\nexample in my case\n-vm\nC:\\Program Files\\Java\\jdk-11.0.9\\bin\\javaw.exe\n-vmargs\n    \n\nI had the problem with STS 4 on Windows 10 that needed JVM V11.\nAfter JDK installation, you must add the bin path of the JDK in your PATH environment variable, in the first position, to ensure the system will use the right version.\nScreenshot:\n\n    \n\ncheck if there is a JAVA_HOME variable set. That might point to the Java7 installation and put the eclipse installer on the wrong track\n    \n\nYou can also set the JAVA_HOME in the same cmd that you run the executable Eclipse installation File. Example:\n\n\n  set JAVA_HOME=\"C:\\Program Files\\Java\\jdk1.8.0_171\" \n  \"C:\\Users\\XX\\Downloads\\eclipse-inst-win64.exe\"\n\n    \n\nrecently I got the same issue with STS. i have issued below command in cmd with admin rights:\nC:\\WINDOWS\\system32&gt;set JAVA_HOME=\"C:\\Program Files\\Java\\jdk-15.0.2\\bin\"\nits resolved my problem. hope it will work for you.\n    \n\nJust as an FYI:\nInstalling multiple JDKs/JREs led to Eclipse detecting the wrong one (jdk 8 instead of the required 11).\nAside from setting JAVA_HOME I had to reorder the PATH variable, to have the 11 before the 8.\n    \n\nOne solution if none of the others work: You have two types of system variables, user and the system. You need to remove all the paths (relative and absolute) that point to any version of java from the system variable part. Worked for me!\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Can Maven collect all the dependent JARs for a project to help with application deployment?", "id": 1633, "answers": [{"answer_id": 1621, "document_id": 1207, "question_id": 1633, "text": "To do it from the command line just do:\n$ mvn dependency:copy-dependencies -DoutputDirectory=OUTPU", "answer_start": 581, "answer_category": null}], "is_impossible": false}], "context": "I'm just starting to use Maven, (evaluating it, really) and I need to be able to quickly generate a JAR file for my application and a directory with all the dependencies (for example, lib) so that I can deploy those two to be run in a stand-alone manner. Generating the JAR file with the proper manifest is easy, but I do not know how to get Maven to copy the dependencies for the current project into a lib directory that I can deploy.\nTake a look at maven's dependency plugin, specifically the copy-dependencies goal. The usage section describes how to do exactly what you want.\nTo do it from the command line just do:\n$ mvn dependency:copy-dependencies -DoutputDirectory=OUTPU\n\nSince this is for a stand-alone Java applications, I am not interested in deploying to a Maven repository, that is also fairly trivial, or at least easily googleable.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Cannot install Aptana Studio 3.6 on Windows", "id": 726, "answers": [{"answer_id": 729, "document_id": 416, "question_id": 726, "text": "Uninstall any nodejs version.\n2.\tInstall https://nodejs.org/dist/v0.10.36/x64/node-v0.10.36-x64.msi.\n3.\tInstall Aptana.\n4.\tCode...", "answer_start": 371, "answer_category": null}], "is_impossible": false}], "context": "I'd like to use Aptana Studio for Rails development under Windows. I currently have different dev tools & ide's up and running (like git/ruby/jdk) and I'd like to install Aptana Studio as well but I can't. After downloading and running installer, it starts properly and after I choose destination dir, it starts downloading prequisities.  have some issue, the fix is:\n1.\tUninstall any nodejs version.\n2.\tInstall https://nodejs.org/dist/v0.10.36/x64/node-v0.10.36-x64.msi.\n3.\tInstall Aptana.\n4.\tCode...\ngreetings!\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to install pip for Python 3.6 on Ubuntu 16.10?", "id": 724, "answers": [{"answer_id": 727, "document_id": 414, "question_id": 724, "text": "sudo add-apt-repository ppa:jonathonf/python-3.6  # (only for 16.04 LTS)", "answer_start": 428, "answer_category": null}], "is_impossible": false}], "context": "I'd like to start by pointing out that this question may seem like a duplicate, but it isn't. All the questions I saw here were regarding pip for Python 3 and I'm talking about Python 3.6. The steps used back then don't work for Python 3.6. \nLet's suppose that you have a system running Ubuntu 16.04, 16.10, or 17.04, and you want Python 3.6 to be the default Python.\nIf you're using Ubuntu 16.04 LTS, you'll need to use a PPA:\nsudo add-apt-repository ppa:jonathonf/python-3.6  # (only for 16.04 LTS)\n\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How can I install Perl modules without root privileges?", "id": 1162, "answers": [{"answer_id": 1155, "document_id": 739, "question_id": 1162, "text": "Just run\n$ cpanm --local-lib=~/perl5 local::lib && eval $(perl -I ~/perl5/lib/perl5/ -Mlocal::lib)\nOpen another terminal and run\n$ env |grep PERL", "answer_start": 585, "answer_category": null}], "is_impossible": false}], "context": "I am on a Linux machine where I have no root privileges. I want to install some packages through CPAN into my home directory so that when I run Perl, it will be able to see it.\nI ran cpan, which asked for some coniguration options. It asked for some directory, which it suggested ~/perl \"for non-root users\". Still, when I try to install a package, it fails at the make install step, because I don't have write access to /usr/lib/perl5/whatever.\nHow can I configure CPAN so that I can install packages into my home directory?\nIf you have this installed your sys admin deserves a beer!\nJust run\n$ cpanm --local-lib=~/perl5 local::lib && eval $(perl -I ~/perl5/lib/perl5/ -Mlocal::lib)\nOpen another terminal and run\n$ env |grep PERL\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Windows error 2 occured while loading the Java VM", "id": 1227, "answers": [{"answer_id": 1220, "document_id": 803, "question_id": 1227, "text": "You should launch the installer with the following command line parameters:\nLAX_VM\nFor example: InstallXYZ.exe LAX_VM \"C:\\Program Files (x86)\\Java\\jre6\\bin\\java.exe\".", "answer_start": 676, "answer_category": null}], "is_impossible": false}], "context": "I've been trying to install this ARToolkit from Qualcomm: https://ar.qualcomm.at/qdevnet/ (Windows \".exe\" version) on a Windows 7 64bits platform, and I keep getting the error:\nWindows error 2 occured while loading the Java VM\nThe program trying to install this program is InstallAnywhere, but I can't seem to find any documentation about this error. The dialogue box name when the crash happens is LaunchAnywhere\nI was just wondering if anyone had seen this kind of error before and if so, how to solve it. The very few answers I can find online relate to Win98/ME issues thus making them irrelevant.\nI'm currently using JDK 1.7.0_04 and jre 1.7 if this is somehow relevant.\nYou should launch the installer with the following command line parameters:\nLAX_VM\nFor example: InstallXYZ.exe LAX_VM \"C:\\Program Files (x86)\\Java\\jre6\\bin\\java.exe\".This works perfectly, not sure why it was voted down\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Best way to deploy Visual Studio application that can run without installing", "id": 744, "answers": [{"answer_id": 745, "document_id": 432, "question_id": 744, "text": "\"Publish\" the application .\n2.\tBut instead of using the produced installer, find the produced files .\n3.\tZip that folder", "answer_start": 637, "answer_category": null}], "is_impossible": false}], "context": "I wrote a fairly simple application with C#/.NET and can't figure out a good way to publish it. It's a sort of a \"tool\" that users would only run once, or run every few months. Because of this, I'm hoping that there is a way I could deploy it where it wouldn't need installing to run (it could just be run by double-clicking an EXE file straight after downloading).\nHowever, it still needs (somehow) to include the correct version of .NET, libraries, etc. so it will run correctly. I know this is included when using ClickOnce, but that still installs the application onto the user's computer.\nIt is possible and is deceptively easy:\n1.\t\"Publish\" the application .\n2.\tBut instead of using the produced installer, find the produced files .\n3.\tZip that folder .\nAn added advantage is that, as a ClickOnce application, it does not require administrative privileges to run (if your application follows the normal guidelines for which folders to use for application data, etc.).\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Maven check for updated dependencies in repository", "id": 1876, "answers": [{"answer_id": 1862, "document_id": 1447, "question_id": 1876, "text": "The Maven Versions plugin and it's display-dependency-updates mojo are what you're looking for:\nmvn versions:display-dependency-updates", "answer_start": 339, "answer_category": null}], "is_impossible": false}], "context": "Is there a Maven plugin that allows you to check if there are newer versions of dependencies available in the repository?\nSay, you are using dependency X with version 1.2. Now a new version of X is released with version 1.3. I'd like to know, based on the dependencies used in my project, which dependencies have newer versions available.\nThe Maven Versions plugin and it's display-dependency-updates mojo are what you're looking for:\nmvn versions:display-dependency-updates\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Wix - Setting Install Folder correctly", "id": 1814, "answers": [{"answer_id": 1799, "document_id": 1385, "question_id": 1814, "text": "You can see this document: https://wixtoolset.org/documentation/manual/v3/wixui/dialog_reference/wixui_installdir.html.", "answer_start": 518, "answer_category": null}], "is_impossible": false}], "context": "I'm creating a program which is being installed by Wix, using VS 2010 and I've already got the product.wxs ready.\nThe problem is when the user installs the program and changes the value of the installation folder, the files of the \"Bin\" and \"Icons\" are installed to their correct path, but the Myapp target is installed to a fix location which was defined at the start as the default installation path.\nWhy do only the bin and icon files installed to the correct folder the user wanted, but the myapp target does not?\nYou can see this document: https://wixtoolset.org/documentation/manual/v3/wixui/dialog_reference/wixui_installdir.html.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Deploy Create-React-App on Nginx", "id": 1725, "answers": [{"answer_id": 1713, "document_id": 1298, "question_id": 1725, "text": "\u2022\tserve CRA through custom express server\n\u2022\tchange hostname in package.json\n\u2022\tadd basename to /admin for react-router\n\u2022\tdefine the following proxy pass:\nlocation / {\n  proxy_pass http://localhost:3000;\n}\nlocation /admin {\n  proxy_pass http://localhost:3001;\n}\nlocation /admin/ {\n  proxy_pass http://localhost:3001/;\n}", "answer_start": 551, "answer_category": null}], "is_impossible": false}], "context": "I'm attempting to deploy my create-react-app SPA on a Digital Ocean droplet with Ubuntu 14.04 and Nginx. Per the static server deployment instructions, I can get it working when I run serve -s build -p 4000, but the app comes down as soon as I close the terminal. It is not clear to me from the create-react-app repo readme how to keep it running forever, similar to something like forever.\nWithout running serve, I get Nginx's 502 Bad Gateway error.\n1\nI was hosting NextJS as main app on / and wanted to host CRA on /admin route. Here is what I did:\n\u2022\tserve CRA through custom express server\n\u2022\tchange hostname in package.json\n\u2022\tadd basename to /admin for react-router\n\u2022\tdefine the following proxy pass:\nlocation / {\n  proxy_pass http://localhost:3000;\n}\nlocation /admin {\n  proxy_pass http://localhost:3001;\n}\nlocation /admin/ {\n  proxy_pass http://localhost:3001/;\n}\n\nRelated articles:\n\u2022\tCRA Deployment\n\u2022\tReact-Router\n\u2022\tMultiple SPAs\n\u2022\tAnother StackOverflow question\n\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Pushing Rails with SQLite3 to Heroku fails", "id": 1109, "answers": [{"answer_id": 1101, "document_id": 686, "question_id": 1109, "text": "You should make sure you don't include sqlite in your Gemfile in production environments.", "answer_start": 158, "answer_category": null}], "is_impossible": false}], "context": "I experience the same scenario as described in Heroku deployment issue when I try to deploy my Rails 3 app to Heroku and sqlite3 is defined in the gems file.\nYou should make sure you don't include sqlite in your Gemfile in production environments.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Free install wizard software", "id": 787, "answers": [{"answer_id": 783, "document_id": 470, "question_id": 787, "text": " I have been using Inno Setup for several years now. It's mature enough that it has a lot of plug-ins", "answer_start": 62, "answer_category": null}], "is_impossible": false}], "context": "Is there something like InstallShield that I can use for free? I have been using Inno Setup for several years now. It's mature enough that it has a lot of plug-ins. I've found that the forums/newsgroups are very good at answering all the questions I've had so far.", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "distributing vc redist running installer when already installed causes prob", "id": 1922, "answers": [{"answer_id": 1909, "document_id": 1495, "question_id": 1922, "text": "Although not for Inno installer, this related question provides a solution for Visual Studio 2010 setup project. The issue is due to the SP1 of vcredist changed the product code, while Visual Studio uses the old code. That is why the already installed check fails. \n\nVisual C++ 2010 Runtime Libraries prerequisite keeps popping up on a VS 2010 created installer", "answer_start": 1698, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nUsing the Inno installer, we distribute the VC++ redistributable with our app so we can run it automatically. We've found that running it on a system where it's already installed asks us to repair/undo the installation which is going to totally confuse users.\n\nIs there a way around this? Maybe a flag on the installer or something?\n\nThanks.\n    \n\nTry the /q flag\nhttp://support.microsoft.com/kb/227091 (assuming you're installing it via calling msiexec.exe on their redistributable)\n    \n\nFor some reason none of the above answers worked for me.  This did, however:\n\n[Run]\nFilename: {tmp}\\vcredist_x86.exe; Parameters: \"/passive /Q:a /c:\"\"msiexec /qb /i vcredist.msi\"\" \"; StatusMsg: Installing 2010 RunTime...\n[Files]\nSource: vendor/vcredist_x86.exe; DestDir: {tmp}\n\n\nI got the hint for it by running  vcredist_x86.exe /?, as noted at the bottom of this thread: http://social.msdn.microsoft.com/Forums/en-US/vcgeneral/thread/a8d4d5b4-7927-4c86-95e8-3cd8b3018ae8/\n\nAs a further note, another possible option for redistribution is to just static link against (your version of) the msvcrt see comments in http://blogs.msdn.com/b/vcblog/archive/2007/10/12/how-to-redistribute-the-visual-c-libraries-with-your-application.aspx (though MS apparently frowns on this type of static linking, at least you won't need a dll).\n\nDistributing the Visual C++ Runtime Libraries (MSVCRT) may also be useful.\n\nAlso note that if you can guarantee you'll have control, you can just include msvcr100.dll in the same directory as your executable and that would work, too, though not a very standard solution.\n    \n\nThis thread resolved my issues:\n\n(taken from the comment in the other answer)\n    \n\nAlthough not for Inno installer, this related question provides a solution for Visual Studio 2010 setup project. The issue is due to the SP1 of vcredist changed the product code, while Visual Studio uses the old code. That is why the already installed check fails. \n\nVisual C++ 2010 Runtime Libraries prerequisite keeps popping up on a VS 2010 created installer\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Capistrano - can't deploy my database.yml", "id": 1261, "answers": [{"answer_id": 1253, "document_id": 832, "question_id": 1261, "text": "Your deploy.rb should have a task that symlinks the newly deployed release's database.yml to the on in the shared directory.", "answer_start": 368, "answer_category": null}], "is_impossible": false}], "context": "What am I missing here/where should I look for a failure? The credentials to database on the server should be right.\nWhen I try to deploy my app with capistrano, I'll get this error:\nfailed: \"sh -c 'cp /var/www/my_app/releases/20120313115055/config/database.staging.yml /var/www/my_app/releases/20120313115055/config/database.yml'\" on IP_ADDR\nMy database.yml ie empty\nYour deploy.rb should have a task that symlinks the newly deployed release's database.yml to the on in the shared directory.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Best practices for new Rails deployments on Linux?", "id": 553, "answers": [{"answer_id": 555, "document_id": 278, "question_id": 553, "text": "This website may solve your problem: http://www.loudthinking.com/posts/30-myth-1-rails-is-hard-to-deploy.", "answer_start": 298, "answer_category": null}], "is_impossible": false}], "context": "I've used straight Mongrel, I've used Mongrel clusters behind Apache, I've looked at Thin, and I'm becoming very intrigued by Passenger. I've looked at Nginx, too. I've looked at MRI, Ruby Enterprise Edition, Rubinius, and JRuby. There are a lot of options, each claiming to be the new holy grail.\nThis website may solve your problem: http://www.loudthinking.com/posts/30-myth-1-rails-is-hard-to-deploy.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "how do i find a list of homebrews installable packages", "id": 1906, "answers": [{"answer_id": 1893, "document_id": 1477, "question_id": 1906, "text": "em separately\n\nbrew list --formula\nbrew list --cask\n\nAvailable Packages\nList\n\nbrew formulae, list all available formulae\nbrew casks, list all casks (These two commands are fast cause it's implemented in", "answer_start": 1608, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nRecently I installed Brew. How can I retrieve a list of available brew packages to install?\n    \n\nbrew help will show you the list of commands that are available.\nbrew list will show you the list of installed packages. You can also append formulae, for example brew list postgres will tell you of files installed by postgres (providing it is indeed installed).\nbrew search &lt;search term&gt; will list the possible packages that you can install. brew search post will return multiple packages that are available to install that have post in their name.\nbrew info &lt;package name&gt; will display some basic information about the package in question.\n    \n\nFrom the man page:\n\n\nsearch, -S text|/text/\nPerform a substring search of formula names for text. If text is surrounded with slashes,\nthen it is interpreted as a regular expression. If no search term is given,\nall available formula are displayed.\n\n\n\nFor your purposes, brew search will suffice.\n    \n\nPlease use Homebrew Formulae page to see the list of installable packages.\nhttps://formulae.brew.sh/formula/\n\nTo install any package =&gt; command to use is :\n\nbrew install node\n    \n\nSince Homebrew 2.6.0 released in 2020.12.01, brew list behaves a little differently. This is one of the reason I post the following answer.\nInstalled Packages\nHomebrew describes packages as formulae. There's another kind of package know as cask, which is used for GUI apps installed by brew install --cask.\nFormerly, brew list will only list installed formulae. After 2.6.0, brew list will list both of packages and casks. You may wanna check them separately\n\nbrew list --formula\nbrew list --cask\n\nAvailable Packages\nList\n\nbrew formulae, list all available formulae\nbrew casks, list all casks (These two commands are fast cause it's implemented in Bash not Ruby)\n\nSearch\n# search local installed\nbrew search --formula keyword\nbrew serach --cask keyword\n\n# search online available\nbrew search --formulae keyword\nbrew serach --casks keyword\n\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "One big executable or many small DLL's", "id": 1103, "answers": [{"answer_id": 1095, "document_id": 680, "question_id": 1103, "text": "A single executable has a huge positive impact on maintainability. It is easier to debug, deploy (size issues aside) and diagnose in the field. As you point out, it completely sidesteps DLL hell.", "answer_start": 169, "answer_category": null}], "is_impossible": false}], "context": "Over the years my application has grown from 1MB to 25MB and I expect it to grow further to 40, 50 MB. I don't use DLL's, but put everything in this one big executable.\nA single executable has a huge positive impact on maintainability. It is easier to debug, deploy (size issues aside) and diagnose in the field. As you point out, it completely sidesteps DLL hell.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to implement WiX installer upgrade?", "id": 1567, "answers": [{"answer_id": 1556, "document_id": 1144, "question_id": 1567, "text": "In the newest versions (from the 3.5.1315.0 beta), you can use the MajorUpgrade element instead of using your own.", "answer_start": 384, "answer_category": null}], "is_impossible": false}], "context": "At work we use WiX for building installation packages. We want that installation of product X would result in uninstall of the previous version of that product on that machine.\nI've read on several places on the Internet about a major upgrade but couldn't get it to work. Can anyone please specify the exact steps that I need to take to add uninstall previous version feature to WiX?\nIn the newest versions (from the 3.5.1315.0 beta), you can use the MajorUpgrade element instead of using your own.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "how to automate installer testing", "id": 1383, "answers": [{"answer_id": 1372, "document_id": 953, "question_id": 1383, "text": "If you have the installer runnable from the command line, it's easy to have a script to call it automatically.\n\nThen you can use a web app testing tool to see it the install was successful, like this one http://seleniumhq.org/ For this you will need an unique way to test a new install - like a page with the current version", "answer_start": 1262, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI'm wondering if anyone has any best practices for automating the testing of installers on various machines with potentially different hardware / software profiles and by specifying various options to the installer.  The idea would be that I could write \"unit test like\" code to set up a machine, run the installer, then test that certain things are true.  Tests might look similar to:\n\nTest:\n    Boot Machine without IIS\n    Run Installer\n    Assert Installer Had Errors\n\nTest:\n    Boot Machine with IIS\n    Run Installer\n    Assert Installer Ran\n\nTest_Fixture:\n    SetUp:\n        Boot Machine with IIS\n\n    Test:\n        Run Installer without IIS install\n        Assert Website Not Installed\n\n    Test:\n        Run Installer with IIS install\n        Assert Website Installed\n\n\nI know I could create lots of VMs, but waiting for a VM to boot for each functional test sounds like way more work than I want.  What I really want is a way to virtualize the installer environment.  Any suggestions?  \n    \n\nWe have created a set of VMs and find it is very easy to manage.  We run the tests for 13 different Windows installers over night.  The VMs we have created our very bare bones, so it is possible to run a number of tests in parallel.\n    \n\nIf you have the installer runnable from the command line, it's easy to have a script to call it automatically.\n\nThen you can use a web app testing tool to see it the install was successful, like this one http://seleniumhq.org/ For this you will need an unique way to test a new install - like a page with the current version.\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "creating a dual mac win autorun cd", "id": 1360, "answers": [{"answer_id": 1349, "document_id": 928, "question_id": 1360, "text": "You can use hdiutil\n\neg.\n\nhdiutil makehybrid -o [output-file] [input-folder] -iso -hfs -hide-iso [mac-only-file] -hide-hfs [windows-only-files]\n", "answer_start": 427, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nHow do I create a CD that opens a \"Drag this icon to the Application folder to install\" on the Mac and autoruns an installer on Windows?\n    \n\nJordan Brough has a really good write-up on his blog about this:\n\nhttp://jordan.broughs.net/archives/2008/03/creating-cross-platform-windows-and-mac-installer-cds\n\nFor what it's worth, this was the first result returned from a fairly simple google search...\n    \n\nYou can use hdiutil\n\neg.\n\nhdiutil makehybrid -o [output-file] [input-folder] -iso -hfs -hide-iso [mac-only-file] -hide-hfs [windows-only-files]\n\n\nWhat you will want to do is hide the windows specific files from the HFS partition and hide the Mac only files from the ISO.\n\nThen you would use an autorun file as your normally would on Windows.\n\nOn the Mac side there are many applications you can buy for creating a Finder window that looks a certain way but all these changes can be made within finder.  You then will need to copy the DS_Store file to the CD and finder will automatically apply any changes that you have made.\n\nAlso using -hfs-openfolder will cause it to open automatically when inserted on the mac.\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to install Poppler on Windows?", "id": 820, "answers": [{"answer_id": 815, "document_id": 502, "question_id": 820, "text": "With anaconda installed on windows one can simply execute:\nconda install -c conda-forge poppler", "answer_start": 277, "answer_category": null}], "is_impossible": false}], "context": "The most recent version of ScraperWiki depends on Poppler (or so the GitHub says). Unfortunately, it only specifies how to get it on macOS and Linux, not Windows.\nA quick googling turned up nothing too promising. Does anyone know how to get Poppler on Windows for ScraperWiki? With anaconda installed on windows one can simply execute:\nconda install -c conda-forge poppler\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How should I manage deployments with kubernetes", "id": 1295, "answers": [{"answer_id": 1287, "document_id": 866, "question_id": 1295, "text": "My colleague has a good blog post about this topic:\nhttp://blog.jonparrott.com/building-a-paas-on-kubernetes/", "answer_start": 712, "answer_category": null}], "is_impossible": false}], "context": "I am hoping to find a good way to automate the process of going from code to a deployed application on my kubernetes cluster.\nIn order to build and deploy my app I need to first build the docker image, tag it, and then push it to ECR. I then need to update my deployment.yaml with the new tag for the docker image and run the deployment with kubectl apply -f deployment.yaml.\nThis will go and perform a rolling deployment on the kubernetes cluster updating the pods to the new version of the container image, once this deployment has completed I may need to do other application specific things such as running database migrations, or cache clear/warming which may or may not need to run for a given deployment.\nMy colleague has a good blog post about this topic:\nhttp://blog.jonparrott.com/building-a-paas-on-kubernetes/\nBasically, Kubernetes is not a Platform-as-a-Service, it's a toolkit on which you can build your own Platform-a-as-Service. It's not very opinionated by design, instead it focuses on solving some tricky problems with scheduling, networking, and coordinating containers, and lets you layer in your opinions on top of it.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "what is the difference between a docker container and an ansible playbook", "id": 1428, "answers": [{"answer_id": 1417, "document_id": 1003, "question_id": 1428, "text": "They are completely different things. Ansible is used to automate configuration and management of machines/containers an Docker is a lightweight container system for Linux.", "answer_start": 2248, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nIt seems to me that both tools are used to easily install and automatically configure applications.\n\nHowever, I've limitedly used Docker and haven't used Ansible at all. So I'm a little confused. \n\nWhenever I search for a comparison between these two technologies, I find details about how to use these technologies in combination.\n    \n\nThere are many reasons most articles talk about using them together.\n\nThink of Ansible as a way of installing and configuring a machine where you can go back and tweak any individual step of that install and configuration in the future. You can then scale that concept out to many machines as you are able to manage.\n\nA key difference where Ansible has an advantage is that it can not just manage the internals of the machine, but also manage the other systems such as networking, DNS, monitoring etc that surround the machine.\n\nBuilding out many machines via Ansible takes pretty much as much time to do 50 machines as it does to make 1, as all 50 will be created step by step. If you are running a rolling deploy across multiple environments its this build step by step that takes up time.\n\nNow think of Docker as having built one of those individual machines - installed and configured and ready to be deployed wherever a docker system is available (which is pretty much everywhere these days). The drawback here is you don't get to manage all the other aspects needed around making docker containers actually work, and tweaking them long term isn't as much fun as it sounds if you haven't automated the configuration (hence Ansible helps here).\n\nScaling from 1 to 50 Docker machines once you have already created the initial image is blindingly fast in comparison to the step by step approach Ansible takes, and this is most obvious during a rolling deploy of many machines in smaller groups.\n\nEach has its drawbacks in either ability or speed. Combine them both however and it can be pretty awesome. As no doubt with most of the articles you have already read, I would recommend looking at using Ansible to create (and update) your base Docker container(s) and then using Ansible to manage the rollout of whatever scale of containers you need to satisfy your applications usage.\n    \n\nThey are completely different things. Ansible is used to automate configuration and management of machines/containers an Docker is a lightweight container system for Linux.\n\nhttp://www.ansible.com/home\n\nhttps://www.docker.com/\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Can't get Jenkins to start using Tomcat", "id": 1253, "answers": [{"answer_id": 1246, "document_id": 825, "question_id": 1253, "text": "To fix this, create the .jenkins manually (with correct own/grp permissions to tomcat) and restart tomcat.", "answer_start": 507, "answer_category": null}], "is_impossible": false}], "context": "I'm having a problem trying to deploy Jenkins war to Tomcat. I'm using CentOS with Java 1.6.0_28, Tomcat 6.0.24 and last version of jenkins as of January 21, 2014.\nI think the problem is more related to Jenkins because of the log but not sure. When I google the error only find the classes that fire the exception but no solution. Here is the log. Any help is appreciated.\nJenkins tries to create /usr/share/tomcat7/.jenkins to store its data, but the directory isnt created, so it throws NoHome Exception.\nTo fix this, create the .jenkins manually (with correct own/grp permissions to tomcat) and restart tomcat.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Unit testing code with a file system dependency", "id": 1892, "answers": [{"answer_id": 1879, "document_id": 1463, "question_id": 1892, "text": "You don\u00b4t need to test (in this unit test) whether a file is correctly unzipped, your method takes that for granted. The interfaces are valuable by itself because they provide abstractions that you can program against, rather than implicitly or explicitly relying on one concrete implementation.", "answer_start": 841, "answer_category": null}], "is_impossible": false}], "context": "I am writing a component that, given a ZIP file, needs to:\nUnzip the file.\nFind a specific dll among the unzipped files.\nLoad that dll through reflection and invoke a method on it.\nI'd like to unit test this component.\nYay! Now it's testable; I can feed in test doubles (mocks) to the DoIt method. But at what cost? I've now had to define 3 new interfaces just to make this testable. And what, exactly, am I testing? I'm testing that my DoIt function properly interacts with its dependencies. It doesn't test that the zip file was unzipped properly, etc.\nIt doesn't feel like I'm testing functionality anymore. It feels like I'm just testing class interactions.\nMy question is this: what's the proper way to unit test something that is dependent on the file system?\nedit I'm using .NET, but the concept could apply Java or native code too.\t\nYou don\u00b4t need to test (in this unit test) whether a file is correctly unzipped, your method takes that for granted. The interfaces are valuable by itself because they provide abstractions that you can program against, rather than implicitly or explicitly relying on one concrete implementation.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Android SDK install: Java SE development Kit (JDK) not found [duplicate]", "id": 708, "answers": [{"answer_id": 712, "document_id": 399, "question_id": 708, "text": "Windows 7 32 bit has the same behaviour as Windows XP SP3. Hit BACK and then NEXT again and you're able to install the Android SDK", "answer_start": 535, "answer_category": null}], "is_impossible": false}], "context": "I have installed Java runtime 6 to C:\\src\\libraries\\jre6.\nI installed Java JDK to C:\\src\\libraries\\javasdk.\nMy %PATH% is set to:\nPATH=C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\system32 \\WindowsPowerShell\\v1.0;C:\\src\\libraries\\javasdk\\bin;C:\\src\\libraries\\javasdk;C: \\src\\libraries\\javasdk\\jdk\\bin;C:\\src\\libraries\\javasdk\\jdk;C:\\src\\libraries\\jav asdk;\nWhen I run the Android SDK installer I get the error:\n\"Java SE development Kit (JDK) not found\"\nI'm at a loss as to what the Android SDK cannot really find.\nWindows 7 32 bit has the same behaviour as Windows XP SP3. Hit BACK and then NEXT again and you're able to install the Android SDK. Strangely, this worked on my machine! \n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Sort maven dependencies in Eclipse", "id": 1888, "answers": [{"answer_id": 1875, "document_id": 1459, "question_id": 1888, "text": "You just need to tick the checkbox Sort library entries alphabetically in Package Explorer under Preferences \u2192 Java \u2192 Appearance", "answer_start": 157, "answer_category": null}], "is_impossible": false}], "context": "Just wanted to know if it is possible in Eclipse to sort Maven dependencies by alphabetical order?\nIt's bothering me to have a list of 200 jars not ordered.\nYou just need to tick the checkbox Sort library entries alphabetically in Package Explorer under Preferences \u2192 Java \u2192 Appearance\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to install JQ on Mac on the command line?", "id": 1571, "answers": [{"answer_id": 1560, "document_id": 1148, "question_id": 1571, "text": "If you want to know the exact command just search your package on https://brewinstall.org and you will get the set of commands needed to install that package.", "answer_start": 189, "answer_category": null}], "is_impossible": false}], "context": "I need to know the most efficient way of installing JQ on Mac (El Capitan). The code is downloaded to my Mac but I would like to know how I can install and operate it via the command line.\nIf you want to know the exact command just search your package on https://brewinstall.org and you will get the set of commands needed to install that package.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "when installing jre with MSI enterprise,how to  uninstall the JRE with the Java Removal Tool?", "id": 404, "answers": [{"answer_id": 411, "document_id": 172, "question_id": 404, "text": "use the Add/Remove Programs utility in the Microsoft Windows Control Panel. The Java Removal Tool is integrated with the uninstallation process, and it will guide you through the removal of older JREs.", "answer_start": 1592, "answer_category": null}], "is_impossible": false}, {"question": "when installing jre with MSI enterprise,how to  uninstall the JRE with  the online Java Uninstall tool?", "id": 405, "answers": [{"answer_id": 412, "document_id": 172, "question_id": 405, "text": "go to https://www.java.com/en/download/uninstallapplet.jsp\nThe Java Uninstall tool helps you improve your computer security by finding and uninstalling older versions of Java. The Uninstall tool shows you a list of the Java versions on your computer and then removes those that are out-of-date.", "answer_start": 1854, "answer_category": null}], "is_impossible": false}, {"question": "when installing jre with MSI enterprise,how to  uninstall  the JRE from the Command Line?", "id": 406, "answers": [{"answer_id": 413, "document_id": 172, "question_id": 406, "text": "msiexec /x {MSI product code of JRE} ", "answer_start": 3053, "answer_category": null}], "is_impossible": false}], "context": "Use the MSI Enterprise JRE Installer to Install the JRE\nYou can use the Microsoft Windows Installer (MSI) Enterprise JRE Installer to install and uninstall the Java Runtime Environment (JRE) for Windows.\nNote:\nThe MSI Enterprise JRE Installer is available as part of Oracle Java SE Subscription and other legacy products (such as Oracle Java SE Advanced or Oracle Java SE Suite), and is only available to customers for download through My Oracle Support (MOS).\nThe Microsoft Windows Installer (MSI) Enterprise JRE Installer enables you to install the JRE across your enterprise. Because it fully supports Windows Installer 3.0, it is fully compatible with system management software, such as Systems Management Server (SMS) and Systems Center Configuration Manager (SCCM). These software management suites enable you to securely deploy software across your enterprise. In addition to the features and options that you can specify with the MSI Enterprise JRE Installer, you can specify a Java Usage Tracker configuration file and a deployment rule set.\nThis section includes the following topics:\n\u2022\tSystem Requirements\n\u2022\tInstalling the JRE from the MSI Enterprise Installer\n\u2022\tInstalling the JRE from the Command Line\n\u2022\tCreating a Log File\n\u2022\tPerforming a Static Installation of the JRE\n\u2022\tUninstalling the JRE with Java Removal and Uninstall Tools\n\u2022\tUninstalling the JRE from the Command Line\nUninstalling the JRE with Java Removal and Uninstall Tools\nYou can uninstall the JRE by using either the Java Removal Tool or the Java Uninstall tool.\n\u2022\tTo uninstall the JRE with the Java Removal Tool, use the Add/Remove Programs utility in the Microsoft Windows Control Panel. The Java Removal Tool is integrated with the uninstallation process, and it will guide you through the removal of older JREs.\n\u2022\tTo uninstall the JRE with the online Java Uninstall tool, go to https://www.java.com/en/download/uninstallapplet.jsp\nThe Java Uninstall tool helps you improve your computer security by finding and uninstalling older versions of Java. The Uninstall tool shows you a list of the Java versions on your computer and then removes those that are out-of-date.\nNote:\nThe Java Uninstall tool will not run if your system administrator specified a deployment rule set in your organization.\nThe deployment rule set enables enterprises to directly manage their Java desktop environment and continue using legacy business applications in an environment of ever-tightening Java applet and Java Web Start application security policies. The deployment rule set enables administrators to specify rules for applets and Java Web Start applications. These rules might specify that a specific JRE version must be used. Consequently, the Java Uninstall tool will not run if it detects a deployment rule set that ensures required JREs are not uninstalled.\nSee Deployment Rule Set in the Java Platform, Standard Edition Deployment Guide.\nUninstalling the JRE from the Command Line\nYou can uninstall the JRE from the command line.\nRun the following command to uninstall the JRE:\nCopymsiexec /x {MSI product code of JRE} \nIn the command, use the MSI product code of the JRE version that you want to uninstall. In the following examples, the values that are in braces are the MSI product code of the JRE that you want to uninstall. The text in bold represents the JRE version that you want to uninstall.\nExamples for JDK 8:\n\u2022\tThe following command uninstalls the 32-bit JRE, version 1.8.0_25:\nCopymsiexec /x {26A24AE4-039D-4CA4-87B4-2F83218025F0} \n\u2022\tThe following command uninstalls the 64-bit JRE, version 1.8.0_25:\nCopymsiexec /x {26A24AE4-039D-4CA4-87B4-2F86418025F0} \n\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to specify a repository for a dependency in Maven", "id": 1868, "answers": [{"answer_id": 1854, "document_id": 1439, "question_id": 1868, "text": "Your opinion is not possible. Maven checks the repositories in their declaration order until a given artifact gets resolved (or not).\nSome repository manager can do something approaching this though. For example, Nexus has a routes feature that does something equivalent.", "answer_start": 405, "answer_category": null}], "is_impossible": false}], "context": "In projects with several dependencies and repositories, the try-and-error approach of Maven for downloading dependencies is a bit cumbersome and slow, so I was wondering if there is any way to set an specific repo for some declared dependencies.\nFor example, I want for bouncycastle to check directly BouncyCastle's Maven repo at http://repo2.maven.org/maven2/org/bouncycastle/ instead of official Maven.\nYour opinion is not possible. Maven checks the repositories in their declaration order until a given artifact gets resolved (or not).\nSome repository manager can do something approaching this though. For example, Nexus has a routes feature that does something equivalent.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Which webserver to use with Django?", "id": 1125, "answers": [{"answer_id": 1118, "document_id": 702, "question_id": 1125, "text": "Here's an example of configuring nginx to serve static files directly and then pass dynamic requests to Python:From https://code.djangoproject.com/wiki/DjangoAndNginx:", "answer_start": 456, "answer_category": null}], "is_impossible": false}], "context": "I am asking this question because I am a beginner and I've read almost 90% of articles speaking about Django, but the problem is: Django was made and had problems for deploying, it is python, and python is not PHP! When reading Django tutorials, a beginner is in big problem.\nThe key point that is being made is that Django/Python should not serve your static resources. \"Two servers\" could be different physical servers, or instances, or virtual servers. Here's an example of configuring nginx to serve static files directly and then pass dynamic requests to Python:From https://code.djangoproject.com/wiki/DjangoAndNginx:\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "What's the difference between Red/Black deployment and Blue/Green Deployment?", "id": 317, "answers": [{"answer_id": 326, "document_id": 130, "question_id": 317, "text": "Both blue/green and red/black deployment represent the same concept.While the first is the most common term, the latter seems to be used mainly in Netflix and their tools (like Spinnaker).They apply only to cloud, virtualized or containerized services in the sense your infrastructure has to be automatable in order to make sense of this approach.", "answer_start": 347, "answer_category": null}], "is_impossible": false}], "context": "I've heard both used to describe the idea of deploying an update on new machines while keeping old machines active, ready to rollback if an issue occurs. I've also heard it used to describe sharing load between updated services and old service, again for the purpose of a rollbacks \u2014sometimes terminating inactive older patches and sometimes not. Both blue/green and red/black deployment represent the same concept.While the first is the most common term, the latter seems to be used mainly in Netflix and their tools (like Spinnaker).They apply only to cloud, virtualized or containerized services in the sense your infrastructure has to be automatable in order to make sense of this approach.", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to make index.html not to cache when the site contents are changes in AngularJS website?", "id": 459, "answers": [{"answer_id": 468, "document_id": 192, "question_id": 459, "text": "Yes, that is the correct way. You have to set the Cache-Control header to let the browsers know that they don't have to cache any content for that request.\n<meta http-equiv=\"Cache-control\" content=\"no-cache, no-store, must-revalidate\">\n<meta http-equiv=\"Pragma\" content=\"no-cache\">\nWhat is the Python equivalent of Tomcat?", "answer_start": 376, "answer_category": null}], "is_impossible": false}], "context": "Normally for .js and .css file we will append a version during build like xx.js?v=123, and then after website deploy, we can get the new version of js and CSS. But I don't see a place talking about how to make the index.html file upgrade when website deployment happen. And we do see in IE that the HTML content should have been changed but it still use the old HTML content.\nYes, that is the correct way. You have to set the Cache-Control header to let the browsers know that they don't have to cache any content for that request.\n<meta http-equiv=\"Cache-control\" content=\"no-cache, no-store, must-revalidate\">\n<meta http-equiv=\"Pragma\" content=\"no-cache\">\nWhat is the Python equivalent of Tomcat?\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "I want to install JRE 8 on Oracle Solaris 11 using IPS packages, how can I know whether the package is available from your IPS publisher?", "id": 212, "answers": [{"answer_id": 220, "document_id": 116, "question_id": 212, "text": "$ pkg list -a jre-8\n\nNAME (PUBLISHER)        VERSION                    IFO\ndeveloper/java/jre-8    1.8.0.0-0.183.0.0.0.0.0    ---\nIf you see an \"i\" in the I column, then the package is already installed.", "answer_start": 1580, "answer_category": null}], "is_impossible": false}, {"question": "I want to install JRE 8 on Oracle Solaris 11 using IPS packages, If I have the Software Installation rights profile, what shall I do?", "id": 213, "answers": [{"answer_id": 221, "document_id": 116, "question_id": 213, "text": "$ pfexec pkg install jre-8", "answer_start": 2335, "answer_category": null}], "is_impossible": false}, {"question": "I want to install JRE 8 on Oracle Solaris 11 using IPS packages, If I prefer to execute a privileged command., what shall I do?", "id": 214, "answers": [{"answer_id": 222, "document_id": 116, "question_id": 214, "text": "$ sudo pkg install jre-8", "answer_start": 2614, "answer_category": null}], "is_impossible": false}, {"question": "I want to install JRE 8 on Oracle Solaris 11 using IPS packages, If I have the root role, what shall I do ?", "id": 215, "answers": [{"answer_id": 223, "document_id": 116, "question_id": 215, "text": "# pkg install jre-8", "answer_start": 2808, "answer_category": null}], "is_impossible": false}, {"question": "how to install IPS packages when install JRE8 on solaris 11?", "id": 216, "answers": [{"answer_id": 224, "document_id": 116, "question_id": 216, "text": "Make sure the jre-8 package is available from your IPS publisher.\n\n$ pkg list -a jre-8\n\nNAME (PUBLISHER)        VERSION                    IFO\ndeveloper/java/jre-8    1.8.0.0-0.183.0.0.0.0.0    ---\nIf you see an \"i\" in the I column, then the package is already installed.\n\nThis package is available from the solaris publisher at pkg.oracle.com and also from other publisher origins. If you see a message that no such package is found, use the pkg publisher command to check your publisher origin and contact your system administrator or Oracle Support representative.\n\nMake sure you have permission to install IPS packages.\n\nUse the profiles command to list the rights profiles that are assigned to you. If you have the Software Installation rights profile, you can use the pfexec command to install and update packages.\n\n$ pfexec pkg install jre-8\nOther rights profiles also provide installation privilege, such as System Administrator rights profile.\n\nDepending on the security policy at your site, you might be able to use the sudo command with your user password to execute a privileged command.\n\n$ sudo pkg install jre-8\nUse the roles command to list the roles that are assigned to you. If you have the root role, you can use the su command with the root password to assume the root role.\n\n# pkg install jre-8", "answer_start": 1513, "answer_category": null}], "is_impossible": false}, {"question": "which file shall I download for installing JRE 8 release on the Oracle Solaris platform?", "id": 217, "answers": [{"answer_id": 225, "document_id": 116, "question_id": 217, "text": "Download File(s)\tArchitecture\tWho Can Install\njre-8uversion-solaris-sparcv9.tar.gz\t64-bit SPARC\tanyone\njre-8uversion-solaris-x64.tar.gz\t64-bit x64, EM64T\tanyone", "answer_start": 2982, "answer_category": null}], "is_impossible": false}, {"question": "How to install a JRE archive binary Manually?", "id": 218, "answers": [{"answer_id": 226, "document_id": 116, "question_id": 218, "text": "Download the bundle.\n\nBefore the file can be downloaded, you must accept the license agreement. The archive binaries can be installed by anyone in any location that you can write to.\n\nThe .tar.gz archive file (also called a tarball) is a file that can be simultaneously uncompressed and extracted in one step.\n\nChange directory to the location where you want the JRE to be installed.\n\nMove the .tar.gz archive binaries to the current directory.\n\nUnpack the tarball and install the JRE:\n\nOn SPARC processors:\n\n% gzip -dc jre-8uversion-solaris-sparcv9.tar.gz | tar xf -\nOn x64/EM64T processors:\n\n% gzip -dc jre-8uversion-solaris-x64.tar.gz | tar xf -", "answer_start": 3348, "answer_category": null}], "is_impossible": false}], "context": "Skip to Content\nHome PageJava SoftwareJava SE DownloadsJava SE 8 DocumentationSearch\nJava Platform, Standard Edition Installation Guide\nContents    Previous    Next\n4 JRE 8 Installation on the Oracle Solaris Operating System\nThis page describes how to install JRE 8 on Oracle Solaris operating systems.\n\nThe page contains these topics:\n\n\"System Requirements\"\n\n\"Installation Instructions Notation\"\n\n\"JRE 8 Installation Instructions for Oracle Solaris 11 using IPS packages\"\n\n\"Manual JRE 8 Installation Instructions\"\n\nSystem Requirements\nThis version of the JRE is supported on the Oracle Solaris 10 Update 9 or later OS, Oracle Solaris 11 Express OS, and Oracle Solaris 11 OS. For supported processors and browsers, see http://www.oracle.com/technetwork/java/javase/certconfig-2095354.html.\n\nInstallation Instructions Notation\nFor any text on this page containing the following notation, you must substitute the appropriate JRE update version number for the notation.\n\nversion\nFor example, if you are installing update JRE 8 update release 1, the following string representing the name of the bundle:\n\njre-8uversion-solaris-sparcv9.tar.gz\nbecomes:\n\njre-8u1-solaris-sparcv9.tar.gz\nNote that, as in the preceding example, the version number is sometimes preceded with the letter u, for example, 8u1, and sometimes it is preceded with an underscore, for example, jre1.8.0_01.\n\nJRE 8 Installation Instructions for Oracle Solaris 11 using IPS packages\nTo install JRE 8 on Oracle Solaris 11, install the jre-8 package:\n\nMake sure the jre-8 package is available from your IPS publisher.\n\n$ pkg list -a jre-8\n\nNAME (PUBLISHER)        VERSION                    IFO\ndeveloper/java/jre-8    1.8.0.0-0.183.0.0.0.0.0    ---\nIf you see an \"i\" in the I column, then the package is already installed.\n\nThis package is available from the solaris publisher at pkg.oracle.com and also from other publisher origins. If you see a message that no such package is found, use the pkg publisher command to check your publisher origin and contact your system administrator or Oracle Support representative.\n\nMake sure you have permission to install IPS packages.\n\nUse the profiles command to list the rights profiles that are assigned to you. If you have the Software Installation rights profile, you can use the pfexec command to install and update packages.\n\n$ pfexec pkg install jre-8\nOther rights profiles also provide installation privilege, such as System Administrator rights profile.\n\nDepending on the security policy at your site, you might be able to use the sudo command with your user password to execute a privileged command.\n\n$ sudo pkg install jre-8\nUse the roles command to list the roles that are assigned to you. If you have the root role, you can use the su command with the root password to assume the root role.\n\n# pkg install jre-8\nManual JRE 8 Installation Instructions\nThe following table lists the options available for downloading the JRE 8 release on the Oracle Solaris platform.\n\nDownload File(s)\tArchitecture\tWho Can Install\njre-8uversion-solaris-sparcv9.tar.gz\t64-bit SPARC\tanyone\njre-8uversion-solaris-x64.tar.gz\t64-bit x64, EM64T\tanyone\nYou can install a JRE archive binary in any location that you can write to. It will not displace the system version of the Java platform provided by the Oracle Solaris OS.\n\nFollow these steps to install:\n\nDownload the bundle.\n\nBefore the file can be downloaded, you must accept the license agreement. The archive binaries can be installed by anyone in any location that you can write to.\n\nThe .tar.gz archive file (also called a tarball) is a file that can be simultaneously uncompressed and extracted in one step.\n\nChange directory to the location where you want the JRE to be installed.\n\nMove the .tar.gz archive binaries to the current directory.\n\nUnpack the tarball and install the JRE:\n\nOn SPARC processors:\n\n% gzip -dc jre-8uversion-solaris-sparcv9.tar.gz | tar xf -\nOn x64/EM64T processors:\n\n% gzip -dc jre-8uversion-solaris-x64.tar.gz | tar xf -\nThe JRE is installed in a directory called jre1.8.0_version in the current directory. For example, for the JRE 8 update 1 release, the directory is named: jre1.8.0_01.\n\nThe JRE documentation is a separate download. See http://www.oracle.com/technetwork/java/javase/downloads/index.html#docs.\n\nContents    Previous    Next\nCopyright \u00a9 1993, 2021, Oracle and/or its affiliates. All rights reserved. | Cookie \u559c\u597d\u8bbe\u7f6e | Ad Choices.Contact Us", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "ssdt post deployment script run once", "id": 1340, "answers": [{"answer_id": 1330, "document_id": 909, "question_id": 1340, "text": "You can't tell the project to only run the latest version of a script if there are multiple scripts present because they're all built into one large PreDeploy.sql or PostDeploy.sql file. You'd need something within each section/script that would tell it where to look to know whether or not to run. That could be an existing data check, a table check, version check, or something else, but it would need to know somewhere what to use to know whether or not to run.\n", "answer_start": 495, "answer_category": null}], "is_impossible": false}], "context": "I am new to SQL Server Database Tools and may be making incorrect assumptions about what the post deployment scripts are doing.. so correct me if I am wrong.\nAs far as I am aware the post deployment script will be expected to run after every deployment, not just a single deployment.\nIf I want to have the post deployment script run a script only one time is there a way to do this without also requiring a version or history table in the database that logs when these scripts were already ran?\nYou can't tell the project to only run the latest version of a script if there are multiple scripts present because they're all built into one large PreDeploy.sql or PostDeploy.sql file. You'd need something within each section/script that would tell it where to look to know whether or not to run. That could be an existing data check, a table check, version check, or something else, but it would need to know somewhere what to use to know whether or not to run.\n\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How do I download Java for Solaris?", "id": 170, "answers": [{"answer_id": 177, "document_id": 101, "question_id": 170, "text": "Go to java.com and click on the Free Java Download button.\nThe Solaris Manual downloads page appears. Before the file can be downloaded, you must accept the license agreement. The archive binaries can be installed by anyone in any location that you can write to.\nDownload the bundle.\nThe .tar.gz archive file (also called a tarball) is a file that can be simultaneously uncompressed and extracted in one step.", "answer_start": 914, "answer_category": null}], "is_impossible": false}, {"question": "I am installing Java for Solaris, how to  unpack the tarball On 64-bit SPARC processors?", "id": 171, "answers": [{"answer_id": 178, "document_id": 101, "question_id": 171, "text": "gzip -dc jre-8u73-solaris-sparcv9.tar.gz | tar xf -", "answer_start": 1640, "answer_category": null}], "is_impossible": false}, {"question": "I am installing Java for Solaris, how to  unpack the tarball on x64/EM64T processors?", "id": 172, "answers": [{"answer_id": 179, "document_id": 101, "question_id": 172, "text": "gzip -dc jre-8u73-solaris-x64.tar.gz | tar xf -", "answer_start": 1718, "answer_category": null}], "is_impossible": false}], "context": "How do I download and install Java for Solaris?\nThis article applies to:\nPlatform(s): Solaris SPARC, Solaris x86\nJava version(s): 8.0\nSolaris System Requirements\nSee supported Java 8 System Configurations for information about supported platforms, operating systems, desktop managers, and browsers.\n\nDownload\nThe instructions below are for installing version Java 8 Update 73 (8u73). If you are installing another version, make sure you change the version number appropriately when you type the commands at the terminal. Example: For Java 8u79 replace 8u73 with 8u79. Note that, as in the preceding example, the version number is sometimes preceded with the letter u and sometimes it is preceded with an underbar, for example, jre1.8.0_73.\n\nYou can install a JRE archive binary in any location that you can write to. It will not displace the system version of the Java platform provided by the Oracle Solaris OS.\n\nGo to java.com and click on the Free Java Download button.\nThe Solaris Manual downloads page appears. Before the file can be downloaded, you must accept the license agreement. The archive binaries can be installed by anyone in any location that you can write to.\nDownload the bundle.\nThe .tar.gz archive file (also called a tarball) is a file that can be simultaneously uncompressed and extracted in one step.\n\n\nInstall\nChange directory to the location where you would like the JRE to be installed.\ncd directory_path_name\nFor example, to install the software in the /usr/java directory:\ncd /usr/java\n\nMove the .tar.gz archive binaries to the current directory.\n\nUnpack the tarball and install Java\nOn 64-bit SPARC processors:\ngzip -dc jre-8u73-solaris-sparcv9.tar.gz | tar xf -\n\nOn x64/EM64T processors:\ngzip -dc jre-8u73-solaris-x64.tar.gz | tar xf -\nThe JRE is installed in a directory called jre1.8.0_<version> in the current directory. For example, for the JRE 8 update 73 release, the directory is named jre1.8.0_73\n\nThe JRE documentation is a separate download. See http://www.oracle.com/technetwork/java/javase/downloads/index.html#docs.", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Dll in both the bin and the gac, which one gets used?", "id": 323, "answers": [{"answer_id": 332, "document_id": 136, "question_id": 323, "text": "You should double check that your dll's are signed or not. GAC takes precedence over codebase and probe. Once it finds the signed version in GAC it stops. I tested this with 4.0 and the MSDN doc is accurate, first BindingRedirect, then GAC, then CodeBase, then Probe for StrongName assemblies \u2013 CodeCowboyOrg.", "answer_start": 211, "answer_category": null}], "is_impossible": false}], "context": "We have a web application that's deployed to many websites with only frontend changes, the shared backend portion has it's DLL in the GAC so we only have to update that one dll and all the sites get the update. You should double check that your dll's are signed or not. GAC takes precedence over codebase and probe. Once it finds the signed version in GAC it stops. I tested this with 4.0 and the MSDN doc is accurate, first BindingRedirect, then GAC, then CodeBase, then Probe for StrongName assemblies \u2013 CodeCowboyOrg.\n\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Git: auto pull from repository?", "id": 1677, "answers": [{"answer_id": 1664, "document_id": 1250, "question_id": 1677, "text": "On unix-likes you can create cron job that calls \"git pull\" (every day or every week or whatever) on your machine. On windows you could use task scheduler or \"AT\" command to do the same thing.", "answer_start": 465, "answer_category": null}], "is_impossible": false}], "context": "Is there any way to set up git such that it listens for updates from a remote repo and will pull whenever something changes? The use case is I want to deploy a web app using git (so I get version control of the deployed application) but want to put the \"central\" git repo on Github rather than on the web server (Github's interface is just soooo nice).\nHas anyone gotten this working? How does Heroku do it? My Google-fu is failing to give me any relevant results.\nOn unix-likes you can create cron job that calls \"git pull\" (every day or every week or whatever) on your machine. On windows you could use task scheduler or \"AT\" command to do the same thing. Why not? Imagine you have a little Rasperry Pi in your home and no static IP available. What other option have you got then running a cron or runwhen job?\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Installing PDO driver on MySQL Linux server", "id": 650, "answers": [{"answer_id": 655, "document_id": 343, "question_id": 650, "text": "On Ubuntu you should be able to install the necessary PDO parts from apt using sudo apt-get install php5-mysql\nThere is no limitation between using PDO and mysql_ simultaneously. You will however need to create two connections to your DB, one with mysql_ and one using PDO.", "answer_start": 728, "answer_category": null}], "is_impossible": false}], "context": "I was suggested, not long ago, to change my code to use PDO in order to parameterize my queries and safely save HTML in the database.\nWell, here are the main problems:\n1.\tI looked at http://php.net/manual/en/ref.pdo-mysql.php, and I don't really get where I should put that $ ./configure --with-pdo-mysql string...\n2.\tThe site I'm building actually only requires PDO for one page. While I may consider re-writing it, it would take a while and I need the pages to be running soon, so I can't turn off MySQL completely. If I do install PDO, will I still be able to use mysql_* handlers?\nThe server in question is running PHP Version 5.4.6-1ubuntu1 and Apache/2.2.22 (Ubuntu). I'm also running a phpMyAdmin database, if it matters.On Ubuntu you should be able to install the necessary PDO parts from apt using sudo apt-get install php5-mysql\nThere is no limitation between using PDO and mysql_ simultaneously. You will however need to create two connections to your DB, one with mysql_ and one using PDO. It's worth mentioning that you need to restart apache afterwards for the changes to apply. \n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Are all of the ASP.NET MVC 4 RC assemblies really necessary for a ASP.NET WebAPI RC deployment?", "id": 1241, "answers": [{"answer_id": 1234, "document_id": 813, "question_id": 1241, "text": "You don't need all those assemblies.Just create an empty ASP.NET Project and add the Microsoft.AspNet.WebApi NuGet package.", "answer_start": 561, "answer_category": null}], "is_impossible": false}], "context": "When I created a \"Empty WebAPI\" project in Visual Studio 2010, several of the new assemblies to support the MVC web pages were added as references\nI am currently doing a bin deploy to the server with all of the assemblies it took until the JIT compliation errors ceased.\nI understand the FileNotFoundException. I understand that the runtime is trying to resolve that reference.\nMy question(s): Why is it necessary to carry around the MVC Razor assemblies when all you are trying to create is a WebAPI site? Is there another dependency that needs to be removed?\nYou don't need all those assemblies.Just create an empty ASP.NET Project and add the Microsoft.AspNet.WebApi NuGet package.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "ruby 1 9 ri on windows knows nothing about any classes using rvm?", "id": 1433, "answers": [{"answer_id": 1422, "document_id": 1007, "question_id": 1433, "text": "    \n\nIf you are using ", "answer_start": 2041, "answer_category": null}], "is_impossible": false}, {"question": "ruby 1 9 ri on windows knows nothing about any classes", "id": 1434, "answers": [{"answer_id": 1423, "document_id": 1007, "question_id": 1434, "text": "ame/.rdoc&lt;/ul&gt;\n\n\nNonetheless, there was no physical .rdoc folder (C:/Users/username/.rdoc)\nRu", "answer_start": 2341, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI'm using Windows XP SP2, and installed Ruby through Ruby 1.9 one click installer. Then when I try to using ri, I get the following response, can anyone help me with my problem? \n\nC:\\Documents and Settings\\eyang&gt;ruby --version\nruby 1.9.1p243 (2009-07-16 revision 24175) [i386-mingw32]\n\nC:\\Documents and Settings\\eyang&gt;ri --version\nri 2.2.2\n\nC:\\Documents and Settings\\eyang&gt;ri String\nUpdating class cache with 0 classes...\nNothing known about String\n\nC:\\Documents and Settings\\eyang&gt;ri\nUpdating class cache with 0 classes...\nNo ri data found\n\nIf you've installed Ruby yourself, you need to generate documentation using:\n\n  make install-doc\n\nfrom the same place you ran `make` to build ruby.\n\nIf you installed Ruby from a packaging system, then you may need to\ninstall an additional package, or ask the packager to enable ri generation.\n\nC:\\Documents and Settings\\eyang&gt;\n\n\nBy the way, when I try to use gem, I got the following error messages too, anyone can explain it?\n\nC:\\Documents and Settings\\eyang&gt;gem --version\n1.3.5\n\nC:\\Documents and Settings\\eyang&gt;gem query --remote\n\n*** REMOTE GEMS ***\n\nERROR:  While executing gem ... (Errno::ENOMEM)\n    Not enough space - &lt;STDOUT&gt;\n\nC:\\Documents and Settings\\eyang&gt;\n\n    \n\nGo to the same folder where your ruby is installed. Then do:\n\nrdoc --all --ri\n\n    \n\nRubyInstaller do not bundle RI documentation, as it increased the size of the distribution and the time to install the package.\n\nInstead, we bundled CHM (Windows Help) files for both Core and StdLib API.\n\nThis was discussed in the RubyInstaller group and the decision was made on that base.\n\nAs for your other point, two things: you need to provide a gem name or part of it, since there are 12K gems in RubyForge.\n\nAlso, depending on your console configuration (Latin or something) the Not Enough space error will be related to the terminal itself, not RubyGems.\n    \n\nwindows rubyinstaller doesn't come with the ri for core by default, so install the rdoc-data gem, then it will have it.\n    \n\nIf you are using rvm try $ rvm docs generate-ri\n    \n\nWhat might be helpful:\n\n\nWhen I ran ri.cmd -l command, it worked, though there were no known Classes/Modules\n\nWhen I ran ri.cmd --list-doc-dirs, I've got:\n\nC:/Ruby25-x64/share/ri/2.5.0/system\nC:/Ruby25-x64/share/ri/2.5.0/site\nC:/Users/username/.rdoc&lt;/ul&gt;\n\n\nNonetheless, there was no physical .rdoc folder (C:/Users/username/.rdoc)\nRunning rdoc --all --ri as recommended @GregMoreno did the trick\n\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to manually install a pypi module without pip/easy_install?", "id": 644, "answers": [{"answer_id": 649, "document_id": 337, "question_id": 644, "text": "1.\tDownload the package\n2.\tunzip it if it is zipped\n3.\tcd into the directory containing setup.py\n4.\tIf there are any installation instructions contained in documentation contianed herein, read and follow the instructions OTHERWISE\n5.\ttype in python setup.py install", "answer_start": 658, "answer_category": null}], "is_impossible": false}], "context": "I want to use the gntp module to display toaster-like notifications for a C/C++ software. I want to package all the dependencies for the soft to be self-executable on a another computer.\nThe gntp module is only available through the pip installer, which cannot be used (The computer which is running the soft do not have any internet connection) : how can I install it from the sources ?\nI would prefer not to force the user to install easy_install/pip and manually add the pip path to the %PATH.\nPS : I'm using Python 2.7 on a Windows machine.\nGeneral dependencies installation\nNow you can navigate to the more-itertools direcotry and install it as normal.\n1.\tDownload the package\n2.\tunzip it if it is zipped\n3.\tcd into the directory containing setup.py\n4.\tIf there are any installation instructions contained in documentation contianed herein, read and follow the instructions OTHERWISE\n5.\ttype in python setup.py install\nYou may need administrator privileges for step 5. What you do here thus depends on your operating system. For example in Ubuntu you would say sudo python setup.py install\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "how do i reinstall a base r package e g stats graphics utils etc", "id": 1420, "answers": [{"answer_id": 1409, "document_id": 994, "question_id": 1420, "text": "create another identical installation of R (on another machine, or in a non-default location); locate the relevant directories in your new installation and copy them over to your existing installation.\nrestore the relevant directories from your backup.\n", "answer_start": 1798, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI have been using the \"stats\" package in R 3.0.1 without any problems . But today i deleted the \"stats\" folder from the R library location and now I can't install it any more. I tried doing the same thing with other packages but I could install everything except \"stats\"\n\ninstall.packages(\"stats\", dep = TRUE, repos=\"http://cran.cs.wwu.edu\")\n\n## Installing package into \u2018%Default R Lib Installation Path%\u2019   \n## (as \u2018lib\u2019 is unspecified)  \n## Warning in install.packages :   package \u2018stats\u2019 is not available (for R version 3.0.1)\n\n\nI also tried downloading it from other sources like \"http://cran.ma.imperial.ac.uk/\" but nothing works . Any ideas?\n    \n\n(Since SO is nagging me not to continue the comment thread, I will post an answer.)\n\nI believe that this problem will apply to any base package (but not to those installed from repositories, and probably not to Recommended packages): I am deeply skeptical that stats is the only package. It should occur for any of the packages in this list:\n\nrownames(subset(as.data.frame(installed.packages()),Priority==\"base\"))\n\n\nI'm sure it's theoretically possible to re-install a base package from scratch, but it will be much easier to re-install R. At a guess, it would take me about 15 minutes to re-install R, and I would feel lucky if I figured out how to re-install a base package on its own in less than an hour.\n\nI'm pretty sure that re-installing R will not affect previously-installed packages: see e.g. http://cran.r-project.org/bin/windows/base/rw-FAQ.html#How-do-I-UNinstall-R_003f; that link is about uninstalling rather than re-installing, but this seems relevant:\n\n\n  Uninstalling R only removes files from the initial installation, not (for example) packages you have installed or updated. \n\n\nOther choices would be\n\n\ncreate another identical installation of R (on another machine, or in a non-default location); locate the relevant directories in your new installation and copy them over to your existing installation.\nrestore the relevant directories from your backup.\n\n\nPS: obviously if you are doing this on a client's machine it would be a good idea to test my advice first ...\n    \n\nWhat worked for me was to copy the entire package folder from another to my R.home() directory.\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Restart WPF application after click-once update", "id": 1113, "answers": [{"answer_id": 1105, "document_id": 690, "question_id": 1113, "text": "You should see the following website:http://robrelyea.wordpress.com/2007/07/24/application-restart-for-wpf/.", "answer_start": 108, "answer_category": null}], "is_impossible": false}], "context": "How to restart WPF application after it has been updated using click-once, i need to start the new version!\nYou should see the following website:http://robrelyea.wordpress.com/2007/07/24/application-restart-for-wpf/.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Custom distutils commands", "id": 492, "answers": [{"answer_id": 496, "document_id": 220, "question_id": 492, "text": "Sure, you can extend distutils with new commands. In your distutil configuration file, add:\n [global]\n command-packages=foo.bar", "answer_start": 260, "answer_category": null}], "is_impossible": false}], "context": "I have a library called \"example\" that I'm installing into my global site-packages directory. However, I'd like to be able to install two versions, one for production and one for testing (I have a web application and other things that are versioned this way).\nSure, you can extend distutils with new commands. In your distutil configuration file, add:\n [global]\n command-packages=foo.bar\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Web deployment task failed. Could not connect...server did not respond", "id": 1685, "answers": [{"answer_id": 1673, "document_id": 1258, "question_id": 1685, "text": "1.\topen \"Add or remove\" programs\n2.\tClick on \"Microsoft Web Deploy\"\n3.\tClick on \"Change\" button (besides \"Uninstall\")\n4.\tClick on \"Next\" in\n \n5.\tClick on \"Change\" in\n \n6.\tAdd the features marked with red X\n \n7.\tFinish the installation", "answer_start": 1895, "answer_category": null}], "is_impossible": false}], "context": "I've been publishing my Lightswitch app using Visual Studio 2012 RC to my localhost (Win 7, SQL 2008 R2, IIS 7.5) just fine. Now I'm trying to publish to a remote server (Win 2008 R2, SQL 2008 R2, IIS 7.5) and I'm having trouble.\nWhen I try to Publish, I receive the following error:\nWeb deployment task failed. (Could not connect to the remote computer (\"###.###.###.###\") using the specified process (\"Web Deployment Agent Service\") because the server did not respond. Make sure that the process (\"Web Deployment Agent Service\") is started on the remote computer. Learn more at: http://go.microsoft.com/fwlink/?LinkId=221672#ERROR_COULD_NOT_CONNECT_TO_REMOTESVC.)\nI've checked each of the things the \"learn more\" link suggests to check. I have verified that MsDepSvc and WMSVC are both running and Ports 80 and 8172 are both responding to port scans. In the Publish settings, for the Service URL, I'm using the IP address (http://###.###.###.###) of the remote machine. For the User Name, I'm using DomainName\\Administrator which is what I use to log on using RDP.\n had the same problem with Web Deploy 3.5 when I installed it with \"Web Platform Installer 5.0\"\nWhen I tried to publish from Visual Studio I got this error:\n---------------------------\nMicrosoft Visual Studio\n---------------------------\nCould not connect to the remote computer (\"10.0.3.102\") using the specified process \n(\"Web Management Service\") because the server did not respond. Make sure that the process \n(\"Web Management Service\") is started on the remote computer.  Learn more at: \nhttp://go.microsoft.com/fwlink/?LinkId=221672#ERROR_COULD_NOT_CONNECT_TO_REMOTESVC. \nThe remote server returned an error: (403) Forbidden.\n---------------------------\nOK   \n---------------------------\nI double checked the services, both was running. I switched off the firewall also and still the same error.\nHow I solved this problem:\n1.\topen \"Add or remove\" programs\n2.\tClick on \"Microsoft Web Deploy\"\n3.\tClick on \"Change\" button (besides \"Uninstall\")\n4.\tClick on \"Next\" in\n \n5.\tClick on \"Change\" in\n \n6.\tAdd the features marked with red X\n \n7.\tFinish the installation\nResult: Publish from Visual Studio is working\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": " how to Enable and Configure 32-bit Java for Linux within firefox?", "id": 167, "answers": [{"answer_id": 174, "document_id": 100, "question_id": 167, "text": "If you want to use Java within Firefox, you need to manually create a symbolic link from the plugin file in the release to one of the locations that Firefox expects. For Firefox version 21 and higher, you must create the symbolic link in your home directory, ~/.mozilla/plugins. Beginning with Firefox version 21, creating the symbolic link in the plugins subdirectory of the Firefox application's directory is not supported.", "answer_start": 177, "answer_category": null}], "is_impossible": false}, {"question": "do I need to Uninstall previous installations of Java Plugin when I Enable and Configure 32-bit Java for Linux within firefox?", "id": 168, "answers": [{"answer_id": 175, "document_id": 100, "question_id": 168, "text": "Only one Java Plugin can be used at a time. When you want to use a different plugin, or version of a plugin, remove the symbolic links to any other versions and create a fresh symbolic link to the new one.\n\nRemove the symbolic links (or move them to another directory) to javaplugin-oji.so and libnpjp2.so from the Firefox plugins directory.", "answer_start": 753, "answer_category": null}], "is_impossible": false}, {"question": "How to Create a symbolic link to the Java Plugin  for 32-bit linux?", "id": 169, "answers": [{"answer_id": 176, "document_id": 100, "question_id": 169, "text": "Go to the Firefox plugins directory\ncd ~/.mozilla/plugins\nCreate the plugins directory if it does not exist.\nCreate the symbolic link\n32-bit plugin:\nln -s Java installation directory/lib/i386/libnpjp2.so .\n64-bit plugin:\nln -s Java installation directory/lib/amd64/libnpjp2.so .", "answer_start": 1171, "answer_category": null}], "is_impossible": false}], "context": "\nHow do I download and install 32-bit Java for Linux?\nEnable and Configure\nFirefox\nWhen you install the Java platform, the Java plugin file is included as part of that install. If you want to use Java within Firefox, you need to manually create a symbolic link from the plugin file in the release to one of the locations that Firefox expects. For Firefox version 21 and higher, you must create the symbolic link in your home directory, ~/.mozilla/plugins. Beginning with Firefox version 21, creating the symbolic link in the plugins subdirectory of the Firefox application's directory is not supported.\n\nTo configure the Java Plugin follow these steps:\nExit Firefox browser if it is already running.\nUninstall any previous installations of Java Plugin.\nOnly one Java Plugin can be used at a time. When you want to use a different plugin, or version of a plugin, remove the symbolic links to any other versions and create a fresh symbolic link to the new one.\n\nRemove the symbolic links (or move them to another directory) to javaplugin-oji.so and libnpjp2.so from the Firefox plugins directory.\n\nCreate a symbolic link to the Java Plugin in the Firefox plugins directory\nGo to the Firefox plugins directory\ncd ~/.mozilla/plugins\nCreate the plugins directory if it does not exist.\nCreate the symbolic link\n32-bit plugin:\nln -s Java installation directory/lib/i386/libnpjp2.so .\n64-bit plugin:\nln -s Java installation directory/lib/amd64/libnpjp2.so .\n\nExample\nIf the Java is installed at this directory:\n/usr/java/Java installation directory\nThen type in the terminal window to go to the browser plug-in directory:\ncd ~/.mozilla/plugins/\nEnter the following command to create a symbolic link to the Java Plug-in for Firefox:\nln -s /usr/java/Java installation directory/lib/i386/libnpjp2.so .\n\nStart the Firefox browser or restart it if it is already up.\n\nIn Firefox, type about:plugins in the Location bar to confirm that the Java Plugin is loaded. You can also click the Tools menu to confirm that Java Console is there.\nTest Installation\nTo test that Java is installed and working properly on your computer, run this test applet.", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How do I tell if .NET 3.5 SP1 is installed?", "id": 844, "answers": [{"answer_id": 839, "document_id": 523, "question_id": 844, "text": "Download this application  .NET Checker and run it. This harmless code signed application will tell us if you have .NET 4.5, 4.6, 4.7 or 4.8 (including minor versions). ", "answer_start": 274, "answer_category": null}], "is_impossible": false}], "context": "Do you have .NET?\nYou have at least 4.6 on your machine. We can only infer this as the user agent of your browser doesn't give us the version. Try the .Net Checker application to get more accurate version information.\n\n\n Get .NET  .NET Checker\nWhat about .NET 4.5 or later?\nDownload this application  .NET Checker and run it. This harmless code signed application will tell us if you have .NET 4.5, 4.6, 4.7 or 4.8 (including minor versions). Don't trust us? Here's the code.\n What happened\nThis site looked at your browser's \"UserAgent\" and figured out what version (if any) of the .NET Framework you have (or don't have) installed, then calculated the total size if you chose to download the .NET Framework.\n\nThere's no database, no cookies, and nothing about your computer has been stored or kept. We just look at the information your browser already reports about your computer and make a suggestion as to the best .NET Framework download for you.\n\n Offline Download\nIf you are a developer and are distributing your code on CD or DVD, you might want to download the .NET 4.8 Offline on your media. The download is about 69.3 MB\n Online Download\nIf your users have internet connectivity, the .NET Framework is only between 31.3 and 69.3 megs. Why such a wide range? Well, it depends on if they already have some version of .NET. If you point your users to the online setup for the .NET 4.8 Web, that 1.4 MB download will automatically detect and download the smallest archive possible to get the job done.\nAre you a .NET Programmer?\nIf you're a programmer/developer, you might be trying to figure out which .NET Framework for your users to use.\n\nSometimes finding the right .NET Framework is confusing because different kinds of machines (x86, x64, ia64) that may or may not have different versions of .NET already on them.\n\nIf you look for .NET Downloads on Microsoft's site, it might look like the .NET Framework is 200+ megs. It's not. Those big downloads are the Complete Offline Versions of every version of the .NET Framework for every kind of machine possible. The big .NET download includes x86, x64, and ia64. It includes .NET 2.0, 3.0, and 3.5 code for all systems all in one super-archive. The downloads for .NET 4.5, 4.6, 4.7 and 4.8 are even smaller.\n\nWhy would you EVER want to download the whole archive? Only if you're a developer and you want to distribute the .NET Framework the widest possible audience in a format like a CD or DVD.\nYour User Agent\nFor technical or debugging purposes, this is exactly what your browser said about itself:\nMozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/95.0.4638.69 Safari/537.36\nIntegration\nWant SmallestDotNet functionality for your own site? Add this chunk of JavaScript, it'll spit out HTML and you can style to taste.\n<script type=\"text/javascript\" src=\"https://www.smallestdotnet.com/javascript.ashx\"></script>\nPrefer a JavaScript Object (JSON) to detect .NET Framework installations? Try this instead:\n<script type=\"text/javascript\" src=\"https://www.smallestdotnet.com/javascriptdom.ashx\"></script>\nGet examples on how to use the JSON object on Scott's Blog.", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How do you install an APK file in the Android emulator?", "id": 138, "answers": [{"answer_id": 146, "document_id": 84, "question_id": 138, "text": "You can simply copy .apk file in your SDK's platform-tools/ directory,then install the .apk on the emulator by using cmd(on windows): adb install <path_to_your_bin>.apk.", "answer_start": 136, "answer_category": null}], "is_impossible": false}], "context": "I finally managed to obfuscate my Android application, now I want to test it by installing the APK file and running it on the emulator. You can simply copy .apk file in your SDK's platform-tools/ directory,then install the .apk on the emulator by using cmd(on windows): adb install <path_to_your_bin>.apk.", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to restart puma after deploy?", "id": 1642, "answers": [{"answer_id": 1630, "document_id": 1216, "question_id": 1642, "text": "You can restart manually using the following command\nbundle exec pumactl -P /home/deploy/.pids/puma.pid restart", "answer_start": 222, "answer_category": null}], "is_impossible": false}], "context": "I'm using Rails, Puma, Capistrano3. I have installed the gem capistrano3-puma as well. I started Puma with Puma Jungle https://github.com/puma/puma/tree/master/tools/jungle/upstart\nHow do I restart Puma during deployment?\nYou can restart manually using the following command\nbundle exec pumactl -P /home/deploy/.pids/puma.pid restart\nMake sure you point to the correct pid path.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Is there a deployment tool similar to Fabric written in JavaScript?", "id": 518, "answers": [{"answer_id": 520, "document_id": 243, "question_id": 518, "text": "Flightplan looks very interesting and is inspired by Fabric. Its documentation has extensive examples and is worth checking out.", "answer_start": 514, "answer_category": null}], "is_impossible": false}], "context": "I put together a mobile development stack that is almost entirely using Javascript on node.js. With the only exception of SASS (prefer it to LESS) and Fabric. I prefer not to pollute my development directory and as I have to combine and minify JS and CSS anyway, I thought I could also use node.js to serve my code.\nI would like to reduce my dependence on Ruby and/or Python. I don't really use all features of Fabric so I have the hope of replacing it. But I couldn't find any similar tool written in Javascript.\nFlightplan looks very interesting and is inspired by Fabric. Its documentation has extensive examples and is worth checking out.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Node.js setup for easy deployment and updating", "id": 554, "answers": [{"answer_id": 556, "document_id": 279, "question_id": 554, "text": " You might be better off, for production use, to look at something like Cluster. You might not want the cluster features but it also includes other production features such as zero downtime restarts, logging, workers, etc.", "answer_start": 192, "answer_category": null}], "is_impossible": false}], "context": "We're currently developing a website (TYPO3 under Apache) for a customer that is supported by a node.js/socket.io application that provides realtime updates to the content served from the CMS. You might be better off, for production use, to look at something like Cluster. You might not want the cluster features but it also includes other production features such as zero downtime restarts, logging, workers, etc.", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "InnoSetup: Getting AppName in [Code] section", "id": 1773, "answers": [{"answer_id": 1759, "document_id": 1344, "question_id": 1773, "text": "It's a build-time constant, not an install-time value. So you can use the Inno Setup Preprocessor add-on to define such constants. (You can install it easily via the QuickStart pack).", "answer_start": 696, "answer_category": null}], "is_impossible": false}], "context": "I'm creating an installer using InnoSetup, and writing some custom handlers in a [Code] section. In one of the handlers, I would like to be able to retrieve the value of the AppName (or, potentially, the value of other parameters) defined in the [Setup] section. Is there a way for me to do this? I've looked though the documentation, but I haven't found anything that would allow me to do this. Our InnoSetup files are actually generated by our build process, which stitches together fragments that are common between all of our programs and that are program specific, so it would be inconvenient to have to define constants in the code for each program. Is there any convenient way to do this?\nIt's a build-time constant, not an install-time value. So you can use the Inno Setup Preprocessor add-on to define such constants. (You can install it easily via the QuickStart pack).\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "npm install gives unauthorized name or password is incorrect error", "id": 1234, "answers": [{"answer_id": 1227, "document_id": 810, "question_id": 1234, "text": "Remove .npmrc from my home directory and it works!", "answer_start": 18, "answer_category": null}], "is_impossible": false}], "context": "Found the answer.\nRemove .npmrc from my home directory and it works!\nThanks to mcollina https://github.com/mcollina at https://github.com/isaacs/npm/issues/2778\nI had a an odd issue where I got the credentials error and it was because npm login had cached bad credentials and then npm logout didn't clear the credentials (as evidenced by \"npm login\" always showing my default details).\nNPM stores your login under the globals NPM_CONFIG_EMAIL and NPM_CONFIG__AUTH. If you do \"npm config ls -la\" and see email under \"environment configs\" you may have the same issue I had (where npm logout didn't work).\nUnset the globals NPM_CONFIG__AUTH and NPM_CONFIG_EMAIL and then npm login again and it should work. (in terminal it's just \"unset NPM_CONFIG__AUTH\")\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Automatically create requirements.txt\t", "id": 1820, "answers": [{"answer_id": 1805, "document_id": 1391, "question_id": 1820, "text": "You can use the following code to generate a requirements.txt file:\npip install pipreqs\npipreqs /path/to/project", "answer_start": 312, "answer_category": null}], "is_impossible": false}], "context": "Sometimes I download the python source code from github and don't know how to install all the dependencies. If there is no requirements.txt file I have to create it by hands.\nThe question is:\nGiven the python source code directory is it possible to create requirements.txt automatically from the import section?\nYou can use the following code to generate a requirements.txt file:\npip install pipreqs\npipreqs /path/to/project\nmore info related to pipreqs can be found here.\nSometimes you come across pip freeze, but this saves all packages in the environment including those that you don't use in your current project.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Read local file in Intellij IDEA JavaEE web application deployed with tomcat", "id": 1331, "answers": [{"answer_id": 1321, "document_id": 900, "question_id": 1331, "text": "First, create your java project structure correctly, place HelloWorld.java under src/main/java.\nPut your resources under /src/main/resources.\nLoad the resources with:\n.getClass().getClassLoader().getResource(fileName)", "answer_start": 170, "answer_category": null}], "is_impossible": false}], "context": "I have a simple JavaEE web application project in Intellij and I need to read a file in src.main.resources folder. Follow is a screen shot of what my project looks like.\nFirst, create your java project structure correctly, place HelloWorld.java under src/main/java.\nPut your resources under /src/main/resources.\nLoad the resources with:\n.getClass().getClassLoader().getResource(fileName)\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to deploy a meteor application to my own server?", "id": 1701, "answers": [{"answer_id": 1689, "document_id": 1274, "question_id": 1701, "text": "Try Meteor Up too\nWith that you can deploy into any Ubuntu server. This uses meteor build command internally. And used by many for deploying production apps.", "answer_start": 430, "answer_category": null}], "is_impossible": false}], "context": "How to deploy a meteor application to my own server?\nflavour 1: the development and deployment server are the same;\nflavour 2: the development server is one (maybe my localhost) and the deployment server is another (maybe a VPS in the cloud);\nflavour 3: I want to make a \"meteor hosting\" domain, just like \"meteor.com\". Is it possible? How?\nUpdate:\nI'm running Ubuntu and I don't want to \"demeteorize\" the application. Thank you.\nTry Meteor Up too\nWith that you can deploy into any Ubuntu server. This uses meteor build command internally. And used by many for deploying production apps.\nI created Meteor Up to allow developers to deploy production quality Meteor apps until Galaxy comes.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Secure distribution of NodeJS applications", "id": 315, "answers": [{"answer_id": 325, "document_id": 128, "question_id": 315, "text": "Yes you can create a binary format. V8 allows you to pre-compile JavaScript. Note that this might have a bunch of weird side-effects on assumptions made by node core.", "answer_start": 313, "answer_category": null}], "is_impossible": false}], "context": "We build serverside applications in NodeJS for clients, that have often to be hosted on the client's servers. Distributing source code means clients can easily steal our solution and stop paying licensing fees. This opens up the possibility of easy reverse-engineering or reuse of our apps without our awareness. Yes you can create a binary format. V8 allows you to pre-compile JavaScript. Note that this might have a bunch of weird side-effects on assumptions made by node core. Just because you distribute the binary doesn't protect you againsts theft. They can still steal the binary code or disassemble it. This is protection through obscurity which is no protection at all.", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to deploy after a build with TeamCity?", "id": 1687, "answers": [{"answer_id": 1675, "document_id": 1260, "question_id": 1687, "text": "1.\tInstall WebDeploy\n2.\tEnable Web config transforms\n3.\tConfigure TeamCity BuildRunner\n4.\tConfigure TeamCity Build Dependencies", "answer_start": 692, "answer_category": null}], "is_impossible": false}], "context": "I'm setting up TeamCity as my build server.\nI have my project set up, it is updating correctly from subversion, and building ok.\nSo what's next?\nIdeally, I'd like to have it auto deploy to a test server, with a manual deploy to a live/staging server.\nWhat's the best way to go about this?\nSince I am using C#/ASP.Net, should I add a Web Deployment project to my solution?\nc#asp.netdeploymentteamcity\nThis article explains how to call Microsoft's WebDeploy tool from TeamCity to deploy a web application to a remote web server. I've been using it to deploy to a test web server and run selenium tests on check-in.\nhttp://www.mikevalenty.com/automatic-deployment-from-teamcity-using-webdeploy/\n1.\tInstall WebDeploy\n2.\tEnable Web config transforms\n3.\tConfigure TeamCity BuildRunner\n4.\tConfigure TeamCity Build Dependencies\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "maven : Failed to install metadata project Could not parse metadata maven-metadata-local.xml: only whitespace content allowed before start tag", "id": 1211, "answers": [{"answer_id": 1204, "document_id": 787, "question_id": 1211, "text": "Go to the the path where the maven-metadata-local.xml is. Delete the project folder along with the xml and build the project.", "answer_start": 496, "answer_category": null}], "is_impossible": false}], "context": "Failed to execute goal org.apache.maven.plugins:maven-install-plugin:2.4:install (default-install) on project : Failed to install metadata project:1.0-SNAPSHOT/maven-metadata.xml: Could not parse metadata C:\\Users.m2\\project\\1.0-SNAPSHOT\\maven-metadata-local.xml: only whitespace content allowed before start tag and not \\u0 (position: START_DOCUMENT seen \\u0... @1:1) -> [Help 1]\nI just wanted to document this error on the internet. There was no much help or I didn't search properly.\nAnswer :\nGo to the the path where the maven-metadata-local.xml is. Delete the project folder along with the xml and build the project.\nIt worked for me!\n\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How do I find the install time and date of Windows?", "id": 1563, "answers": [{"answer_id": 1552, "document_id": 1140, "question_id": 1563, "text": "In regedit.exe you just need to:\nHKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\\", "answer_start": 290, "answer_category": null}], "is_impossible": false}], "context": "This might sound like a little bit of a crazy question, but how can I find out (hopefully via an API/registry key) the install time and date of Windows?\nThe best I can come up with so far is to look at various files in C:\\Windows and try to guess... but that's not exactly a nice solution.\nIn regedit.exe you just need to:\nHKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\\\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "what is the best way to install conda on macos apple mac", "id": 1499, "answers": [{"answer_id": 1488, "document_id": 1078, "question_id": 1499, "text": "brew cask install anaconda\nexport PATH=\"/usr/local/anaconda3/bin:$PATH\"", "answer_start": 383, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nWhat is the recommended approach for installing Anaconda on Mac?\n\nI tried with brew cask install anaconda which after a while returns anaconda was successfully installed!.\n\nAfter that - trying conda command returns command not found: conda.\n\nIs there any post step installation that needs to be done?\nAnd what is recommended way to install Conda on MacOS?\n    \n\n\nbrew cask install anaconda\nexport PATH=\"/usr/local/anaconda3/bin:$PATH\"\n\n    \n\nI would say that the recommended way to install anaconda is to use the official anaconda installer, which can be downloaded from the link I just posted. I've done it several times, never had a problem, and it walks you through it (including an option to automatically add it to your PATH).\n    \n\nI don't know about other people but I've had issue downloading conda/miniconda etc for a few hours now. For some reason it decided to install at ~/opt when using the graphical installer (i.e. the .dmg file). I've been through the uninstall here How to uninstall Anaconda completely from macOS and additionall did an rm -rf ~/opt command. Seems that without this its not actually uninstalled (you might also have to change your PATH or .bash_profile or .bashrc until your path is virigin again before you start your re-installation installation). Seems that using the command line installer is what works:\n\n\nAnaconda3 will now be installed into this location:\n/Users/brandBrandoParetoopareto/anaconda3\n\n  - Press ENTER to confirm the location\n  - Press CTRL-C to abort the installation\n  - Or specify a different location below\n\n[/Users/brandBrandoParetoopareto/anaconda3] &gt;&gt;&gt; \nPREFIX=/Users/brandBrandoParetoopareto/anaconda3\n\nUnpacking payload ...\nCollecting package metadata (current_repodata.json): done                                                                                                                                                                                                                                                                                            \nSolving environment: done\n\n\nSo for that download it from the official link then do:\n\nsh Anaconda3-2020.02-MacOSX-x86_64.sh \n\n\ndo sh I believe is the right thing because I might have had issue in the past when I did bash instead...plus if you are using a different shell like zsh I am not sure what you'd need to do, but I'd get sh would be safest.\n\nAfter the installation is done you should do:\n\nconda init &lt;SHELL-NAME&gt;\n\n\nso that conda is initialized correctly (so far that seems to only modify my .bash_profile and my PATH variable). Unfortunately, it seems the previous uninstallation attempts didn't remove the code the previous conda init had added from my .bash_profile so I removed it manually using vim.\n\nThis is what I get after doing that:\n\nconda init bash\n\nno change     /Users/brandBrandoParetoopareto/anaconda3/condabin/conda\nno change     /Users/brandBrandoParetoopareto/anaconda3/bin/conda\nno change     /Users/brandBrandoParetoopareto/anaconda3/bin/conda-env\nno change     /Users/brandBrandoParetoopareto/anaconda3/bin/activate\nno change     /Users/brandBrandoParetoopareto/anaconda3/bin/deactivate\nno change     /Users/brandBrandoParetoopareto/anaconda3/etc/profile.d/conda.sh\nno change     /Users/brandBrandoParetoopareto/anaconda3/etc/fish/conf.d/conda.fish\nno change     /Users/brandBrandoParetoopareto/anaconda3/shell/condabin/Conda.psm1\nno change     /Users/brandBrandoParetoopareto/anaconda3/shell/condabin/conda-hook.ps1\nno change     /Users/brandBrandoParetoopareto/anaconda3/lib/python3.7/site-packages/xontrib/conda.xsh\nno change     /Users/brandBrandoParetoopareto/anaconda3/etc/profile.d/conda.csh\nmodified      /Users/brandBrandoParetoopareto/.bash_profile\n\n==&gt; For changes to take effect, close and re-open your current shell. &lt;==\n\n\n\nif you are using vs-code integrated terminal like I am you need to press the trash can button. Doing bash seems to NOT re-run your .bash_profile so make sure you do what it would consider \"closing your terminal and re-opening it completely\". \n\nThat should be all you need to do I believe. Perhaps you also need to make sure you have the most recent version of mac OS.\n\n\n\nExtra tips hints\n\n\nMake sure conda init modified your .bash_profile correctly. For me for some reason it added it's stuff AFTER it ran my .bashrc and thus when my .bashrc tried activating my environment it wouldn't do it as it would say conda wasn't initialized correctly (and thus nio matter how many times I re-ran conda init &lt;SHELL&gt; it wouldn't fix it. I don't know why that happened but that's how it was.\nI avoided the dmg/graphical installation since it seemed to install it at non-standard places ~/opt\nIf conda is still acting weird it might be because of the way your .bashrc modifies the PATH env variable. What worked for me was removing lines that modified my path in .bashrc (AND having the code conda init added before my .bashrc was ran).\n\n    \n\nAfter installation using the graphical installation, everything sits in the ~/opt directory, as mentioned in some previous answers. If this is OK for you, all you need to do to use the command line conda is add ~/opt/anaconda3/bin in your path. This can be done by adding\nexport PATH=\"${PATH}:~/opt/anaconda3/bin\"\n\nat the end of your rc file (~/.zshrc or ~/.bashrc).\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to automate migration (schema and data) for PHP/MySQL application", "id": 465, "answers": [{"answer_id": 474, "document_id": 198, "question_id": 465, "text": "What you want to do is create a 'db_schema_versions' table:\nCREATE TABLE db_schema_versions (\n  `table` varchar(255) NOT NULL PRIMARY KEY, \n  `version` INT NOT NULL\n)", "answer_start": 411, "answer_category": null}], "is_impossible": false}], "context": "I have an application in PHP/MySQL. I am searching for an automated way upgrading database behind the application. I don't need to have the compatibility with older versions once it is upgraded.\nI have read jeff's and K. Scott Allen's articles on this.\nI am still not sure how to implement this for a PHP/MySQL application.\nIs there any simple and good process for this?\nYou could do the same without classes..\nWhat you want to do is create a 'db_schema_versions' table:\nCREATE TABLE db_schema_versions (\n  `table` varchar(255) NOT NULL PRIMARY KEY, \n  `version` INT NOT NULL\n)\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Cython Install GCC error", "id": 1736, "answers": [{"answer_id": 1723, "document_id": 1308, "question_id": 1736, "text": "You need the developer version of Python - i.e. the Python header files (Python.h)\nsudo apt-get install python-dev", "answer_start": 211, "answer_category": null}], "is_impossible": false}], "context": "Trying to install Cython on a small VPS running Ubuntu Server. Did\nsudo apt-get install gcc\nWhy should I need a 'development version of Python'? Running Python 2.6.5 (r265:79063, Apr 16 2010, 13:57:41). Thanks!\nYou need the developer version of Python - i.e. the Python header files (Python.h)\nsudo apt-get install python-dev\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "how to create a remote git repository from a local one", "id": 1522, "answers": [{"answer_id": 1511, "document_id": 1099, "question_id": 1522, "text": "cd     /outside_of_any_repo\nmkdir  my_remote.git\ncd     my_remote.git\ngit init --bare\n\n\n\nand then\n\ncd  /your_path/original_repo\ngit remote add origin /outside_of_any_repo/my_remote.git\ngit push --set-upstream origin master", "answer_start": 3721, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI have a local Git repository. I would like to make it available on a remote, ssh-enabled, server. How do I do this?\n    \n\nI think you make a bare repository on the remote side, git init --bare, add the remote side as the push/pull tracker for your local repository (git remote add origin URL), and then locally you just say git push origin master. Now any other repository can pull from the remote repository.\n    \n\nIn order to initially set up any Git server, you have to export an existing repository into a new bare repository \u2014 a repository that doesn\u2019t contain a working directory. This is generally straightforward to do. In order to clone your repository to create a new bare repository, you run the clone command with the --bare option. By convention, bare repository directories end in .git, like so:\n\n$ git clone --bare my_project my_project.git\nInitialized empty Git repository in /opt/projects/my_project.git/\n\n\nThis command takes the Git repository by itself, without a working directory, and creates a directory specifically for it alone.\n\nNow that you have a bare copy of your repository, all you need to do is put it on a server and set up your protocols. Let\u2019s say you\u2019ve set up a server called git.example.com that you have SSH access to, and you want to store all your Git repositories under the /opt/git directory. You can set up your new repository by copying your bare repository over:\n\n$ scp -r my_project.git user@git.example.com:/opt/git\n\n\nAt this point, other users who have SSH access to the same server which has read-access to the /opt/git directory can clone your repository by running\n\n$ git clone user@git.example.com:/opt/git/my_project.git\n\n\nIf a user SSHs into a server and has write access to the /opt/git/my_project.git directory, they will also automatically have push access. Git will automatically add group write permissions to a repository properly if you run the git init command with the --shared option.\n\n$ ssh user@git.example.com\n$ cd /opt/git/my_project.git\n$ git init --bare --shared\n\n\nIt is very easy to take a Git repository, create a bare version, and place it on a server to which you and your collaborators have SSH access. Now you\u2019re ready to collaborate on the same project.\n    \n\nA note for people who created the local copy on Windows and want to create a corresponding remote repository on a Unix-line system, where text files get LF endings on further clones by developers on Unix-like systems, but CRLF endings on Windows.\n\nIf you created your Windows repository before setting up line-ending translation  then you have a problem. Git's default setting is no translation, so your working set uses CRLF but your repository (i.e. the data stored under .git) has saved the files as CRLF too.  \n\nWhen you push to the remote, the saved files are copied as-is, no line ending translation occurs. (Line ending translation occurs when files are commited to a repository, not when repositories are pushed). You end up with CRLF in your Unix-like repository, which is not what you want.\n\nTo get LF in the remote repository you have to make sure LF is in the local repository first, by re-normalizing your Windows repository.  This will have no visible effect on your Windows working set, which still has CRLF endings, however when you push to remote, the remote will get LF correctly.\n\nI'm not sure if there's an easy way to tell what line endings you have in your Windows repository - I guess you could test it by setting core.autocrlf=false and then cloning (If the repo has LF endings, the clone will have LF too).\n    \n\nThere is an interesting difference between the two popular solutions above:\n\n\nIf you create the bare repository like this:\n\n\ncd     /outside_of_any_repo\nmkdir  my_remote.git\ncd     my_remote.git\ngit init --bare\n\n\n\nand then\n\ncd  /your_path/original_repo\ngit remote add origin /outside_of_any_repo/my_remote.git\ngit push --set-upstream origin master\n\n\nThen git sets up the configuration in 'original_repo' with this relationship:\n\noriginal_repo origin --&gt; /outside_of_any_repo/my_remote.git/\n\n\nwith the latter as the upstream remote.  And the upstream remote doesn't have any other remotes in its configuration.\n\n\nHowever, if you do it the other way around:\n\n\n(from in directory original_repo)\ncd ..\ngit clone --bare original_repo  /outside_of_any_repo/my_remote.git\n\n\n\nthen 'my_remote.git' winds up with its configuration having 'origin' pointing back to 'original_repo' as a remote, with a remote.origin.url equating to local directory path, which might not be appropriate if it is going to be moved to a server.\n\nWhile that \"remote\" reference is easy to get rid of later if it isn't appropriate, 'original_repo' still has to be set up to point to 'my_remote.git' as an up-stream remote (or to wherever it is going to be shared from).  So technically, you can arrive at the same result with a few more steps with approach #2.  But #1 seems a more direct approach to creating a \"central bare shared repo\" originating from a local one, appropriate for moving to a server, with fewer steps involved.  I think it depends on the role you want the remote repo to play.  (And yes, this is in conflict with the documentation here.)\n\nCaveat:  I learned the above (at this writing in early August 2019) by doing a test on my local system with a real repo, and then doing a file-by-file comparison between the results.  But!  I am still learning, so there could be a more correct way.  But my tests have helped me conclude that #1 is my currently-preferred method.\n    \n\n\n  A remote repository is generally a bare repository\u2009\u2014\u2009a Git repository\n  that has no working directory. Because the repository is only used as\n  a collaboration point, there is no reason to have a snapshot checked\n  out on disk; it\u2019s just the Git data. In the simplest terms, a bare\n  repository is the contents of your project\u2019s .git directory and\n  nothing else.\n\n\nYou can make a bare git repository with the following code:\n\n$ git clone --bare /path/to/project project.git\n\n\nOne options for having a remote git repository is using SSH protocol:\n\n\n  A common transport protocol for Git when self-hosting is over SSH.\n  This is because SSH access to servers is already set up in most\n  places\u2009\u2014\u2009and if it isn\u2019t, it\u2019s easy to do. SSH is also an\n  authenticated network protocol and, because it\u2019s ubiquitous, it\u2019s\n  generally easy to set up and use.\n  \n  To clone a Git repository over SSH, you can specify an ssh:// URL\n  like this:\n\n$ git clone ssh://[user@]server/project.git\n\n  \n  Or you can use the shorter scp-like syntax for the SSH protocol:\n\n$ git clone [user@]server:project.git\n\n  \n  In both cases above, if you don\u2019t specify the optional username, Git\n  assumes the user you\u2019re currently logged in as.\n  \n  The Pros\n  \n  The pros of using SSH are many. First, SSH is relatively easy to set\n  up\u2009\u2014\u2009SSH daemons are commonplace, many network admins have experience\n  with them, and many OS distributions are set up with them or have\n  tools to manage them. Next, access over SSH is secure\u2009\u2014\u2009all data\n  transfer is encrypted and authenticated. Last, like the HTTPS, Git and\n  Local protocols, SSH is efficient, making the data as compact as\n  possible before transferring it.\n  \n  The Cons\n  \n  The negative aspect of SSH is that it doesn\u2019t support anonymous access\n  to your Git repository. If you\u2019re using SSH, people must have SSH\n  access to your machine, even in a read-only capacity, which doesn\u2019t\n  make SSH conducive to open source projects for which people might\n  simply want to clone your repository to examine it. If you\u2019re using it\n  only within your corporate network, SSH may be the only protocol you\n  need to deal with. If you want to allow anonymous read-only access to\n  your projects and also want to use SSH, you\u2019ll have to set up SSH for\n  you to push over but something else for others to fetch from.\n\n\nFor more information, check the reference:\nGit on the Server - The Protocols\n    \n\nYou need to create a directory on a remote server. Then use \"git init\" command to set it as a repository. This should be done for each new project you have (each new folder)\n\nAssuming you have already setup and used git using ssh keys, I wrote a small Python script, which when executed from a working directory will set up a remote and initialize the directory as a git repo. Of course, you will have to edit script (only once) to tell it server and Root path for all repositories.\n\nCheck here - https://github.com/skbobade/ocgi\n    \n\nIn current code folder.\ngit remote add origin http://yourdomain-of-git.com/project.git\ngit push --set-upstream origin master\n\nThen review by\ngit remote --v\n\n    \n\nNormally you can set up a git repo by just using the init command\n\ngit init\n\n\nIn your case, there is already a repo on a remote available. Dependent on how you access your remote repo ( with username inside the url or a ssh key which handles verification ) use just the clone command:\n\ngit clone git@[my.url.com]:[git-repo-name].git\n\n\nThere are also other ways to clone the repo. This way you call it if you have a ssh key setup on your machine which verifies on pulling your repository. There are other combinations of the url if you want to include your password and username inside to login into your remote repository.\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to install APK from PC?", "id": 876, "answers": [{"answer_id": 871, "document_id": 557, "question_id": 876, "text": "adb install <path_to_apk>", "answer_start": 2955, "answer_category": null}], "is_impossible": false}], "context": "111\n\n\n20\nI want to install an APK from PC to Android device. And because of user's Android and generally technical skills, I need to do it as automatically (silently) as possible. So how do I send an APK from PC to Android and start install there?\n\nandroid\ninstallation\napk\nShare\nImprove this question\nFollow\nedited May 1 '18 at 11:19\n\nGrimthorr\n6,37655 gold badges4040 silver badges5252 bronze badges\nasked Mar 15 '12 at 10:44\n\nuser1254836\n1,79933 gold badges1212 silver badges99 bronze badges\ntech-papers.org/run_android_apps_on_chrome \u2013 \nBalwinder SIngh\n Oct 4 '14 at 12:26\nUse Airdroid app. play.google.com/store/apps/details?id=com.sand.airdroid \u2013 \nAshokchakravarthi Nagarajan\n Apr 23 '15 at 12:05\n3\nI had this same problem in the context of software development (needed to install file manager on a very reluctant device for which I was developing an app that would access the sd card), so I think it's not off-topic. \u2013 \nAntonio\n Oct 30 '15 at 12:45\n-I'm facing issue to update in same way.... -update version and generate signed apk -when tap to install it prompts for updates but at the end show \"App not install\"??? any solution for this as i'm developing an app and wan to update on my physical device. \u2013 \nMuhammad Zeshan Ghafoor\n Jun 12 '20 at 17:02\nAdd a comment\n5 Answers\n\n112\n\nConnect Android device to PC via USB cable and turn on USB storage.\nCopy .apk file to attached device's storage.\nTurn off USB storage and disconnect it from PC.\nCheck the option Settings \u2192 Applications \u2192 Unknown sources OR Settings > Security > Unknown Sources.\nOpen FileManager app and click on the copied .apk file. If you can't fine the apk file try searching or allowing hidden files. It will ask you whether to install this app or not. Click Yes or OK.\nThis procedure works even if ADB is not available.\n\nShare\nImprove this answer\nFollow\nedited Sep 26 '18 at 0:33\n\nJess\n21.1k1818 gold badges115115 silver badges133133 bronze badges\nanswered Mar 15 '12 at 10:54\n\nYugandhar Babu\n10k99 gold badges3939 silver badges6565 bronze badges\n14\nIf you don't have a file-browser, you can't install file-browser to install files. However, you CAN open a .apk by browsing with... THE BROWSER: file://path/to/app.apk \u2013 \nMichael Paulukonis\n Aug 16 '13 at 1:15\nIf you don't have a FileManager app installed you can download and install it from Google Play. It's free, but it shows advertising. It's able to install .apk files. \u2013 \nRenniePet\n Sep 24 '13 at 20:30\nIt's a very useful answer generally but this process can't be automated easily, probably this is the reason why wasn't it accepted. \u2013 \npcjuzer\n Feb 18 '15 at 12:10\nIf you do not have a file manager and you cannot install it then it is possible to upload apk via bluetooth. Once transferred tap the file to install. \u2013 \nLeos Literak\n Feb 19 '15 at 11:34\n@LeosLiterak Most phones have a download-manager-app so you don't need an filebrowser for installing. \u2013 \nWuerfelDev\n Jul 8 '15 at 17:30\nShow 3 more comments\n\n109\n\nadb install <path_to_apk>\nhttp://developer.android.com/guide/developing/tools/adb.html#move\n\nShare\nImprove this answer\nFollow\nedited Jul 1 '15 at 16:29\n\nLimon Monte\n46.3k4444 gold badges167167 silver badges198198 bronze badges\nanswered Mar 15 '12 at 10:45\n\nOllie C\n27.5k3434 gold badges126126 silver badges212212 bronze badges\n2\nAnd when I need to install apk from their PC? \u2013 \nuser1254836\n Mar 15 '12 at 10:49\n2\nYou can host your apk at [http://]yoursite/your.apk then ask them to open this link in browser. Then it shall download the apk file and ask for permission to install. \u2013 \nCalvin\n Mar 15 '12 at 11:39 \n9\nIf they use Gmail, you can email the APK to them, and when they receive the email in the Android Gmail client the email has an \"Install\" button on it. Installing apps via Gmail is very easy, however do note that before anyone can install an app from a source other than the Android Market, they must set the \"Unknown sources\" setting on their device, to allow installation of apps from places other than the Android Market (Google Play!) \u2013 \nOllie C\n Mar 15 '12 at 11:54\n4\nRemember in 4.2+ enable first adb Debugging. Quoted from above link: In order to use adb with a device connected over USB, you must enable USB debugging in the device system settings, under Developer options. On Android 4.2 and higher, the Developer options screen is hidden by default. To make it visible, go to Settings > About phone and tap Build number seven times. Return to the previous screen to find Developer options at the bottom. \u2013 \nOlaf\n Aug 11 '14 at 16:09 \n2\nCan't access that in China. What if the site goes down. Post a local tutorial. Why is this even marked as an answer? -1 \u2013 \nWolfpack'08\n Dec 2 '14 at 5:04\nShow 1 more comment\n\n9\n\nJust connect the device to the PC with a USB cable, then copy the .apk file to the device. On the device, touch the APK file in the file explorer to install it.\n\nYou could also offer the .apk on your website. People can download it, then touch it to install.\n\nShare\nImprove this answer\nFollow\nanswered Mar 15 '12 at 10:51\n\nAlexander van Oostenrijk\n4,23722 gold badges1818 silver badges3737 bronze badges\nAdd a comment\n\n\n6\n\n3 Ways to Install Applications On Android Without The Market\n\nAnd don't forget to enable Unknown sources in your Android device Settings, before installing apk, else Android platform will not allow you to install apk directly\n\nenter image description here", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How do I completely uninstall a ClickOnce application from my computer?", "id": 1135, "answers": [{"answer_id": 1128, "document_id": 712, "question_id": 1135, "text": "You can try using this command to clear the ClickOnce cache:\nrundll32 dfshim CleanOnlineAppCache", "answer_start": 0, "answer_category": null}], "is_impossible": false}], "context": "You can try using this command to clear the ClickOnce cache:\nrundll32 dfshim CleanOnlineAppCache\nWhile experimenting with trying to publish both a Foo and Foo Beta version of my application via ClickOnce I managed to clobber the existing and previously-working-fine Foo 1.0.0 install (replacing it with Foo Beta 1.0.5, which does not work) due to using the same GUID, assembly name, product name or something.\nOK, honest mistake. In an attempt to revert this I then uninstalled Foo Beta using the Windows 7 add/remove programs.\nMy computer is now in a state where no instance of Foo shows up in Windows 7 Add/Remove programs. However, Foo Beta is still shown as installed. In addition, I am unable to re-install Foo 1.0.0, because it thinks that a newer version of the program (Foo Beta 1.0.5) already exists on my computer - instead I get the `Unable to install because a newer version of this product is already installed' message.\nHow can I get my computer to a state where neither Foo nor Foo Beta have ever been installed on it, so that I can then go and re-install Foo?\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Error Installing Jekyll - Native Extension Build", "id": 786, "answers": [{"answer_id": 782, "document_id": 469, "question_id": 786, "text": "apt-get install ruby-dev", "answer_start": 298, "answer_category": null}], "is_impossible": false}], "context": "I'm having some trouble installing jekyll. Can't quite figure out how to patch the missing link. I think it's an update to Ruby, but RVM is having trouble installing alternate versions of ruby as well. I had the same error on Ubuntu and this helped me sort it out. You must have ruby-dev installed\napt-get install ruby-dev. it will prompt you to install these tools. After that just follow SrBlanco\u00b4s answer. That solved the problem for me.\nGood luck.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to install trusted CA certificate on Android device?", "id": 1574, "answers": [{"answer_id": 1563, "document_id": 1151, "question_id": 1574, "text": "Prior to Android KitKat you have to root your device to install new certificates.", "answer_start": 554, "answer_category": null}], "is_impossible": false}], "context": "I have created my own CA certificate and now I want to install it on my Android Froyo device (HTC Desire Z), so that the device trusts my certificate.\nAndroid stores CA certificates in its Java keystore in /system/etc/security/cacerts.bks. I copied the file to my computer, added my certificate using portecle 1.5 and pushed it back to the device.\nNow, Android does not seem to reload the file automatically. I have read in several blog posts that I need to restart the device. Doing so results in the file being overwritten with the original one again.\nPrior to Android KitKat you have to root your device to install new certificates.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "installing pyqt4 on mac osx mavericks", "id": 1439, "answers": [{"answer_id": 1428, "document_id": 1011, "question_id": 1439, "text": "sudo python3.5 -m pip install PyQt5", "answer_start": 1709, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nhttp://www.riverbankcomputing.com/software/pyqt/download\n\nI tried many solutions on internet including\n\nbrew install qt\nbrew install sip\nbrew install pyqt\n\n\nto successfully install it on osx. \nI am using PyCharm IDE to python development and want to install it for python 3.\n\nI just cannot download pyqt4 libraries to mac,\nis there some specific steps i need to follow?\nEasy way? Hard way? anything...\n\nSorry, I am new to mac world.\n    \n\nYou can install it with homebrew using the --with-python3 flag:\n\nunset PYTHONPATH\nbrew install sip --with-python3\nbrew install pyqt --with-python3\n\n\nAnd relink the site packages if necessary.\n    \n\nThe easiest way I've found is to use MacPorts. Once installed, simply run\n\nsudo port install py34-pyqt4\n\n\nand it will do the rest - install Python 3.4, pyqt4, and all the dependencies. You'll need to configure PyCharm to use the MacPorts version of Python (found in /opt/local/bin), but after that you should be all set. There are many modules available through MacPorts, and for those that aren't you can always install py34-pip.\n    \n\nI know this is from a year ago but this may help someone...\n\nNote: This is for PyQT5 and Python 3. It is an alternative to using homebrew.\n\nBackground\n\nIf you installed Python 3.x, it installs in a separate directory (leaving your Mac version alone). After adding the new directories to your path, most people can just use python3.5 (or whatever version) to access it without having to change over python alias.\n\nAlso note that Python comes with pip right out of the box...\n\nRead more on Python 3 for Mac here.\n\nAnswer the question already\n\nNow, all that said, you can simply use the following to install via pip:\n\nsudo python3.5 -m pip install PyQt5\n\n\nYou'll likely have to use sudo. Output should look like:\n\nCollecting PyQt5\n  Downloading PyQt5-5.7-cp35-cp35m-macosx_10_6_intel.whl (79.4MB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 79.4MB 18kB/s \nCollecting sip (from PyQt5)\n  Downloading sip-4.18.1-cp35-cp35m-macosx_10_6_intel.whl (46kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51kB 9.8MB/s \nInstalling collected packages: sip, PyQt5\nSuccessfully installed PyQt5-5.7 sip-4.18.1\n\n\nDon't forget to use the -m option. It allows for library modules to be run as scripts. From the --help entry:\n\n-m mod : run library module as a script (terminates option list)\n\n\nNote: Older versions of PyQT cannot be installed via pip.\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Installing Ruby on Mac OS X 10.8.2", "id": 1190, "answers": [{"answer_id": 1183, "document_id": 766, "question_id": 1190, "text": "All I had to do was:\nbrew update\nbrew tap homebrew/dupes\nbrew install apple-gcc42\nrvm get stable\nrvm install 1.9.3-p374", "answer_start": 394, "answer_category": null}], "is_impossible": false}], "context": "I have referred to Installing Ruby on Rails - Mac OS Lion and followed Alain Beauvois's reply and got pretty much everything up. The only difference is I am using 1.9.3 instead of 1.9.2.\nI have created .bash_profile and even made sure that is there by open -e .bash_profile and even added the line as stated by Alain Beauvois.\nI had the exact same error, but am using brew instead of MacPorts. All I had to do was:\nbrew update\nbrew tap homebrew/dupes\nbrew install apple-gcc42\nrvm get stable\nrvm install 1.9.3-p374\nI didn't have to set the CC environment variable as some instructions point out. HTH!\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to install Watchman on Windows (win10)?", "id": 1194, "answers": [{"answer_id": 1187, "document_id": 770, "question_id": 1194, "text": "Installation is as simple as:\nPS C:\\> choco install watchman", "answer_start": 418, "answer_category": null}], "is_impossible": false}], "context": "I am trying to install Watchman to my computer for npm for React Native. I opened the docs http://facebook.github.io/watchman/docs/install.html and can't understand how it should be done.\nCould you please explain me more detailed?\nFor those using Chocolatey (this is not published on the website yet, but found the info in the github repo):\nWatchman is available to install via the Chocolatey Windows package manager. Installation is as simple as:\nPS C:\\> choco install watchman\nReference: https://github.com/facebook/watchman/blob/573b18b86b88759f31e83fa2e1837539d138de22/website/_docs/install.markdown#installing-on-windows-via-chocolatey\nBeware that installing Chocolatey itself is somewhat tedious, but here is the link if you'd like to give it a go: https://chocolatey.org/courses/installation\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "I've been usually installed python packages through pip.\n", "id": 1548, "answers": [{"answer_id": 1537, "document_id": 1125, "question_id": 1548, "text": "Are you using OS X and Homebrew? The Homebrew python page https://github.com/Homebrew/brew/blob/master/docs/Homebrew-and-Python.md calls out a known issue with pip and a work around.", "answer_start": 136, "answer_category": null}], "is_impossible": false}], "context": "I've been usually installed python packages through pip.\nFor Google App Engine, I need to install packages to another target directory.\nAre you using OS X and Homebrew? The Homebrew python page https://github.com/Homebrew/brew/blob/master/docs/Homebrew-and-Python.md calls out a known issue with pip and a work around.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "What is the minimal set of privileges required to deploy artifacts to Nexus 3?", "id": 495, "answers": [{"answer_id": 499, "document_id": 223, "question_id": 495, "text": "You can read more about Privileges in the Security chapter: https://help.sonatype.com/repomanager3/security/privileges", "answer_start": 168, "answer_category": null}], "is_impossible": false}], "context": "I'm using Nexus Repository Manager 3.1.0-04, and I want to create a user to just have deployment permissions. What are the minimal set of privileges required for that? You can read more about Privileges in the Security chapter: https://help.sonatype.com/repomanager3/security/privileges", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "maven profiles or spring profiles?", "id": 546, "answers": [{"answer_id": 548, "document_id": 271, "question_id": 546, "text": "Maven profiles would provide a build-time solution, while SpringFramework profiles would provide a runtime alternative. I think this is the first question one may ask himself: if he wants to have a single package that can be deployed in different environments, or if he wants the build tool to provide different packages according to the destination environment.", "answer_start": 290, "answer_category": null}], "is_impossible": false}], "context": "a lot java applications are build with maven. maven has Profiles concept, it is really handy to build release package for different environments. e.g. dev/test/prod using different path / jndiname / security rule / properties files... I think I don't have to list codes here to explain it.\nMaven profiles would provide a build-time solution, while SpringFramework profiles would provide a runtime alternative. I think this is the first question one may ask himself: if he wants to have a single package that can be deployed in different environments, or if he wants the build tool to provide different packages according to the destination environment.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How do I install package.json dependencies in the current directory using npm", "id": 1650, "answers": [{"answer_id": 1638, "document_id": 1224, "question_id": 1650, "text": "Running:\nnpm install\nfrom inside your app directory ", "answer_start": 156, "answer_category": null}], "is_impossible": false}], "context": "I have a web app: fooapp. I have a package.json in the root. I want to install all the dependencies in a specific node_modules directory. How do I do this?\nRunning:\nnpm install\nfrom inside your app directory (i.e. where package.json is located) will install the dependencies for your app, rather than install it as a module, as described here. These will be placed in ./node_modules relative to your package.json file (it's actually slightly more complex than this, so check the npm docs here).\nYou are free to move the node_modules dir to the parent dir of your app if you want, because node's 'require' mechanism understands this. However, if you want to update your app's dependencies with install/update, npm will not see the relocated 'node_modules' and will instead create a new dir, again relative to package.json.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "c 2017 compiler could not be created error message while opening project", "id": 2009, "answers": [{"answer_id": 1995, "document_id": 1600, "question_id": 2009, "text": "I would strongly recommend that you first try possible solutions suggested by @Sara-MSFT before doing clean reinstall just in case if it works", "answer_start": 2216, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI've Visual Studio 2017 community edition. I have a C# project created using Visual Studio 2015. When I try to open the project in VS 2017 I get an error message prompt:\n\n---------------------  Microsoft Visual Studio\n\nProject 'dataStructureInCSharp' could not be opened because the Visual C# 2017 compiler could not be created.\nPlease re-install Visual Studio.\n--------------------------- OK\n\nI'm trying to obtain help if anyone else has faced similar issue. I feel going the uninstall and then reinstall route is very costly for me and would try that option last if I've got no other resort.\nWhat I've done so far :\n\nTried starting visual studio with administrative privileges\n\nBut problem remained same.\nI tried creating a new console project solution from scratch but in that case I get very same error and an additional error error also shown below:\n\nSystem Environment: Windows 7 Ultimate Service Pack 1\n    \n\nYou can try to close all VS 2017 instances and delete the folder %localappdata%\\Microsoft\\VisualStudio\\15.0_xxxx\\ComponentModelCache, then open VS to create a new Console project.\n\nOr \n\nplease re-run the VS 2017 installer as administrator, click the icon beside \u2018Launch\u2019 button and choose \u2018Repair\u2019 to repair as shown below:\n\n\n    \n\nJust to brief the history of my problem, I had first installed Visual Studio(VS) 2017 community when it was in RC stage. This was first time when I saw the workload based UI of visual studio installation. Initially I simply chose .Net desktop development workload to get started as I was interested in creating only console applications to get my hands dirty.\n\nInitially it was all working well. One fine day I added all other workloads I was interested in namely Universal Windows Platform Development, Azure Development, ASP.NET and web development,Node.js development, and Mobile development with .Net. I'm not sure if there any of the specific workloads to be blamed for the issue I've posted.\n\nJust to avoid the case if RC and RTM release builds might not have messed up my entire environment, I simply uninstalled the entire stuff, rebooted my machine and installed it again from scratch from latest RTM release for Visual Studio Community.\n\nI would strongly recommend that you first try possible solutions suggested by @Sara-MSFT before doing clean reinstall just in case if it works. It can save you couple of hours required in whole reinstallation process if it works.\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "how to install gcc on windows 7 machine?", "id": 1570, "answers": [{"answer_id": 1559, "document_id": 1147, "question_id": 1570, "text": "You should download mingw-get and simply issue:\nmingw-get install gcc.\nSee the Getting Started page.", "answer_start": 374, "answer_category": null}], "is_impossible": false}], "context": "I have MinGW on my windows 7 machine. I wish to install and use complete gcc for C compiler. I found there is no single pre-compiled ready-made installation file for this purpose. I checked the following page : http://gcc.gnu.org/install/ It is difficult and I find it above my level of understanding. Could any one please provide me step by step guidance along with links?\nYou should download mingw-get and simply issue:\nmingw-get install gcc.\nSee the Getting Started page.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "why does the uac dialog take a long time to be displayed", "id": 1399, "answers": [{"answer_id": 1388, "document_id": 971, "question_id": 1399, "text": "    \n\nEverytime you install software using Windows Installer, a restore point is created prior to do the actual installation. Ref: http://msdn.microsoft.com/en-us/library/aa372060.aspx\n\nYou can turn this off in the registry: http://msdn.microsoft.com/en-us/library/aa36975", "answer_start": 1183, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI am working on a Windows installer using the WIX toolkit, and it takes FOR EVER (15-45 seconds) for the UAC dialog to pop up during an install. is there any way to speed this up?\n    \n\nThanks to joegrage for pointing me in the right direction on this one.  The trick to this seems to be the MSIFASTINSTALL property.\n\nThe MSIFASTINSTALL property can be used to reduce the time required to install a large Windows Installer package. The property can be set on the command line or in the Property table to configure operations that the user or developer determines are non-essential for the installation.\n\nThe value of the MSIFASTINSTALL property can be a combination of the following values.\n\nvalue  Meaning \n-----  -----------------------------------------------------------\n0      Default value\n1      No system restore point is saved for this installation.\n2      Perform only File Costing and skip checking other costs.\n4      Reduce the frequency of progress messages.\n\n\nIn WIX, you can use a combination of these values like so:\n\n&lt;Property Id=\"MSIFASTINSTALL\" Value=\"3\" /&gt;\n\n\nA more detailed write up about this property can be found on this blog post.\n    \n\nEverytime you install software using Windows Installer, a restore point is created prior to do the actual installation. Ref: http://msdn.microsoft.com/en-us/library/aa372060.aspx\n\nYou can turn this off in the registry: http://msdn.microsoft.com/en-us/library/aa369758.aspx\n    \n\nThis happens because Windows is checking if the package has a digital signature.\n\nUnfortunately the digital signature verification algorithm is not very good. Also, its performance depends on the package size. So a bigger package will have a longer delay.\n\nTo avoid the delay you can add a simple EXE bootstrapper to your MSI. It\u2019s purpose is to request elevation through an application manifest and launch the MSI next to it. If you don\u2019t include your MSI in it, the bootstrapper will have a small size. So the digital signature check will be very fast.\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "error in installing matplotlib fatal error c1083", "id": 1411, "answers": [{"answer_id": 1400, "document_id": 986, "question_id": 1411, "text": "python -m pip install -U matplotlib==3.2.0rc1", "answer_start": 1548, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI am relatively new to Python coding and want to learn about statistics and data management in Python. For this I would like to install Matplotlib, which is giving me some issues. \n\nI see other people having this issue, but I have not fully understood how to fix it. \n\nTo install i use\n\npip install matplotlib\n\n\nI have the following specs installed\n\n\nWindows 10\nPython 3.8\nMicrosoft Studio 2019\n\n\nThe first error i got was to install Microsoft Studio, so I did that. \nI have also attempted to update pip\n\nBUILDING MATPLOTLIB \n matplotlib: yes [3.1.1] \n python: yes [3.8.0 (tags/v3.8.0:fa919fd, Oct 14 2019, 19:21:23) [MSC v.1916 32 bit (Intel)]] \n platform: yes [win32] \n\n...\n\n checkdep_freetype2.c\n    src/checkdep_freetype2.c(1): fatal error C1083: Cannot open include file: 'ft2build.h': No such file or directory\n    error: command 'C:\\\\Program Files (x86)\\\\Microsoft Visual Studio\\\\2019\\\\BuildTools\\\\VC\\\\Tools\\\\MSVC\\\\14.23.28105\\\\bin\\\\HostX86\\\\x86\\\\cl.exe' failed with exit status 2\n    ----------------------------------------\nERROR: Command errored out with exit status 1:\n\n    \n\nYou have python 3.8, not python 3.7.\nBut there are no python 3.8 wheels available for matplotlib 3.1.1 on pypi. So best remove python 3.8 completely and install python 3.7.\nWhen you then run python -m pip install matplotlib it will install the compiled version from the wheels, so there is no need to compile anything yourself or have Microsoft Studio available.\n    \n\nAfter spending a lot of time on the issue, this helped me to solve it:\n\npython -m pip install -U matplotlib==3.2.0rc1\n\n    \n\nFYI: the matplotlib website installation instructions has some info on installing from source.\n\nFor Windows it states setting include path and link path:\n\nset CL=/IC:\\directory\\containing\\ft2build.h ...\nset LINK=/LIBPATH:C:\\directory\\containing\\freetype.lib ...\n\n    \n\nAs a workaround you may install matplotlib on Windows using the 'Unofficial Windows Binaries for Python Extension Packages' with pip install &lt;downloaded_filename&gt;.\n\nTested on Python 3.8, Windows 10 and matplotlib-3.2\n\nhttps://www.lfd.uci.edu/~gohlke/pythonlibs/#matplotlib\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "how do i create a custom dialog in wix for user input", "id": 1915, "answers": [{"answer_id": 1902, "document_id": 1487, "question_id": 1915, "text": "Create a property\nHave the UI control set this property\nThe name attribute on the service will reference the property, ie [Ser", "answer_start": 1605, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI'm using WiX to create an installer for a windows service. It's desirable that the name of service that gets installed and displayed in Services is configurable at install time. \n\nFor example, this is what I'm thinking (wix xml snip):\n\n&lt;ServiceInstall \n    Id=\"MyServiceInstaller\" \n    Name=\"NAME_PASSED_FROM_DIALOG\" \n    Type=\"ownProcess\" \n    Start=\"auto\" \n    ErrorControl=\"normal\" \n    Description=\"My Service\" \n    Account=\"localsystem\"/&gt; \n\n&lt;ServiceControl\n    Id=\"StartMyServiceInstaller\" \n    Name=\"NAME_PASSED_FROM_DIALOG\" \n    Start=\"install\"\n    Wait=\"no\" /&gt;\n\n&lt;ServiceControl\n    Id=\"StopMyServiceInstaller\" \n    Name=\"NAME_PASSED_FROM_DIALOG\" \n    Remove=\"uninstall\"\n    Stop=\"both\"\n    Wait=\"yes\" /&gt;\n\n\nNAME_PASSED_FROM_DIALOG is something I would like to hook up to a custom dialog that gets created and gets displayed to the person installing the service so they can set/modify the service name. I think this is very similar to the WIXUI_INSTALLDIR property that gets set and passed to the WixUI_InstallDir Dialog Set. \n\nMy question is:\n\nHow do I create a custom UI dialog that can accept user input which gets passed into runtime of the installer?\n    \n\nHave fun with UI! \n\nEdit: The original link to answer doesn't exist anymore. FireGiant (the maintainers of Wix) some examples for part of this process, but it's doesn't completely answer this question. There is one further tutorial (UPDATE Aug.2018: Link resurrected from Wayback Machine) that does go most of the way to answer this question.\n\nA high level overview of what will be happening is:\n\n\nCreate a property\nHave the UI control set this property\nThe name attribute on the service will reference the property, ie [ServiceNameProperty].\n\n\nHowever this is complex, and the way that is suggested to create a new UI dialog, is to take an existing dialog, make a clone of it, and then edit with new text, controls and use it to populate the property.\n    \n\nTry to use \nWixEdit\nthat is nice tool for creating UI\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Pip Install not installing into correct directory?", "id": 1206, "answers": [{"answer_id": 1199, "document_id": 782, "question_id": 1206, "text": "1.\tupdate path with correct python\n2.\tuninstall pip using python -m pip uninstall pip setuptools\n3.\trestart windows didn't work until a restart", "answer_start": 270, "answer_category": null}], "is_impossible": false}], "context": "However, it's not in the correct directory\nHow do I get sudo pip install to install into correct directory?\nIn addition, I've tried\nsudo pip install Scrappy\nI get the following message\nThis is what worked for me on Windows. The cause being multiple python installations\n1.\tupdate path with correct python\n2.\tuninstall pip using python -m pip uninstall pip setuptools\n3.\trestart windows didn't work until a restart\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to make an installer for my C# application?", "id": 779, "answers": [{"answer_id": 776, "document_id": 463, "question_id": 779, "text": "Add a new install project to your solution.\n2.\tAdd targets from all projects you want to be installed.\n3.\tConfigure pre-requirements and choose \"Check for .NET 3.5 and SQL Express\" option. Choose the location from where missing components must be installed.\n4.\tConfigure your installer settings - company name, version, copyright, etc.\n5.\tBuild and go!", "answer_start": 152, "answer_category": null}], "is_impossible": false}], "context": "I have created an application (C#, Windows Forms) on Visual Studio 2008, and now I want to make installer of this application. How can this be done?\n1.\tAdd a new install project to your solution.\n2.\tAdd targets from all projects you want to be installed.\n3.\tConfigure pre-requirements and choose \"Check for .NET 3.5 and SQL Express\" option. Choose the location from where missing components must be installed.\n4.\tConfigure your installer settings - company name, version, copyright, etc.\n5.\tBuild and go!\nWhy invent wheels yourself while there is a car ready for you? I just find this tools super easy and intuitive to use: Advanced Installer. This one minute video should be enough to impress you. Here is the illustrative user guide.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "installing cakephp 3 manually, without composer", "id": 1780, "answers": [{"answer_id": 1766, "document_id": 1351, "question_id": 1780, "text": "Packaged app (cakephp/app) releases that include all dependencies (framework (cakephp/cakephp), standard CakePHP plugins (cakephp/debugkit, cakephp/bake, etc), required third party libraries) can be found on GitHub.\nhttps://github.com/cakephp/cakephp/releases", "answer_start": 546, "answer_category": null}], "is_impossible": false}], "context": "I need to install CakePHP 3 in an old-fashioned upload-unzip-run way.\nThe archive I've downloaded from cakephp/cakephp/tags does not contain the default folders like webroot, Model etc., which means it's not complete.\nThe official documentation does not cover this. Here's a relevant Github issue I found, but the person ends up still using Composer.\nThere's also cakephp/app and it seems to include those missing files, but it's not mentioned in cakephp/cakephp's composer.json, and even if I download it I've no idea how to merge the packages.\nPackaged app (cakephp/app) releases that include all dependencies (framework (cakephp/cakephp), standard CakePHP plugins (cakephp/debugkit, cakephp/bake, etc), required third party libraries) can be found on GitHub.\nhttps://github.com/cakephp/cakephp/releases\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "CakePHP Application Deployment", "id": 1252, "answers": [{"answer_id": 1245, "document_id": 824, "question_id": 1252, "text": "You may consider using an IDE that allows you to connect to a server over (S)FTP.", "answer_start": 331, "answer_category": null}], "is_impossible": false}], "context": "I am interested in finding out how folks are deploying their CakePHP applications. I have recently been approached about doing some freelance CakePHP development, which would be a nice opportunity for me to get some experience in the presentation tier (in my FT job I do Java EE development in the business and persistence tiers).\nYou may consider using an IDE that allows you to connect to a server over (S)FTP.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to install APK from PC?", "id": 645, "answers": [{"answer_id": 650, "document_id": 338, "question_id": 645, "text": "1.\tConnect Android device to PC via USB cable and turn on USB storage.\n2.\tCopy .apk file to attached device's storage.\n3.\tTurn off USB storage and disconnect it from PC.\n4.\tCheck the option Settings \u2192 Applications \u2192 Unknown sources OR Settings > Security > Unknown Sources.\n5.\tOpen FileManager app and click on the copied .apk file. If you can't fine the apk file try searching or allowing hidden files. It will ask you whether to install this app or not. Click Yes or OK.", "answer_start": 396, "answer_category": null}], "is_impossible": false}], "context": "I want to install an APK from PC to Android device. And because of user's Android and generally technical skills, I need to do it as automatically (silently) as possible. So how do I send an APK from PC to Android and start install there? Airdroid , android market install the app on android then go onto the computer type in the address given, type in the password given (or scan the QR code). \n1.\tConnect Android device to PC via USB cable and turn on USB storage.\n2.\tCopy .apk file to attached device's storage.\n3.\tTurn off USB storage and disconnect it from PC.\n4.\tCheck the option Settings \u2192 Applications \u2192 Unknown sources OR Settings > Security > Unknown Sources.\n5.\tOpen FileManager app and click on the copied .apk file. If you can't fine the apk file try searching or allowing hidden files. It will ask you whether to install this app or not. Click Yes or OK.\nThis procedure works even if ADB is not available.\nGo to settings and under security (if your running the new ICS or Jellybean) or go to settings->apps->managment and select unknown sources(for gingerbread) then click on (I think) speed install, or something along those lines. it will be on the top of the page slightly towards the left. drag and drop as many .apks as you want then on you android just tap the install buttons that appear. Airdroid is wonderful and does a lot more than just apks.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How do I install a library permanently in Colab?", "id": 810, "answers": [{"answer_id": 805, "document_id": 492, "question_id": 810, "text": "ou can install the library in Google Drive. Then add the path to sys.path.\nThen you can install a library, for example, jdc, and specify the target.\n!pip install --target=$nb_path jdc\nLater, when you run the notebook again, you can skip the !pip install line. You can just import jdc and use it", "answer_start": 273, "answer_category": null}], "is_impossible": false}], "context": "In Google Colaboratory, I can install a new library using !pip install package-name. But when I open the notebook again tomorrow, I need to re-install it every time.\nIs there a way to install a library permanently? No need to spend time installing every time to use? Yes. You can install the library in Google Drive. Then add the path to sys.path.\nThen you can install a library, for example, jdc, and specify the target.\n!pip install --target=$nb_path jdc\nLater, when you run the notebook again, you can skip the !pip install line. You can just import jdc and use it. Here's an example notebook.\nhttps://colab.research.google.com/drive/1KpMDi9CjImudrzXsyTDAuRjtbahzIVjq\nBTW, I really like jdc's %%add_to. It makes working with a big class much easier.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Android Studio - local path doesn't exist", "id": 1665, "answers": [{"answer_id": 1652, "document_id": 1238, "question_id": 1665, "text": "Try this:\n1.\tClose IDE\n2.\tRemove .idea folder and all .iml files in the project.\n3.\tRestart the IDE and re-import the project.", "answer_start": 439, "answer_category": null}], "is_impossible": false}], "context": "After reading some posts here, I changed the output path to the build/apk folder, but the issue remains. Instead of the created projectname-debug-unaligned.apk he is looking for the projectname.apk and I have no idea how and where I can tell him to change the name he is looking for. Any ideas?\nPS: Yes I have read several posts which are shown in the list on the right side. The restart of the IDE worked for my coworker, but not for me.\nTry this:\n1.\tClose IDE\n2.\tRemove .idea folder and all .iml files in the project.\n3.\tRestart the IDE and re-import the project.\nOriginal post: https://code.google.com/p/android/issues/detail?id=59018\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Deploy app through Google Play Store in a private channel without Google Apps?", "id": 1057, "answers": [{"answer_id": 1053, "document_id": 638, "question_id": 1057, "text": "I would just deploy your app using Google Play \"Beta testing and staged rollout\":\nhttps://support.google.com/googleplay/android-developer/answer/3131213?hl=en", "answer_start": 499, "answer_category": null}], "is_impossible": false}], "context": "I'm building an Android app which for various reasons cannot be listed publicly on the Google Play store.\nI have found information about the Google Play Private Channel but if I understand it correctly, this can only be used when all your users are within a Google Apps domain (which mine are not).\nAre there any similar methods that I could use to deploy my app (and get the benefit of easy installation for users, an updating mechanism etc.), or is my only option to publish an APK file manually?\nI would just deploy your app using Google Play \"Beta testing and staged rollout\":\nhttps://support.google.com/googleplay/android-developer/answer/3131213?hl=en\nYour users have to join a Google Group or Google + community that you create so you can limit you audience and make the app non-public on Google Play but still allows your non-public users to update the app via Google Play instead of manual APK installation.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Ruby on Rails Beta 3 Install on Snow Leopard - file not found", "id": 1791, "answers": [{"answer_id": 1777, "document_id": 1363, "question_id": 1791, "text": "All you need to run is gem install rdoc, follow its instructions, then run the install rails command again and you're good. The problem is due to rdoc failing to install when installed with rails, if installed manually it works fine.\nSource: http://gist.github.com/565967", "answer_start": 424, "answer_category": null}], "is_impossible": false}], "context": "I tried to install the new beta on my system with the command:\nsudo gem install rails --pre\nbut no matter what I tried, I still get this damn error:\nSuccessfully installed rails-3.0.0.beta3\n1 gem installed\nInstalling ri documentation for rails-3.0.0.beta3...\nFile not found: lib\nSince I'm very new to ruby, I really don't know what to do.\nHow can I finish installing this? Is this installed already? Why does it abort here?\nAll you need to run is gem install rdoc, follow its instructions, then run the install rails command again and you're good. The problem is due to rdoc failing to install when installed with rails, if installed manually it works fine.\nSource: http://gist.github.com/565967\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to check that the anaconda package was properly installed", "id": 1754, "answers": [{"answer_id": 1741, "document_id": 1326, "question_id": 1754, "text": "You can determine which version of python you are running when you get the error by looking at the results of which python from the commandline.", "answer_start": 644, "answer_category": null}], "is_impossible": false}], "context": "I'm completely new to Python and want to use it for data analysis. I just installed Python 2.7 on my mac running OSX 10.8. I need the NumPy, SciPy, matplotlib and csv packages. I read that I could simply install the Anaconda package and get all in one. So I went ahead and downloaded/installed Anaconda 1.7.\nHowever, when I type in: import numpy as np\nI get an error telling me that there is no such module. I assume this has to do with the location of the installation, but I can't figure out how to: A. Check that everything is actually installed properly B. Check the location of the installation.\nAny pointers would be greatly appreciated!\nYou can determine which version of python you are running when you get the error by looking at the results of which python from the commandline.\nIt is likely that you are running the system version (although recent versions Mac OS X include numpy in its system python), rather than Anaconda's python distribution. If this is the case, you need to modify your PATH as suggested by Anaconda at the end of the install process. Assuming it was installed in ~/anaconda, you would need to add something like:\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Automatically Deploy From GitHub To Server On Push.", "id": 467, "answers": [{"answer_id": 476, "document_id": 200, "question_id": 467, "text": "You probably want to use GitHub's post-receive hooks.", "answer_start": 185, "answer_category": null}], "is_impossible": false}], "context": "We have a VPS on Linode, and code hosted on GitHub. How do we setup so when we push to GitHub, it also pushes automatically to our Linode server. We are using PHP on the Linode server.\nYou probably want to use GitHub's post-receive hooks.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "how to update ios6 enterprise apps over the air", "id": 1515, "answers": [{"answer_id": 1504, "document_id": 1091, "question_id": 1515, "text": "on for everytime \n    \n\nFor app updates, the existing app data persists if the bundle ID for the app stays the same. \n\nApple explains Enterprise App Deployment and app updates here: http://help.apple.com/iosdeployment-a", "answer_start": 6511, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nHello we have developed our first enterprise app recently. We are using the \"In-House\" option to create the distribution certificate. Client is not using the app yet. But he will be using it soon. Meanwhile i got a question. He will use the app and in future if there are any updates to the app from our side, we want the client to have it updated on his side as well . Like right now I have  apps installed on my iPhone. I get update from AppStore saying the XYZ app has been updated. So i install the update. Now if our client is using the app and has saved some data on it(our app uses core data, and we built it in a way that client can store some data on the device) we want to send an update to him that would install the update, but not erase any existing client data.Is that possible? How do i do it?I am using over the air installation right now to install the app. We have a secure server, where the .ipa and .plist files are present and we have a download html page. Client clicks the link and the app gets installed. Please let me know if you need more information. Thanks.\n    \n\nYes, it is possible. When you deploy an Enterprise application it requires a plist that contains metadata about the application. This metadata includes the version number that you can use to check for updates. \n\nBOOL updateAvailable = NO;\nNSDictionary *updateDictionary = [NSDictionary dictionaryWithContentsOfURL:\n                                  [NSURL URLWithString:@\"http://www.example.com/pathToPlist\"]];\n\nif(updateDictionary)\n{\n    NSArray *items = [updateDictionary objectForKey:@\"items\"];\n    NSDictionary *itemDict = [items lastObject];\n\n    NSDictionary *metaData = [itemDict objectForKey:@\"metadata\"];\n    NSString *newversion = [metaData valueForKey:@\"bundle-version\"];\n    NSString *currentversion = [[[NSBundle mainBundle] infoDictionary] objectForKey:@\"CFBundleVersion\"];\n\n    updateAvailable = [newversion compare:currentversion options:NSNumericSearch] == NSOrderedDescending;\n}\n\n\nOnce you detect the update is available navigate the user to the download URL \n\n\n  itms-services://?action=download-manifest&amp;url=&lt;url-path-to-plist&gt;\n\n\nand it will install over the existing version leaving all data in-tact and even upgrade the CoreData database if you setup auto migration and make changes.\n    \n\nThanks to Joe for a fantastic response. Here is the extended version translated to swift. You can put it inside of the viewDidLoad of your main view controller\n\nlet plistUrl = \"https://example.com/example.plist\"\nlet installationUrl = \"itms-services://?action=download-manifest&amp;amp;url=https://example.com/example.plist\"\n\n\noverride func viewDidLoad() {\n    super.viewDidLoad()\n\n    //Check for the updates        \n    checkForUpdates()\n}\n\nfunc checkForUpdates() {\n    let qualityOfServiceClass = QOS_CLASS_BACKGROUND\n    let backgroundQueue = dispatch_get_global_queue(qualityOfServiceClass, 0)\n    dispatch_async(backgroundQueue, {\n        let updateDictionary = NSDictionary(contentsOfURL: NSURL(string: self.plistUrl)!)!\n\n        let items = updateDictionary[\"items\"]\n        let itemDict = items?.lastObject as! NSDictionary\n        let metaData = itemDict[\"metadata\"] as! NSDictionary\n        let serverVersion = metaData[\"bundle-version\"] as! String\n        let localVersion = NSBundle.mainBundle().infoDictionary![\"CFBundleVersion\"] as! String\n        let updateAvailable = serverVersion.compare(localVersion, options: .NumericSearch) == .OrderedDescending;\n\n        if updateAvailable {\n            self.showUpdateDialog(serverVersion)\n        }\n    })\n}\n\nfunc showUpdateDialog(serverVersion: String) {\n    dispatch_async(dispatch_get_main_queue(), { () -&gt; Void in\n        let alertController = UIAlertController(title: \"New version of Example available!\", message:\n            \"Example \\(serverVersion) has been released. Would you like to download it now?\", preferredStyle: UIAlertControllerStyle.Alert)\n        alertController.addAction(UIAlertAction(title: \"Not now\", style: .Cancel,handler: nil))\n        alertController.addAction(UIAlertAction(title: \"Update\", style: .Default, handler: { (UIAlertAction) in\n            UIApplication.sharedApplication().openURL(NSURL(string: self.installationUrl)!)\n        }))\n\n        self.presentViewController(alertController, animated: true, completion: nil)\n    })\n}\n\n    \n\nJust distribute the update the same way you distribute the original.  The user retains the data from the earlier version.\n    \n\nUpdated  answer of @Vlad in swift 4 \n\n var plistUrl = \"https://xxxxxxxxxxxxxxfgfgf/ex.plist\"\n\nvar installationUrl = URL(string : \"itms-services://?action=download-manifest&amp;url=https://xxxxxxxxxxxxxxfgfgf/ex.plist\")\n\n\n   func checkForUpdates() {\n       let qos = DispatchQoS(qosClass: .background, relativePriority: 0)\n       let backgroundQueue = DispatchQueue.global(qos: qos.qosClass)\n    backgroundQueue.async(execute: {\n        let updateDictionary = NSDictionary(contentsOf: NSURL(string: self.plistUrl)! as URL)!\n\n           let items = updateDictionary[\"items\"]\n        let itemDict = (items as AnyObject).lastObject as! NSDictionary\n           let metaData = itemDict[\"metadata\"] as! NSDictionary\n           let serverVersion = metaData[\"bundle-version\"] as! String\n\n        print (\"serverVersion=\",serverVersion)\n        let localVersion = Bundle.main.infoDictionary![\"CFBundleVersion\"] as! String\n        print (\"localVersion=\",localVersion)\n\n           let updateAvailable = serverVersion.compare(localVersion, options: .numeric) == .orderedDescending\n        print(\"version is newer\")\n\n           if updateAvailable {\n            self.showUpdateDialog(serverVersion: serverVersion)\n           }\n       })\n   }\n\n   func showUpdateDialog(serverVersion: String) {\n       DispatchQueue.main.async { () -&gt; Void in\n           let alertController = UIAlertController(title: \"New version of  App  available!\", message:\n            \"MCD \\(serverVersion) has been released. Would you like to download it now?\", preferredStyle: UIAlertController.Style.alert)\n        alertController.addAction(UIAlertAction(title: \"Not now\", style: .cancel,handler: nil))\n        alertController.addAction(UIAlertAction(title: \"Update\", style: .default, handler: { (UIAlertAction) in\n            UIApplication.shared.open(self.installationUrl!, options: [:], completionHandler: nil)\n           }))\n\n        self.present(alertController, animated: true, completion: nil)\n    }}\n\n\nbut this is only onetime need more modification for everytime \n    \n\nFor app updates, the existing app data persists if the bundle ID for the app stays the same. \n\nApple explains Enterprise App Deployment and app updates here: http://help.apple.com/iosdeployment-apps/mac/1.1/#app43ad802c\n\nI'd also recommend including an update checker in-app.\n\nTo do that, iVersion is a ios library by Nick Lockwood (aka Charcoal Design) that will help you to do this. It's available here:\nhttps://github.com/nicklockwood/iVersion\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "visual studio 2017 not enough disk space", "id": 1934, "answers": [{"answer_id": 1921, "document_id": 1511, "question_id": 1934, "text": "I found the following tip at https://www.codeproject.com/Messages/5476334/Re-Visual-Studio-and-SSDs.aspx\n  Use symlinks to move some folders from C: to D: or another partition.", "answer_start": 1549, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI'm trying to install visual studio 2017 with Xamarin which requires ~30GB of space and I have this amount available in the directory that VS2017 is currently installed in. I'm trying to basically modify the install to add Xamarin to it, however it says I don't have enough disk space because despite the download directory being in my D drive, it's still trying to install components to my C drive. I haven't been able to find anything about this issue online yet.\n\nVisual studio installer:\n\n\n\nFile explorer (notice C drive has less than the required amount available)\n\n\n    \n\nOne possible issue is that not everything installed by VS goes into the VS installation folder.  There are generally (I'm simplifying a little bit here) two types of install packages that will get pulled in:\n\n\nInstance-based packages affect this instance of VS, but no others.  These are things like your Visual Studio shell, most of the components you select, and well, anything that gets installed to the path you've specified.\nGlobal packages or \"singleton packages\" as they're sometimes called are installed once and shared across all installs of VS2017.  These can include things like SDKs, any components that are installed to the system registry or GAC, and could also include large tools like emulators.\n\n\nThere's a good chance that the Xamarin features are pulling in many of the 2nd type, which would get installed to Program Files by default.  I don't know of a way to specify an alternative install path for these types of packages.\n    \n\nI found the following tip at https://www.codeproject.com/Messages/5476334/Re-Visual-Studio-and-SSDs.aspx\n  Use symlinks to move some folders from C: to D: or another partition.\n\n\ncreate a new folder \"D:_moved_from_C_ProgramFilesX86\"\nmove \"C:\\Program Files (x86)\\Microsoft SDKs\" to \"D:_moved_from_C_ProgramFilesX86\\Microsoft SDKs\"\nrun this command from a command prompt, running as administrator:\nmklink /J \"C:\\Program Files (x86)\\Microsoft SDKs\" \"D:\\_moved_from_C_ProgramFilesX86\\Microsoft SDKs\"\nrepeat the same with \"C:\\Program Files (x86)\\Windows Kits\"\n\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Building a compiled application with Docker", "id": 1249, "answers": [{"answer_id": 1242, "document_id": 821, "question_id": 1249, "text": "I had difficulties automating our build with docker-compose, and I ended up using docker build for everything\nThree layers for building\nRun \u2192 develop \u2192 build\nThen I copy the build outputs into the 'deploy' image:\nRun \u2192 deploy", "answer_start": 247, "answer_category": null}], "is_impossible": false}], "context": "I am building a server, written in C++ and want to deploy it using Docker with docker-compose. What is the \"right way\" to do it? Should I invoke make from Dockerfile or build manually, upload to some server and then COPY binaries from Dockerfile?\nI had difficulties automating our build with docker-compose, and I ended up using docker build for everything\nThree layers for building\nRun \u2192 develop \u2192 build\nThen I copy the build outputs into the 'deploy' image:\nRun \u2192 deploy\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Web setup MSI fails on Windows Server 2008", "id": 1805, "answers": [{"answer_id": 1790, "document_id": 1376, "question_id": 1805, "text": "Make sure you have the IIS 6 compatibility options installed with IIS 7 in Windows 2008. That should install the COM components that your MSI is trying to access.", "answer_start": 249, "answer_category": null}], "is_impossible": false}], "context": "I have built a web setup project in VS2008 which installs my ASP.NET/Silverlight app into IIS. This works fine everywhere except on Windows Server 2008.\nI've tried googling the various error codes, but I'm not having much luck.\nWhat is the problem?\nMake sure you have the IIS 6 compatibility options installed with IIS 7 in Windows 2008. That should install the COM components that your MSI is trying to access.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to make one module depend on another module artifact?", "id": 1865, "answers": [{"answer_id": 1851, "document_id": 1436, "question_id": 1865, "text": "Looks like it should work to me. But you might try mvn install instead of mvn package.", "answer_start": 654, "answer_category": null}], "is_impossible": false}], "context": "I have maven multiple-module project.\n A: parent.\n    B: child1.\n    C: child2.\nB will be packaged to get jar file and then c will use this jar file to compile the code.\nIn B, if I run mvn package, it will create b.jar (stays in B/target/jars not in B/target -for another purpose).\nIn C, I need to use that b.jar to compile the code.\nNow, from A, when I run: mvn package. First, I am successful to create b.jar file for B.\nBut when it come to C's compilation phase, it looks like C doesn't recognize b.jar in the classpath (the compilation gets errors because C's code can not import the class file from B).\nMy question is: How can I solve this problem?\nLooks like it should work to me. But you might try mvn install instead of mvn package.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "error installing angular using npm due to require from string", "id": 1994, "answers": [{"answer_id": 1980, "document_id": 1578, "question_id": 1994, "text": "n\n\nrm ./package-lock.json\ngit commit -am \"Removed broken package-lock.json file.\"\ngit push heroku maste", "answer_start": 2261, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nHow to resolve this error as I am not able to install Angular.\n\nPlease see the below exception:\n\nC:\\Users\\absin\\node&gt;npm install -g @angular/cli\nnpm ERR! code ETARGET\nnpm ERR! notarget No matching version found for require-from-string@^1.1.0\nnpm ERR! notarget In most cases you or one of your dependencies are requesting\nnpm ERR! notarget a package version that doesn't exist.\nnpm ERR! notarget\nnpm ERR! notarget It was specified as a dependency of 'cosmiconfig'\nnpm ERR! notarget\n\nnpm ERR! A complete log of this run can be found in:\nnpm ERR!     C:\\Users\\absin\\AppData\\Roaming\\npm-cache\\_logs\\2018-01-06T19_07_00_000Z-debug.log\n\n\neven after temporal solution: \n\nnpm install https://github.com/floatdrop/require-from-string/tarball/v1.1.0 --save\nnpm install\n\n\nstill getting this \n    \n\nas stated in the issue link: \n\ntemporal solution:\n\nnpm install https://github.com/floatdrop/require-from-string/tarball/v1.1.0 --save\nnpm install\n\n\nUPDATE:\n\nIt appears they are working on it. The require-from-string page used to return a 404, but at least now it loads the correct page on NPM's website: https://www.npmjs.com/package/require-from-string\n\nStill doesn't appear to be working through npm install yet.\n\nUPDATE 2:\n\nAn official response from NPM: https://status.npmjs.org/incidents/41zfb8qpvrdj\n\nUPDATE 3: issue seems to be resolved. \n    \n\nEven if https://status.npmjs.org/ says that it s all ok now,\n\nthe problem persist here\n    \n\n\n\nsome problem with NPM registry some of the packages got deleted.\nThey are restoring it... it will be available shortly\n\nTemp solution use the missing package from already running projects that or older projects.\n    \n\nI had the same issue. So when I used yarn it worked well and not having any problem to create new projects also.\n\nyarn global add @angular/cli\n\n\nagain, when you create new app ng new newapp it will fail because at this moment npm is not able to find a package called require-from-string. But the you can run yarn to install all the packages. Yarn will ask which version of the require-from-string is needed. you can choose it.\n    \n\nnpm is having issues.  This should just work.\n    \n\nnpm registry issue is now fixed according to https://status.npmjs.org/incidents/41zfb8qpvrdj\n\nSolution\n\nrm ./package-lock.json\ngit commit -am \"Removed broken package-lock.json file.\"\ngit push heroku master  \n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "are there iterators and loops in puppet?", "id": 581, "answers": [{"answer_id": 587, "document_id": 306, "question_id": 581, "text": "Older versions of the puppet language have no support for loops.\nBut you can use an array instead of a simple string for the title and declare several resources at the same time with the same params", "answer_start": 89, "answer_category": null}], "is_impossible": false}], "context": "When I define(?) a resource e.g. to ensure dir structure, are there any loops available?\nOlder versions of the puppet language have no support for loops.\nBut you can use an array instead of a simple string for the title and declare several resources at the same time with the same params\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Github Access Error - AggregateException encountered", "id": 1079, "answers": [{"answer_id": 1071, "document_id": 656, "question_id": 1079, "text": "You need to update git. See here: The last comment from Whoisj. I had the same problem in the morning. It's easy: just download and install git again. ", "answer_start": 239, "answer_category": null}], "is_impossible": false}], "context": "I am using a github Repo for almost one year and since this morning I am not able to push my code to the remote repo. I get following error in the command line:\nfatal: AggregateException encountered. Mindestens ein Fehler ist aufgetreten.\nYou need to update git. See here: The last comment from Whoisj. I had the same problem in the morning. It's easy: just download and install git again. \n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Do you know the Maven profile for mvnrepository.com?", "id": 1830, "answers": [{"answer_id": 1816, "document_id": 1401, "question_id": 1830, "text": "you have to add this repository to the repositories used by your project, in your pom.xml :\n<project>\n  ...\n  <repositories>\n    <repository>\n      <id>my-alternate-repository</id>\n      <url>http://myrepo.net/repo</url>\n    </repository>\n  </repositories>\n  ...\n</project>", "answer_start": 351, "answer_category": null}], "is_impossible": false}], "context": "I am trying to include some dependencies in my Maven project. These dependencies are not available in the default Maven 2 repository http://repo1.maven.org/maven2/.\nThey are available at http://mvnrepository.com/.\nBut I couldn't find the profile for this site to include in my settings.xml.\nDoes anyone know what this repository's URL and profile is?\nyou have to add this repository to the repositories used by your project, in your pom.xml :\n<project>\n  ...\n  <repositories>\n    <repository>\n      <id>my-alternate-repository</id>\n      <url>http://myrepo.net/repo</url>\n    </repository>\n  </repositories>\n  ...\n</project>\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "installing a windows service from a visual studio installer project", "id": 1377, "answers": [{"answer_id": 1366, "document_id": 947, "question_id": 1377, "text": "To find these right click project-&gt;\"view\"-&gt;\"custom actions\" under there it needs the primary output added to the f", "answer_start": 1431, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nA colleague has written a Windows Application and left me to do the installers.  I have created the installer project through Visual Studio and added the primary output of the service project to the new project.\n\nWhen I run the installer it creates the correct folders and copies the dlls, exe and config file in, but it doesn't do the actual install of the service.\n\nThe service isn't listed in the Services window, and if I double click on the exe I'm told I need to run installutil to install the service.\n\nHow do I make the installer do this bit for me?  I found this article:\n\nhttp://www.codeproject.com/KB/install/InstallService.aspx \n\nbut that seems overly complex for what I would expect to be pretty basic.\n    \n\nI used this article:\n\nHow to create a Setup project for a Windows Service in Visual Basic .NET or in Visual Basic 2005\n\nFelt pretty dumb that I couldn't figure it all out until I went through all of the steps in this article. It's not a trivial exercise by any means.\n    \n\nFor those who are looking for updated instructions for Visual Studio 2010 (instructions in answer are for VS 2005) check the following link:\n\nWalkthrough: Creating a Windows Service Application in the Component Designer (note that \"other versions\" [VS 2005, VS2008] are available from the same link)\n    \n\nI had this issue in my case the problem was I neglected to add the custom actions for the installer project. To find these right click project-&gt;\"view\"-&gt;\"custom actions\" under there it needs the primary output added to the folders.\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Installing & using the Android NDK in Eclipse", "id": 828, "answers": [{"answer_id": 823, "document_id": 510, "question_id": 828, "text": "First, inside your terminal you need to run the NDK build script on your project. cd into the root of your project directory and then execute the ndk-build script within that directory.\nFor example:\ncd ~/workspace/hello-jni\n./~/android-ndk-1.5_r1/ndk-build", "answer_start": 114, "answer_category": null}], "is_impossible": false}], "context": "As simply as I can describe it, building an Android app from within Eclipse that uses the NDK requires two steps.\nFirst, inside your terminal you need to run the NDK build script on your project. cd into the root of your project directory and then execute the ndk-build script within that directory.\nFor example:\ncd ~/workspace/hello-jni\n./~/android-ndk-1.5_r1/ndk-build\nI've been running the Android SDK for a while now in Eclipse (MAC OSX). I've downloaded the NDK and installed the C/C++ tools in Eclipse, but could anyone guide me on using the NDK? For example, do I just create an Android project like normal and build it with the NDK instead?\nReally could do with a decent tutorial if anyone know of any.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Android Studio installation on Windows 7 fails, no JDK found", "id": 1534, "answers": [{"answer_id": 1523, "document_id": 1111, "question_id": 1534, "text": "You may need to Add a system variable JDK_HOME with value c:\\Program Files\\Java\\jdk1.7.0_21\\ worked for me. The latest Java release can be downloaded here.\nAdditionally, make sure the variable JAVA_HOME is also set with the above location.", "answer_start": 487, "answer_category": null}], "is_impossible": false}], "context": "I downloaded Android Studio and attempted to launch the program.\nThis is running on Windows 7 64-bit with Java 1.7. During the installation my Java 1.7 is detected, and the rest of the installation goes through just fine. However, when attempting to launch the application from the desktop icon, nothing happens. Looking at the task manager, a new process from the CMD is loaded. This is because it's attempting to run the batch file studio.bat.\nWhen I execute via CMD, I get the error:\nYou may need to Add a system variable JDK_HOME with value c:\\Program Files\\Java\\jdk1.7.0_21\\ worked for me. The latest Java release can be downloaded here.\nAdditionally, make sure the variable JAVA_HOME is also set with the above location.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "how to pip install cairocffi", "id": 1524, "answers": [{"answer_id": 1513, "document_id": 1101, "question_id": 1524, "text": "\n    \n\nCouldn't get it to work, even with libffi6 libffi-dev installed. Finally got ", "answer_start": 11836, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nHow do I install cairocffi through pip?\n\ncairocffi is a CFFI-based drop-in replacement for Pycairo https://github.com/SimonSapin/cairocffi. \n\nI'm trying to install it on Ubuntu 14.04:\n\nalvas@ubi:~$ cat /etc/*-release\nDISTRIB_ID=Ubuntu\nDISTRIB_RELEASE=14.04\nDISTRIB_CODENAME=trusty\nDISTRIB_DESCRIPTION=\"Ubuntu 14.04.2 LTS\"\nNAME=\"Ubuntu\"\nVERSION=\"14.04.2 LTS, Trusty Tahr\"\nID=ubuntu\nID_LIKE=debian\nPRETTY_NAME=\"Ubuntu 14.04.2 LTS\"\nVERSION_ID=\"14.04\"\nHOME_URL=\"http://www.ubuntu.com/\"\nSUPPORT_URL=\"http://help.ubuntu.com/\"\nBUG_REPORT_URL=\"http://bugs.launchpad.net/ubuntu/\"\n\n\nI've tried installing with the standard pip command but I get this:\n\n$ sudo pip install cairocffi\nThe directory '/home/alvas/.cache/pip/log' or its parent directory is not owned by the current user and the debug log has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\nThe directory '/home/alvas/.cache/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\nThe directory '/home/alvas/.cache/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\nCollecting cairocffi\n  Downloading cairocffi-0.6.tar.gz (75kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 77kB 34kB/s \nCollecting cffi&gt;=0.6 (from cairocffi)\n  Downloading cffi-0.9.2.tar.gz (209kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 212kB 97kB/s \nRequirement already satisfied (use --upgrade to upgrade): pycparser in /usr/local/lib/python3.4/dist-packages (from cffi&gt;=0.6-&gt;cairocffi)\nInstalling collected packages: cffi, cairocffi\n  Running setup.py install for cffi\n    Complete output from command /usr/bin/python3 -c \"import setuptools, tokenize;__file__='/tmp/pip-build-d3kjzf__/cffi/setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace('\\r\\n', '\\n'), __file__, 'exec'))\" install --record /tmp/pip-ll323a3c-record/install-record.txt --single-version-externally-managed --compile:\n    Package libffi was not found in the pkg-config search path.\n    Perhaps you should add the directory containing `libffi.pc'\n    to the PKG_CONFIG_PATH environment variable\n    No package 'libffi' found\n    Package libffi was not found in the pkg-config search path.\n    Perhaps you should add the directory containing `libffi.pc'\n    to the PKG_CONFIG_PATH environment variable\n    No package 'libffi' found\n    Package libffi was not found in the pkg-config search path.\n    Perhaps you should add the directory containing `libffi.pc'\n    to the PKG_CONFIG_PATH environment variable\n    No package 'libffi' found\n    Package libffi was not found in the pkg-config search path.\n    Perhaps you should add the directory containing `libffi.pc'\n    to the PKG_CONFIG_PATH environment variable\n    No package 'libffi' found\n    Package libffi was not found in the pkg-config search path.\n    Perhaps you should add the directory containing `libffi.pc'\n    to the PKG_CONFIG_PATH environment variable\n    No package 'libffi' found\n    running install\n    running build\n    running build_py\n    creating build\n    creating build/lib.linux-x86_64-3.4\n    creating build/lib.linux-x86_64-3.4/cffi\n    copying cffi/commontypes.py -&gt; build/lib.linux-x86_64-3.4/cffi\n    copying cffi/lock.py -&gt; build/lib.linux-x86_64-3.4/cffi\n    copying cffi/api.py -&gt; build/lib.linux-x86_64-3.4/cffi\n    copying cffi/verifier.py -&gt; build/lib.linux-x86_64-3.4/cffi\n    copying cffi/__init__.py -&gt; build/lib.linux-x86_64-3.4/cffi\n    copying cffi/cparser.py -&gt; build/lib.linux-x86_64-3.4/cffi\n    copying cffi/backend_ctypes.py -&gt; build/lib.linux-x86_64-3.4/cffi\n    copying cffi/vengine_gen.py -&gt; build/lib.linux-x86_64-3.4/cffi\n    copying cffi/gc_weakref.py -&gt; build/lib.linux-x86_64-3.4/cffi\n    copying cffi/ffiplatform.py -&gt; build/lib.linux-x86_64-3.4/cffi\n    copying cffi/model.py -&gt; build/lib.linux-x86_64-3.4/cffi\n    copying cffi/vengine_cpy.py -&gt; build/lib.linux-x86_64-3.4/cffi\n    running build_ext\n    building '_cffi_backend' extension\n    creating build/temp.linux-x86_64-3.4\n    creating build/temp.linux-x86_64-3.4/c\n    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -g -fstack-protector --param=ssp-buffer-size=4 -Wformat -Werror=format-security -D_FORTIFY_SOURCE=2 -fPIC -DUSE__THREAD -I/usr/include/ffi -I/usr/include/libffi -I/usr/include/python3.4m -c c/_cffi_backend.c -o build/temp.linux-x86_64-3.4/c/_cffi_backend.o\n    c/_cffi_backend.c:13:17: fatal error: ffi.h: No such file or directory\n     #include &lt;ffi.h&gt;\n                     ^\n    compilation terminated.\n    error: command 'x86_64-linux-gnu-gcc' failed with exit status 1\n\n    ----------------------------------------\n    Command \"/usr/bin/python3 -c \"import setuptools, tokenize;__file__='/tmp/pip-build-d3kjzf__/cffi/setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace('\\r\\n', '\\n'), __file__, 'exec'))\" install --record /tmp/pip-ll323a3c-record/install-record.txt --single-version-externally-managed --compile\" failed with error code 1 in /tmp/pip-build-d3kjzf__/cffi\n\n\nI've manually checked the permission and I realized that there's no write access permission. Why is that so? And why isn't sudo working to overwrite the permission?\n\n$ ls -la .cache/pip/log/\ntotal 60\ndrwxrwxr-x 2 alvas alvas  4096 Feb  3 10:51 .\ndrwx------ 4 alvas alvas  4096 Apr 12 23:16 ..\n-rw-rw-r-- 1 alvas alvas 49961 Apr 12 23:18 debug.log\n\n\nWhen I tried sudo -H pip install cairoffi, I got:\n\nsudo -H pip install cairocffi\nCollecting cairocffi\n  Using cached cairocffi-0.6.tar.gz\nCollecting cffi&gt;=0.6 (from cairocffi)\n  Downloading cffi-0.9.2.tar.gz (209kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 212kB 29kB/s \nRequirement already satisfied (use --upgrade to upgrade): pycparser in /usr/local/lib/python3.4/dist-packages (from cffi&gt;=0.6-&gt;cairocffi)\nInstalling collected packages: cffi, cairocffi\n  Running setup.py install for cffi\n    Complete output from command /usr/bin/python3 -c \"import setuptools, tokenize;__file__='/tmp/pip-build-2sv6pbsp/cffi/setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace('\\r\\n', '\\n'), __file__, 'exec'))\" install --record /tmp/pip-xk4kkjrj-record/install-record.txt --single-version-externally-managed --compile:\n    Package libffi was not found in the pkg-config search path.\n    Perhaps you should add the directory containing `libffi.pc'\n    to the PKG_CONFIG_PATH environment variable\n    No package 'libffi' found\n    Package libffi was not found in the pkg-config search path.\n    Perhaps you should add the directory containing `libffi.pc'\n    to the PKG_CONFIG_PATH environment variable\n    No package 'libffi' found\n    Package libffi was not found in the pkg-config search path.\n    Perhaps you should add the directory containing `libffi.pc'\n    to the PKG_CONFIG_PATH environment variable\n    No package 'libffi' found\n    Package libffi was not found in the pkg-config search path.\n    Perhaps you should add the directory containing `libffi.pc'\n    to the PKG_CONFIG_PATH environment variable\n    No package 'libffi' found\n    Package libffi was not found in the pkg-config search path.\n    Perhaps you should add the directory containing `libffi.pc'\n    to the PKG_CONFIG_PATH environment variable\n    No package 'libffi' found\n    running install\n    running build\n    running build_py\n    creating build\n    creating build/lib.linux-x86_64-3.4\n    creating build/lib.linux-x86_64-3.4/cffi\n    copying cffi/commontypes.py -&gt; build/lib.linux-x86_64-3.4/cffi\n    copying cffi/lock.py -&gt; build/lib.linux-x86_64-3.4/cffi\n    copying cffi/api.py -&gt; build/lib.linux-x86_64-3.4/cffi\n    copying cffi/verifier.py -&gt; build/lib.linux-x86_64-3.4/cffi\n    copying cffi/__init__.py -&gt; build/lib.linux-x86_64-3.4/cffi\n    copying cffi/cparser.py -&gt; build/lib.linux-x86_64-3.4/cffi\n    copying cffi/backend_ctypes.py -&gt; build/lib.linux-x86_64-3.4/cffi\n    copying cffi/vengine_gen.py -&gt; build/lib.linux-x86_64-3.4/cffi\n    copying cffi/gc_weakref.py -&gt; build/lib.linux-x86_64-3.4/cffi\n    copying cffi/ffiplatform.py -&gt; build/lib.linux-x86_64-3.4/cffi\n    copying cffi/model.py -&gt; build/lib.linux-x86_64-3.4/cffi\n    copying cffi/vengine_cpy.py -&gt; build/lib.linux-x86_64-3.4/cffi\n    running build_ext\n    building '_cffi_backend' extension\n    creating build/temp.linux-x86_64-3.4\n    creating build/temp.linux-x86_64-3.4/c\n    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -g -fstack-protector --param=ssp-buffer-size=4 -Wformat -Werror=format-security -D_FORTIFY_SOURCE=2 -fPIC -DUSE__THREAD -I/usr/include/ffi -I/usr/include/libffi -I/usr/include/python3.4m -c c/_cffi_backend.c -o build/temp.linux-x86_64-3.4/c/_cffi_backend.o\n    c/_cffi_backend.c:13:17: fatal error: ffi.h: No such file or directory\n     #include &lt;ffi.h&gt;\n                     ^\n    compilation terminated.\n    error: command 'x86_64-linux-gnu-gcc' failed with exit status 1\n\n    ----------------------------------------\n    Command \"/usr/bin/python3 -c \"import setuptools, tokenize;__file__='/tmp/pip-build-2sv6pbsp/cffi/setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace('\\r\\n', '\\n'), __file__, 'exec'))\" install --record /tmp/pip-xk4kkjrj-record/install-record.txt --single-version-externally-managed --compile\" failed with error code 1 in /tmp/pip-build-2sv6pbsp/cffi\n\n\nAs @MattDMo suggested, i've tried apt-get install libffi but it still didn't work out:\n\nalvas@ubi:~$ sudo apt-get install libffi libffi-dev\nReading package lists... Done\nBuilding dependency tree       \nReading state information... Done\nE: Unable to locate package libffi\n\n\nBut there isn't any libffi on the package manager, so i've tried libffi-dev:\n\nalvas@ubi:~$ sudo apt-get install libffi-dev\nReading package lists... Done\nBuilding dependency tree       \nReading state information... Done\nThe following NEW packages will be installed:\n  libffi-dev\n0 upgraded, 1 newly installed, 0 to remove and 3 not upgraded.\nNeed to get 99.8 kB of archives.\nAfter this operation, 323 kB of additional disk space will be used.\nGet:1 http://archive.ubuntu.com/ubuntu/ trusty/main libffi-dev amd64 3.1~rc1+r3.0.13-12 [99.8 kB]\nFetched 99.8 kB in 1s (76.3 kB/s)     \nSelecting previously unselected package libffi-dev:amd64.\n(Reading database ... 492855 files and directories currently installed.)\nPreparing to unpack .../libffi-dev_3.1~rc1+r3.0.13-12_amd64.deb ...\nUnpacking libffi-dev:amd64 (3.1~rc1+r3.0.13-12) ...\nProcessing triggers for man-db (2.6.7.1-1ubuntu1) ...\nProcessing triggers for doc-base (0.10.5) ...\nProcessing 1 added doc-base file...\nProcessing triggers for install-info (5.2.0.dfsg.1-2) ...\nSetting up libffi-dev:amd64 (3.1~rc1+r3.0.13-12) ...\n\n\nIt installs libffi-dev successfully but cairoffi is still not installing:\n\nalvas@ubi:~$ sudo -H pip install cairoffi\nCollecting cairoffi\n  Could not find a version that satisfies the requirement cairoffi (from versions: )\n  No matching distribution found for cairoffi\nalvas@ubi:~$ sudo -H pip3 install cairoffi\nCollecting cairoffi\n  Could not find a version that satisfies the requirement cairoffi (from versions: )\n  No matching distribution found for cairoffi\n\n    \n\nIt's right in the error message:\n\nNo package 'libffi' found\n\n\nYou'll need to install libffi and libffi-dev through your distro's package manager (yum, apt-get, whatever) before the pip installation will work. Their names may very slightly from platform to platform.\n    \n\nCouldn't get it to work, even with libffi6 libffi-dev installed. Finally got it working with:\n\npip3 install -U pip\npip3 install -U setuptools\npip3 install --no-cache-dir cairocffi\n\n\n--no-cache-dir was the trick, thanks to: https://github.com/Kozea/cairocffi/issues/125#issuecomment-476295293\n    \n\nFor me, on Windows 10 using python 3.5.3rc1, pip install cairocffi was failing. Doing pip install cffi was sufficient to let it succeed.\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Automating Wordpress Development and Deployment", "id": 1245, "answers": [{"answer_id": 1238, "document_id": 817, "question_id": 1245, "text": "You can use Hudson CI Server (http://hudson-ci.org/) to to automated and manual builds using subversion checkout tasks, phing, and phpunit, etc... Basically, the Hudson server pulls code from subversion depending on what you want to deploy, and it rsync's the files to be deployed from the CI server out to the destination server.", "answer_start": 508, "answer_category": null}], "is_impossible": false}], "context": "Has anyone ever worked on a WordPress project with multiple developers in different locations? Are there best practices around distributed development teams and automated deployments?\nI have a team of varying degrees of developers, including plugin developers, theme developers, and simple CSS style tweakers, in a few different locations, and I would like to setup a good system for everyone to be able to work on their separate pieces and continuously deploy changes without disturbing anyone else's code.\nYou can use Hudson CI Server (http://hudson-ci.org/) to to automated and manual builds using subversion checkout tasks, phing, and phpunit, etc... Basically, the Hudson server pulls code from subversion depending on what you want to deploy, and it rsync's the files to be deployed from the CI server out to the destination server.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "java standalone application in n systems without manual installation", "id": 1942, "answers": [{"answer_id": 1929, "document_id": 1521, "question_id": 1942, "text": "Java Web Start is the best solution", "answer_start": 1904, "answer_category": null}], "is_impossible": false}], "context": "\n\n                    \n\n            \n\n        \n\n            \n\n                \n\n                    \n\n                        Closed. This question is off-topic. It is not currently accepting answers.\n\n                        \n\n                    \n\n                \n\n            \n\n        \n\n            \n\n        \n\n                \n\n                    \n\n                \n\n            \n\n                \n\n                    Want to improve this question? Update the question so it's on-topic for Stack Overflow.\n\n                \n\n                    Closed 9 years ago.\n\n\n\n            \n\n        \n\n            \n\n                    \n\n                        Improve this question\n\n                    \n\n            \n\n\n\n\n\n    \n\n\n\nI've done a java application for a hospital. The application is an ERP software. The application is to be installed in around n systems. \nBut installing the same application in all these systems is a bit difficult and it requires a lot of time.\n\nEven if that has been done, in the future, if the application requires even a little modification, then again the modified application should be replaced in all those systems in the hospital. I'm using mysql as my database, which i've placed within the main hospital server from where all other systems access it through the network.\n\nThrough RMI method we can access it somehow, but RMI is a bit too slow.\n\nCan anyone tell me some solutions or suggestion to achieve a solution for this problem?\n\nIn the case of java web application we can deploy the application war file within a main server and all the systems within a LAN or any other network can get the application through the browser. Is there any similar solution in the case for my Desktop Java Swing application which is a exe, jar based app.\n\nHow to get a Java Deskop Swing application in an around 200 systems without manual installation?\n\nThanks in advance.\n    \n\nJava Web Start is the best solution. http://docs.oracle.com/javase/tutorial/deployment/webstart/index.html\n    \n\nUsing Java Web Start is one the way to solve your problem. Try this to learn Java Web Start.\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Submodule file links in SSDT predeploy or postdeploy script producing a 72001 error", "id": 1341, "answers": [{"answer_id": 1331, "document_id": 910, "question_id": 1341, "text": "You can use Peter Schott's suggested fix in the original question's comments - using relative paths back to the submodule on disk instead of the \"virtual\" link inside of the actual Visual Studio SQL project.", "answer_start": 928, "answer_category": null}], "is_impossible": false}], "context": "In a SQL Server Data Tools (SSDT) project in Visual Studio, we have a \"core\" set of SQL objects that are included in each SQL project we do - kind of like a class library. We keep these \"core\" SQL objects in a separate Git repo, and then include them in other projects as a Git submodule.\nTrying to build the project in this manner produces a SQL72001 error in Visual Studio (\"The included file does not exist\"). Obviously if we physically place the ReferenceDataScript1.sql file in the directory (without having a reference), it builds just fine.\nOptions we've explored include having a non-Build \"buffer\" script between the PostDeploy master and the core subscripts (same error), and having pre and post build actions set to physically copy files back and forth from the submodule to the project to satisfy the build engine (a little too hacky for our taste).\nHas anyone run into this issue, or have a serviceable workaround?\nYou can use Peter Schott's suggested fix in the original question's comments - using relative paths back to the submodule on disk instead of the \"virtual\" link inside of the actual Visual Studio SQL project.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to handle dependency injection in a WPF/MVVM application", "id": 1863, "answers": [{"answer_id": 1849, "document_id": 1434, "question_id": 1863, "text": "You cannot set the DataContext property in XAML and also do dependency injection. Instead you have other alternatives.\nIf you application is based on a simple hierarchical view-model you can construct the entire view-model hierarchy.", "answer_start": 580, "answer_category": null}], "is_impossible": false}], "context": "I am starting a new desktop application and I want to build it using MVVM and WPF.\nI am also intending to use TDD.\nThe problem is that I don\u00b4t know how I should use an IoC container to inject my dependencies on my production code.\nThe problem is when it comes to use it in the real application. I know that I must have an IoC container that links a default implementation for the IStorage interface, but how would I do that?\nHow can I correctly 'tell' WPF to inject dependencies in that case?\nAlso, suppose I need an instance of SomeViewModel from my C# code, how should I do it?\nYou cannot set the DataContext property in XAML and also do dependency injection. Instead you have other alternatives.\nIf you application is based on a simple hierarchical view-model you can construct the entire view-model hierarchy.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to deploy war file to tomcat using command prompt?", "id": 1726, "answers": [{"answer_id": 1714, "document_id": 1299, "question_id": 1726, "text": " First add a user role in tomcat-users.xml for role manager-script.\nThen to undeploy current app you can use\nwget http://username:password@localhost:portnumber/manager/text/undeploy?path=/appname -O - -q\nTo deploy\nwget http://username:password@localhost:portnumber/manager/text/deploy?path=/appname&war=file", "answer_start": 111, "answer_category": null}], "is_impossible": false}], "context": "I have created a war file and put into tomcat/webapps. How to deploy a war file to tomcat using command prompt? First add a user role in tomcat-users.xml for role manager-script.\nThen to undeploy current app you can use\nwget http://username:password@localhost:portnumber/manager/text/undeploy?path=/appname -O - -q\nTo deploy\nwget http://username:password@localhost:portnumber/manager/text/deploy?path=/appname&war=file\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "unable to install pg gem", "id": 1616, "answers": [{"answer_id": 1603, "document_id": 1190, "question_id": 1616, "text": "You may need to install the postgresql-devel package, this will solve the issue of pg_config missing.\nsudo apt-get install libpq-dev", "answer_start": 58, "answer_category": null}], "is_impossible": false}], "context": "I tried using gem install pg but it doesn't seem to work.\nYou may need to install the postgresql-devel package, this will solve the issue of pg_config missing.\nsudo apt-get install libpq-dev\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Is there any way to ignore INSTALL_FAILED_VERSION_DOWNGRADE on application install with the Android Debug Bridge?", "id": 1595, "answers": [{"answer_id": 1584, "document_id": 1171, "question_id": 1595, "text": "It appears the latest version of adb tools has an \"allow downgrade flag\" that isn't shown in the adb help, but it is shown in the \"pm\" help on the device. So use: adb install -r -d <link to apk>", "answer_start": 477, "answer_category": null}], "is_impossible": false}], "context": "It seems like the most recent Android 4.2 has introduced this error condition on installation when one attempts to install an APK with a lower version. In prior versions of Android, one would be able to install older APK's simply via adb install -r <link to APK>. For debugging purposes, I frequently need to re-test older APK's; and the -r flag would replace the older build in older Android versions. Is there a work-around here to ignore [INSTALL_FAILED_VERSION_DOWNGRADE]?\nIt appears the latest version of adb tools has an \"allow downgrade flag\" that isn't shown in the adb help, but it is shown in the \"pm\" help on the device. So use: adb install -r -d <link to apk>\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "error: This is probably not a problem with npm. There is likely additional logging output above", "id": 517, "answers": [{"answer_id": 519, "document_id": 242, "question_id": 517, "text": "You should delete node_modules and package-lock.json then command: npm cache clean \u2013force.", "answer_start": 90, "answer_category": null}], "is_impossible": false}], "context": "This is probably not a problem with npm. There is likely additional logging output above.\nYou should delete node_modules and package-lock.json then command: npm cache clean \u2013force.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Gradle - getting the latest release version of a dependency", "id": 1838, "answers": [{"answer_id": 1824, "document_id": 1409, "question_id": 1838, "text": "Gradle currently does not support Maven's RELEASE (which is rarely used and deprecated) but it does support Ivy's latest.release. However, the general recommendation is to build against exact versions. Otherwise, the build can become a lottery.", "answer_start": 601, "answer_category": null}], "is_impossible": false}], "context": "What would be the easiest way to tell Gradle the following:\nRetrieve 'junit' dependency and take its latest 'release' version.\nManaging Maven and Ivy repositories is sort of new to me. I tried the following steps and they result in Could not resolve dependency ... error:\nWrite compile \"junit:junit:latest.release\" with repositories set to only mavenCentral() (however, it works if I say \"junit:junit:4.10\").\nWrite compile \"junit:junit:latest.release\" with repository set the following way:\nMaybe I misunderstand something. Why would getting the latest version of the dependency be such a hard task?\n\tGradle currently does not support Maven's RELEASE (which is rarely used and deprecated) but it does support Ivy's latest.release. However, the general recommendation is to build against exact versions. Otherwise, the build can become a lottery.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to set Python's default version to 3.x on OS X? ", "id": 1613, "answers": [{"answer_id": 1600, "document_id": 1187, "question_id": 1613, "text": "You can solve it by symbolic link.\nunlink /usr/local/bin/python\nln -s /usr/local/bin/python3.3 /usr/local/bin/python", "answer_start": 126, "answer_category": null}], "is_impossible": false}], "context": "I'm running Mountain Lion and the basic default Python version is 2.7. I downloaded Python 3.3 and want to set it as default.\nYou can solve it by symbolic link.\nunlink /usr/local/bin/python\nln -s /usr/local/bin/python3.3 /usr/local/bin/python\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Django newbie deployment question - ImportError: Could not import settings 'settings'", "id": 1279, "answers": [{"answer_id": 1271, "document_id": 850, "question_id": 1279, "text": "Your apache configuration should look like this:\n<Location \"/mysite\">\n    SetHandler python-program\n    PythonHandler django.core.handlers.modpython\n    SetEnv DJANGO_SETTINGS_MODULE mysite.settings\n    PythonOption django.root /mysite\n    PythonPath \"['/root/djangoprojects/', '/root/djangoprojects/mysite','/root/djangoprojects/mysite/polls', '/var/www'] + sys.path\"\n    PythonDebug On\n</Location>", "answer_start": 274, "answer_category": null}], "is_impossible": false}], "context": "How do I add settings location to sys.path such that it persistent across sessions ?\nI have read umpteen no of post with people having the same issue it and I have tried a lot completely beats me as to what I need to do.\nLooking for some help.\nThanks in advance Ankur Gupta\nYour apache configuration should look like this:\n<Location \"/mysite\">\n    SetHandler python-program\n    PythonHandler django.core.handlers.modpython\n    SetEnv DJANGO_SETTINGS_MODULE mysite.settings\n    PythonOption django.root /mysite\n    PythonPath \"['/root/djangoprojects/', '/root/djangoprojects/mysite','/root/djangoprojects/mysite/polls', '/var/www'] + sys.path\"\n    PythonDebug On\n</Location>\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Which directive provides Custom error documents?", "id": 414, "answers": [{"answer_id": 422, "document_id": 175, "question_id": 414, "text": "ErrorDocument directive", "answer_start": 1172, "answer_category": null}], "is_impossible": false}, {"question": "How to enable Multi Language Custom Error?", "id": 417, "answers": [{"answer_id": 425, "document_id": 175, "question_id": 417, "text": "In your server configuration file, you'll see a line such as:\n# Multi-language error messages\n#Include conf/extra/httpd-multilang-errordoc.conf\nUncommenting this Include line will enable this\nfeature, and provide language-negotiated error messages, based on\nthe language preference set in the client browser.", "answer_start": 5020, "answer_category": null}], "is_impossible": false}], "context": "\n\nCustom Error Responses\n\n\n\n\n\n\nModules | Directives | FAQ | Glossary | Sitemap\nApache HTTP Server Version 2.4\n\n\n\nApache > HTTP Server > Documentation > Version 2.4Custom Error Responses\nAlthough the Apache HTTP Server provides generic error responses\nin the event of 4xx or 5xx HTTP status codes, these responses are\nrather stark, uninformative, and can be intimidating to site users.\nYou may wish to provide custom error responses which are either\nfriendlier, or in some language other than English, or perhaps which\nare styled more in line with your site layout.\nCustomized error responses can be defined for any HTTP status\ncode designated as an error condition - that is, any 4xx or 5xx\nstatus.\nAdditionally, a set of values are provided, so\nthat the error document can be customized further based on the\nvalues of these variables, using Server\nSide Includes. Or, you can have error conditions handled by a\ncgi program, or other dynamic handler (PHP, mod_perl, etc) which\nmakes use of these variables.\n\nConfiguration\nAvailable Variables\nCustomizing Error Responses\nMulti Language Custom Error Documents\n\n\n\nConfiguration\nCustom error documents are configured using the ErrorDocument directive,\nwhich may be used in global,\nvirtualhost, or directory context. It may be used in .htaccess files\nif AllowOverride is set to\nFileInfo.\nErrorDocument 500 \"Sorry, our script crashed. Oh dear\"\nErrorDocument 500 /cgi-bin/crash-recover\nErrorDocument 500 http://error.example.com/server_error.html\nErrorDocument 404 /errors/not_found.html\nErrorDocument 401 /subscription/how_to_subscribe.html\nThe syntax of the ErrorDocument directive is:\nErrorDocument <3-digit-code> <action>\nwhere the action will be treated as:\n\nA local URL to redirect to (if the action begins with a \"/\").\nAn external URL to redirect to (if the action is a valid URL).\nText to be displayed (if none of the above). The text must be\nwrapped in quotes (\") if it consists of more than one word.\n\nWhen redirecting to a local URL, additional environment variables\nare set so that the response can be further customized. They are not sent to\nexternal URLs.\n\n\nAvailable Variables\nRedirecting to another URL can be useful, but only if some\ninformation can be passed which can then be used to explain or log\nthe error condition more clearly.\nTo achieve this, when the error redirect is sent, additional\nenvironment variables will be set, which will be generated from\nthe headers provided to the original request by prepending\n'REDIRECT_' onto the original header name. This provides the error\ndocument the context of the original request.\nFor example, you might receive, in addition to more usual\nenvironment variables, the following.\n\nREDIRECT_HTTP_ACCEPT=*/*, image/gif, image/jpeg, image/png\nREDIRECT_HTTP_USER_AGENT=Mozilla/5.0 Fedora/3.5.8-1.fc12 Firefox/3.5.8\nREDIRECT_PATH=.:/bin:/usr/local/bin:/sbin\nREDIRECT_QUERY_STRING=\nREDIRECT_REMOTE_ADDR=121.345.78.123\nREDIRECT_REMOTE_HOST=client.example.com\nREDIRECT_SERVER_NAME=www.example.edu\nREDIRECT_SERVER_PORT=80\nREDIRECT_SERVER_SOFTWARE=Apache/2.2.15\nREDIRECT_URL=/cgi-bin/buggy.pl\n\nREDIRECT_ environment variables are created from\nthe environment variables which existed prior to the\nredirect. They are renamed with a REDIRECT_\nprefix, i.e., HTTP_USER_AGENT becomes\nREDIRECT_HTTP_USER_AGENT.\nREDIRECT_URL, REDIRECT_STATUS, and\nREDIRECT_QUERY_STRING are guaranteed to be set, and\nthe other headers will be set only if they existed prior to the\nerror condition.\nNone of these will be\nset if the ErrorDocument target is an\nexternal redirect (anything starting with a\nscheme name like http:, even if it refers to the same host\nas the server).\n\n\nCustomizing Error Responses\nIf you point your ErrorDocument to some variety of\ndynamic handler such as a server-side include document, CGI\nscript, or some variety of other handler, you may wish to use the\navailable custom environment variables to customize this\nresponse.\nIf the ErrorDocument specifies a local redirect to a CGI\nscript, the script should include a \"Status:\"\nheader field in its output in order to ensure the propagation\nall the way back to the client of the error condition that\ncaused it to be invoked. For instance, a Perl ErrorDocument\nscript might include the following:\n...\nprint  \"Content-type: text/html\\n\";\nprintf \"Status: %s Condition Intercepted\\n\", $ENV{\"REDIRECT_STATUS\"};\n...\nIf the script is dedicated to handling a particular error\ncondition, such as 404\u00a0Not\u00a0Found, it can\nuse the specific code and error text instead.\nNote that if the response contains Location:\nheader (in order to issue a client-side redirect), the script\nmust emit an appropriate Status: header\n(such as 302\u00a0Found). Otherwise the\nLocation: header may have no effect.\n\n\nMulti Language Custom Error Documents\nProvided with your installation of the Apache HTTP Server is a\ndirectory of custom error documents translated into 16 different\nlanguages. There's also a configuration file in the\nconf/extra configuration directory that can be included\nto enable this feature.\nIn your server configuration file, you'll see a line such as:\n# Multi-language error messages\n#Include conf/extra/httpd-multilang-errordoc.conf\nUncommenting this Include line will enable this\nfeature, and provide language-negotiated error messages, based on\nthe language preference set in the client browser.\nAdditionally, these documents contain various of the\nREDIRECT_ variables, so that additional information can\nbe provided to the end-user about what happened, and what they can\ndo now.\nThese documents can be customized to whatever degree you wish to\nprovide more useful information to users about your site, and what\nthey can expect to find there.\nmod_include and mod_negotiation\nmust be enabled to use this feature.\n\n\nCopyright 2021 The Apache Software Foundation.Licensed under the Apache License, Version 2.0.\nModules | Directives | FAQ | Glossary | Sitemap\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Missing production secret_key_base in rails", "id": 548, "answers": [{"answer_id": 550, "document_id": 273, "question_id": 548, "text": "We ourselves should generate a secret key (by rake secret) then create an environment variables for SECRET_KEY_BASE by running following command from command prompt: rhc set-env SECRET_KEY_BASE=3dc8b0885b3043c0e38aa2e1dc64******************** -a myapp.\nafter running this command, connect to your server via SSH and run env so you should see your SECRET_KEY_BASE in the list.\nNow restart you app rhc app-stop myapp and rhc app-start myapp, then you are good to go.", "answer_start": 183, "answer_category": null}], "is_impossible": false}], "context": "I have recently deployed an app and got internal server error because of missing production secret_key_base. After hours of testing, I managed to solve this problem with two methods.\nWe ourselves should generate a secret key (by rake secret) then create an environment variables for SECRET_KEY_BASE by running following command from command prompt: rhc set-env SECRET_KEY_BASE=3dc8b0885b3043c0e38aa2e1dc64******************** -a myapp.\nafter running this command, connect to your server via SSH and run env so you should see your SECRET_KEY_BASE in the list.\nNow restart you app rhc app-stop myapp and rhc app-start myapp, then you are good to go.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Compiling php with curl, where is curl installed?", "id": 796, "answers": [{"answer_id": 792, "document_id": 479, "question_id": 796, "text": "In order to compile with cURL, you need libcurl header files (.h files). They are usually found in /usr/include/curl. They generally are bundled in a separate development package", "answer_start": 224, "answer_category": null}], "is_impossible": false}], "context": "for 64 bit ubuntu 17.** and after, curl is moved to /usr/include/x86_64-linux-gnu/curl, so just make a symlink cd /usr/include sudo ln -s x86_64-linux-gnu/curl. None of these will allow you to compile PHP with cURL enabled.\nIn order to compile with cURL, you need libcurl header files (.h files). They are usually found in /usr/include/curl. They generally are bundled in a separate development package.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How do i change the start in path of a shortcut for nsis?", "id": 1746, "answers": [{"answer_id": 1733, "document_id": 1318, "question_id": 1746, "text": "You can try this:\nSection \"Desktop Shortcut\" SHORTCUT\n     SetOutPath \"$INSTDIR\"\n     CreateShortcut \"$DESKTOP\\${FULL_APP_NAME}.lnk\" \"$INSTDIR\\${APP_NAME}.exe\" \"\" \"$ICONDIR\\${DESKICO}\"\nSectionEnd", "answer_start": 355, "answer_category": null}], "is_impossible": false}], "context": "I have an nsis installer script for the application im working on and it can place a shortcut on the desktop and in the start menu folder but each shortcut has the wrong start in path and as such the app saves data files to where the short cut is.\nIs there an easy way to change the start in path as the documentation was less than helpful on the matter?\nYou can try this:\nSection \"Desktop Shortcut\" SHORTCUT\n     SetOutPath \"$INSTDIR\"\n     CreateShortcut \"$DESKTOP\\${FULL_APP_NAME}.lnk\" \"$INSTDIR\\${APP_NAME}.exe\" \"\" \"$ICONDIR\\${DESKICO}\"\nSectionEnd\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How can I create an MSI setup?", "id": 748, "answers": [{"answer_id": 749, "document_id": 436, "question_id": 748, "text": "/usr/local/mysql/bin/mysql -u root -p", "answer_start": 367, "answer_category": null}], "is_impossible": false}], "context": "I downloaded the mysql dmg file and went through the wizard to run. Done. I have also started mysql server under system preferences.\nThe purpose of me doing this is to work through the exercises of my SQL text book. The terminal commands are new to me but I think once I can actually get started, working through the exercises should be OK. In the terminal, I typed:\n/usr/local/mysql/bin/mysql -u root -p\nI was then prompted to enter the temporary password that was given to me upon completion of the installation.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "install python and make in cygwin", "id": 1774, "answers": [{"answer_id": 1760, "document_id": 1345, "question_id": 1774, "text": "Look into cygwin native package manager, devel category. You should find make and python there.", "answer_start": 217, "answer_category": null}], "is_impossible": false}], "context": "I have installed Cygwin Terminal in OS Windows. But I need to install also python and make in cygwin.All of these programs are needed to run petsc library. Does Someone know how to install these components in cygwin?\nLook into cygwin native package manager, devel category. You should find make and python there.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to modify capistrano deploy to automatically run migrations in Rails 3.0", "id": 1099, "answers": [{"answer_id": 1091, "document_id": 676, "question_id": 1099, "text": "You can add:\nafter \"deploy:update_code\", \"deploy:migrate\"\nto your config/deploy.rb.", "answer_start": 149, "answer_category": null}], "is_impossible": false}], "context": "Right now, I have to run cap deploy and cap deploy:migrations if there are migrations to be run.\nhow I modify the cap deploy task to run migrations.\nYou can add:\nafter \"deploy:update_code\", \"deploy:migrate\"\nto your config/deploy.rb.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Deploying SQL Server Databases from Test to Live", "id": 510, "answers": [{"answer_id": 511, "document_id": 235, "question_id": 510, "text": "You should use normal versioning (using subversion, but any revision control should work). This way, I not only get the benefit of versioning, but updating live from dev/stage is the same process for code and database - tags, branches and so on work all the same.", "answer_start": 605, "answer_category": null}], "is_impossible": false}], "context": "I wonder how you guys manage deployment of a database between 2 SQL Servers, specifically SQL Server 2005. Now, there is a development and a live one. As this should be part of a buildscript (standard windows batch, even do with current complexity of those scripts, i might switch to PowerShell or so later), Enterprise Manager/Management Studio Express do not count.\nWould you just copy the .mdf File and attach it? I am always a bit careful when working with binary data, as this seems to be a compatiblity issue (even though development and live should run the same version of the server at all time).\nYou should use normal versioning (using subversion, but any revision control should work). This way, I not only get the benefit of versioning, but updating live from dev/stage is the same process for code and database - tags, branches and so on work all the same.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to set up MiniTest?", "id": 816, "answers": [{"answer_id": 811, "document_id": 498, "question_id": 816, "text": "But the most basic way you can run a set of MiniTest tests is with ruby itself:\n$ ruby myfileoftests.rb", "answer_start": 400, "answer_category": null}], "is_impossible": false}], "context": "I'm a fairly novice tester, but have been trying to get better at TDD in Rails.\nRSpec works great, but my tests are pretty slow. I've heard that MiniTest is a lot faster, and the MiniTest/Spec DSL looks pretty similar to how I'm used to working with RSpec, so I thought I'd give it a try.\nNote that watchr or spork are not requirements for running tests. They're a convenience for doing autotesting. But the most basic way you can run a set of MiniTest tests is with ruby itself:\n$ ruby myfileoftests.rb\nHowever, I have not been able to find anything on the web that provides a walkthrough of how to setup and run Minitest. I learned how to test from the RSpec book, and I have no idea how Test::Unit or MiniTest are supposed to work. I have the gem in my gemfile, I've written a few simple tests, but I have no idea where to put them or how to run them. I figure this is one of those things that's so obvious nobody has bothered to write it down...\nCan anyone explain to me how to setup some some Minitest/spec files and get them running so I can compare the performance against Rspec?\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How do I find the fully qualified name of an assembly?", "id": 516, "answers": [{"answer_id": 518, "document_id": 241, "question_id": 516, "text": "If you can load the assembly into a .NET application, you can do:\ntypeof(SomeTypeInTheAssembly).Assembly.FullName\nIf you cannot then you can use ildasm.exe and it will be in there somewhere:\nildasm.exe MyAssembly.dll /text", "answer_start": 120, "answer_category": null}], "is_impossible": false}], "context": "I've managed to get my PublicKeyToken using the sn.exe in the SDK, but I'ld like to easily get the full qualified name.\nIf you can load the assembly into a .NET application, you can do:\ntypeof(SomeTypeInTheAssembly).Assembly.FullName\nIf you cannot then you can use ildasm.exe and it will be in there somewhere:\nildasm.exe MyAssembly.dll /text\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "installing qt on linux cannot find lgl", "id": 1397, "answers": [{"answer_id": 1386, "document_id": 969, "question_id": 1397, "text": "sudo apt-get install libgl1-mesa-dev", "answer_start": 1948, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI'm having a hard time trying to install Qt on linux. I downloaded the .run file on the website and installed Qt. However, when I try to compile the default Hello World project using Qtcreator, I get the following :\n\nerror cannot find -lGL\n\n\nI was able to solve the problem by issuing the command :\n\nsudo apt-get install libqt4-dev\n\n\nBut, I'm not satisfied with the solution as I want to use Qt5 and the name of the lib I downloaded implies version 4. Can someone explain what is going on and tell me if my solution is correct? If not, what should I do to get a working Qt on Linux.\n\nAdditional question\n\nThe correct answer, as provided by LtWorf, was to install libgl-dev. For future problems of this sort, can someone tell me how I should have guessed that I had to download this particular library? And why are there some libs with -dev at the end? What do they provide?\n    \n\nWell it is trying to link with libgl and doesn't find it. You should install libgl-dev.\n\n-l is a linker option, it tells the linker to use a certain library.\nFor example you can have -lmagic meaning that you want to use libmagic.\n\nNormally all libraries are called libsomething, and on debian you will find 3 packages called:\nlibsomething\nlibsomething-dbg\nlibsomething-dev\n\nThe 1st one is the library, the second one is the library compiled with the debug symbols, so you can make sense of stacktraces more easily, and the final one is the development package, it contains the .h files so you can link to the library.\n    \n\nsudo apt-get install libgl-dev \n    \n\nOn Fedora 17, I did:\n\nsudo yum install mesa-libGL-devel\n\n    \n\nDo you have libgl-dev installed? If not install it and it should work.\n    \n\nThose other posters are correct, but on some systems, the lib to install is named differently.  I just dealt with a 32bit Ubuntu 14.04.5 LTS system, and libgl-dev was not available.\n\nInstead, I needed to install the libgl1-mesa-dev package via:\n\nsudo apt-get install libgl1-mesa-dev\n\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "gradlew command not found?", "id": 1558, "answers": [{"answer_id": 1547, "document_id": 1135, "question_id": 1558, "text": "Try running gradle wrapper --gradle-version 2.13 Remember to change 2.13 to your gradle version number. After running this command, you should see new scripts added to your project folder. You should be able to run the wrapper with ./gradlew build to build your code. Please refer to this guid for more information https://spring.io/guides/gs/gradle/.", "answer_start": 375, "answer_category": null}], "is_impossible": false}], "context": "I am working on a Java project with gradlew. I use Ubuntu Linux as my OS. When I run \"gradle\" it runs, and gives me info. But when I run \"gradlew\", it outputs as \"No command 'gradlew' found, did you mean: Command 'gradle' from package 'gradle' (universe) gradlew: command not found\"\nI did my research, I have jdk, and I did sudo apt-get install gradle. I am totally clueless\nTry running gradle wrapper --gradle-version 2.13 Remember to change 2.13 to your gradle version number. After running this command, you should see new scripts added to your project folder. You should be able to run the wrapper with ./gradlew build to build your code. Please refer to this guid for more information https://spring.io/guides/gs/gradle/.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Packaging and Deploying Scala Applications", "id": 1105, "answers": [{"answer_id": 1097, "document_id": 682, "question_id": 1105, "text": "The simplest (and the most consistent) way to package a Scala application, is packaging into a JAR (like you do with normal Java applications).", "answer_start": 135, "answer_category": null}], "is_impossible": false}], "context": "What is the simplest way to package a Scala application for use on a desktop PC? I'm guessing that would be in the form of a jar file.\nThe simplest (and the most consistent) way to package a Scala application, is packaging into a JAR (like you do with normal Java applications).\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Silent installation of a MSI package", "id": 663, "answers": [{"answer_id": 668, "document_id": 356, "question_id": 663, "text": "You should be able to use the /quiet or /qn options with msiexec to perform a silent install.", "answer_start": 408, "answer_category": null}], "is_impossible": false}], "context": "I have a MSI package that I need to install if the package is not already installed. Also I need to install it silently. The package prompts user for:\n\u2022\tInstallation location (C:\\Program Files\\Foobar)\n\u2022\tInstall type: minimal and full (minimal)\nI need to override these two parameters using command line parameters or some other method. So how do I go about these two issues. I'll use VBScript for scripting.\nYou should be able to use the /quiet or /qn options with msiexec to perform a silent install.\nMSI packages export public properties, which you can set with the PROPERTY=value syntax on the end of the msiexec parameters.\nFor example, this command installs a package with no UI and no reboot, with a log and two properties:\nmsiexec /i c:\\path\\to\\package.msi /quiet /qn /norestart /log c:\\path\\to\\install.log PROPE\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "ImportError: No module named tensorflow", "id": 684, "answers": [{"answer_id": 689, "document_id": 377, "question_id": 684, "text": "Try installing tensorflow again with the whatever version you want and with option --ignore-installed like:\npip install tensorflow==1.2.0 --ignore-installed", "answer_start": 301, "answer_category": null}], "is_impossible": false}], "context": "I had a more basic problem when I received this error.\nThe \"Validate your installation\" instructions say to type: python\nHowever, I have both 2.7 and 3.6 installed. Because I used pip3 to install tensorflow, I needed to type: python3\nUsing the correct version, I could import the \"tensorflow\" module. Try installing tensorflow again with the whatever version you want and with option --ignore-installed like:\npip install tensorflow==1.2.0 --ignore-installed\nI solved same issue using this command.\n\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "python-dev installation error: ImportError: No module named apt_pkg", "id": 1609, "answers": [{"answer_id": 1596, "document_id": 1183, "question_id": 1609, "text": "You can solve it by this:\n/usr/lib/python3/dist-packages# cp apt_pkg.cpython-34m-i386-linux-gnu.so apt_pkg.so", "answer_start": 366, "answer_category": null}], "is_impossible": false}], "context": "I am Debian user, and I want to install python-dev, but when I run the code in the shell as a root:\n# aptitude install python-dev\nI get the following error:\nTraceback (most recent call last):       \n  File \"/usr/bin/apt-listchanges\", line 28, in <module>\n    import apt_pkg\nImportError: No module named apt_pkg\nWhat seems to be the problem and how can I resolve it?\nYou can solve it by this:\n/usr/lib/python3/dist-packages# cp apt_pkg.cpython-34m-i386-linux-gnu.so apt_pkg.so\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Why am I getting an error before configuring a device in Visual Studio 2015?", "id": 1350, "answers": [{"answer_id": 1340, "document_id": 919, "question_id": 1350, "text": "You should use the following method:\n1.Project properties\n2.Debugging\n3.Remote Computer Name, drop down and select Configure... enter image description here\n4.Successfully shows the device configuration wizard, and was able to provision the target computer.", "answer_start": 428, "answer_category": null}], "is_impossible": false}], "context": "I want to deploy my driver for testing. I have provisioned my target computer for testing (although this shouldn't matter because I am not even at that step yet). On my host computer I open Visual Studio and go to: Driver > Test > Configure Devices\nThis looks like a bug in my Visual Studio (I have just recently updated to VS Update 1).\nI am going to uninstall and reinstall since I have already tried repair. Any other ideas?\nYou should use the following method:\n1.Project properties\n2.Debugging\n3.Remote Computer Name, drop down and select Configure... enter image description here\n4.Successfully shows the device configuration wizard, and was able to provision the target computer.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How do I install a POM in my local m2 repository?", "id": 1748, "answers": [{"answer_id": 1735, "document_id": 1320, "question_id": 1748, "text": "You need to specify -Dfile, you can tell mvn install:install-file to ignore the file and just install the POM with -Dpackaging=pom", "answer_start": 242, "answer_category": null}], "is_impossible": false}], "context": "I need to install a parent POM (without a JAR file) into my local .m2 repository. mvn install:install-file won't let me do that, it always asks for a valid -Dfile=<path-to-jar>.\nHow can I install tmp/dependency-management-1.0.0-SNAPSHOT.pom?\nYou need to specify -Dfile, you can tell mvn install:install-file to ignore the file and just install the POM with -Dpackaging=pom\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Create MSI or setup project with Visual Studio 2012", "id": 1544, "answers": [{"answer_id": 1533, "document_id": 1121, "question_id": 1544, "text": "To create setup projects in Visual Studio 2012 with InstallShield Limited Edition, watch this video:https://devblogs.microsoft.com/visualstudio/visual-studio-installer-projects-extension.aspx", "answer_start": 450, "answer_category": null}], "is_impossible": false}], "context": "I create a small application and I would like to create one MSI file.\nIn Visual Studio 2010 you have this project type under:\nOther Project Types -> Setup and Deployment -> Visual studio Installer -> Setup Project\nBut the only thing you got in Visual Studio 2012 is \"Enable InstallShield Limited Edition\".\nYou can change the .NET Framework, but nothing changes.\nWhy is it not there any more? And how can I get it back? Is there a new way to do this?\nTo create setup projects in Visual Studio 2012 with InstallShield Limited Edition, watch this video:https://devblogs.microsoft.com/visualstudio/visual-studio-installer-projects-extension.aspx\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "how to install use regexkitlite for an iphone application", "id": 1929, "answers": [{"answer_id": 1916, "document_id": 1503, "question_id": 1929, "text": "Add libicucore.A.dylib to your projects Frameworks group", "answer_start": 1095, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI'm trying to understand how to get my iPhone application working with RegexKitLite, I tried searching Google for how to install it but couldn't find anything that explains it clearly. Hopefully this will become a step-by-step guide for anyone searching for it in the future.\n\nAlright so according to the documentation it says:\n\n\n  The two files, RegexKitLite.h and\n  RegexKitLite.m, and linking against\n  the /usr/lib/libicucore.dylib ICU\n  shared library is all that is\n  required.\n\n\nSo I downloaded the .h and .m files, now I am confused about the whole \"linking against...\" part. Could someone please clarify?\n\nThere's a link in the documentation to the ICU from apple's website which contains a make file among others. Do I run this make file? Do I have to be an administrator when I run it? What do I do once/if the files are \"made\"? How do I \"link against\" this? Is this done in XCode? Does it need to be done for every project that needs it? Once I've done all that, I assume all I need to do is #import the .h file and start using it, is this correct?\n\nThanks\n    \n\nAdd libicucore.A.dylib to your projects Frameworks group. It can be found in:\n\n/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS2.1.sdk/usr/lib/libicucore.A.dylib \n\n\nObviously, you'll need to change the version of the iPhoneOS2.1.sdk to reflect the version of the iPhone OS you're building for, and change the iPhoneOS.platform to iPhoneSimulator.platform if you're building in the simulator.\n\nAfter that you can just add the RegexKitLite source files to your project and start using them.\n\nThe \"whole linking against...\" part is handled by adding the libicucore library to your Frameworks group.\n    \n\nYou really shouldn't be using the Frameworks groups to add the ICU library to your project, it's not a Framework. Double-click on the project icon in Groups &amp; Files pane in Xcode and go to the Build tab of the Project Info window, go to the Linking sub-section of the tab, double click on the Other Linker Flags field and add -licucore to the flags using the popup window. \n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Rails - Invalid Authenticity Token After Deploy", "id": 1110, "answers": [{"answer_id": 1102, "document_id": 687, "question_id": 1110, "text": "After extensive work by EngineYard (they're awesome!) they were able to diagnose the issue. The root cause of this issue is a bug with mongrel clusters. Mongrel doesn't seem to see the first post request after being started. EngineYard did extensive work to diagnose.", "answer_start": 563, "answer_category": null}], "is_impossible": false}], "context": "We're using EngineYard Cloud to deploy our Ruby on Rails application. We are running Rails v2.3.3.\nEngineYard Cloud deploys to AWS instances in a manner similar to Capistrano. After each deploy, we're running into Invalid Authenticity Token errors. Specifically, any user that has previously visited our application and then visits after the deploy and then tries to submit a form gets an invalid authenticity token error. This error persists until they reset their cookies for the site. After they reset their cookies, the site works as expected with no errors.\nAfter extensive work by EngineYard (they're awesome!) they were able to diagnose the issue. The root cause of this issue is a bug with mongrel clusters. Mongrel doesn't seem to see the first post request after being started. EngineYard did extensive work to diagnose.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Force browsers reload after cache headers were set for home page", "id": 1290, "answers": [{"answer_id": 1282, "document_id": 861, "question_id": 1290, "text": "So in the end I managed to fix it by adding a modified etag header. Looks like switching cache-control to no-store and removing the etag as a whole didn't help, but having cache-control: no-store and etag with a different value forced the test Chrome browser to reload.", "answer_start": 844, "answer_category": null}], "is_impossible": false}], "context": "I have a SPA and accidentally when launching I set cache headers not only for the assets, but also for the main page on the domain (like www.mysite.com).\nMy JS and CSS assets are versioned and they correctly get a unique filename on build to bust caches, but the problem is, that because my homepage is browser-cached too, the visitor never gets the updated HTML. They just always get the old HTML requesting the old assets.\nI have fixed the headers now, but all the users that visited my site previously are getting the old version, even when they click \"reload\" in browser. I managed to force it when I open DevTools and click \"Empty cache and hard reload\", but that's not what my users will normally do. So the question is - do I have any chance to force browsers to reload the main page on the domain after the cache headers were set once?\nSo in the end I managed to fix it by adding a modified etag header. Looks like switching cache-control to no-store and removing the etag as a whole didn't help, but having cache-control: no-store and etag with a different value forced the test Chrome browser to reload.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How do I create a nice-looking DMG for Mac OS X using command-line tools?", "id": 1562, "answers": [{"answer_id": 1551, "document_id": 1139, "question_id": 1562, "text": "You should take a look at the readme at the official website:\nhttps://github.com/LinusU/node-appdmg", "answer_start": 482, "answer_category": null}], "is_impossible": false}], "context": "I need to create a nice installer for a Mac application. I want it to be a disk image (DMG), with a predefined size, layout and background image.\nI need to do this programmatically in a script, to be integrated in an existing build system (more of a pack system really, since it only create installers. The builds are done separately).\nI already have the DMG creation done using \"hdiutil\", what I haven't found out yet is how to make an icon layout and specify a background bitmap.\nYou should take a look at the readme at the official website:\nhttps://github.com/LinusU/node-appdmg\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "pip cannot uninstall <package>: \"It is a distutils installed project\"", "id": 758, "answers": [{"answer_id": 758, "document_id": 445, "question_id": 758, "text": "pip install --ignore-installed pyOpenSSL\nThis will install the package with latest version and then if you try to install,\n  pip install twilio", "answer_start": 481, "answer_category": null}], "is_impossible": false}], "context": "I tried to install the Twilio module:\nsudo -H pip install twilio\nAnd I got this error:\nInstalling collected packages: pyOpenSSL\n  Found existing installation: pyOpenSSL 0.13.1\nCannot uninstall 'pyOpenSSL'. It is a distutils installed project and             \nthus we cannot accurately determine which files belong to it which \nwould lead to only a partial uninstall.\nAnyone know how to uninstall pyOpenSSL?\nI had the same error and was able to resolve using the following steps:\n  pip install --ignore-installed pyOpenSSL\nThis will install the package with latest version and then if you try to install,\n  pip install twilio\nIt will work.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to install Android SDK on Ubuntu?", "id": 878, "answers": [{"answer_id": 873, "document_id": 559, "question_id": 878, "text": "sudo apt update && sudo apt install android-sdk\nThe location of Android SDK on Linux can be any of the following:\n\n/home/AccountName/Android/Sdk\n\n/usr/lib/android-sdk\n\n/Library/Android/sdk/\n\n/Users/[USER]/Library/Android/sdk", "answer_start": 1544, "answer_category": null}], "is_impossible": false}, {"question": "how to install android sdk on ubuntu?", "id": 880, "answers": [{"answer_id": 875, "document_id": 559, "question_id": 880, "text": "sudo snap install androidsdk", "answer_start": 7871, "answer_category": null}], "is_impossible": false}], "context": "For my Ubuntu machine, I downloaded the latest version of Android SDK from this page.\n\nAfter extracting the downloaded .tgz file, I was trying to search for installation instructions and found:\n\nTo get started on Linux:\n\nUnpack the .zip file you've downloaded. The SDK files are download separately to a user-specified directory.\n\nMake a note of the name and location of the SDK directory on your system\u2014you will need to refer to the SDK directory later when using the SDK tools from the command line.\n\nWhat exactly are we supposed to do?\n\nandroid\nlinux\nubuntu\ninstallation\napt-get\nShare\nImprove this question\nFollow\nedited Jun 20 '20 at 9:12\n\nCommunityBot\n111 silver badge\nasked Jan 1 '16 at 14:17\n\nxameeramir\n23.6k1919 gold badges119119 silver badges195195 bronze badges\n1\nthere's an easy install paolorotolo.github.io/android-studio -- or check this tutorial on how do it manualy -- youtube.com/watch?v=qfinKxwYYZs \u2013 \nTasos\n Jan 1 '16 at 14:41 \n@Tasos Any idea about the maintenance and long term support paolorotolo's Android studio? This looks more of a personal project :( \u2013 \nxameeramir\n Jan 1 '16 at 15:00\nAndroid Studio itself alerts you when there is a new update/upgrade so you do it from there. I dont think the person modified AS \u2013 \nTasos\n Jan 1 '16 at 15:33 \nfrom my last comment -- however you can ask that question directly to the person here -- github.com/PaoloRotolo/android-studio/issues \u2013 \nTasos\n Jan 1 '16 at 15:43 \n@Tasos Yeah sure, done now! \u2013 \nxameeramir\n Jan 6 '16 at 7:13\nShow 1 more comment\n8 Answers\n\n169\n\nOption 1:\n\nsudo apt update && sudo apt install android-sdk\nThe location of Android SDK on Linux can be any of the following:\n\n/home/AccountName/Android/Sdk\n\n/usr/lib/android-sdk\n\n/Library/Android/sdk/\n\n/Users/[USER]/Library/Android/sdk\n\nOption 2:\n\nDownload the Android Studio.\n\nExtract downloaded .zip file.\n\nThe extracted folder name will read somewhat like android-studio\n\nTo keep navigation easy, move this folder to Home directory.\n\nAfter moving, copy the moved folder by right clicking it. This action will place folder's location to clipboard.\n\nUse Ctrl Alt T to open a terminal\n\nGo to this folder's directory using cd /home/(USER NAME)/android-studio/bin/\n\nType this command to make studio.sh executable: chmod +x studio.sh\n\nType ./studio.sh\n\nA pop up will be shown asking for installation settings. In my particular case, it is a fresh install so I'll go with selecting I do not have a previous version of Studio or I do not want to import my settings.\n\nIf you choose to import settings anyway, you may need to close any old project which is opened in order to get a working Android SDK.\n\n./studio.sh popup\n\nFrom now onwards, setup wizard will guide you.\n\nAndroid studio setup wizard\n\nAndroid Studio can work with both Open JDK and Oracle's JDK (recommended). Incase, Open JDK is installed the wizard will recommend installing Oracle Java JDK because some UI and performance issues are reported while using OpenJDK.\n\nThe downside with Oracle's JDK is that it won't update with the rest of your system like OpenJDK will.\n\nThe wizard may also prompt about the input problems with IDEA .\n\nSelect install type\n\nSelect Android studio install type\n\nVerify installation settings\n\nVerify Android studio installation settings\n\nAn emulator can also be configured as needed.\n\nAndroid studio emulator configuration prompt\n\nThe wizard will start downloading the necessary SDK tools\n\nThe wizard may also show an error about Linux 32 Bit Libraries, which can be solved by using the below command:\n\nsudo apt-get install libc6:i386 libncurses5:i386 libstdc++6:i386 lib32z1\n\nAfter this, all the required components will be downloaded and installed automatically.\n\nAfter everything is upto the mark, just click finish\n\nCompleted installation of Android studio\n\nTo make a Desktop icon, go to 'Configure' and then click 'Create Desktop Entry'\n\nCreating Android studio desktop icon\n\nCreating Android studio desktop icon for one or multiple users\n\nsource\n\nShare\nImprove this answer\nFollow\nedited Nov 12 '20 at 15:10\n\nAwaaaaarghhh\n19133 silver badges1515 bronze badges\nanswered Jan 6 '16 at 7:40\n\nxameeramir\n23.6k1919 gold badges119119 silver badges195195 bronze badges\n1\n@NiklasRosencrantz We did not tried importing settings so can't say anything about it. \u2013 \nxameeramir\n Jan 11 '18 at 5:47\n2\nwhat is the install location of Android SDK, if we use the option 1 ? \u2013 \nPratik Singhal\n Mar 8 '19 at 19:45 \n2\n@PratikSinghal Mostly this /home/AccountName/Android/Sdk \u2013 \nxameeramir\n Mar 10 '19 at 18:07\n1\n@student i use option 1 but cant find it in /home/accountname any idea ? \u2013 \nzukijuki\n Jul 21 '19 at 2:29\n2\nFor Ubuntu, with option 1, I found the sdk there: /usr/lib/android-sdk \u2013 \nSharcoux\n Mar 3 '20 at 22:48\nShow 6 more comments\n\n80\n\nTo install it on a Debian based system simply do\n\n# Install latest JDK\nsudo apt install default-jdk\n\n# install unzip if not installed yet\nsudo apt install unzip\n\n# get latest sdk tools - link will change. go to https://developer.android.com/studio/#downloads to get the latest one\ncd ~\nwget https://dl.google.com/android/repository/sdk-tools-linux-4333796.zip\n\n# unpack archive\nunzip sdk-tools-linux-4333796.zip\n\nrm sdk-tools-linux-4333796.zip\n\nmkdir android-sdk\nmv tools android-sdk/tools\nThen add the Android SDK to your PATH, open ~/.bashrc in editor and add the following lines into the file\n\n# Export the Android SDK path \nexport ANDROID_HOME=$HOME/android-sdk\nexport PATH=$PATH:$ANDROID_HOME/tools/bin\nexport PATH=$PATH:$ANDROID_HOME/platform-tools\n\n# Fixes sdkmanager error with java versions higher than java 8\nexport JAVA_OPTS='-XX:+IgnoreUnrecognizedVMOptions --add-modules java.se.ee'\nRun\n\nsource ~/.bashrc\nShow all available sdk packages\n\nsdkmanager --list\nIdentify latest android platform (here it's 28) and run\n\nsdkmanager \"platform-tools\" \"platforms;android-28\"\nNow you have adb, fastboot and the latest sdk tools installed\n\nShare\nImprove this answer\nFollow\nedited Nov 27 '18 at 21:15\nanswered Nov 27 '18 at 21:10\n\nluckyhandler\n9,33522 gold badges4343 silver badges5454 bronze badges\n3\nThe export JAVA_OPTS=.... caused an error when running sdkmanager : \"Error: Could not find or load main class java.se.ee\". Removing it fixed the issue. \u2013 \nDavid Robson\n Feb 13 '19 at 23:05\n1\nAbsolutely nowhere else mentions the sdk-tools download. Thank you! \u2013 \nDustin Hansen\n Feb 24 '19 at 19:03\nIn case this would prove useful to anyone passing by, if you do not want to include export JAVA_OPTS=... for some reason (like sdkmanager still failing even with it in the startup files, e.g. .bashrc and .zshrc, already, which happened to me in my other machine), you could use SDKman to install different versions of Java and use Java 8 only when you run sdkmanager. \u2013 \nSean Francis N. Ballais\n Apr 5 '19 at 11:10\nstackoverflow.com/questions/54287619/\u2026 \u2013 \nOtto\n Aug 21 '19 at 7:53\nsudo apt install android-sdk put the sdk into /usr/lib/android-sdk \u2013 \nSharcoux\n Mar 3 '20 at 23:11\nAdd a comment\n\n32\n\nThere is no need to download any binaries or files or follow difficult installation instructions.\n\nAll you really needed to do is:\n\nsudo apt update && sudo apt install android-sdk\nUpdate: Ubuntu 18.04 only\n\nShare\nImprove this answer\nFollow\nedited Jun 8 '18 at 12:05\nanswered Apr 11 '18 at 15:34\n\nMacroMan\n1,9012121 silver badges3131 bronze badges\n4\nsays I need to accept the licence... I have no idea how to do this :C \u2013 \nRicardoE\n May 31 '18 at 8:00\n37\nHow to accept license? there's no sdkmanager and android files. \u2013 \nNewbie\n Dec 24 '18 at 3:22\n3\nwhere will the sdk be placed btw? \u2013 \nthekucays\n Feb 19 '19 at 8:23\n6\n@thekucays Mine went into /usr/lib/android-sdk/ on Ubuntu 18.04 \u2013 \nMacroMan\n Feb 19 '19 at 12:34\n6\nThis seems very incomplete; sdkmanager is missing so we cannot install any components or platforms. \u2013 \ndurka42\n Jul 22 at 3:37\nShow 3 more comments\n\n21\n\nAndroid SDK Manager\n\nGet it from the Snap Store\n\nsudo snap install androidsdk\nUsage\nYou can use the sdkmanager to perform the following tasks.\n\nList installed and available packages\nandroidsdk --list [options]\nInstall packages\nandroidsdk packages [options]\nThe packages argument is an SDK-style path as shown with the --list command, wrapped in quotes (for example, \"build-tools;29.0.0\" or \"platforms;android-28\"). You can pass multiple package paths, separated with a space, but they must each be wrapped in their own set of quotes.\n\nFor example, here's how to install the latest platform tools (which includes adb and fastboot) and the SDK tools for API level 28:\n\nandroidsdk \"platform-tools\" \"platforms;android-28\"\nAlternatively, you can pass a text file that specifies all packages:\n\nandroidsdk --package_file=package_file [options]\nThe package_file argument is the location of a text file in which each line is an SDK-style path of a package to install (without quotes).\n\nTo uninstall, simply add the --uninstall flag:\n\nandroidsdk --uninstall packages [options]\nandroidsdk --uninstall --package_file=package_file [options]\nUpdate all installed packages\n\nandroidsdk --update [options]\nNote\nandroidsdk it is snap wraper of sdkmanager all options of sdkmanager work with androidsdk\n\nLocation of installed android sdk files : /home/user/AndroidSDK\n\nSee all sdkmanager options in google documentation\n\nShare\nImprove this answer\nFollow\nedited Dec 4 '20 at 9:20\nanswered Jan 18 '20 at 11:42\n\nAndrey Yankovich\n61277 silver badges1212 bronze badges\nAdd a comment\n\n12\n\nIf you are on Ubuntu 17.04 (Zesty), and you literally just need the SDK (no Android Studio), you can install it like on Debian:\n\nsudo apt install android-sdk android-sdk-platform-23\nexport ANDROID_HOME=/usr/lib/android-sdk\nIn build.gradle, change compileSdkVersion to 23 and buildToolsVersion to 24.0.0\nrun gradle build\nShare\nImprove this answer\nFollow", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to skip the release or specific modules in the maven repo", "id": 534, "answers": [{"answer_id": 536, "document_id": 259, "question_id": 534, "text": "To understand you correctly, you still want to run the test modules, but your don't want to deploy/release them. The maven-deploy-plugin to the rescue! To artifact will still be built, but not deployed.", "answer_start": 342, "answer_category": null}], "is_impossible": false}], "context": "So first i run the parent module, then I run the module which builds the api, then the modules which depend on the api, then a test module which contains tools to test and at the end I run a assembly/distribution where I package some modules in a archive. Because of some problems I can not really change the way and it works so far perfect.\nTo understand you correctly, you still want to run the test modules, but your don't want to deploy/release them. The maven-deploy-plugin to the rescue! To artifact will still be built, but not deployed.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Installing PhantomJS on Mac", "id": 837, "answers": [{"answer_id": 832, "document_id": 519, "question_id": 837, "text": "If you are using Homebrew, you can type:\nbrew tap homebrew/cask\nbrew cask install phantomjs", "answer_start": 0, "answer_category": null}], "is_impossible": false}], "context": "If you are using Homebrew, you can type:\nbrew tap homebrew/cask\nbrew cask install phantomjs\nAlso tried installing the binary from the downloads website, but nothing. What am I missing? End goal is to use casperjs but currently casper is asking.It was migrated from homebrew/core to homebrew/cask. You can access it again by running: brew tap homebrew/cask And then you can install it by running: brew cask install phantomjs\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Deploying CherryPy (daemon)", "id": 485, "answers": [{"answer_id": 491, "document_id": 215, "question_id": 485, "text": "There is a Daemonizer plugin for CherryPy included by default which is useful for getting it to start but by far the easiest way for simple cases is to use the cherryd script: https://bitbucket.org/cherrypy/cherrypy/src/default/cherrypy/cherryd.", "answer_start": 311, "answer_category": null}], "is_impossible": false}], "context": "I've followed the basic CherryPy tutorial (http://www.cherrypy.org/wiki/CherryPyTutorial). One thing not discussed is deployment.\nHow can I launch a CherryPy app as a daemon and \"forget about it\"? What happens if the server reboots?\nIs there a standard recipe? Maybe something that will create a service script\nThere is a Daemonizer plugin for CherryPy included by default which is useful for getting it to start but by far the easiest way for simple cases is to use the cherryd script: https://bitbucket.org/cherrypy/cherrypy/src/default/cherrypy/cherryd.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Maven copy local file to remote server using SSH", "id": 1703, "answers": [{"answer_id": 1691, "document_id": 1276, "question_id": 1703, "text": " I suggest to use an embedded antrun build step. In this step, you can do anything using the normal ant syntax which you'd use in build.xml", "answer_start": 267, "answer_category": null}], "is_impossible": false}], "context": "Can Maven copy local file to a remote server using SSH?\nI want to specify location in maven configuration file and to copy that file (or files) to server each time deploy phase is executed. Maven is not a generic tool, it's a tool to make your build process reusable. I suggest to use an embedded antrun build step. In this step, you can do anything using the normal ant syntax which you'd use in build.xml.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Error when deploying an artifact in Nexus", "id": 1669, "answers": [{"answer_id": 1656, "document_id": 1242, "question_id": 1669, "text": "Try changing the version of your artefact to end with -SNAPSHOT.", "answer_start": 315, "answer_category": null}], "is_impossible": false}], "context": "Im' getting an error when deploying an artifact in my own repository in a Nexus server: \"Failed to deploy artifacts: Could not transfer artifact\" \"Failed to transfer file http:///my_artifact. Return code is: 400\". Just to create a separate answer. The answer is actually found in a comment for the accepted answer.\nTry changing the version of your artefact to end with -SNAPSHOT.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "What interfaces do Django support?", "id": 50, "answers": [{"answer_id": 53, "document_id": 60, "question_id": 50, "text": "Django currently supports two interfaces: WSGI and ASGI.", "answer_start": 765, "answer_category": null}], "is_impossible": false}, {"question": "Why does django need interface?", "id": 51, "answers": [{"answer_id": 54, "document_id": 60, "question_id": 51, "text": "Django, being a web framework, needs a web server in order to operate. And\nsince most web servers don\u2019t natively speak Python, we need an interface to\nmake that communication happen.", "answer_start": 582, "answer_category": null}], "is_impossible": false}], "context": "\n\n\n\n\n\nDeploying Django\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDjango 3.2.9.dev documentation\n\nHome  |\nTable of contents  |\nIndex  |\nModules\n\n\n\u00ab previous\n|\nup\n|\nnext \u00bb\n\n\n\n\n\n\nDeploying Django\u00b6\nDjango is full of shortcuts to make Web developers\u2019 lives easier, but all\nthose tools are of no use if you can\u2019t easily deploy your sites. Since Django\u2019s\ninception, ease of deployment has been a major goal.\nThere are many options for deploying your Django application, based on your\narchitecture or your particular business needs, but that discussion is outside\nthe scope of what Django can give you as guidance.\nDjango, being a web framework, needs a web server in order to operate. And\nsince most web servers don\u2019t natively speak Python, we need an interface to\nmake that communication happen.\nDjango currently supports two interfaces: WSGI and ASGI.\n\nWSGI is the main Python standard for communicating between Web servers and\napplications, but it only supports synchronous code.\nASGI is the new, asynchronous-friendly standard that will allow your\nDjango site to use asynchronous Python features, and asynchronous Django\nfeatures as they are developed.\n\nYou should also consider how you will handle static files for your application, and how to handle\nerror reporting.\nFinally, before you deploy your application to production, you should run\nthrough our deployment checklist to ensure that your\nconfigurations are suitable.\n\n\nHow to deploy with WSGI\nHow to use Django with Gunicorn\nHow to use Django with uWSGI\nHow to use Django with Apache and mod_wsgi\nAuthenticating against Django\u2019s user database from Apache\nThe application object\nConfiguring the settings module\nApplying WSGI middleware\n\n\nHow to deploy with ASGI\nHow to use Django with Daphne\nHow to use Django with Hypercorn\nHow to use Django with Uvicorn\nThe application object\nConfiguring the settings module\nApplying ASGI middleware\n\n\nDeployment checklist\nRun manage.py check --deploy\nCritical settings\nEnvironment-specific settings\nHTTPS\nPerformance optimizations\nError reporting\n\n\n\n\n\n\n\n\n\n\n\nPrevious topic\nWriting a custom storage system\nNext topic\nHow to deploy with WSGI\n\nThis Page\n\nShow Source\n\n\n\nQuick search\n\n\n\n\n\n\n\n\n\n\n\n\nLast update:\nOct 05, 2021\n\n\n\n\n\u00ab previous\n|\nup\n|\nnext \u00bb\n\n\n\n\n\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to install MongoDB on Debian?", "id": 202, "answers": [{"answer_id": 210, "document_id": 111, "question_id": 202, "text": "ise Edition using the\napt pac", "answer_start": 3128, "answer_category": null}], "is_impossible": false}, {"question": "How to remove MongoDB databases on Debian?", "id": 204, "answers": [{"answer_id": 212, "document_id": 111, "question_id": 204, "text": "/log/mongodbsudo rm -r /var", "answer_start": 8668, "answer_category": null}], "is_impossible": false}, {"question": "How to remove MongoDB log files on Debian?", "id": 203, "answers": [{"answer_id": 211, "document_id": 111, "question_id": 203, "text": "nd log files.sudo rm -r /var", "answer_start": 8640, "answer_category": null}], "is_impossible": false}], "context": "Install MongoDB Enterprise Edition on Debian&lt;iframe src=\"https://www.googletagmanager.com/ns.html?id=GTM-GDFN\" height=\"0\" width=\"0\" style=\"display: none; visibility: hidden\" aria-hidden=\"true\"&gt;&lt;/iframe&gt;Close \u00d7MongoDB ManualVersion 5.0IntroductionInstallationInstall MongoDB Community EditionInstall MongoDB EnterpriseInstall on LinuxInstall on Red HatInstall on UbuntuInstall on DebianInstall using .tgz TarballInstall on SUSEInstall on AmazonInstall on macOSInstall on WindowsInstall with DockerUpgrade MongoDB Community to MongoDB EnterpriseVerify Integrity of MongoDB PackagesMongoDB Shell (mongosh)MongoDB CRUD OperationsAggregationData ModelsTransactionsIndexesSecurityChange StreamsReplicationShardingAdministrationStorageFrequently Asked QuestionsReferenceRelease NotesTechnical SupportNavigationInstall MongoDB > Install MongoDB Enterprise > Install MongoDB Enterprise on LinuxInstall MongoDB Enterprise Edition on Debian\u00b6On this pageOverviewConsiderationsInstall MongoDB Enterprise EditionRun MongoDB Enterprise EditionUninstall MongoDBAdditional InformationNoteMongoDB AtlasMongoDB Atlas\nis a hosted MongoDB service option in the cloud which requires no\ninstallation overhead and offers a free tier to get started.Overview\u00b6Use this tutorial to install MongoDB 5.0 Enterprise Edition\nusing the apt package manager.MongoDB Enterprise Edition\nis available on select platforms and contains support for several\nfeatures related to security and monitoring.MongoDB Version\u00b6This tutorial installs MongoDB 5.0 Enterprise\nEdition. To install a different version of MongoDB Enterprise,\nuse the version drop-down menu in the upper-left corner of this page to\nselect the documentation for that version.Considerations\u00b6Platform Support\u00b6MongoDB 5.0 Enterprise Edition supports the following\n64-bit Debian releases on\nx86_64 architecture:Debian 10 \"Buster\"Debian 9 \"Stretch\"MongoDB only supports the 64-bit versions of these platforms.See Supported Platforms for more information.NoteWindows Subsystem for Linux (WSL) SupportTo run MongoDB in Windows Subsystem for Linux (WSL), refer to the\nWSL documentation.Production Notes\u00b6Before deploying MongoDB in a production environment, consider the\nProduction Notes document which offers\nperformance considerations and configuration recommendations for\nproduction MongoDB deployments.Official MongoDB Packages\u00b6To install MongoDB Enterprise on your Debian system, these\ninstructions will use the official mongodb-enterprise package, which is\nmaintained and supported by MongoDB Inc. The official mongodb-enterprise\npackage always contains the latest version of MongoDB, and is available\nfrom its own dedicated repo.ImportantThe mongodb package provided by Debian is not\nmaintained by MongoDB Inc. and conflicts with the official\nmongodb-enterprise package. If you have already installed the mongodb\npackage on your Debian system, you must first uninstall\nthe mongodb package before proceeding with these instructions.See MongoDB Enterprise Edition Packages for the complete list of\nofficial packages.Install MongoDB Enterprise Edition\u00b6Follow these steps to install MongoDB Enterprise Edition using the\napt package manager.1Import the public key used by the package management system.\u00b6From a terminal, issue the following command to import the\nMongoDB public GPG Key from https://www.mongodb.org/static/pgp/server-5.0.asc:wget -qO - https://www.mongodb.org/static/pgp/server-5.0.asc | sudo apt-key add -The operation should respond with an OK.However, if you receive an error indicating that gnupg is not\ninstalled, you can:Install gnupg and its required libraries using the following command:sudo apt-get install gnupgOnce installed, retry importing the key:wget -qO - https://www.mongodb.org/static/pgp/server-5.0.asc | sudo apt-key add -2Create a /etc/apt/sources.list.d/mongodb-enterprise.list file for MongoDB.\u00b6Create the list file using the command appropriate for your version\nof Debian:If you'd like to install MongoDB Enterprise packages from a\nparticular release series, you can\nspecify the release series of a version of MongoDB that is supported\nfor your Debian build in the repository configuration. For example,\nto restrict your system to the 4.2 release series, add the following\nrepository:3Reload local package database.\u00b6Issue the following command to reload the local package database:sudo apt-get update4Install the MongoDB Enterprise packages.\u00b6Install MongoDB Enterprise.\u00b6Issue the following command:sudo apt-get install -y mongodb-enterpriseInstall a specific release of MongoDB Enterprise.\u00b6To install a specific release, you must specify each component package\nindividually along with the version number, as in the\nfollowing example:sudo apt-get install -y mongodb-enterprise=5.0.2 mongodb-org-database=5.0.2 mongodb-enterprise-server=5.0.2 mongodb-enterprise-shell=5.0.2 mongodb-enterprise-mongos=5.0.2 mongodb-enterprise-tools=5.0.2If you only install mongodb-enterprise=5.0.2 and do not include the\ncomponent packages, the latest version of each MongoDB package will be\ninstalled regardless of what version you specified.Pin a specific version of MongoDB Enterprise.\u00b6Although you can specify any available version of MongoDB,\napt-get upgrades the packages when a newer version\nbecomes available. To prevent unintended upgrades, pin the\npackage. To pin the version of MongoDB at the currently\ninstalled version, issue the following command sequence:echo \"mongodb-enterprise hold\" | sudo dpkg --set-selectionsecho \"mongodb-enterprise-server hold\" | sudo dpkg --set-selectionsecho \"mongodb-enterprise-database hold\" | sudo dpkg --set-selectionsecho \"mongodb-enterprise-shell hold\" | sudo dpkg --set-selectionsecho \"mongodb-enterprise-mongos hold\" | sudo dpkg --set-selectionsecho \"mongodb-enterprise-tools hold\" | sudo dpkg --set-selectionsRun MongoDB Enterprise Edition\u00b6By default, a MongoDB instance stores:its data files in /var/lib/mongodbits log files in /var/log/mongodbIf you installed via the package manager, these default directories are\ncreated during the installation.If you installed manually by downloading the tarballs, you can create\nthe directories using mkdir -p <directory> or sudo mkdir -p\n<directory> depending on the user that will run MongoDB. (See your\nlinux man pages for information on mkdir and sudo.)By default, MongoDB runs using the mongodb user account. If you\nchange the user that runs the MongoDB process, you must also modify\nthe permission to the /var/lib/mongodb and /var/log/mongodb\ndirectories to give this user access to these directories.To specify a different log file directory and data file directory, edit\nthe systemLog.path and storage.dbPath settings in\nthe /etc/mongod.conf. Ensure that the user running MongoDB has\naccess to these directories.Most Unix-like operating systems limit the system resources that a\nprocess may use. These limits may negatively impact MongoDB operation,\nand should be adjusted. See UNIX ulimit Settings for the recommended\nsettings for your platform.NoteStarting in MongoDB 4.4, a startup error is generated if the\nulimit value for number of open files is under 64000.Procedure\u00b6Follow these steps to run MongoDB Enterprise Edition on your system.\nThese instructions assume that you are using the official mongodb-enterprise\npackage -- not the unofficial mongodb package provided by\nDebian --  and are using the default settings.Init SystemTo run and manage your mongod process, you will be using\nyour operating system's built-in init system. Recent versions of\nLinux tend to use systemd (which uses the systemctl command),\nwhile older versions of Linux tend to use System V init (which uses\nthe service command).If you are unsure which init system your platform uses, run the\nfollowing command:ps --no-headers -o comm 1Then select the appropriate tab below based on the result:systemd - select the systemd (systemctl) tab below.init - select the System V Init (service) tab below.Uninstall MongoDB\u00b6To completely remove MongoDB from a system, you must remove the MongoDB\napplications themselves, the configuration files, and any directories containing\ndata and logs. The following section guides you through the necessary steps.WarningThis process will completely remove MongoDB, its configuration, and all\ndatabases. This process is not reversible, so ensure that all of your\nconfiguration and data is backed up before proceeding.1Stop MongoDB.\u00b6Stop the mongod process by issuing the following command:sudo service mongod stop2Remove Packages.\u00b6Remove any MongoDB packages that you had previously installed.sudo apt-get purge mongodb-enterprise*3Remove Data Directories.\u00b6Remove MongoDB databases and log files.sudo rm -r /var/log/mongodbsudo rm -r /var/lib/mongodbAdditional Information\u00b6Localhost Binding by Default\u00b6By default, MongoDB launches with bindIp set to\n127.0.0.1, which binds to the localhost network interface. This\nmeans that the mongod can only accept connections from\nclients that are running on the same machine. Remote clients will not be\nable to connect to the mongod, and the mongod will\nnot be able to initialize a replica set unless this value is set\nto a valid network interface.This value can be configured either:in the MongoDB configuration file with bindIp, orvia the command-line argument --bind_ipWarningBefore binding to a non-localhost (e.g. publicly accessible)\nIP address, ensure you have secured your cluster from unauthorized\naccess. For a complete list of security recommendations, see\nSecurity Checklist. At minimum, consider\nenabling authentication and\nhardening network infrastructure.For more information on configuring bindIp, see\nIP Binding.MongoDB Enterprise Edition Packages\u00b6MongoDB Enterprise Edition is available from its own dedicated\nrepository, and contains the following officially-supported packages:Package NameDescriptionmongodb-enterpriseA metapackage that automatically installs the component\npackages listed below.mongodb-enterprise-databaseA metapackage that automatically installs the component\npackages listed below.Package NameDescriptionmongodb-enterprise-serverContains the mongod daemon and associated\nconfiguration and init scripts.mongodb-enterprise-mongosContains the mongos daemon.mongodb-enterprise-shellContains the mongo shell.mongodb-enterprise-cryptdContains the mongocryptd\nbinarymongodb-mongoshContains the MongoDB Shell (mongosh).mongodb-enterprise-toolsA metapackage that automatically installs the component\npackages listed below:Package NameDescriptionmongodb-database-toolsContains the following MongoDB database tools:mongodumpmongorestorebsondumpmongoimportmongoexportmongostatmongotopmongofilesmongodb-enterprise-database-tools-extraContains the following MongoDB support tools:mongoldapmongokerberosinstall_compass scriptmongodecrypt binary\u00a9 MongoDB, Inc 2008-present. MongoDB, Mongo, and the leaf logo are registered trademarks of MongoDB, Inc.Give Feedback\u2190 \u00a0Install MongoDB Enterprise on Ubuntu using .tgz TarballInstall MongoDB Enterprise on Debian using .tgz Tarball\u00a0\u2192On this pageOverviewConsiderationsInstall MongoDB Enterprise EditionRun MongoDB Enterprise EditionUninstall MongoDBAdditional Information\n\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to install PyInstaller?", "id": 1761, "answers": [{"answer_id": 1748, "document_id": 1333, "question_id": 1761, "text": "To install PyInstaller you shuold:\n1.Go to your command prompt (Start -> Run -> cmd)\n2.type the following command cd c:\\python27\\scripts press enter,this should be where your pip.exe file is located.\n3.Once you are in this directory type pip install pyinstaller press enter", "answer_start": 438, "answer_category": null}], "is_impossible": false}], "context": "I want to use PyInstaller. I could always create bin files with pyinstaller.py [args].\nIt's not a package with an __init__.py file,\nit has no setup.py\nand it doesn't work to create a folder, put it in my PYTHONPATH, put pyinstaller\\[files] in that folder and then make a call to python pyinstaller\\pyinstaller.py.\nSo now I'm out of ideas how to install PyInstaller so that I don't have to work with absolute paths. Do you have any ideas?\nTo install PyInstaller you shuold:\n1.Go to your command prompt (Start -> Run -> cmd)\n2.type the following command cd c:\\python27\\scripts press enter,this should be where your pip.exe file is located.\n3.Once you are in this directory type pip install pyinstaller press enter\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to install Java 8 on Mac", "id": 139, "answers": [{"answer_id": 147, "document_id": 85, "question_id": 139, "text": "Oracle has a poor record for making it easy to install and configure Java, but using Homebrew, the latest OpenJDK (Java 14) can be installed with: brew install --cask adoptopenjdk8.", "answer_start": 129, "answer_category": null}], "is_impossible": false}], "context": "I want to do some programming with the latest JavaFX, which requires Java 8.  I'm using IntelliJ 13 CE and Mac OS X 9 Mavericks.\nOracle has a poor record for making it easy to install and configure Java, but using Homebrew, the latest OpenJDK (Java 14) can be installed with: brew install --cask adoptopenjdk8.\nExisting users of Homebrew may encounter Error: Cask adoptopenjdk8 exists in multiple taps due to prior workarounds with different instructions. This can be solved by fully specifying the location with brew install --cask adoptopenjdk/openjdk/adoptopenjdk8.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How can I setup & run PhantomJS on Ubuntu?", "id": 1561, "answers": [{"answer_id": 1550, "document_id": 1138, "question_id": 1561, "text": "PhantomJS is on npm. You can run this command to install it globally:\nnpm install -g phantomjs-prebuilt  ", "answer_start": 352, "answer_category": null}], "is_impossible": false}], "context": "I set up PhantomJS and recorded it to video: https://www.dailymotion.com/video/xnizmh_1_webcam\nBuild instructions: http://phantomjs.org/build.html\nIs there anything wrong in my setup?\nAfter I set it up I read the quick start tutorial and tried to write this code\nphantomjs hello.js \nIt gives me \"command not found\" error. How can I solve this problem?\nPhantomJS is on npm. You can run this command to install it globally:\nnpm install -g phantomjs-prebuilt  \n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Collectstatic error while deploying Django app to Heroku", "id": 1699, "answers": [{"answer_id": 1687, "document_id": 1272, "question_id": 1699, "text": "1.\tdisable the collectstatic during a deploy\nheroku config:set DISABLE_COLLECTSTATIC=1\n2.\tdeploy\ngit push heroku master\n3.\trun migrations (django 1.10 added at least one)\nheroku run python manage.py migrate\n4.\trun collectstatic using bower\nheroku run 'bower install --config.interactive=false;grunt prep;python manage.py collectstatic --noinput'\n5.\tenable collecstatic for future deploys\nheroku config:unset DISABLE_COLLECTSTATIC\n6.\ttry it on your own (optional)\nheroku run python manage.py collectstatic", "answer_start": 334, "answer_category": null}], "is_impossible": false}], "context": "I'm trying to deploy a Django app to Heroku, it starts to build, download and installs everything, but that's what I get when it comes to collecting static files.\n86\nI just updated to Django 1.10 today and had the exact same problem. Your static settings are identical to mine as well.\nThis worked for me, run the following commands:\n1.\tdisable the collectstatic during a deploy\nheroku config:set DISABLE_COLLECTSTATIC=1\n2.\tdeploy\ngit push heroku master\n3.\trun migrations (django 1.10 added at least one)\nheroku run python manage.py migrate\n4.\trun collectstatic using bower\nheroku run 'bower install --config.interactive=false;grunt prep;python manage.py collectstatic --noinput'\n5.\tenable collecstatic for future deploys\nheroku config:unset DISABLE_COLLECTSTATIC\n6.\ttry it on your own (optional)\nheroku run python manage.py collectstatic\nfuture deploys should work as normal from now on\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "error while creating new rails 3 project require cannot load such file o", "id": 1920, "answers": [{"answer_id": 1907, "document_id": 1493, "question_id": 1920, "text": "4).  Fixed with:\n\nsudo apt-get install libssl-dev\nrvm remove 1.9.3\n", "answer_start": 1283, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nWhen I try to create a new project($ rails new first_app) it gives following error after creating directory structure.\n\n...\n...\n      create  vendor/plugins/.gitkeep\n         run  bundle install\n/home/amit/.rvm/rubies/ruby-1.9.3-p0/lib/ruby/site_ruby/1.9.1/rubygems/custom_require.rb:36:in `require': cannot load such file -- openssl (LoadError)\n    from /home/amit/.rvm/rubies/ruby-1.9.3-p0/lib/ruby/site_ruby/1.9.1/rubygems/custom_require.rb:36:in `require'\n    from /home/amit/.rvm/rubies/ruby-1.9.3-p0/lib/ruby/1.9.1/net/https.rb:22:in `&lt;top (required)&gt;'\n...\n...\n\n\nHere is the configuration on Ubuntu 10.04\n\n$ rails -v\nRails 3.2.1\n$ rvm -v\n\nrvm 1.10.2 by Wayne E. Seguin &lt;wayneeseguin@gmail.com&gt;, Michal Papis &lt;mpapis@gmail.com&gt; [https://rvm.beginrescueend.com/]\n\n$ bundle -v\nBundler version 1.0.21\n$ gem -v\n1.8.15\n\n\nPlease help me to resolve this issue.\n    \n\nYou need to bundle your ruby with openssl support. Have a look at http://beginrescueend.com/packages/openssl/\n\nrvm reinstall 1.9.3 --with-openssl-dir=/usr/local\n\nThis requires that you have the openssl headers present on your box. These are named differently across the systems, like libopenssl-dev, libssl-dev, openssl-devel etc..\n    \n\nHad the same problem on Ubuntu Lucid (10.04).  Fixed with:\n\nsudo apt-get install libssl-dev\nrvm remove 1.9.3\nrvm install 1.9.3\n\n    \n\nThis solution I saw in this link worked for me very well.\n\nAssuming RVM is in use\n\nrvm pkg install openssl\n\n\nRemove ruby\n\nrvm remove 1.9.3\n\n\nAnd finally recompile Ruby with openssl\n\nrvm install 1.9.3 --with-openssl-dir=$HOME/.rvm/usr\n\n\nFinally\n\nrvm use 1.9.3 --default\n\n\nI hope this worked for future searches.\n    \n\nIf you're not using RVM, here's how:\n\nsudo apt-get install libssl-dev\n./configure --prefix=/usr/local\nmake\nmake install\n\n    \n\nHere are instructions for Mac OS 10.8 (although, these seem general)\nhttps://gist.github.com/joneslee85/5025729\n\nrvm pkg install openssl\nrvm reinstall 1.9.3 --with-openssl-dir=$rvm_path/usr\n\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How can I pass a password from a bash script to aptitude for installing mysql?", "id": 1744, "answers": [{"answer_id": 1731, "document_id": 1316, "question_id": 1744, "text": "You can use the following command:\nsudo debconf-set-selections <<< 'mysql-server-5.1 mysql-server/root_password password your_password'\nsudo debconf-set-selections <<< 'mysql-server-5.1 mysql-server/root_password_again password your_password'\nsudo apt-get -y install mysql-server", "answer_start": 182, "answer_category": null}], "is_impossible": false}], "context": "I am writing a simple bash script to install MySQL on Ubuntu.\nHowever MySQL prompts for a password and confirmation. How do I pass along a root password. Is there an echo I can use?\nYou can use the following command:\nsudo debconf-set-selections <<< 'mysql-server-5.1 mysql-server/root_password password your_password'\nsudo debconf-set-selections <<< 'mysql-server-5.1 mysql-server/root_password_again password your_password'\nsudo apt-get -y install mysql-server\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "rails 3 install error \"File not found: lib\"", "id": 1191, "answers": [{"answer_id": 1184, "document_id": 767, "question_id": 1191, "text": "sudo gem install rails --no-rdoc --no-ri", "answer_start": 218, "answer_category": null}], "is_impossible": false}], "context": "When installing Rails 3.0.1 i always get following error message, although i can use the installation.\nI'm not sure if there's something wrong, I'm irritated by the error message.\nThat didn't work for me, but this did sudo gem install rails --no-rdoc --no-ri\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "War deployment on Tomcat takes ages", "id": 1352, "answers": [{"answer_id": 1342, "document_id": 921, "question_id": 1352, "text": "I upload the WAR to my home directory, cd to /usr/local/tomcat, then run the following commands:\nbin/shutdown.sh\nrm webapps/ROOT.war\nrm -rf webapps/ROOT\ncp ~/ROOT.war webapps\nbin/startup.sh", "answer_start": 639, "answer_category": null}], "is_impossible": false}], "context": "I have a Grails application, built to a war file (~30mb). When I attempt to deploy the war file on Tomcat 6 via the application manager, it takes upwards of 10 minutes to deploy, or hangs indefinitely. When it hangs I can restart Tomcat and the app is usually deployed, however sometimes I have to repeat the process. I've also noticed that during deployment, the Java process maxes out the CPU and the RAM is at ~10-15%.\nI'm fairly new to Java, so I don't know if this is normal, but I can't imagine how it could be. Is there something I can do to make this run smoother/faster? Is there a better way to deploy than Tomcat's app manager?\nI upload the WAR to my home directory, cd to /usr/local/tomcat, then run the following commands:\nbin/shutdown.sh\nrm webapps/ROOT.war\nrm -rf webapps/ROOT\ncp ~/ROOT.war webapps\nbin/startup.sh\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How do I run Visual Studio as an administrator by default?", "id": 1622, "answers": [{"answer_id": 1609, "document_id": 1196, "question_id": 1622, "text": "Copied and pasted from http://www.sevenforums.com/tutorials/11841-run-administrator.html, the Using Advanced Properties section. This will allow you to always have the program run as an administrator when you open it.", "answer_start": 292, "answer_category": null}], "is_impossible": false}], "context": "I recently discovered that even while logged into my personal laptop as an administrator, Visual Studio does not run in administrator mode and you need to explicitly use Run As Administrator.\nIs there a way to make it run as an administrator by default, other than creating a shortcut, etc.?\nCopied and pasted from http://www.sevenforums.com/tutorials/11841-run-administrator.html, the Using Advanced Properties section. This will allow you to always have the program run as an administrator when you open it.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "What are the Team City best practices for multistage deployment?", "id": 579, "answers": [{"answer_id": 585, "document_id": 304, "question_id": 579, "text": "The problem is simply that all permissions in TeamCity are applied against the project and never the build so if you've got one project with all your builds, there's no ability to apply permissions granularity to dev versus production builds. I've previously dealt with this in two ways:\nHandle it socially. Everyone knows what their responsibilities are and you don't run what you're not meant to run. If you do, it's audited and traceable back to YOU. Work fine when there's maturity, a clear idea of responsibilities and not compliance requirement that prohibits it.\nCreate separate projects. I don't like having to do this but it does fix the problem. You can still use artifacts from another project and means you simply end up with one project containing builds that deploy to environments you're happy for all the devs to access and another project to sensitive environments. The downside is that if the production build fails, the very people you probably want support from won't be able to access it!\n", "answer_start": 850, "answer_category": null}], "is_impossible": false}], "context": "We're using Team City and only have Continuous Integration setup with our development environment. I don't want to save artifacts for every development deployment that Team City does. I want an assigned person to be able to fire a build configuration that will deploy a certain successful development deployment to our staging server.\nThen, I want each staging deployment to save artifacts. When a staging deployment passes UAT, I want to deploy that package to Production.\nI'm not sure how to set this up in Team City. I'm using version 6.5.4, and I'm aware there's a \"Promote...\" action/trigger, but I think it depends on saved artifacts. I don't want to save development deployments each time as artifacts, but I do want the person running the staging deployment to be able to specify which successful development deployment to deploy to staging.\nThe problem is simply that all permissions in TeamCity are applied against the project and never the build so if you've got one project with all your builds, there's no ability to apply permissions granularity to dev versus production builds. I've previously dealt with this in two ways:\nHandle it socially. Everyone knows what their responsibilities are and you don't run what you're not meant to run. If you do, it's audited and traceable back to YOU. Work fine when there's maturity, a clear idea of responsibilities and not compliance requirement that prohibits it.\nCreate separate projects. I don't like having to do this but it does fix the problem. You can still use artifacts from another project and means you simply end up with one project containing builds that deploy to environments you're happy for all the devs to access and another project to sensitive environments. The downside is that if the production build fails, the very people you probably want support from won't be able to access it!\n\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How do I package a Mac OS application for install?", "id": 1161, "answers": [{"answer_id": 1154, "document_id": 738, "question_id": 1161, "text": "Use a PKG if you need to install files aside from your application bundle. In all other cases use a DMG that prompts the user to copy the application into the Applications folder", "answer_start": 242, "answer_category": null}], "is_impossible": false}], "context": "What's the \"standard\" way of packaging an app for install on Mac OS? Is one of the above the Apple-recommended way?\nThanks.\nAssuming you don't want to or can't go the App Store route, both PKG and DMG are common ways to distribute a program. Use a PKG if you need to install files aside from your application bundle. In all other cases use a DMG that prompts the user to copy the application into the Applications folder. But a lot of your users will not understand that they need to do that (unless your target audience is solely knowledgeable computer users). They will run your application from the disk image. Ideally in this case, your program will detect that it is running from a disk image and offer to copy itself into the Applications folder.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Is there a way to enforce a deployment order in tomcat6?", "id": 1282, "answers": [{"answer_id": 1274, "document_id": 853, "question_id": 1282, "text": "One solution is to use something like ZeroConf and register your services when they start up, and then have the dependent apps look for when these services come available and have them connect and do what ever they need to do when the service is ready. This is how I have been handling multiple dependent services for years now. I have Python, Java and Erlang services all discovering each other via ZeroConf seemlessly.", "answer_start": 275, "answer_category": null}], "is_impossible": false}], "context": "I have 3 wars in my webapp folder. Two of them are built on services of the third one. I'm in a testing environment, i.e. I don't have control over their architectures, so I'm no able to change a thing. So...\nQuestion: Is there a way to enforce a deployment order in tomcat?\nOne solution is to use something like ZeroConf and register your services when they start up, and then have the dependent apps look for when these services come available and have them connect and do what ever they need to do when the service is ready. This is how I have been handling multiple dependent services for years now. I have Python, Java and Erlang services all discovering each other via ZeroConf seemlessly.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Install phpunit on windows", "id": 1147, "answers": [{"answer_id": 1140, "document_id": 724, "question_id": 1147, "text": "it should be installed as a local (for your project) development package: composer require --dev phpunit/phpunit ^9.3", "answer_start": 110, "answer_category": null}], "is_impossible": false}], "context": "How to install phpunit?\nI read documentation https://github.com/sebastianbergmann/phpunit, but have an error. it should be installed as a local (for your project) development package: composer require --dev phpunit/phpunit ^9.3.\nI wasted a lot of time trying to use pear (the proscribed method to get MakeGood working with Eclipse IDE), only to discover the repository for phpunit is no longer available. This information should be nearer the top of this page so people do not waste their time too.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Where did CUDA get installed in my computer?", "id": 809, "answers": [{"answer_id": 804, "document_id": 491, "question_id": 809, "text": "locate cuda | grep /cuda$\nor\nfind / -type d -name cuda 2>/dev/null", "answer_start": 294, "answer_category": null}], "is_impossible": false}], "context": "The thing is the folder /usr/local/cuda* does not exist after successful CUDA installation. Further trying to install cuda says that it is already the newest version. Usually, it is /usr/local/cuda. If this is not the case, you can try to locate cuda. If you want to find directories only, run\nlocate cuda | grep /cuda$\nor\nfind / -type d -name cuda 2>/dev/null\nFor me, it turned out to be in /opt/cuda-7.5\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Skip database migration while deploying Rails application using Capistrano 3", "id": 1255, "answers": [{"answer_id": 1247, "document_id": 826, "question_id": 1255, "text": "According to the doc, you can require just what you need manually:\n# Capfile\nrequire 'capistrano/bundler' \nrequire 'capistrano/rails/assets'\n# require 'capistrano/rails/migrations'", "answer_start": 283, "answer_category": null}], "is_impossible": false}], "context": "When we run cap deploy, it runs all the migrations during deployment. We have to point the application to existing DB and don't want modify existing DB.\nCan anybody suggest how can we skip the migration step while deploying the application?\nI suppose you are using capistrano/rails.\nAccording to the doc, you can require just what you need manually:\n# Capfile\nrequire 'capistrano/bundler' \nrequire 'capistrano/rails/assets'\n# require 'capistrano/rails/migrations'\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "alpine linux cant install lapack dev on python3 5 alpine3 4", "id": 1930, "answers": [{"answer_id": 1917, "document_id": 1504, "question_id": 1930, "text": "Alpine v3.5. \nSo, my suggestion is to install lapack-dev package from the nearest repository by time. In this case you shouldn't fa", "answer_start": 1512, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI'm using docker python:3.5-alpine3.4 image and trying to install lapack-dev but it keeps failing. It is complaining that it can't find libgfortran.so.5. However, I've tried installing libgfortran and that does not seem to fix the problem.\n\n(1/1) Installing libgfortran (5.3.0-r0)\nOK: 33 MiB in 37 packages\nfetch http://dl-cdn.alpinelinux.org/alpine/v3.4/main/x86_64/APKINDEX.tar.gz\nfetch http://dl-cdn.alpinelinux.org/alpine/v3.4/main/x86_64/APKINDEX.tar.gz\nfetch http://dl-cdn.alpinelinux.org/alpine/v3.4/community/x86_64/APKINDEX.tar.gz\nfetch http://dl-cdn.alpinelinux.org/alpine/v3.4/community/x86_64/APKINDEX.tar.gz\nfetch http://dl-8.alpinelinux.org/alpine/edge/community/x86_64/APKINDEX.tar.gz\nfetch http://dl-8.alpinelinux.org/alpine/edge/community/x86_64/APKINDEX.tar.gz\nWARNING: This apk-tools is OLD! Some packages might not function properly.\n\nERROR: unsatisfiable constraints:\n  so:libgfortran.so.5 (missing):\n\n    required by:\n      lapack-3.8.0-r1[so:libgfortran.so.5]\n\n\nAny ideas how I can fix this? Here is the relevant RUN step.\n\nFROM python:3.5-alpine3.4\n\nRUN echo \"http://dl-8.alpinelinux.org/alpine/edge/community\" &gt;&gt; /etc/apk/repositories \\\n  &amp;&amp; apk update \\\n  &amp;&amp; apk add --update-cache --no-cache libgcc libquadmath musl \\\n  &amp;&amp; apk add --update-cache --no-cache libgfortran \\\n  &amp;&amp; apk add --update-cache --no-cache lapack-dev\n\n    \n\npython:3.5-alpine3.4 docker image is based on Alpine v3.4. lapack-dev package was appeared only in Alpine v3.5. \nSo, my suggestion is to install lapack-dev package from the nearest repository by time. In this case you shouldn't face issues with outdated dependencies. \nAnd it works pretty well.\n\nThe final Dockerfile is:\n\nFROM python:3.5-alpine3.4\n\nRUN echo \"http://dl-cdn.alpinelinux.org/alpine/v3.5/community\" &gt;&gt; /etc/apk/repositories \\\n  &amp;&amp; apk update \\\n  &amp;&amp; apk add --update-cache --no-cache libgcc libquadmath musl \\\n  &amp;&amp; apk add --update-cache --no-cache libgfortran \\\n  &amp;&amp; apk add --update-cache --no-cache lapack-dev\n\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Why do so many programs have both a setup.exe and a setup.msi?", "id": 738, "answers": [{"answer_id": 739, "document_id": 426, "question_id": 738, "text": "In very basic words, you can deliver just the .msi file and it will install. but .exe will not work without the .msi", "answer_start": 551, "answer_category": null}], "is_impossible": false}], "context": "I have always wondered about this. So many application setups have a zip file that you unzip, and in it are a bunch of files, among other things an exe and an msi. What is the difference? They are often even about the same size. I am never really sure which one to execute, sometimes I do the exe and sometimes the msi, and it usually works with either one. But does one of them do anything that the other doesn't do? And if not, isn't it kind of a waste having two files that does the same thing? Especially when thinking about download size, etc...\nIn very basic words, you can deliver just the .msi file and it will install. but .exe will not work without the .msi\nNot sure if this should be here or on ServerFault, or maybe neither, but I figured since developers usually are the ones creating setup files, then developers might know why this is like it is =)\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "removing bad installs from add remove programs", "id": 1430, "answers": [{"answer_id": 1419, "document_id": 1005, "question_id": 1430, "text": "If you can't find it in either of the folders in the answer, you can do a Ctrl+F (Edit - Find...) and search for the exact display name. I had this issue when making a bootstrapper and I was very frustrated after searching line by line through each mentioned folder, not realizing there was a search function. It ended up being under one of the folders in HKEY_USERS instead of HKEY_LOCAL_MACHINE for me", "answer_start": 1835, "answer_category": null}], "is_impossible": false}, {"question": "  \"SOURCEMGMT: Source is invalid due to missing/inaccessible package\" ", "id": 1431, "answers": [{"answer_id": 1420, "document_id": 1005, "question_id": 1431, "text": " check HKLM\\software\\classes\\installer\\products if your uninstall process is failing even after deleting the keys", "answer_start": 2250, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI've created a custom boot-strapper for my application using Wix and Burn, but in the time it took to learn I managed to install several early variants in such a way that they won't uninstall.  I think I created the problem by running Engine.Apply before PlanComplete had been called.\n\nWhere is the information that builds the list in Add/Remove programs and what is the best way to manually remove orphaned rows?\n\nUpdate - I should have said I'm on 64 bit Windows 7 Enterprise, Service Pack 1.\n    \n\nHKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Uninstall or HKEY_LOCAL_MACHINE\\SOFTWARE\\Wow6432Node\\Microsoft\\Windows\\CurrentVersion\\Uninst\u200c\u200ball- this is the location where the add remove programs gets populated. If you remove the entry from the registry it would take out the entry. You can delete the key from here as described below and also physically locate and delete the files/folders.\n\nIn Registry Editor, locate the registry keys mentioned above.\n\nEach key listed under Uninstall in the left pane of Registry Editor represents a program that is displayed in the Currently installed programs list of the Add or Remove Programs tool.To determine which program that each key represents, click the key, and then view the following values in the details pane on the right:\n\nDisplayName: The value data for the DisplayName key is the name that is listed in Add or Remove Programs.\n\n-and-\n\nUninstallString: The value data for the UninstallString key is the program that is used to uninstall the program.\n\nAfter you identify the registry key that represents the program that you removed but which is still displayed in the Currently installed programs list of Add or Remove Programs, right-click the key in the left pane of the Registry Editor window, and then click Delete.\n\nMicrosoft Link  \n    \n\nIf you can't find it in either of the folders in the answer, you can do a Ctrl+F (Edit - Find...) and search for the exact display name. I had this issue when making a bootstrapper and I was very frustrated after searching line by line through each mentioned folder, not realizing there was a search function. It ended up being under one of the folders in HKEY_USERS instead of HKEY_LOCAL_MACHINE for me.\n    \n\nAlso check HKLM\\software\\classes\\installer\\products if your uninstall process is failing even after deleting the keys from Isiah4110's answer. It will resolve this lingering problem:\n\n\n  \"SOURCEMGMT: Source is invalid due to missing/inaccessible package\" \n\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "ignoring gems because its extensions are not built", "id": 1963, "answers": [{"answer_id": 1949, "document_id": 1545, "question_id": 1963, "text": "This error occurs when you switch the ruby version", "answer_start": 2257, "answer_category": null}], "is_impossible": false}], "context": "\n\n                    \n\n            \n\n        \n\n            \n\n                \n\n                    \n\n                        This question already has answers here:\n\n                        \n\n                    \n\n                \n\n            \n\n                    \n\n                        Ignoring GEM because its extensions are not built\n\n                            \n\n                                (22 answers)\n\n                            \n\n                    \n\n                Closed 3 years ago.\n\n        \n\n\n\n\n\n    \n\n\n\nI installed the rails in my machine\n\nruby 2.3.1\n\nrails 5.1.4\n\nUbuntu 16.04 os\n\nAfter rails installation, When i check rails version rails -v\nIt throws Ignoring \"Some gems list\" because its extensions are not built\n\nIgnoring nokogiri-1.8.1 because its extensions are not built.  Try: gem pristine nokogiri --version 1.8.1\nIgnoring bindex-0.5.0 because its extensions are not built.  Try: gem pristine bindex --version 0.5.0\nIgnoring byebug-9.1.0 because its extensions are not built.  Try: gem pristine byebug --version 9.1.0\nIgnoring curb-0.9.4 because its extensions are not built.  Try: gem pristine curb --version 0.9.4\nIgnoring executable-hooks-1.3.2 because its extensions are not built.  Try: gem pristine executable-hooks --version 1.3.2\nIgnoring ffi-1.9.18 because its extensions are not built.  Try: gem pristine ffi --version 1.9.18\nIgnoring gem-wrappers-1.3.2 because its extensions are not built.  Try: gem pristine gem-wrappers --version 1.3.2\nIgnoring gem-wrappers-1.2.7 because its extensions are not built.  Try: gem pristine gem-wrappers --version 1.2.7\nIgnoring nio4r-2.2.0 because its extensions are not built.  Try: gem pristine nio4r --version 2.2.0\nIgnoring pg-0.21.0 because its extensions are not built.  Try: gem pristine pg --version 0.21.0\nIgnoring websocket-driver-0.6.5 because its extensions are not built.  Try: gem pristine websocket-driver --version 0.6.5\nRails 5.1.4\n\n\nAfter new rails application created successfully\nWhen i run bundle install \nIt throws errror message like:\n\nAn error occurred while installing json (1.8.6), and Bundler cannot continue.\nMake sure that `gem install json -v '1.8.6'` succeeds before bundling.\n\n    \n\nI ran the command gem pristine --all to remove all errors\n\nThis error occurs when you switch the ruby version\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to install python packages without root privileges?", "id": 1224, "answers": [{"answer_id": 1217, "document_id": 800, "question_id": 1224, "text": "You can do that with a command such as\npip install --user numpy\nor from source\npython setup.py install --user", "answer_start": 653, "answer_category": null}], "is_impossible": false}], "context": "I am using numpy / scipy / pynest to do some research computing on Mac OS X. For performance, we rent a 400-node cluster (with Linux) from our university so that the tasks could be done parallel. The problem is that we are NOT allowed to install any extra packages on the cluster (no sudo or any installation tool), they only provide the raw python itself.\nHow can I run my scripts on the cluster then? Is there any way to integrate the modules (numpy and scipy also have some compiled binaries I think) so that it could be interpreted and executed without installing packages?\nYou don't need root privileges to install packages in your home directory. You can do that with a command such as\npip install --user numpy\nor from source\npython setup.py install --user\nSee https://stackoverflow.com/a/7143496/284795\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Java Jar file: use resource errors: URI is not hierarchical", "id": 1693, "answers": [{"answer_id": 1681, "document_id": 1266, "question_id": 1693, "text": "File f = new File(getClass().getResource(\"/MyResource\").toEx", "answer_start": 176, "answer_category": null}], "is_impossible": false}], "context": "If for some reason you really need to create a java.io.File object to point to a resource inside of a Jar file, the answer is here: https://stackoverflow.com/a/27149287/155167\nFile f = new File(getClass().getResource(\"/MyResource\").toEx\nI have deployed my app to jar file. When I need to copy data from one file of resource to outside of jar file, I do this code:\nURL resourceUrl = getClass().getResource(\"/resource/data.sav\");\nFile src = new File(resourceUrl.toURI()); //ERROR HERE\nFile dst = new File(CurrentPath()+\"data.sav\");  //CurrentPath: path of jar file don't include jar file name\nFileInputStream in = new FileInputStream(src);\nFileOutputStream out = new FileOutputStream(dst);\n // some excute code here\nThe error I have met is: URI is not hierarchical. this error I don't meet when run in IDE.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Wait on the Database Engine recovery handle failed. Check the SQL server error log for potential causes", "id": 803, "answers": [{"answer_id": 798, "document_id": 485, "question_id": 803, "text": "1.\tuninstall SQL Server\n2.\tin Regional Settings / Management / System Locale, \"Beta: UTF-8 support\" should be OFF\n3.\tre-install SQL Server\n4.\tLet Windows install the patch.\nAnd all was well.", "answer_start": 256, "answer_category": null}], "is_impossible": false}], "context": "This post is high up when you google that error message, which I got when installing security patch KB4505224 on SQL Server 2017 Express i.e. None of the above worked for me, but did consume several hours trying.\nThe solution for me, partly from here was:\n1.\tuninstall SQL Server\n2.\tin Regional Settings / Management / System Locale, \"Beta: UTF-8 support\" should be OFF\n3.\tre-install SQL Server\n4.\tLet Windows install the patch.\nAnd all was well.\nMore on this here.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "installing cygwin: setup.ini missing from http://mirrors.kernel.org", "id": 729, "answers": [{"answer_id": 732, "document_id": 419, "question_id": 729, "text": "The mirror should be the full path http://mirrors.kernel.org/sourceware/cygwin\nIf you get complaints about the .ini being from a newer version of setup, you'll need to find a newer setup.exe somewhere on the net that you trust.", "answer_start": 338, "answer_category": null}], "is_impossible": false}], "context": "I'm trying to install cygwin on a windows 2008 server. I managed to get a hold of the cygwin setup.exe version 2.721.\nSince cygwin.com is down at the moment, i tried several mirrors found through the google cache of the cygwin mirrors. I ran into the same problem like this guy: Help needed installing cygwin: may be ini file problem. 28\nThe mirror should be the full path http://mirrors.kernel.org/sourceware/cygwin\nIf you get complaints about the .ini being from a newer version of setup, you'll need to find a newer setup.exe somewhere on the net that you trust. (And cygwin guys? Grrr for not including setup.exe in your mirrors and for not signing your exe!)\n\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "how to specify the command-line options  for the Windows Java Runtime Environment?", "id": 359, "answers": [{"answer_id": 366, "document_id": 165, "question_id": 359, "text": "\njre [INSTALLCFG=configuration_file_path] [options]\njre refers to the JRE Windows Offline Installer base file name (for example, jre-8u05-windows-i586.exe).\n\nINSTALLCFG=configuration_file_path specifies the path of the installer configuration file. See \"Installing With a Configuration File\" for more information.\n\noptions are options with specified values separated by spaces. Use the same options as listed in Table 20-1, \"Configuration File Options\". In addition, you may use the option /s for the JRE Windows Offline Installer to perform a silent installation.", "answer_start": 1205, "answer_category": null}], "is_impossible": false}, {"question": "how to avoid JRE  being overwritten by a newer version for windows?", "id": 360, "answers": [{"answer_id": 367, "document_id": 165, "question_id": 360, "text": "(by specifying the command-line or configuration file option STATIC=1), then the Java Auto Update feature will leave that JRE installed during a Java update. A later version of the same JRE family will be installed in a separate directory.", "answer_start": 1839, "answer_category": null}], "is_impossible": false}, {"question": "when installing jre for windows, how to create a log file\uff1f", "id": 361, "answers": [{"answer_id": 368, "document_id": 165, "question_id": 361, "text": ". To create a log file describing the installation, append /L C:\\pathsetup.log to the install command and scroll to the end of the log file to verify.", "answer_start": 2590, "answer_category": null}], "is_impossible": false}], "context": "17 Windows JRE Installer Options\nThis page describes options for installing, configuring, and creating a log file for the Windows Java Runtime Environment.\n\nThis page contains the following topics:\n\n\"Introduction\"\n\n\"Command-Line Installation\"\n\n\"Static Installation\"\n\n\"Creating a Log File\"\n\nSee \"JDK 8 and JRE 8 Installation Start Here\" for general information about installing JDK 8 and JRE 8.\n\nIntroduction\nThis page describes options for installation of the Java SE Runtime Environment (JRE) on Windows 32-bit platform. It is intended for:\n\nSystem administrators deploying the JRE with Java Plug-in and Java Web Start technologies on multiple PCs in their Intranet without user interaction.\n\nVendors having products requiring the JRE. The JRE can be silently (non-interactively from the command line) installed with their product.\n\nJRE installers are built using Microsoft Window Installer (MSI) 2.0 technology. MSI contains built-in support for silent installations. This topic explains how to manually install the JRE using the .exe file that runs the MSI.\n\nCommand-Line Installation\nThis section describes the command-line options for the JRE Windows Offline Installer. Run the installer as follows:\n\njre [INSTALLCFG=configuration_file_path] [options]\njre refers to the JRE Windows Offline Installer base file name (for example, jre-8u05-windows-i586.exe).\n\nINSTALLCFG=configuration_file_path specifies the path of the installer configuration file. See \"Installing With a Configuration File\" for more information.\n\noptions are options with specified values separated by spaces. Use the same options as listed in Table 20-1, \"Configuration File Options\". In addition, you may use the option /s for the JRE Windows Offline Installer to perform a silent installation.\n\nStatic Installation\nIf you perform a static installation of the JRE (by specifying the command-line or configuration file option STATIC=1), then the Java Auto Update feature will leave that JRE installed during a Java update. A later version of the same JRE family will be installed in a separate directory. This mode ensures that vendors, who require a specific version of the JRE for their product, can be certain that the JRE will not be overwritten by a newer version.\n\nThe default installation directory of a static JRE is C:\\Program Files (x86)\\Java\\jren (for 32-bit versions) or C:\\Program Files\\Java\\jren (for 64-bit versions), where n is the full Java SE release and update number (for example, n = 1.8.0_20 for release 8 update 20).\n\nCreating a Log File\nUse a log file to verify that an installation succeeded. To create a log file describing the installation, append /L C:\\pathsetup.log to the install command and scroll to the end of the log file to verify.\n\nThe following is an example of creating a log file:\n\njre-8-windows-i586.exe /s /L C:\\pathsetup.log\nThis example causes the log to be written to the pathsetup.log file.", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Free software for Windows installers: NSIS vs. WiX?", "id": 776, "answers": [{"answer_id": 774, "document_id": 461, "question_id": 776, "text": "If you want to get an installer done today, with the minimum amount of overhead, use NSIS. Simple scripting language, good documentation, fast.\nIf you want to build MSI files, integrate with the Windows Installer transactional system, and have plenty of time to devote to learning the declarative model used by Windows Installer, then check out WiX.", "answer_start": 221, "answer_category": null}], "is_impossible": false}], "context": "I'm need to choose a software package for installing software. NSIS and WiX seem promising. Which one would you recommend over the other and why?\nFeel free to offer something else if you think it's better than these two.\nIf you want to get an installer done today, with the minimum amount of overhead, use NSIS. Simple scripting language, good documentation, fast.\nIf you want to build MSI files, integrate with the Windows Installer transactional system, and have plenty of time to devote to learning the declarative model used by Windows Installer, then check out WiX.\nYou should take a look at InstallJammer. Not only is it free, it's cross-platform and very easy to use. Most common actions don't require any scripting at all, but with a powerful scripting language underneath the hood, you can make an install do just about anything you want.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to change database design in a deployed application?", "id": 1262, "answers": [{"answer_id": 1254, "document_id": 833, "question_id": 1262, "text": "Firstly, many database designers create views to code against, rather than coding directly to the tables. This allows tables to be altered (split or merged, etc) while only requiring that the views are updated. You may want to investigate database refactoring techniques for this.\nSecondly, you can indeed add versioning information to the database (commonly done as a 'version' table with a single field). Updating the database can be done through code or through scripts. One system I worked on would automatically check the database version and then progressively update the schema through versions in code until it matched the required version for the runtime. This was quite an undertaking.", "answer_start": 769, "answer_category": null}], "is_impossible": false}], "context": "I'm creating a C#/WPF 4 application using a SQL Compact Edition database as a backend with the Entity Framework and deploying with ClickOnce.\nI'm fairly new to applications using databases, though I don't suspect I'll have much problem designing and building the original database. However, I'm worried that in the future I'll need to add or change some functionality which will require me to change the database design after the database is already deployed and the user has data in the database.\nQuestions\nIs it even possible to push an updated database design out to users via a clickonce update in the same way it is for code changes?\nIf I did, how would the user's data be affected?\nHow is this sort of thing done in real situations? What are some best-practices?\nFirstly, many database designers create views to code against, rather than coding directly to the tables. This allows tables to be altered (split or merged, etc) while only requiring that the views are updated. You may want to investigate database refactoring techniques for this.\nSecondly, you can indeed add versioning information to the database (commonly done as a 'version' table with a single field). Updating the database can be done through code or through scripts. One system I worked on would automatically check the database version and then progressively update the schema through versions in code until it matched the required version for the runtime. This was quite an undertaking.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Does the .Net Framework 4.0 Installer include the .Net Framework 3.5?", "id": 835, "answers": [{"answer_id": 830, "document_id": 517, "question_id": 835, "text": "On XP SP2 with FW 4 installed running FW3.5 application gives a message:\nUnable to find a version of the runtime to run this application.\nSo the answer is no", "answer_start": 218, "answer_category": null}], "is_impossible": false}], "context": "Do .Net components that were compiled against the .Net Framework 3.5 run on a system that has only .Net Framework 4.0 installed?\nOr in other words, does the .Net Framework 4.0 Installer include the .Net Framework 3.5? On XP SP2 with FW 4 installed running FW3.5 application gives a message:\nUnable to find a version of the runtime to run this application.\nSo the answer is no.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Mac install and open mysql using terminal?", "id": 859, "answers": [{"answer_id": 854, "document_id": 539, "question_id": 859, "text": "sudo sh -c 'echo /usr/local/mysql/bin > /etc/paths.d/mysql'", "answer_start": 1542, "answer_category": null}], "is_impossible": false}], "context": "The Treehouse Community is a meeting place for developers, designers, and programmers of all backgrounds and skill levels to get support. Collaborate here on code errors or bugs that you need feedback on, or asking for an extra set of eyes on your latest project. Join thousands of Treehouse students and alumni in the community today. (Note: Only Treehouse students can comment or ask questions, but non-students are welcome to browse our conversations.)\n\nLooking to learn something new?\nTreehouse offers a seven day free trial for new students. Get access to thousands of hours of content and a supportive community. Start your free trial today.\n\nRuby\nActiveRecord Basics\nMigrations and Relationships\nMigrations\nAndrew Carr\nPosted on Feb 19, 2015 by Andrew Carr\nSays: 'mysql' command not found\nHey! I've complete the Database Foundations block but so I'm 90% sure mysql should be recognized as a valid command, and I've also made sure it's running, but for some reason I can't perform the 'mysql -uroot' command. Any advice? Thanks.\n\nAndrew Carr\nAndrew Carr \non Feb 19, 2015\nOther posts have suggested using homebrew. But I'm hesitant to use it since I don't know why I should, per se, when I've already downloaded mysql (specifically mysql workbench and mysql 5.6). Am I missing anything here? Thanks, again!\n\nK Jo\nK Jo \non Mar 2, 2015\nI had this same issue. After trying a number of things to no avail, I found this link: http://apple.stackexchange.com/questions/98999/mysql-not-working-on-mountain-lion\n\nthen ran the suggested command:\n\nsudo sh -c 'echo /usr/local/mysql/bin > /etc/paths.d/mysql'\n\nIt will then prompt you for your password. After your enter that, press cmd+t to open a new terminal tab, then type mysql -uroot.\n\nI'm relatively new to this, so I don't really know why the above command worked, but i'm just happy it does for me since the issue was starting to get annoying. If anyone is able to break this command down and illustrate what it is doing, it would be helpful.\n\n3 Answers\nJohn MageePLUS\nJohn Magee \non Feb 19, 2015\nHomebrew is a good thing to install because it takes care of all of your updates on A LOT of things you might or might not install. Every day, something you've got running in your back ground may be updated or deleted and running a quick 'brew update' on your command line takes care of that (and you don't have to run it every day - but just saying - it does update things almost every day). New things that show up will be easier to install (usually) through homebrew.\n\nAlso - try mysql -u root -p (include the -p so it asks for the password, and keep the space, and make sure you mapped the mysql 'path' properly as well\n\nAndrew Carr\nAndrew Carr \non Feb 19, 2015\nHow do I ensure the mysql path is mapped properly?\n\nJohn Magee\nJohn Magee \non Feb 19, 2015\nAndrew\n\nI apologize, but that I can't tell you. I just remember from installing (years ago because I use it so much) on my computer that you had to write some code so that the mysql command mapped to the proper location of the mysql app.\n\nJohn Magee\nJohn Magee \non Feb 24, 2015\nAndrew - not sure if you're still looking - but you have to map the mysql folder to your $PATH variable. This requires you to know how to add to your $PATH variable and to know where you SQL executable is stored.\n\nIf you don't want to take the whole console course - this video should do the trick\n\nhttps://teamtreehouse.com/library/console-foundations/environment-and-redirection/environment-variables\n\nAndrew Carr\nAndrew Carr \non Feb 25, 2015\nHey! So I was thinking it'd involve that. That being said, I don't want to accidentally reassign something I'll significantly regret. That being said, I'm tempted to put in this command:\n\nls -s \"/usr/local/mysql/bin/mysql\" ~/bin/mysql\n\nThat being said, I have no guarantee that I've made the right association. Would you agree that is the right input?\n\nThanks!\n\nJohn Magee\nJohn Magee \non Feb 25, 2015\nAndrew - I don't know those commands - like I said - learning the console course - priarmily a mac guy never been a huge console user just followed the directions\n\nHere's what I'd do\n\nwhich mysql will tell you path to the mysql exe file\nFind your config file\nIn config file add text to change your $PATH variable (or bashrc file) For instance I have this line in my bashrc file for herokus export PATH=\"/usr/local/heroku/bin:$PATH\" Just substitute the path to your mySQL where my heroku path is.\nDo not forget :$PATH so as to append this to your already existing path.\n\nPosting to the forum is only allowed for members with active accounts.\nPlease sign in or sign up to post.", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "What interfaces does Django support", "id": 28, "answers": [{"answer_id": 30, "document_id": 44, "question_id": 28, "text": "Django currently supports two interfaces: WSGI and ASGI.\nWSGI is the main Python standard for communicating between Web servers and\napplications, but it only supports synchronous code.\nASGI is the new, asynchronous-friendly standard that will allow your\nDjango site to use asynchronous Python features, and asynchronous Django\nfeatures as they are developed.", "answer_start": 596, "answer_category": null}], "is_impossible": false}], "context": "Django is full of shortcuts to make Web developers\u2019 lives easier, but all\nthose tools are of no use if you can\u2019t easily deploy your sites. Since Django\u2019s\ninception, ease of deployment has been a major goal.\nThere are many options for deploying your Django application, based on your\narchitecture or your particular business needs, but that discussion is outside\nthe scope of what Django can give you as guidance.\nDjango, being a web framework, needs a web server in order to operate. And\nsince most web servers don\u2019t natively speak Python, we need an interface to\nmake that communication happen.\nDjango currently supports two interfaces: WSGI and ASGI.\nWSGI is the main Python standard for communicating between Web servers and\napplications, but it only supports synchronous code.\nASGI is the new, asynchronous-friendly standard that will allow your\nDjango site to use asynchronous Python features, and asynchronous Django\nfeatures as they are developed.\nYou should also consider how you will handle static files for your application, and how to handle\nFinally, before you deploy your application to production, you should run\nthrough our deployment checklist to ensure that your\nHow to use Django with Gunicorn\nHow to use Django with Apache and mod_wsgi\nAuthenticating against Django\u2019s user database from Apache\nConfiguring the settings module\nHow to use Django with Hypercorn\nConfiguring the settings module\nWriting a custom storage system", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Is virtualenv recommended for django production server?", "id": 329, "answers": [{"answer_id": 337, "document_id": 141, "question_id": 329, "text": " I would do it that way if you ever think you'll run more than one project on the webserver. As soon as you have two projects you run the risk of a future upgrade of any python package breaking the other site.", "answer_start": 146, "answer_category": null}], "is_impossible": false}], "context": "I have always been using virtualenv for testing my app in localhost since I have isolated environment and can safely test new release of packages. I would do it that way if you ever think you'll run more than one project on the webserver. As soon as you have two projects you run the risk of a future upgrade of any python package breaking the other site.", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Deploy to iPhone without running", "id": 1643, "answers": [{"answer_id": 1631, "document_id": 1217, "question_id": 1643, "text": "Xcode -> Product -> Scheme -> Edit Scheme -> Run -> Info -> Wait for executable to be launched", "answer_start": 110, "answer_category": null}], "is_impossible": false}], "context": "How do I deploy my app down to the iPhone (for testing purposes) from XCode without actually running the app? Xcode -> Product -> Scheme -> Edit Scheme -> Run -> Info -> Wait for executable to be launched. NOTE - To be fair, this answer is the same as the one for this question. I didn't mark this question as a duplicate though, because you might have a different reason for wanting to do this. Still, if you want a more detailed explanation, look there.", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "ALTER TABLE in Magento setup script without using SQL", "id": 723, "answers": [{"answer_id": 726, "document_id": 413, "question_id": 723, "text": " If you subscribe exclusively to the above two ideas, you should never be changing the data store.", "answer_start": 282, "answer_category": null}], "is_impossible": false}], "context": "I would like to know how best to add/modify/remove a column or index to/from a table in this manner, but without relying on SQL? Is it even possible?\nFurthermore, what other actions can only be done in SQL?\nSo, the problem is an ALTER TABLE statement directly changes the datastore. If you subscribe exclusively to the above two ideas, you should never be changing the data store. (which, in the case of adding a column or an index means using EAV models exclusively, using the Setup Resources to manage changes, and accepting Magento's indexing).\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "jar file is not install on the samsung mobiles but install on the nokia", "id": 1995, "answers": [{"answer_id": 1981, "document_id": 1579, "question_id": 1995, "text": "It's not jar problem, but server problem: your server is not configured to accept an unsigned jar file in case of samsung handset. Add some permissions in header of server page to download jar file on samsung set like: \n\nContent-Type: application/java-archive jar.", "answer_start": 2123, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI am facing a very strange problem that my J2ME application .jar file  is downloaded normally on the Nokia mobiles but on Samsung and Motorola it is not downloaded. But if i install the .jar without downloading on the Samsung or any other handset it worked.\nIf some has any idea then guide.\nLink of my app is m.hellosunshine.in   \n    \n\nYour server is configured to reply with \n\nContent-Type: text/plain\n\n\nfor the jar file (http://m.hellosunshine.in/J2ME/HelloSunShine.jar), but it should be configured to reply with\n\nContent-Type: application/java-archive jar\n\n\nSee e.g. http://www.summet.com/blog/2007/11/04/mime-types-for-hosting-j2me-jar-and-jad-files/\n    \n\nIt very simple they are 3 possibilities \n\n\nMost Samsung do not support J2ME applications\nFor Samsung phones that support J2ME .. moths times to install an application it required both .JAD and .JAR files .. on the phone .. Lunch the .jad when both files are on the phone or your url must point to the .jad file\n\n\nA typical Content type looks like this \n\napplication/java-archive .jar\ntext/vnd.sun.j2me.app-descriptor .jad\n\n\n\nWhen you JAR file is higher than specified application memory it does not run \n\n\nThanks\n:)\n    \n\nEach Device has a specified amount of Heap Memory. A Heap Memory is a kind of RAM for Jar file to run. I Believe that your Samsung device doesn't have enough Heap Memory. \n\nMost of the devices has 2MB Heap Memory for running Jar Files.\n\nNokia E-Series Phones allows unlimited Heap Memory, so you can easily execute your 1.5 MB Jar file in any E-Series mobile. \n\nThere is a utility called ProGuard. It allows your Jar file to compress in such a tiny format that you can run it easily. It will reduce your Jar file to 300-400KB and then you can easily run your application in Samsung Device too.\n    \n\nTry to reduce the size of the jar file\n\nalso verify the content in jad file if all the details provided are correct\n    \n\nthis problem not in jar but in code which direct it to download so in header of file add content signed and follow this link ( How to Force a file to download using PHP on mobile Browsers?)\n    \n\nIt's not jar problem, but server problem: your server is not configured to accept an unsigned jar file in case of samsung handset. Add some permissions in header of server page to download jar file on samsung set like: \n\nContent-Type: application/java-archive jar.\n\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Inno Setup: How to automatically uninstall previous installed version?", "id": 674, "answers": [{"answer_id": 679, "document_id": 367, "question_id": 674, "text": " attempt to remove previous versions' icons\n[InstallDelete]\nType: filesandordirs; Name: {group}\\*;", "answer_start": 270, "answer_category": null}], "is_impossible": false}], "context": "I'm using Inno Setup to create an installer.\nI want the installer to automatically uninstall the previous installed version, instead of overwriting it. How can I do that?\nIf you \"just want to remove the old icons\" (because yours have changed/updated) you can use this:\n; attempt to remove previous versions' icons\n[InstallDelete]\nType: filesandordirs; Name: {group}\\*;\nThis is run \"at the beginning of installation\" so basically removes the old icons, and your new ones will still be installed there after this is completely done.\nI just do this with every install \"in case anything has changed\" icon wise (it all gets reinstalled anyway).\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How can I deploy a .NET application that uses ODAC without installing the whole component to the user?", "id": 557, "answers": [{"answer_id": 559, "document_id": 282, "question_id": 557, "text": "If the concern is about having to install the Oracle client and the ODAC, you can use the Oracle Instant Client? That's the smallest footprint method for installing the Oracle client. You'll also need the ODAC xcopy supplement.", "answer_start": 339, "answer_category": null}], "is_impossible": false}], "context": "I have written a C# application that connects to an Oracle 10g database. Using Oracle Data Access Component 11.2 \"ODAC\", it works perfectly on my machine.\nAnd now I want to deploy the application and install it in another \"clean machine\" that has the .NET Framework only! And I don't want to install the whole ODAC component to the user! \nIf the concern is about having to install the Oracle client and the ODAC, you can use the Oracle Instant Client? That's the smallest footprint method for installing the Oracle client. You'll also need the ODAC xcopy supplement.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Alternative to HttpUtility for .NET 3.5 SP1 client framework?", "id": 1301, "answers": [{"answer_id": 1292, "document_id": 871, "question_id": 1301, "text": "You can use System.Net library in C# 4.0 Client Profile:\nUri.EscapeDataString(...)\nWebUtility.HtmlEncode(...)", "answer_start": 602, "answer_category": null}], "is_impossible": false}], "context": "It'd be really nice to target my Windows Forms app to the .NET 3.5 SP1 client framework. But, right now I'm using the HttpUtility.HtmlDecode and HttpUtility.UrlDecode functions, and the MSDN documentation doesn't point to any alternatives inside of, say, System.Net or something.\nSo, short from reflectoring the source code and copying it into my assembly---which I don't think would be worth it---are there alternatives inside of the .NET 3.5 SP1 client framework that you know of, to replace this functionality? It seems a bit strange that they'd restrict these useful functions to server-only code.\nYou can use System.Net library in C# 4.0 Client Profile:\nUri.EscapeDataString(...)\nWebUtility.HtmlEncode(...)\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Best practices for deploying Java webapps with minimal downtime?", "id": 473, "answers": [{"answer_id": 482, "document_id": 206, "question_id": 473, "text": "This is dependant on your application architecture.One of my applications sits behind a load-balancing proxy, where I perform a staggered deployment - effectively eradicating downtime.", "answer_start": 196, "answer_category": null}], "is_impossible": false}], "context": "The rsync minimizes the amount of data sent from the development machine to the live environment. Uploading the entire .war file takes over ten minutes, whereas an rsync takes a couple of seconds.This is dependant on your application architecture.One of my applications sits behind a load-balancing proxy, where I perform a staggered deployment - effectively eradicating downtime.", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "wix retaining registry settings on major upgrade", "id": 1931, "answers": [{"answer_id": 1918, "document_id": 1505, "question_id": 1931, "text": "cts is scheduled?\n    \n\nUse RegistrySearch to load the current values into properties then use [PROPERTY] in RegistryValue to write those values. If there isn't an older product installed, the properties will ke", "answer_start": 1861, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nWe are using WiX 3.5 to build an installer for one of our products. For simplicity, we handle version upgrades via a major upgrade, like so:\n\n&lt;MajorUpgrade AllowSameVersionUpgrades=\"yes\" DowngradeErrorMessage=\"Laterversionfound\" /&gt;\n\n\nWe are not specifying the Schedule attribute, which means the RemoveExistingProducts action should run after \"InstallValidate\" - meaning a full uninstall of the old version will take place, before installing the new version.\n\nWe install some HKLM registry settings, which the user must configure following installation. Because the major upgrade performs a full uninstall followed by a reinstall, we are losing the user-defined settings in the registry. Ideally, we need to be able to keep these across upgrades.\n\nMy registry key components look like this:\n\n&lt;Component Id=\"regserver\" Guid=\"[guid]\"&gt;\n    &lt;RegistryValue Root=\"HKLM\" Key=\"Software\\Our Company\\Our Product\" Name=\"Server\" Value=\"\" Type=\"string\" KeyPath=\"yes\" /&gt;\n&lt;/Component&gt;\n\n\nI've tried setting the NeverOverwrite property of the components to \"yes\" but this has the unfortunate effect of failing to recreate the keys - presumably because it checks to see if the keys exist before the uninstall happens (which obviously they do), then they get removed with the uninstall, but not recreated again.\n\nI've also tried setting the \"RemoveFeatures\" attribute on the MajorUpgrade element to remove everything but the reg keys. This leaves two versions of the product installed though, as the feature containing the reg keys belongs to the old version.\n\nMy next step is to try scheduling the RemoveExistingProducts option at a different point, although i'm expecting a certain amount of pain with some of our custom actions.\n\nSo my question is, is there any way to achieve what we need, without changing where RemoveExistingProducts is scheduled?\n    \n\nUse RegistrySearch to load the current values into properties then use [PROPERTY] in RegistryValue to write those values. If there isn't an older product installed, the properties will keep their default values.\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "wix installer should always run as administrator", "id": 1983, "answers": [{"answer_id": 1969, "document_id": 1568, "question_id": 1983, "text": "which required administrator privileges.\n\nUsing CustomAction \u2192 Impersonate=\"no\" worked for me as mentioned in post ht", "answer_start": 1573, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI developed a custom installer with WiX for a .NET WPF application. It works fine if I right-click and run as administrator, however when running without, some components fail to install due to insufficient privileges.\n\nThe components include SQL Server Express 2008 R2, FoxIt Reader, an ActiveX component and some others. It also requires that some SQL scripts are ran on the newly installed database - anyway, they all require administrator privileges.\n\n\n\nI tried adding the InstallScope=\"perMachine\" and InstallPrivileges=\"elevated\" attributes to the Package node, but this didn't seem to make a difference.\n\nI'm sure it's something silly, but I couldn't find anything in the reference or online.\n    \n\nI think if you just add\n\n&lt;Property Id=\"MSIUSEREALADMINDETECTION\" Value=\"1\" /&gt;\n\n\nit should solve the problem. Let me know if not and I can do some more checking.\n    \n\nAdd this to your package element\n\n&lt;Property Id=\"ALLUSERS\" Value=\"1\" /&gt;    &lt;!--equals to install=\"permachine\" at package element but this element depricated --&gt;\n&lt;Property Id=\"MSIUSEREALADMINDETECTION\" Value=\"1\" /&gt; \n\n&lt;Condition Message=\"Please Run as Administrator.\"&gt;\n      Privileged\n&lt;/Condition&gt;\n\n\nThen creating a simple sfx archive file for msi file with Winrar and these options: \n\n\nSetup tab &gt; Run after execution input: your msi file name\nAdvanced tab &gt; Mark Request Administrative access option checkbox\n\n    \n\nFor me I was supposed to run a registry command to delete a system environment variable via the CustomAction WiX element, which required administrator privileges.\n\nUsing CustomAction \u2192 Impersonate=\"no\" worked for me as mentioned in post https://stackoverflow.com/a/8657472/3205679.\n\nWiX Custom Action code:\n\n&lt;CustomAction Id = \"Uninstall_MYSYSENV\"\n              Directory  = \"INSTALLFOLDER\"\n              ExeCommand = 'cmd.exe /c &amp;quot;reg delete       \"HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Control\\Session    Manager\\Environment\" /v MYSYSENV /f&amp;quot;'\n              Execute    = \"deferred\"\n              Impersonate= \"no\"\n              Return     = \"asyncNoWait\"\n              /&gt;\n\n&lt;InstallExecuteSequence&gt;\n      &lt;Custom Action=\"Uninstall_MYSYSENV\"\n              After=\"InstallInitialize\"&gt;Installed AND NOT UPGRADINGPRODUCTCODE&lt;/Custom&gt;\n&lt;/InstallExecuteSequence&gt;\n\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Windows SDK 7.1 Setup failure", "id": 652, "answers": [{"answer_id": 657, "document_id": 345, "question_id": 652, "text": "In order to deal with this problem, I uninstalled my .NET framework version 4.6 and installed 4. Then I installed the SDK, and the problem was gone.Change both values temporarily to 4.0.30319 and the setup will let you continue. ", "answer_start": 1162, "answer_category": null}], "is_impossible": false}], "context": "I am trying to install Windows SDK for Windows 7 with .NET Framework 4 but when I open the setup I receive an error:\nSome Windows SDK components require the RTM .NET Framework 4. Setup detected a pre-release version of .NET Framework 4. If you continue with Setup, these components will not be installed. If you want to install these components, click Cancel, then install the .NET Framework 4 from https://go.microsoft.com/fwlink/?LinkID=187668 and then rerun Setup.\nClick OK to continue.\nWhen I went to install the .NET Framework 4 it appears a message saying that there is already the .NET Framework 4 on my PC:\nThe Microsoft .NET Framework 4 is already part of the operating system. No need to install the .NET Framework 4 redistributable. More information.\nAn equal or higher version of the .NET Framework 4 has already been installed on the computer.\nI don't know what to do anymore. I am using Windows 10 Enterprise (x64).\n107\nWith Windows 10 x64, the setup is blocked by:\nHKEY_LOCAL_MACHINE\\SOFTWARE\\Wow6432Node\\Microsoft\\NET Framework Setup\\NDP\\v4\\Full\\Version\nHKEY_LOCAL_MACHINE\\SOFTWARE\\Wow6432Node\\Microsoft\\NET Framework Setup\\NDP\\v4\\Client\\Version\nIn order to deal with this problem, I uninstalled my .NET framework version 4.6 and installed 4. Then I installed the SDK, and the problem was gone.Change both values temporarily to 4.0.30319 and the setup will let you continue. Make sure you edit the registry with elevated privileges, otherwise you will not be allowed to change the values.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Maven Installation OSX Error Unsupported major.minor version 51.0", "id": 643, "answers": [{"answer_id": 648, "document_id": 336, "question_id": 643, "text": "export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home", "answer_start": 2693, "answer_category": null}], "is_impossible": false}], "context": "I installed maven by following this steps:(a tutorial)\nJAVA_HOME=/Library/Java/Home\nexport M2_HOME=/Users/steven/zimmermann/maven\nexport M2=$M2_HOME/bin\nexport PATH=$M2:$PATH\n\necho $JAVA_HOME\necho $M2_HOME\necho $M2\necho $PATH\n\nnano .bash_profile\nthen I wrote the echo in the .bash_profile sth like this:\nJAVA_HOME=/usr/libexec/java_home\nM2_HOME=/path/to/your/apache-maven-3.x.x\nM2=/path/to/your/apache-maven-3.x.x/bin\nPATH=/path/to/maven/bin:/$\u2026.bla-bla-bla\u2026\nand also I wrote this in the .bashrc\nexport M2_HOME=/Users/steven/zimmermann/maven\nexport M2=$M2_HOME/bin\nexport PATH=$M2:$PATH\nnow when I want to check the version (mvn -v) I get the following exception: I think there are some versions wrong, but I don't know.\nException in thread \"main\" java.lang.UnsupportedClassVersionError: org/apache/maven/cli/MavenCli : Unsupported major.minor version 51.0\n    at java.lang.ClassLoader.defineClass1(Native Method)\n    at java.lang.ClassLoader.defineClassCond(ClassLoader.java:637)\n    at java.lang.ClassLoader.defineClass(ClassLoader.java:621)\n    at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:141)\n    at java.net.URLClassLoader.defineClass(URLClassLoader.java:283)\n    at java.net.URLClassLoader.access$000(URLClassLoader.java:58)\n    at java.net.URLClassLoader$1.run(URLClassLoader.java:197)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at java.net.URLClassLoader.findClass(URLClassLoader.java:190)\n    at org.codehaus.plexus.classworlds.realm.ClassRealm.loadClassFromSelf(ClassRealm.java:401)\n    at org.codehaus.plexus.classworlds.strategy.SelfFirstStrategy.loadClass(SelfFirstStrategy.java:42)\n    at org.codehaus.plexus.classworlds.realm.ClassRealm.unsynchronizedLoadClass(ClassRealm.java:271)\n    at org.codehaus.plexus.classworlds.realm.ClassRealm.loadClass(ClassRealm.java:254)\n    at org.codehaus.plexus.classworlds.realm.ClassRealm.loadClass(ClassRealm.java:239)\n    at org.codehaus.plexus.classworlds.launcher.Launcher.getMainClass(Launcher.java:144)\n    at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:266)\n    at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)\n    at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)\n    at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)\nAdditional information:\njava -version\njava version \"1.8.0_40\"\nJava(TM) SE Runtime Environment (build 1.8.0_40-b27)\nJava HotSpot(TM) 64-Bit Server VM (build 25.40-b25, mixed mode)\njavac -version\njavac 1.8.0_40\n275\nThe problem is because you haven't set JAVA_HOME in Mac properly. In order to do that, you should do set it like this:\nexport JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home\nIn my case my JDK installation is jdk1.8.0_40, make sure you type yours.\nThen you can use maven commands.\nRegards!\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "sphinx quickstart doesnt work on MACOS?", "id": 1913, "answers": [{"answer_id": 1900, "document_id": 1485, "question_id": 1913, "text": "tion:\nInstalled it with conda as conda install sphinx and now it works ", "answer_start": 1648, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI am trying to install sphinx on a remote machine. \n\nSince I don't have an access to the root, I did this:\n\n$bash\n\n$mkdir -p ~/local/lib/python2.7/site-packages\n\n$export PYTHONPATH=$PYTHONPATH:~/local/lib/python2.7/site-packages\n\n$export PATH=$PATH::~/local/lib/python2.7/site-packages\n\n$easy_install -U --prefix=$HOME/local Sphinx\n\n\nBut apparently, $easy_install doesn't build sphinx-quickstart; when I type \n\n$sphinx-quickstart\n\n\nI get the following message:\n\nbash: sphinx-quickstart: command not found\n\n\nI tried \n\nfind $HOME -name sphinx-quickstart \n\n\nand no result was found. However, I can import sphinx inside python:\n\n$python\n\n\nAnd then\n\n&gt;&gt;import sphinx \n\n\nworks. Any idea why sphinx-quickstart doesn't work?\n    \n\nAn alternative way to invoke sphinx-quickstart is to explicitly load Sphinx's quickstart module. For Sphinx v1.7+:\n\npython -m sphinx.cmd.quickstart\n\n\nFor older versions of Sphinx:\n\npython -m sphinx.quickstart\n\n\nFor example:\n\n$ /c/Python35/python -m sphinx.quickstart\nWelcome to the Sphinx 1.6.2 quickstart utility.\n...\n\n    \n\nI found the solution in this webpage:\n\n\n  \n  User (root/sudo free) installation of Python modules.\n\n\nIn section 3. Python 2.6+ he mentioned that the command line commands are in \n\n~/local/bin\n\n\nAlthough I had put ~/local/lib/python2.7/siste-packages in the path, the ~/local/bin directory was not in the path. So all I did\n\n$export PYTHONPATH=$PYTHONPATH:~/local/bin\n\n\nand now it works.\n\nI don't know why find $HOME -name sphinx-quickstart did not find sphinx-quickstart\n    \n\nIn MacOS (Catalina) and zsh:\n\nI installed sphinx with brew and had the same problem as you.\n\nSolution:\nInstalled it with conda as conda install sphinx and now it works for me\n\n$ sphinx-quickstart\nWelcome to the Sphinx 2.3.0 quickstart utility.\n\n\nInstallation Guide\n    \n\nNote, if you're in a virtual environment and using poetry to run\npoetry run py -m sphinx.cmd.quickstart\nwhere py is the python launcher for Windows.\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Failed to load c++ bson extension", "id": 1834, "answers": [{"answer_id": 1820, "document_id": 1405, "question_id": 1834, "text": "You just need to update npm based on @tobias comment (after installing build-essential)\nCommand:npm update", "answer_start": 135, "answer_category": null}], "is_impossible": false}], "context": "A total node noob here. I've been trying to set up a sample node app but the following error keeps popping up every time I try to run:\nYou just need to update npm based on @tobias comment (after installing build-essential)\nCommand:npm update\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to add a whole directory or project output to WiX package", "id": 732, "answers": [{"answer_id": 733, "document_id": 420, "question_id": 732, "text": "\"$(WIX)bin\\heat.exe\" dir \"$(SourcePath)\" -cg MyFiles -gg -scom -sreg -sfrag -dr INSTALLDIR -out \"$(ProjectDir)Fragments\\FileFragment.wxs\" -var wix.InstallerPath", "answer_start": 868, "answer_category": null}], "is_impossible": false}], "context": "However, what we currently do is use projects output files as the input for the setup project. This lets us easily add Application Files to a directory (for images, samples, and other resources...) and those files are automatically added to the setup when we build.\nI could not find any similar feature in WiX. WiX seems to require one Directory entry and one File entry for each and every directory and file. This would require us to change the WiX source everytime a file is added which, to my eyes, is prohibitive since we have so many of them.\nIs there any integrated way of doing that with WiX or do I have to write my own task that will create a WiX source before calling candle?\nI've been using heat.exe in WIX 3.5 just for that purpose. Last time I checked though, the documentation wasn't up-to-date with 3.5 release so keep that in mind.\nHere is an example:\n\"$(WIX)bin\\heat.exe\" dir \"$(SourcePath)\" -cg MyFiles -gg -scom -sreg -sfrag -dr INSTALLDIR -out \"$(ProjectDir)Fragments\\FileFragment.wxs\" -var wix.InstallerPath\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "What is a CursorResult in SQLAlchemy?", "id": 346, "answers": [{"answer_id": 353, "document_id": 158, "question_id": 346, "text": "The CursorResult class is a subclass of\nResult which contains additional attributes that are\nspecific to the DBAPI cursor object. ", "answer_start": 9552, "answer_category": null}], "is_impossible": false}, {"question": "How to know whether the CursorResult.rowcount is supported?", "id": 347, "answers": [{"answer_id": 354, "document_id": 158, "question_id": 347, "text": "The\nCursorResult.supports_sane_rowcount will indicate this.", "answer_start": 10626, "answer_category": null}], "is_impossible": false}, {"question": "What does delete in SQLAlchemy do?", "id": 348, "answers": [{"answer_id": 355, "document_id": 158, "question_id": 348, "text": "Delete which represents a DELETE statement in SQL, that will\ndelete rows from a table.", "answer_start": 7786, "answer_category": null}], "is_impossible": false}, {"question": "What does update in SQLAlchemy do?", "id": 349, "answers": [{"answer_id": 356, "document_id": 158, "question_id": 349, "text": "Update which represents an UPDATE statement in SQL, that will\nupdate existing data in a table.", "answer_start": 2592, "answer_category": null}], "is_impossible": false}], "context": "\n\n\n\n\n\nUpdating and Deleting Rows with Core\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhome\nfeatures\n\nFeature Overview\nTestimonials\n\n\nblog\nlibrary\n\nReference\n\nReference Documentation\n\n\nArchitecture\nCiting SQLAlchemy\nBlog Posts\nTalks\nTutorials\nRecipes\n\n\ncommunity\n\nGet Support\nParticipate\nDevelop\nCode of Conduct\nGithub\n\n\ndownload\n\nCurrent Release Series (1.4)\nMaintenance Release (1.3)\nDevelopment Access\nLicense\nVersion Numbering\nRelease Status\n\n\n\n\n\n\n\n\n\n\nRelease: 1.4.25\ncurrent release\n\n| Release Date: September 22, 2021\n\n\nSQLAlchemy 1.4 Documentation\n\n\n\n\n\nSQLAlchemy 1.4 Documentation\ncurrent release\n\nContents |\nIndex\n| Download as ZIP file\n\n\n\n\nSearch terms:\n\n\n\n\n\n\n\n\n\n\n\n\n\nSQLAlchemy 1.4 / 2.0 Tutorial\n\n\nEstablishing Connectivity - the Engine\nWorking with Transactions and the DBAPI\nWorking with Database Metadata\nWorking with Data\nInserting Rows with Core\nSelecting Rows with Core or ORM\nUpdating and Deleting Rows with Core\u00b6\nThe update() SQL Expression Construct\nCorrelated Updates\nUPDATE..FROM\nParameter Ordered Updates\n\n\nThe delete() SQL Expression Construct\nMultiple Table Deletes\n\n\nGetting Affected Row Count from UPDATE, DELETE\nUsing RETURNING with UPDATE, DELETE\nFurther Reading for UPDATE, DELETE\n\n\n\n\nData Manipulation with the ORM\nWorking with Related Objects\nFurther Reading\n\nProject Versions\n\n1.4.25\n\n\n\n\n\n\nSQLAlchemy 1.4 / 2.0 Tutorial\nThis page is part of the SQLAlchemy 1.4 / 2.0 Tutorial.\nPrevious: Selecting Rows with Core or ORM   |   Next: Data Manipulation with the ORM\n\n\nUpdating and Deleting Rows with Core\u00b6\nSo far we\u2019ve covered Insert, so that we can get some data into\nour database, and then spent a lot of time on Select which\nhandles the broad range of usage patterns used for retrieving data from the\ndatabase.   In this section we will cover the Update and\nDelete constructs, which are used to modify existing rows\nas well as delete existing rows.    This section will cover these constructs\nfrom a Core-centric perspective.\n\nORM Readers - As was the case mentioned at Inserting Rows with Core,\nthe Update and Delete operations when used with\nthe ORM are usually invoked internally from the Session\nobject as part of the unit of work process.\nHowever, unlike Insert, the Update and\nDelete constructs can also be used directly with the ORM,\nusing a pattern known as \u201cORM-enabled update and delete\u201d; for this reason,\nfamiliarity with these constructs is useful for ORM use.  Both styles of\nuse are discussed in the sections Updating ORM Objects and\nDeleting ORM Objects.\n\n\nThe update() SQL Expression Construct\u00b6\nThe update() function generates a new instance of\nUpdate which represents an UPDATE statement in SQL, that will\nupdate existing data in a table.\nLike the insert() construct, there is a \u201ctraditional\u201d form of\nupdate(), which emits UPDATE against a single table at a time and\ndoes not return any rows.   However some backends support an UPDATE statement\nthat may modify multiple tables at once, and the UPDATE statement also\nsupports RETURNING such that columns contained in matched rows may be returned\nin the result set.\nA basic UPDATE looks like:\n>>> from sqlalchemy import update\n>>> stmt = (\n...     update(user_table).where(user_table.c.name == 'patrick').\n...     values(fullname='Patrick the Star')\n... )\n>>> print(stmt)\nUPDATE user_account SET fullname=:fullname WHERE user_account.name = :name_1\n\n\nThe Update.values() method controls the contents of the SET elements\nof the UPDATE statement.  This is the same method shared by the Insert\nconstruct.   Parameters can normally be passed using the column names as\nkeyword arguments.\nUPDATE supports all the major SQL forms of UPDATE, including updates against expressions,\nwhere we can make use of Column expressions:\n>>> stmt = (\n...     update(user_table).\n...     values(fullname=\"Username: \" + user_table.c.name)\n... )\n>>> print(stmt)\nUPDATE user_account SET fullname=(:name_1 || user_account.name)\n\n\nTo support UPDATE in an \u201cexecutemany\u201d context, where many parameter sets will\nbe invoked against the same statement, the bindparam()\nconstruct may be used to set up bound parameters; these replace the places\nthat literal values would normally go:\n>>> from sqlalchemy import bindparam\n>>> stmt = (\n...   update(user_table).\n...   where(user_table.c.name == bindparam('oldname')).\n...   values(name=bindparam('newname'))\n... )\n>>> with engine.begin() as conn:\n...   conn.execute(\n...       stmt,\n...       [\n...          {'oldname':'jack', 'newname':'ed'},\n...          {'oldname':'wendy', 'newname':'mary'},\n...          {'oldname':'jim', 'newname':'jake'},\n...       ]\n...   )\nBEGIN (implicit)\nUPDATE user_account SET name=? WHERE user_account.name = ?\n[...] (('ed', 'jack'), ('mary', 'wendy'), ('jake', 'jim'))\n<sqlalchemy.engine.cursor.CursorResult object at 0x...>\nCOMMIT\n\n\nOther techniques which may be applied to UPDATE include:\n\nCorrelated Updates\u00b6\nAn UPDATE statement can make use of rows in other tables by using a\ncorrelated subquery.  A subquery may be used\nanywhere a column expression might be placed:\n>>> scalar_subq = (\n...   select(address_table.c.email_address).\n...   where(address_table.c.user_id == user_table.c.id).\n...   order_by(address_table.c.id).\n...   limit(1).\n...   scalar_subquery()\n... )\n>>> update_stmt = update(user_table).values(fullname=scalar_subq)\n>>> print(update_stmt)\nUPDATE user_account SET fullname=(SELECT address.email_address\nFROM address\nWHERE address.user_id = user_account.id ORDER BY address.id\nLIMIT :param_1)\n\n\n\n\nUPDATE..FROM\u00b6\nSome databases such as PostgreSQL and MySQL support a syntax \u201cUPDATE FROM\u201d\nwhere additional tables may be stated directly in a special FROM clause. This\nsyntax will be generated implicitly when additional tables are located in the\nWHERE clause of the statement:\n>>> update_stmt = (\n...    update(user_table).\n...    where(user_table.c.id == address_table.c.user_id).\n...    where(address_table.c.email_address == 'patrick@aol.com').\n...    values(fullname='Pat')\n...  )\n>>> print(update_stmt)\nUPDATE user_account SET fullname=:fullname FROM address\nWHERE user_account.id = address.user_id AND address.email_address = :email_address_1\n\n\nThere is also a MySQL specific syntax that can UPDATE multiple tables. This\nrequires we refer to Table objects in the VALUES clause in\norder to refer to additional tables:\n>>> update_stmt = (\n...    update(user_table).\n...    where(user_table.c.id == address_table.c.user_id).\n...    where(address_table.c.email_address == 'patrick@aol.com').\n...    values(\n...        {\n...            user_table.c.fullname: \"Pat\",\n...            address_table.c.email_address: \"pat@aol.com\"\n...        }\n...    )\n...  )\n>>> from sqlalchemy.dialects import mysql\n>>> print(update_stmt.compile(dialect=mysql.dialect()))\nUPDATE user_account, address\nSET address.email_address=%s, user_account.fullname=%s\nWHERE user_account.id = address.user_id AND address.email_address = %s\n\n\n\n\nParameter Ordered Updates\u00b6\nAnother MySQL-only behavior is that the order of parameters in the SET clause\nof an UPDATE actually impacts the evaluation of each expression.   For this use\ncase, the Update.ordered_values() method accepts a sequence of\ntuples so that this order may be controlled 2:\n>>> update_stmt = (\n...     update(some_table).\n...     ordered_values(\n...         (some_table.c.y, 20),\n...         (some_table.c.x, some_table.c.y + 10)\n...     )\n... )\n>>> print(update_stmt)\nUPDATE some_table SET y=:y, x=(some_table.y + :y_1)\n\n\n\n2\nWhile Python dictionaries are\nguaranteed to be insert ordered\nas of Python 3.7, the\nUpdate.ordered_values() method still provides an additional\nmeasure of clarity of intent when it is essential that the SET clause\nof a MySQL UPDATE statement proceed in a specific way.\n\n\n\n\n\nThe delete() SQL Expression Construct\u00b6\nThe delete() function generates a new instance of\nDelete which represents a DELETE statement in SQL, that will\ndelete rows from a table.\nThe delete() statement from an API perspective is very similar to\nthat of the update() construct, traditionally returning no rows but\nallowing for a RETURNING variant on some database backends.\n>>> from sqlalchemy import delete\n>>> stmt = delete(user_table).where(user_table.c.name == 'patrick')\n>>> print(stmt)\nDELETE FROM user_account WHERE user_account.name = :name_1\n\n\n\nMultiple Table Deletes\u00b6\nLike Update, Delete supports the use of correlated\nsubqueries in the WHERE clause as well as backend-specific multiple table\nsyntaxes, such as DELETE FROM..USING on MySQL:\n>>> delete_stmt = (\n...    delete(user_table).\n...    where(user_table.c.id == address_table.c.user_id).\n...    where(address_table.c.email_address == 'patrick@aol.com')\n...  )\n>>> from sqlalchemy.dialects import mysql\n>>> print(delete_stmt.compile(dialect=mysql.dialect()))\nDELETE FROM user_account USING user_account, address\nWHERE user_account.id = address.user_id AND address.email_address = %s\n\n\n\n\n\nGetting Affected Row Count from UPDATE, DELETE\u00b6\nBoth Update and Delete support the ability to\nreturn the number of rows matched after the statement proceeds, for statements\nthat are invoked using Core Connection, i.e.\nConnection.execute(). Per the caveats mentioned below, this value\nis available from the CursorResult.rowcount attribute:\n>>> with engine.begin() as conn:\n...     result = conn.execute(\n...         update(user_table).\n...         values(fullname=\"Patrick McStar\").\n...         where(user_table.c.name == 'patrick')\n...     )\n...     print(result.rowcount)\nBEGIN (implicit)\nUPDATE user_account SET fullname=? WHERE user_account.name = ?\n[...] ('Patrick McStar', 'patrick')\n1\nCOMMIT\n\n\n\nTip\nThe CursorResult class is a subclass of\nResult which contains additional attributes that are\nspecific to the DBAPI cursor object.  An instance of this subclass is\nreturned when a statement is invoked via the\nConnection.execute() method. When using the ORM, the\nSession.execute() method returns an object of this type for\nall INSERT, UPDATE, and DELETE statements.\n\nFacts about CursorResult.rowcount:\n\nThe value returned is the number of rows matched by the WHERE clause of\nthe statement.   It does not matter if the row were actually modified or not.\nCursorResult.rowcount is not necessarily available for an UPDATE\nor DELETE statement that uses RETURNING.\nFor an executemany execution,\nCursorResult.rowcount may not be available either, which depends\nhighly on the DBAPI module in use as well as configured options.  The\nattribute CursorResult.supports_sane_multi_rowcount indicates\nif this value will be available for the current backend in use.\nSome drivers, particularly third party dialects for non-relational databases,\nmay not support CursorResult.rowcount at all.   The\nCursorResult.supports_sane_rowcount will indicate this.\n\u201crowcount\u201d is used by the ORM unit of work process to validate that\nan UPDATE or DELETE statement matched the expected number of rows, and is\nalso essential for the ORM versioning feature documented at\nConfiguring a Version Counter.\n\n\n\nUsing RETURNING with UPDATE, DELETE\u00b6\nLike the Insert construct, Update and Delete\nalso support the RETURNING clause which is added by using the\nUpdate.returning() and Delete.returning() methods.\nWhen these methods are used on a backend that supports RETURNING, selected\ncolumns from all rows that match the WHERE criteria of the statement\nwill be returned in the Result object as rows that can\nbe iterated:\n>>> update_stmt = (\n...     update(user_table).where(user_table.c.name == 'patrick').\n...     values(fullname='Patrick the Star').\n...     returning(user_table.c.id, user_table.c.name)\n... )\n>>> print(update_stmt)\nUPDATE user_account SET fullname=:fullname\nWHERE user_account.name = :name_1\nRETURNING user_account.id, user_account.name\n>>> delete_stmt = (\n...     delete(user_table).where(user_table.c.name == 'patrick').\n...     returning(user_table.c.id, user_table.c.name)\n... )\n>>> print(delete_stmt)\nDELETE FROM user_account\nWHERE user_account.name = :name_1\nRETURNING user_account.id, user_account.name\n\n\n\n\nFurther Reading for UPDATE, DELETE\u00b6\n\nSee also\nAPI documentation for UPDATE / DELETE:\n\nUpdate\nDelete\n\nORM-enabled UPDATE and DELETE:\n\nORM-enabled UPDATE statements\nORM-enabled DELETE Statements\n\n\n\n\n\nSQLAlchemy 1.4 / 2.0 Tutorial\nNext Tutorial Section: Data Manipulation with the ORM\n\n\n\n\nPrevious:\nSelecting Rows with Core or ORM\nNext:\nData Manipulation with the ORM\n\n\u00a9 Copyright 2007-2021, the SQLAlchemy authors and contributors.\n\n\nflamb\u00e9! the dragon and The Alchemist image designs created and generously donated by Rotem Yaari.\n\nCreated using Sphinx 4.2.0.\n\n\n\n\n\n\n\n\n\n\n\nSQLAlchemy Sponsors\n\n\n\n\n\n\n\n\n\n\nWebsite content copyright \u00a9 by SQLAlchemy authors and contributors.\nSQLAlchemy and its documentation are licensed under the MIT license.\nSQLAlchemy is a trademark of Michael Bayer.  mike(&)zzzcomputing.com\nAll rights reserved.\nWebsite generation by\nzeekofile, with\nhuge thanks to the Blogofile\nproject.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "extract msi from exe", "id": 1525, "answers": [{"answer_id": 1514, "document_id": 1102, "question_id": 1525, "text": ":\n\n\nDownload and install the WiX toolkit (linking to a previous answer with some extra context information on WiX - as well as the download link).\nAfter installing WiX, just open a command prompt, CD to the folder where the setup.exe resides. Then specify the above command and press Enter\nThe output folder will contain a couple of sub-folders containing both extracted MSI and EXE files and manifests and resource file for the Burn GUI (if any existed in the setup.exe file in the first place of course).\nYou can now, in turn, extract the contents of the extracted MSI files (or EXE files). For an MSI that would mean running an admin install - as described be", "answer_start": 5951, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI want to extract the MSI of an EXE setup to publish over a network.\n\nFor example, using Universal Extractor, but it doesn't work for Java Runtime Environment.\n    \n\nFor InstallShield MSI based projects I have found the following to work:\n\nsetup.exe /s /x /b\"C:\\FolderInWhichMSIWillBeExtracted\" /v\"/qn\"\n\n\nThis command will lead to an extracted MSI in a directory you can freely specify and a silently failed uninstall of the product.\n\nThe command line basically tells the setup.exe to attempt to uninstall the product (/x) and do so silently (/s). While doing that it should extract the MSI to a specific location (/b).\n\nThe /v command passes arguments to Windows Installer, in this case the /qn argument. The /qn argument disables any GUI output of the installer.\n    \n\n\n  Quick List: There are a number of common types of setup.exe files. Here are some of them in a \"short-list\". More fleshed-out details here\n  (towards bottom).\n\n\nSetup.exe Extract: (various flavors to try)\n\n\nsetup.exe /a\nsetup.exe /s /extract_all\nsetup.exe /s /extract_all:[path]\nsetup.exe /stage_only\nsetup.exe /extract \"C:\\My work\"\nsetup.exe /x\nsetup.exe /x [path]\nsetup.exe /s /x /b\"C:\\FolderInWhichMSIWillBeExtracted\" /v\"/qn\"\n\ndark.exe -x outputfolder setup.exe\n\n\n\ndark.exe is a WiX binary - install WiX to extract a WiX setup.exe (as of now). More (section 4).\n\nThere is always:\n\n\nsetup.exe /?\n\n\n\n\nReal-world, pragmatic Installshield setup.exe extraction.\nInstallshield: Setup.exe and Update.exe Command-Line Parameters.\nInstallshield setup.exe commands (sample)\nWise setup.exe commands\nAdvanced Installer setup.exe commands.\n\n\nMSI Extract: msiexec.exe / File.msi extraction:\n\n\n msiexec /a File.msi\n msiexec /a File.msi TARGETDIR=C:\\MyInstallPoint /qn\n\n\n\n\nmsiexec.exe command lines.\nMicrosoft's msiexec.exe documentation on MSDN.\n\n\n\n\nMany Setup Tools: It is impossible to cover all the different kinds of possible setup.exe files. They might feature all kinds of different command line switches. There are so many possible tools that can be used. (non-MSI,MSI, admin-tools, multi-platform, etc...).\n\nNSIS / Inno: Commmon, free tools such as Inno Setup seem to make extraction hard (unofficial unpacker, not tried by me, run by virustotal.com). Whereas NSIS seems to use regular archives that standard archive software (7-zip et al) can open and extract.\n\n\n  General Tricks: One trick is to launch the setup.exe and look in the 1) system's temp folder for extracted files.\n  Another trick is to use 2) 7-Zip, WinRAR, WinZip or similar\n  archive tools to see if they can read the format. Some claim success\n  by 3) opening the setup.exe in Visual Studio. Not a technique\n  I use. 4) And there is obviously application repackaging -\n  capturing the changes done to a computer after a setup has run and\n  clean it up - requires a special tool (most of the free ones\n  come and go, Advanced Installer Architect and AdminStudio are big\n  players).\n\n\n\n\n\n  UPDATE: A quick presentation of various deployment tools used to create\n  installers:\n  How to create windows installer (comprehensive links).\n  \n  And a simpler list view of the most used development tools as of now (2018), for quicker reading and overview.\n  \n  And for safekeeping:\n  \n  \n  Create MSI from extracted setup files (towards bottom)\n  Regarding silent installation using Setup.exe generated using Installshield 2013 (.issuite) project file (different kinds of Installshield setup.exe files)\n  What is the purpose of administrative installation initiated using msiexec /a?.\n  \n\n\n\n\nJust a disclaimer: A setup.exe file can contain an embedded MSI, it can be a legacy style (non-MSI) installer or it can be just a regular executable with no means of extraction whatsoever. The \"discussion\" below first presents the use of admin images for MSI files and how to extract MSI files from setup.exe files. Then it provides some links to handle other types of setup.exe files.  Also see the comments section.\n\nUPDATE: a few sections have now been added directly below, before the description of MSI file extract using administrative installation. Most significantly a blurb about extracting WiX setup.exe bundles (new kid on the block). Remember that a \"last resort\" to find extracted setup files, is to launch the installer and then look for extracted files in the temp folder (Hold down Windows Key, tap R, type %temp% or %tmp% and hit Enter) - try the other options first though - for reliability reasons.\n\nApologies for the \"generalized mess\" with all this heavy inter-linking. I do believe that you will find what you need if you dig enough in the links, but the content should really be cleaned up and organized better.\n\nGeneral links: \n\n\nGeneral links for handling different kinds of setup.exe files (towards bottom).\nUninstall and Install App on my Computer silently (generic, but focus on silent uninstall).\nSimilar description of setup.exe files (link for safekeeping - see links to deployment tools).\nA description of different flavors of Installshield setup.exe files (extraction, silent running, etc...)\nWise setup.exe switches (Wise is no longer on the market, but many setup.exe files remain).\n\n\nExtract content:\n\n\nExtract WiX Burn-built setup.exe (a bit down the page) - also see section directly below.\nProgrammatically extract contents of InstallShield setup.exe (Installshield).\n\n\nVendor links: \n\n\nAdvanced Installer setup.exe files.\nInstallshield setup.exe files.\nInstallshield suite setup.exe files.\n\n\n\n\nWiX Toolkit &amp; Burn Bundles (setup.exe files)\n\nTech Note: The WiX toolkit now delivers setup.exe files built with the bootstrapper tool Burn that you need the toolkit's own dark.exe decompiler to extract. Burn is used to build setup.exe files that can install several embedded MSI or executables in a specified sequence. Here is a sample extraction command:\n\ndark.exe -x outputfolder MySetup.exe\n\n\nBefore you can run such an extraction, some prerequisite steps are required:\n\n\nDownload and install the WiX toolkit (linking to a previous answer with some extra context information on WiX - as well as the download link).\nAfter installing WiX, just open a command prompt, CD to the folder where the setup.exe resides. Then specify the above command and press Enter\nThe output folder will contain a couple of sub-folders containing both extracted MSI and EXE files and manifests and resource file for the Burn GUI (if any existed in the setup.exe file in the first place of course).\nYou can now, in turn, extract the contents of the extracted MSI files (or EXE files). For an MSI that would mean running an admin install - as described below.\n\n\n\n\nThere is built-in MSI support for file extraction (admin install)\n\nMSI or Windows Installer has built-in support for this - the extraction of files from an MSI file. This is called an administrative installation. It is basically intended as a way to create a network installation point from which the install can be run on many target computers. This ensures that the source files are always available for any repair operations.\n\nNote that running an admin install versus using a zip tool to extract the files is very different! The latter will not adjust the media layout of the media table so that the package is set to use external source files - which is the correct way. Always prefer to run the actual admin install over any hacky zip extractions. As to compression, there are  actually three different compression algorithms used for the cab files inside the MSI file format: MSZip, LZX, and Storing (uncompressed). All of these are handled correctly by doing an admin install.\n\nImportant: \nWindows Installer caches installed MSI files on the system for repair, modify and uninstall scenarios. Starting with Windows 7 (MSI version 5) the MSI files are now cached full size to avoid breaking the file signature that prevents the UAC prompt on setup launch (a known Vista problem). This may cause a tremendous increase in disk space consumption (several gigabytes for some systems). To prevent caching a huge MSI file, you should run an admin-install of the package before installing. This is how a company with proper deployment in a managed network would do things, and it will strip out the cab files and make a network install point with a small MSI file and files besides it.\n\n\n\nAdmin-installs have many uses\n\nIt is recommended to read more about admin-installs since it is a useful concept, and I have written a post on stackoverflow: What is the purpose of administrative installation initiated using msiexec /a?.\n\nIn essence the admin install is important for:\n\n\nExtracting and inspecting the installer files\n\n\nTo get an idea of what is actually being installed and where\nTo ensure that the files look trustworthy and secure (no viruses - malware and viruses can still hide inside the MSI file though)\n\nDeployment via systems management software (for example SCCM)\nCorporate application repackaging\nRepair, modify and self-repair operations\nPatching &amp; upgrades\nMSI advertisement (among other details this involves the \"run from source\" feature where you can run directly from a network share and you only install shortcuts and registry data)\nA number of other smaller details\n\n\nPlease read the stackoverflow post linked above for more details. It is quite an important concept for system administrators, application packagers, setup developers, release managers, and even the average user to see what they are installing etc...\n\n\n\nAdmin-install, practical how-to\n\nYou can perform an admin-install in a few different ways depending on how the installer is delivered. Essentially it is either delivered as an MSI file or wrapped in an setup.exe file. \n\nRun these commands from an elevated command prompt, and follow the instructions in the GUI for the interactive command lines:\n\n\nMSI files:\n\nmsiexec /a File.msi\n\n\nthat's to run with GUI, you can do it silently too:\n\nmsiexec /a File.msi TARGETDIR=C:\\MyInstallPoint /qn\n\nsetup.exe files:\n\nsetup.exe /a\n\n\n\nA setup.exe file can also be a legacy style setup (non-MSI) or the dreaded Installscript MSI file type - a well known buggy Installshield project type with hybrid non-standards-compliant MSI format. It is essentially an MSI with a custom, more advanced GUI, but it is also full of bugs.\n\nFor legacy setup.exe files the /a will do nothing, but you can try the /extract_all:[path] switch as explained in this pdf. It is a good reference for silent installation and other things as well. Another resource is this list of Installshield setup.exe command line parameters. \n\nMSI patch files (*.MSP) can be applied to an admin image to properly extract its files. 7Zip will also be able to extract the files, but they will not be properly formatted.\n\nFinally - the last resort - if no other way works, you can get hold of extracted setup files by cleaning out the temp folder on your system, launch the setup.exe interactively and then wait for the first dialog to show up. In most cases the installer will have extracted a bunch of files to a temp folder. Sometimes the files are plain, other times in CAB format, but Winzip, 7Zip or even Universal Extractor (haven't tested this product) - may be able to open these.\n    \n\n7-Zip should do the trick.\n\nWith it, you can extract all the files inside the EXE (thus, also an MSI file).\n\nAlthough you can do it with 7-Zip, the better way is the administrative installation as pointed out by Stein \u00c5smul.\n    \n\nI'm guessing this question was mainly about InstallShield given the tags, but in case anyone comes here with the same problem for WiX-based packages (and possibly others), just call the installer with /extract, like so:\n\nC:\\&gt; installer.exe /extract\n\n\nThat'll place the MSI in the folder alongside the installer.\n    \n\nStarting with parameter:\n\nsetup.exe /A\n\n\nasks for saving included files (including MSI).\n\nThis may depend on the software which created the setup.exe.\n    \n\nThe only way to do that is running the exe and collect the MSI. The thing you must take care of is that if you are tranforming the MSI using MST they might get lost. \n\nI use this batch commandline:\n\nSET TMP=c:\\msipath\n\nMD \"%TMP%\"\n\nSET TEMP=%TMP%\n\nstart /d \"c:\\install\" install.exe /L1033\n\nPING 1.1.1.1 -n 1 -w 10000 &gt;NUL\n\nfor /R \"%TMP%\" %%f in (*.msi) do copy \"%%f\" \"%TMP%\"\n\ntaskkill /F /IM msiexec.exe /T\n\n    \n\nLaunch the installer, but don't press the Install &gt; button. Then\n\ncd \"%AppData%\\..\\LocalLow\\Sun\\Java\"\n\n\nand find your MSI file in one of sub-directories (e.g., jre1.7.0_25).\n\nNote that Data1.cab from that sub-directory will be required as well.\n    \n\nThere is no need to use any tool !! We can follow the simple way.  \n\nI do not know which tool built your self-extracting Setup program and so, I will have to provide a general response. \n\nMost programs of this nature extract the package file (.msi) into the TEMP directory. This behavior is the default behavior of InstallShield Developer.\n\nWithout additional information, I would recommend that you simply launch the setup and once the first MSI dialog is displayed, you can examine your TEMP directory for a newly created sub-directory or MSI file. Before cancelling/stopping an installer just copy that MSI file from TEMP folder. After that you can cancel the installation. \n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Do I need administrative permission to install Java?", "id": 154, "answers": [{"answer_id": 161, "document_id": 95, "question_id": 154, "text": "If you are trying to install Java on a personal computer, you may grant your self administrative privilege by going to:\nControl Panel -> User Accounts -> Change An Account\nSelect your account name -> Change My Account Type -> Select Computer Administrator.\n", "answer_start": 480, "answer_category": null}], "is_impossible": false}], "context": "This article applies to:\nPlatform(s): Windows 8, Windows 7, Vista, Windows XP, Windows 10\nJava version(s): 7.0, 8.0\nSYMPTOMS\n\nDuring installation of Java, a dialog box appears:\nYou need Administrative Permission to install Java.\n\nCAUSE\n\nFor Windows XP, Windows Vista and Windows 7 Operating systems, there are different types of user accounts (e.g. guest or administrator). Administrative accounts are needed to install software and make other changes to the computer.\n\nSOLUTION\n\nIf you are trying to install Java on a personal computer, you may grant your self administrative privilege by going to:\nControl Panel -> User Accounts -> Change An Account\nSelect your account name -> Change My Account Type -> Select Computer Administrator.\n\nIf you are trying to install Java on a corporate computer, please contact the network administrator for administrative permission.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Multiple Firefox Versions on Same PC", "id": 703, "answers": [{"answer_id": 707, "document_id": 394, "question_id": 703, "text": "I am using Windows XP. Yes. Download and install them in seperate directories. Then, launch each one individually with the -p flag to set up different profiles for each version (or at least one for testing). Then, after you have two seperate profiles, create an icon for each on your desktop. Right click on the icon and select properties", "answer_start": 318, "answer_category": null}], "is_impossible": false}], "context": "I develop various web apps, use CSS and JavaScript extensively, and need to be able to test them on both FF 3 as well as FF 3.5.\nBut, installing 3.5 overwrites 3.0, so I was wondering if its possible (and if so, how) to run both Firefox 3.0 and 3.5 on the same system, or am i stuck having to use 2 different systems?\nI am using Windows XP. Yes. Download and install them in seperate directories. Then, launch each one individually with the -p flag to set up different profiles for each version (or at least one for testing). Then, after you have two seperate profiles, create an icon for each on your desktop. Right click on the icon and select properties. Since Firefox 57, legacy support is withdrawn and so many useful plugins and addons are let out in the newer versions (57 onwards). I have faced the problem of keeping multiple firefox say, Firefox 56 (legacy supported) and the default, current version, which will be updated regularly by Ubuntu (say) by default.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Creating nice dmg \"installer\" for Mac OS X", "id": 741, "answers": [{"answer_id": 742, "document_id": 429, "question_id": 741, "text": "the basic idea is that you create a blank image using Disk Utility (make it big enough to at least hold your stuff - exact size doesn't matter), open that image using Finder, put your stuff in and arrange it the way you want it (use right-click and Show View Options to set things like icon size or background image)", "answer_start": 279, "answer_category": null}], "is_impossible": false}], "context": "I've made my first Qt application for Mac OS X. Now I want to create nice .dmg file that will allow user to easily install it. I am thinking about something like firefox has (see the picture):  \nI am completly new to this, so I don't even know where to start.\nIt's fairly easy - the basic idea is that you create a blank image using Disk Utility (make it big enough to at least hold your stuff - exact size doesn't matter), open that image using Finder, put your stuff in and arrange it the way you want it (use right-click and Show View Options to set things like icon size or background image). That's almost it - all that remains is to convert that r/w image into a compressed image: eject it and use Convert in Disk Utility to convert it into a compressed image.\n\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Is there any example of using JavaFX Ant Tasks to Package a Swing Application?", "id": 607, "answers": [{"answer_id": 613, "document_id": 326, "question_id": 607, "text": "<taskdef resource=\"com/sun/javafx/tools/ant/antlib.xml\"      \n        uri=\"javafx:com.sun.javafx.tools.ant\"\n        classpath=\"${javafx.sdk.path}/lib/ant-javafx.jar\"/>\n \n<fx:jar destfile=\"dist-web/ColorfulCircles.jar\">\n    <fx:application refid=\"myapp\"/>\n    <fileset dir=\"build/classes/\">\n        <include name=\"**\"/>\n    </fileset>\n</fx:jar>\n \n<fx:deploy width=\"800\" height=\"600\" outdir=\"dist-web\" \n        outfile=\"SwingInterop\">\n    <fx:info title=\"Swing Interop\"/>\n    <!-- Mark application as a Swing app -->\n    <fx:application id=\"myapp\" \n            mainClass=\"swinginterop.SwingInterop\"\n            toolkit=\"swing\"/>\n    <fx:resources>\n        <fx:fileset dir=\"dist-web\" includes=\"SwingInterop.jar\"/>\n    </fx:resources>\n</fx:deploy> ", "answer_start": 2415, "answer_category": null}], "is_impossible": false}, {"question": "How to enable a splash screen when deploying java applications?", "id": 608, "answers": [{"answer_id": 614, "document_id": 326, "question_id": 608, "text": "Explicitly define a code splash callback onGetSplash. \n\u2022\tAdd code to explicitly hide the HTML splash screen when your application is ready, by using LiveConnect to call the JavaScript function dtjava.hideSplash() from Java code. ", "answer_start": 3475, "answer_category": null}], "is_impossible": false}, {"question": "How to embed applets into a web page when deployiong java applications?", "id": 609, "answers": [{"answer_id": 615, "document_id": 326, "question_id": 609, "text": "Applets can be embedded into a web page using the dtjava.embed() function. You must also specify the swing toolkit to indicate a preference of application type for the Deployment Toolkit.", "answer_start": 5800, "answer_category": null}], "is_impossible": false}, {"question": " how to address the problem that there is no code to hide the HTML splash screen when the application is ready when deploying java applications?\n", "id": 610, "answers": [{"answer_id": 616, "document_id": 326, "question_id": 610, "text": "\u2022\tAdd code to explicitly hide the HTML splash screen when your application is ready, by using LiveConnect to call the JavaScript function dtjava.hideSplash() from Java code, as shown in Example 6-6.\n\u2022\tProvide a custom splash callback that does nothing.", "answer_start": 6267, "answer_category": null}], "is_impossible": false}], "context": "You can create Swing applications with embedded JavaFX content. This page describes how to deploy such applications.\nThis topic contains the following sections:\n\u2022\tOverview\n\u2022\tPackaging with JavaFX Ant Tasks\n\u2022\tPackaging without the Packaging Tools\n6.1 Overview\nDevelopers working on existing Swing applications can take advantage of JavaFX features by integrating JavaFX content into their Swing applications. See the tutorial JavaFX and Swing Applications for more information.\nNot all of the techniques discussed in this guide are applicable to Swing applications with JavaFX content, however, the following techniques are relevant:\n\u2022\tThe same packaging tools can be used to package your Swing applications.\n\u2022\tThe Deployment Toolkit can be used to embed your Swing application in a web page or launch it from a browser.\n\u2022\tYour application can be bundled and packaged as an installable package, using the same technique as for self-contained applications.\nDeployment of a Swing application with embedded JavaFX content on the web is similar to deployment of a regular Swing application as a Java Web Start application or applet. See the Java Tutorials lessons on Java applets and Java Web Start applications.\nHowever, to use JavaFX, the deployment descriptor of your application (JNLP file) needs to express a dependency on the JRE.\nAnt tasks are recommended for packaging hybrid applications, as described in Section 6.2, \"Packaging with JavaFX Ant Tasks.\"). Alternatively, you can modify your existing packaging process manually, as described in Section 6.3, \"Packaging without the Packaging Tools.\"\n6.2 Packaging with JavaFX Ant Tasks\nYou can use same set of Ant tasks (see Chapter 5, \"Packaging Basics\") to package Swing applications with integrated JavaFX content. You only need to mark that the application's primary UI toolkit is Swing, using the toolkit=\"swing\" attribute of <fx:application>.\nThe resulting package is similar to the package for pure JavaFX applications. See Section 5.2, \"Base Application Package.\" The only difference is that there are two deployment descriptors - one for deploying as a Swing applet, and another for launching your application using Java Web Start.\nExample 6-1 shows a sample Ant task for a Swing application that contains JavaFX code and uses the toolkit=\"swing\" attribute.\nExample 6-1 Using JavaFX Ant Tasks to Package a Swing Application with Integrated JavaFX Content\n<taskdef resource=\"com/sun/javafx/tools/ant/antlib.xml\"      \n        uri=\"javafx:com.sun.javafx.tools.ant\"\n        classpath=\"${javafx.sdk.path}/lib/ant-javafx.jar\"/>\n \n<fx:jar destfile=\"dist-web/ColorfulCircles.jar\">\n    <fx:application refid=\"myapp\"/>\n    <fileset dir=\"build/classes/\">\n        <include name=\"**\"/>\n    </fileset>\n</fx:jar>\n \n<fx:deploy width=\"800\" height=\"600\" outdir=\"dist-web\" \n        outfile=\"SwingInterop\">\n    <fx:info title=\"Swing Interop\"/>\n    <!-- Mark application as a Swing app -->\n    <fx:application id=\"myapp\" \n            mainClass=\"swinginterop.SwingInterop\"\n            toolkit=\"swing\"/>\n    <fx:resources>\n        <fx:fileset dir=\"dist-web\" includes=\"SwingInterop.jar\"/>\n    </fx:resources>\n</fx:deploy> \n6.2.1 Enabling an HTML Splash Screen\nSwing applications do not have a default preloader, so there is no code to hide the HTML splash screen when the application is ready. Therefore, by default, the generated HTML page does not use an HTML splash screen.\nTo enable a splash screen, do both of the following tasks:\n\u2022\tExplicitly define a code splash callback onGetSplash. \n\u2022\tAdd code to explicitly hide the HTML splash screen when your application is ready, by using LiveConnect to call the JavaScript function dtjava.hideSplash() from Java code. 6.3 Packaging without the Packaging Tools\nIf your project already has existing support for packaging as a Java Web Start application or applet, then it might be easier to modify the template of the JNLP file directly.\n6.3.1 Using the Deployment Toolkit\nIt is recommended that you use the Deployment Toolkit to deploy your application as an applet or launch it from a browser. The Deployment Toolkit simplifies deployment routines, such as ensuring that required JRE is available. The Deployment Toolkit also has several bonus features, such as:\n\u2022\tPassing JVM options or arguments to your application dynamically. See Section 19.3.4, \"Specify Platform Requirements and Pass JVM Options\" and Section 19.3.3, \"Pass Parameters to a Web Application.\"\n\u2022\tUsage procedures are almost the same as for JavaFX applications. The only requirement is to specify the Swing toolkit as one of the platform requirements.\nFor example, Example 6-5 shows web page code for launching a Swing Java Web Start application. This code is the same as that used to launch JavaFX applications.\nExample 6-5 Scripts and Markup in HTML Page for a Java Web Start Application\n<html>\n    <head>\n            <SCRIPT src=\"https://java.com/js/dtjava.js\"></SCRIPT>\n            <script>\n                function launchApplication(jnlpfile) {\n                dtjava.launch(\n                    {     url : jnlpfile,\n                      toolkit : 'swing'\n                    }\n                    {\n                        javafx : '2.2+'\n                    },\n                    {}\n                );\n                return false;\n            }\n        </script>\n    </head>\n    <body>\n        <h2>Test page</h2>\n        <a href='SampleApp.jnlp' \n        onclick=\"return launchApplication('SampleApp.jnlp');\">Click</a> \n        to launch test app.\n    </body>\n</html>\nExample 6-5 is simplistic, but you can use other features of the dtjava.launch() method in the Deployment Toolkit for Swing applications, as needed. For example, you can embed the JNLP file into the web page for faster startup.\nFor applets, the approach is similar. Applets can be embedded into a web page using the dtjava.embed() function. You must also specify the swing toolkit to indicate a preference of application type for the Deployment Toolkit.\nAnother caveat concerns built-in HTML splash support in the Deployment Toolkit. Swing applications do not have a default preloader, so there is no code to hide the HTML splash screen when the application is ready. To address this difference, use either of the following methods:\n\u2022\tAdd code to explicitly hide the HTML splash screen when your application is ready, by using LiveConnect to call the JavaScript function dtjava.hideSplash() from Java code, as shown in Example 6-6.\n\u2022\tProvide a custom splash callback that does nothing.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to skip role executing in Ansible?", "id": 1089, "answers": [{"answer_id": 1081, "document_id": 666, "question_id": 1089, "text": "The variables will always be defined when you use vars_prompt, so \"is defined\" will always be true. What you probably want is something along these lines:\n- name: Deploy Webserver\n  hosts: webservers\n  vars_prompt:\n    - name: run_common\n      prompt: \"Product release version\"\n      default: \"Y\"\n  roles:\n    - { role: common, when: run_common == \"Y\" }", "answer_start": 308, "answer_category": null}], "is_impossible": false}], "context": "I try to write the playbook.yml for my vagrant machine and I'm faced with the following problem. Ansible prompt me to set these variables and I set these variables to null/false/no/[just enter], but the roles is executed no matter! How can I prevent this behavior? I just want no actions if no vars are set.\nThe variables will always be defined when you use vars_prompt, so \"is defined\" will always be true. What you probably want is something along these lines:\n- name: Deploy Webserver\n  hosts: webservers\n  vars_prompt:\n    - name: run_common\n      prompt: \"Product release version\"\n      default: \"Y\"\n  roles:\n    - { role: common, when: run_common == \"Y\" }\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to configure maven project to deploy both snapshot and releases to Nexus?", "id": 488, "answers": [{"answer_id": 493, "document_id": 217, "question_id": 488, "text": "You need to distinguish between the releases and snapshots repository. <distributionManagement> only allows one <repository> and one <snapshotRepository> child.\nhttp://maven.apache.org/pom.html#Distribution_Management", "answer_start": 273, "answer_category": null}], "is_impossible": false}], "context": "I want the artifact deployed to the InternalSnapshots repository when the pom's version is suffixed with -SNAPSHOT and deployed to the InternalReleases repository when it is RELEASE. This should happen using the same pom.xml file and executing the same mvn deploy command.\nYou need to distinguish between the releases and snapshots repository. <distributionManagement> only allows one <repository> and one <snapshotRepository> child.\nhttp://maven.apache.org/pom.html#Distribution_Management\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Cannot find mysql.sock", "id": 740, "answers": [{"answer_id": 741, "document_id": 428, "question_id": 740, "text": "run\n% mysqladmin -p -u <user-name> variables\nand check the 'socket' variable", "answer_start": 374, "answer_category": null}], "is_impossible": false}], "context": "I just had to re-install mysql and I am having a problem starting it up. It cannot find the socket (mysql.sock). The problem is that neither can I. In my Mac OS X 10.4 terminal, I type: locate mysql.sock, and I get back /private/tmp/mysql.sock. It makes sense that the socket file exist in that location, but it actually does not. to answer the first part of your question:\nrun\n% mysqladmin -p -u <user-name> variables\nand check the 'socket' variable\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "laravel/framework requires ext-mbstring", "id": 1165, "answers": [{"answer_id": 1158, "document_id": 742, "question_id": 1165, "text": "Install the required mbstring extension\nsudo apt-get install php-mbstring \nIn Dockerfile you should add:\nRUN docker-php-ext-install mbstring", "answer_start": 225, "answer_category": null}], "is_impossible": false}], "context": "I am using Kali linux and I am having difficulty installing the laravel framework. I havelooked up in different links like this one and manyothers but nothing seems to work. For every this I do. It keeps bringing this error. Install the required mbstring extension\nsudo apt-get install php-mbstring \nIn Dockerfile you should add:\nRUN docker-php-ext-install mbstring\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "install composer on mamp on windows", "id": 1478, "answers": [{"answer_id": 1467, "document_id": 1053, "question_id": 1478, "text": "TEPS:\n\n\nPath for php.exe C:\\MAMP\\bin\\php\\php5.6.0\\php.exe\nPath For php.ini C:\\Users\\Public\\Documents\\Appsolute\\MAMPPRO\\conf\\php5.6.0.ini\nEdit the php.ini from step 2 as necessary (uncomment openssl)\nCopy the php5.6.0.ini from step 2 to the php.exe directory from step 1\nRename this copied php5.6.0.ini to just", "answer_start": 3570, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nSo here is the issue I'm getting:\n\nI'm trying to install Composer and I'm using MAMP on Windows (it does exist since the beginning of January).\n\nAnd like many users of Composer on Windows, I've got this error :\n\nSome settings on your machine make Composer unable to work properly.\nMake sure that you fix the issues listed below and run this script again:\n\nThe openssl extension is missing, which means that secure HTTPS transfers are impossible.\nIf possible you should enable it or recompile php with --with-openssl\"\n\nSo I checked on the right php.ini page if it was unabled and it was!\n\nI can't find any help since there are very few users of MAMP on Windows.\n\nThank you for your help or suggestions\n    \n\n\nOpen php.ini located in your \"\\MAMP\\conf\\\" folder or copy the php.ini into a \\MAMP\\bin\\php\\php[your PHP version number]\\ \nFind \"extension=php_openssl.dll\"\n;extension=php_openssl.dll - remove \";\"\nRestart your MAMP , extension should be loaded after that.\n\n    \n\nHad the same problem.\n\nThe solution is quite easy:\nEnsure that extension=php_openssl.dll is uncommented (Remove ;) and copy the php.ini file at the same directory where the php.exe is located, because Composer searches there for an php.ini File and not in the MAMP conf folder.\n    \n\nStep 1: Go to following Folder\n\n\\MAMP\\bin\\php\\php[your php version you are working on]\n\n\nNote: to check version of php you are using go to Mamp prefrences and then click on php tab and there you can see Standard Version  the version you are using. \n\nmine was \\MAMP\\bin\\php\\php5.6.8\n\n\nStep 2: Inside that folder find file named as \n\n php.ini-production\n\n\nStep 3: Open this file in Notepad to edit \n\nStep 4: Search for by pressing ctrl+F   \n\nextension=php_openssl.dll\n\n\nStep 5: Remove Semicolon(;) before it\n\nStep 6: Restart MAMP and voila You are ready to install Composer.\n    \n\nThis method works as of version 3.2.2:\n\n\nOpen php.ini located in your \\MAMP\\conf\\ folder and copy the php.ini file into \\MAMP\\bin\\php\\php[your PHP version number]\\ (You can find the version number you are using in MAMP's preferences)\nFind extension=php_openssl.dll remove ; if it exists.\nOpen php.ini-production and php.ini-developmen that exist in the same folder that you copied the php.ini file into.\nFind extension=php_openssl.dll remove ; if it exists.\nRestart your MAMP, and the extension should be loaded after that and you should be able to install Composer.\n\n    \n\nI spent hours on this issue, followed the suggested answers and couldn't get it working.  \n\nSo to anyone else getting a bunch of errors, remember MAMP is still in Beta for windows and some issues will run deeper than is practical to fix.\n\n\n  MAMP &amp; MAMP PRO for Windows is available as a Beta version. It may still contain some. We do not recommend using MAMP &amp; MAMP PRO Beta for Windows in a production environment. \n\n\nIn the end I had to switch to WAMP.  \n    \n\nI spent quite a bit of time too. In my Windows 10 MAMP installation the openssl extension was enabled by default in all php.ini files. I got it working by adding the desired PHP version to the Windows system PATH before installing Composer. MAMP PRO has a setting 'PHP in system' path on the 'PHP' tab of control panel.\n    \n\nI was able to figure this out with MAMP PRO on Windows 7, and I assume MAMP would be the same.\n\nThe tricky step is you have to copy the php.ini file to your php.exe location in order to install composer. (MAMP PRO leaves these in separate locations)\n\nFor example I am using php 5.6.0, you can use any version just make the appropriate changes.\n\nSTEPS:\n\n\nPath for php.exe C:\\MAMP\\bin\\php\\php5.6.0\\php.exe\nPath For php.ini C:\\Users\\Public\\Documents\\Appsolute\\MAMPPRO\\conf\\php5.6.0.ini\nEdit the php.ini from step 2 as necessary (uncomment openssl)\nCopy the php5.6.0.ini from step 2 to the php.exe directory from step 1\nRename this copied php5.6.0.ini to just php.ini\n\n\nComposer should then be able to install correctly.\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How make Canopy 2 User Python be your default Python on MAC OS?", "id": 935, "answers": [{"answer_id": 930, "document_id": 582, "question_id": 935, "text": "Please see the following article for further information on creating aliases for the Windows Command Prompt: Setting Command Aliases on Windows", "answer_start": 3957, "answer_category": null}], "is_impossible": false}, {"question": "How make Canopy 2 User Python be your default Python on Linux?", "id": 934, "answers": [{"answer_id": 929, "document_id": 582, "question_id": 934, "text": "Edit your ~/.bashrc file, and enter the following line at the end:\n\nalias canopy_terminal='source /home/<username>/.local/canopy/edm/envs/User/bin/activate' ", "answer_start": 3791, "answer_category": null}], "is_impossible": false}], "context": "Avatar\tJonathan March\nMay 14, 2019 22:17\nFollow\nYou can make Canopy's active Python environment be the default Python for the duration of a single terminal session:\n\nWindows: Open a \"Canopy Command Prompt\" window from the Canopy Tools menu.\n\nMac/Linux: Open a \"Canopy Terminal\" window from the Canopy Tools menu. (Does not work on all Linux systems, see this article.)\n\n\nBackground\nCanopy provides a Python environment named \"User\" where you can run Python code. You can also optionally create and use other Canopy Python environments. These are all installed and managed by EDM as described in \"Where are all of the Python packages in my Canopy Python Environments?\".\n\nWhile the \"User\" Python environment is the default Python inside the Canopy application, Canopy does not set it to be the default Python in your system's standard Command Prompt / Terminal. This is to avoid interfering with the functioning of scripts which may already assume that another Python is the default. \n\nHowever, you may wish to make the Canopy \"User\" or other Canopy Python environment be the default Python on your system, so that you can immediately run Canopy Python (and ipython, jupyter, and other executables) from any command line.\n\nHere are the OS-specific directories where the Canopy User Python executables are usually located. (Note that on Windows, two directories are specified -- the first for python itself, the second for other executables.)\n\n \n\nWindows 7 & above (64-bit)\tC:\\Users\\<username>\\AppData\\Local\\Enthought\\Canopy\\edm\\envs\\User; C:\\Users\\<username>\\AppData\\Local\\Enthought\\Canopy\\edm\\envs\\User\\Scripts\nWindows 7 & above (32-bit)\tC:\\Users\\<username>\\AppData\\Local\\Enthought\\Canopy32\\edm\\envs\\User; C:\\Users\\<username>\\AppData\\Local\\Enthought\\Canopy32\\edm\\envs\\User\\Scripts\nMacOS\t~/Library/Enthought/Canopy/edm/envs/User/bin\nLinux\t~/.local/share/canopy/edm/envs/User/bin\nFor the location of a different Canopy Python environment, replace \"User\" by the other environment name.\n\nTo make Canopy \"User\" Python be the default in a Command Prompt or Terminal, prepend this OS-specific path to your PATH environment variable.\n\n\nTo check whether Canopy Python is your default Python\nStart python or ipython from a terminal session, then enter the commands\n\nimport sys\nsys.prefix\nCheck that the output value matches the path shown in the platform-specific table above (up to the environment name \"User\"). It should also match the value of sys.prefix that is shown when you type the same commands in the Canopy GUI's Python panel.\n\n\nTo set Canopy User Python as your default Python\nTo make Canopy's User Python be your default Python in all subsequent new Terminal/Command sessions, you can begin your PATH environment variable with the platform-specific directory shown in the table above. Here are recommended ways to do this:\n\nMacOS\nEdit your ~/.bashrc file, and enter the following line at the end:\n\nsource '/Users/<username>/Library/Enthought/Canopy/edm/envs/User/bin/activate' \nLinux\nEdit your ~/.bashrc file, and enter the following line at the end:\n\nsource '/home/<username>/.local/canopy/edm/envs/User/bin/activate' \nWindows\nPrepend the Canopy User Python path shown in the table above to your PATH environment variable (usually the User PATH variable, not the System PATH variable), using the techniques described in \"Editing environment variables on Windows\". \n\n \n\nCommand line aliases:\nAlternatively, rather than having the Canopy environment active in your terminal anytime the terminal is opened, it is instead convenient to create a terminal alias to quickly activate the environment when needed. \n\nMacOS\nEdit your ~/.bashrc file, and enter the following line at the end:\n\nalias canopy_terminal='source /Users/<username>/Library/Enthought/Canopy/edm/envs/User/bin/activate'\nLinux\nEdit your ~/.bashrc file, and enter the following line at the end:\n\nalias canopy_terminal='source /home/<username>/.local/canopy/edm/envs/User/bin/activate' \nWindows\nPlease see the following article for further information on creating aliases for the Windows Command Prompt: Setting Command Aliases on Windows\n\n \n\nAfter making these edits, save the .bashrc and then reopen the terminal. Now you should be able to activate the Canopy environment by executing the command:\n\ncanopy_terminal\nThe environment can be deactivated by executing the command:\n\ndeactivate\n \n\nmacOS and Linux: Using Canopy python in an automated cronjob:\nIf you are automating the execution of a python script using cron, you may find that your script is being executed with the system python installation rather than the python provided by Canopy. This is because the cron job has a very limited scope of environment variables passed to it when execution begins. If you have manually edited your PATH variable in your .bashrc, .bash_profile, or .profile, then you will need to source that file before executing your python code in the cronjob.\n\nFor example, to run a script called my_script.py with the python installed in the Canopy User python environment, assuming you have edited your .bash_profile with the correct PATH, you need to source this file then execute the python code. The command you execute with cron would then look similar to this:\n\nsource ~/.bash_profile && python my_script.py\n \n\n\nTo un-set Canopy User Python as the default Python\nTo remove Canopy from your PATH, reverse the (OS-specific) process described above.\n\n \n\nPlease do not enter support requests in article comments\nPlease use article comments for suggestions to improve the article. For individual support requests, please follow these guidelines.", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to install .rpm on Ubuntu? ", "id": 1214, "answers": [{"answer_id": 1207, "document_id": 790, "question_id": 1214, "text": "sudo apt-get install alien\nsudo alien -i package_file.rpm", "answer_start": 723, "answer_category": null}], "is_impossible": false}], "context": "I am an ubuntu user, however I need to install a printer driver which is a RPM (*.rpm) file (Sorry it's the only driver available from the Manufacturer site for Linux).\nIs it possible to install RPM files on ubuntu?\nThanks in advance.\nrom https://help.ubuntu.com/community/RPM/AlienHowto , use the alien package:\nAlien converts an RPM package file into a Debian package file or Alien can install an RPM file directly. This is not the recommended way to install software packages in Ubuntu. If at all possible, install packages from Ubuntu's repositories using Add/Remove, apt-get, or the Synaptic Package Manager. Package dependency conflicts may occur when attempting to install RPM packages.\nThese commands should do it:\nsudo apt-get install alien\nsudo alien -i package_file.rpm\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Understanding Heroku server status 143", "id": 1676, "answers": [{"answer_id": 1663, "document_id": 1249, "question_id": 1676, "text": "It is an idle state when it does not receive any request for a while. When it receives a request it will start again", "answer_start": 221, "answer_category": null}], "is_impossible": false}], "context": "I'm wondering about Heroku server status and can't find any documentation about this topic.\nExample:\nProcess exited with status 143\nCan anyone explain this example? And where would I find resources for future reference?  It is an idle state when it does not receive any request for a while. When it receives a request it will start again.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Stop a web application tomcat from command line", "id": 1116, "answers": [{"answer_id": 1109, "document_id": 693, "question_id": 1116, "text": "The easiest method I know of is to install the Tomcat manager webapp, note the URL for stopping an application, and wget that URL.", "answer_start": 145, "answer_category": null}], "is_impossible": false}], "context": "I'm writing a bash script that automatically deploys an application to a tomcat server. How can I stop the application from bash / command line?\nThe easiest method I know of is to install the Tomcat manager webapp, note the URL for stopping an application, and wget that URL.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Fundamental django project deployment information", "id": 1291, "answers": [{"answer_id": 1283, "document_id": 862, "question_id": 1291, "text": "In general Django projects are deployed\neither as a directory of source files and the accompanying virtualenv, or\nas a self-contained container of some sort (e.g. Docker).", "answer_start": 587, "answer_category": null}], "is_impossible": false}], "context": "I am a newbie in Django deployment and have been given the task of the project deployment on an Ubuntu 18.04 LTS prod server.\nI have already deployed the project using NginX and uWSGI by copying all the project files in a directory with appropriate permissions and also assigned respective permissions to www-data user.\nBut, my question is whether all the files need to be directly copied into the said directory (since python files are interpreted files) or is there any other format, such as a war file for jsp projects, that can be directly deployed instead of copying the directory?\nIn general Django projects are deployed\neither as a directory of source files and the accompanying virtualenv, or\nas a self-contained container of some sort (e.g. Docker).\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How do you avoid storing passwords in version control?", "id": 1042, "answers": [{"answer_id": 1037, "document_id": 623, "question_id": 1042, "text": "In Spring you use the PropertyPlaceholderConfigurer to load the properties file. It just needs to be findable by Spring, which usually just means putting it in an appropriate directory under the application server.", "answer_start": 450, "answer_category": null}], "is_impossible": false}], "context": "What strategy do you use to avoid storing passwords in version control?\nCurrently, I have development/test/production passwords saved in three different files and the appropriate file gets used during deployment. All this is committed to version control, but I'm not too happy with that since not all developers need to know those passwords (especially outsourced ones, which have access only while their projects last, which could be only a month).\nIn Spring you use the PropertyPlaceholderConfigurer to load the properties file. It just needs to be findable by Spring, which usually just means putting it in an appropriate directory under the application server.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "VS2008 Setup Project: Shared (By All Users) Application Data Files?", "id": 592, "answers": [{"answer_id": 598, "document_id": 317, "question_id": 592, "text": "Instead of checking \"Enable ClickOnce Security Settings\" and selecting \"This is a full trust application\", it is possible to change the permissions of your app's CommonAppDataDirectory with a Custom Action under the \"install\" section of a setup project.", "answer_start": 202, "answer_category": null}], "is_impossible": false}], "context": "I'm developing a Windows desktop app in C#/.NET/WPF, using VS 2008. The app is required to install and run on Vista and XP machines. I'm working on a Setup/Windows Installer Project to install the app.\nInstead of checking \"Enable ClickOnce Security Settings\" and selecting \"This is a full trust application\", it is possible to change the permissions of your app's CommonAppDataDirectory with a Custom Action under the \"install\" section of a setup project.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "boost installation", "id": 1987, "answers": [{"answer_id": 1973, "document_id": 1572, "question_id": 1987, "text": "mkdir -pv /tmp/boostinst\ncd /tmp/boostinst/\nwget -c 'http://sourceforge.net/projects/boost/files/boost/1.66.0/boost_1_66_0.tar.bz2/download'\ntar xf download\ncd boost_1_66_0/\n./bootstrap.sh --help\n./bootstrap.sh --show-libraries\n./bootstrap.sh \n\ncheckinstall ./b2 install\n", "answer_start": 800, "answer_category": null}], "is_impossible": false}, {"question": "new boost version installation", "id": 1988, "answers": [{"answer_id": 1974, "document_id": 1572, "question_id": 1988, "text": "sudo apt-get update\nwget -c 'http://sourceforge.net/projects/boost/files/boost/1.50.0/boost_1_50_0.tar.bz2/download'\ntar xf download\ncd boost_1_50_0\n./bootstrap.sh\n./b2 install\n", "answer_start": 1120, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI have a question regarding the installation of the boost libraries.  Is there a package that I can use the sudo apt-get install to install this package.  I searched all of the questions in this forum and using the commands sudo apt-get install libboost1.40-dev I cannot install theh package with this.  Also, I can download it from boost.org but I do not know the correct path to install it too.  I would prefer to install it using the sudo apt-get install commands if possible.  I am using Ubuntu 9.04.\nThanks.\n    \n\nIf you want to run with the latest version, you can do the bjam install as mentioned by Ralf, but I suggest you build a 'pseudo' package so you can\n\n\nuninstall it safely\nprevent/notice conflicts with official/existing boost packages.\n\n\nHere is how to do that:\n\nmkdir -pv /tmp/boostinst\ncd /tmp/boostinst/\nwget -c 'http://sourceforge.net/projects/boost/files/boost/1.66.0/boost_1_66_0.tar.bz2/download'\ntar xf download\ncd boost_1_66_0/\n./bootstrap.sh --help\n./bootstrap.sh --show-libraries\n./bootstrap.sh \n\ncheckinstall ./b2 install\n\n    \n\nOn new boost version there is other way:\n\nsudo apt-get update\nwget -c 'http://sourceforge.net/projects/boost/files/boost/1.50.0/boost_1_50_0.tar.bz2/download'\ntar xf download\ncd boost_1_50_0\n./bootstrap.sh\n./b2 install\n\n    \n\nYou can use command aptitude search libboost to see list of the availiable boost libraries. The last version of boost is 1.42 - maybe that's why you can't find version 1.40. \n\nIf aptitude search command don't give you sufficient results, try sudo aptitude update and then run aptitude search again.\n    \n\nOn my version of Ubuntu (10.04) it's libboost1.40-all-dev\n\nOn your version you've probably got an older version of boost, you should just be able to tab-complete to see which version you can install.\n\nIn any case what I usually do under Ubuntu is \n\nsudo apt-get install bjam\n\n\nExtract the downloaded boost archive to your hard-drive and then cd into the root and\n\nsudo bjam install\n\n\nThis way you can get the newest version of boost, and not the slightly outdated one that is available for your Ubuntu version.\n    \n\nThis is a link which explain step by step on how to install it (give it some time read!)\n\nhttp://www.boost.org/doc/libs/1_41_0/more/getting_started/unix-variants.html\n\nbut your inline shell command might be the simple and easy way for doing it\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Getting a Scala interpreter to work", "id": 1134, "answers": [{"answer_id": 1127, "document_id": 711, "question_id": 1134, "text": "You need to add scala\\bin to your PATH environment variable.\nOn Mac OS X the path to Scala bin is usually: /Users/<your username>/scala/bin and on Windows usually: C:\\Program Files (x86)\\scala\\bin.\nOn Mac OS X use the Terminal and write (using your username):\necho 'export PATH=/Users/<your username>/scala/bin:$PATH' >> ~/.bash_profile", "answer_start": 536, "answer_category": null}], "is_impossible": false}], "context": "I'm very new to Scala. I have downloaded it, got it working in Eclipse where I'll be developing it; but I can't make it work in Terminal.\nAll sites and books say to just type scala - this doesn't work.\nThe website infuriatingly says:\nWe assume that both the Scala software and the user environment are set up correctly.\nHow do I do that bit?\nI'm very new to this, and using Jargon or assuming too much knowledge of frameworks around Scala will ruin a good response; please keep it simple.\n\u2022\tMac OS X (10.6.7)\n\u2022\tScala: 2.9.0.1\nThank you\nYou need to add scala\\bin to your PATH environment variable.\nOn Mac OS X the path to Scala bin is usually: /Users/<your username>/scala/bin and on Windows usually: C:\\Program Files (x86)\\scala\\bin.\nOn Mac OS X use the Terminal and write (using your username):\necho 'export PATH=/Users/<your username>/scala/bin:$PATH' >> ~/.bash_profile\n\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "wix the directory is in the user profile but is not listed in the removefile tab", "id": 1421, "answers": [{"answer_id": 1410, "document_id": 995, "question_id": 1421, "text": "e user profile but is not listed in the RemoveFile table.. But that solution did not help me.\n\nWhat do I need to change?\n\nThank You,\nVenkat Rao\n    \n\nYou'll want to add some RemoveFolder elements under your components to keep ICE64 happy.\n&lt;RemoveFolder Id=\"RemoveThisFolder\" On=\"uninstall\" /&gt;\n\nFor more details about removing per-us", "answer_start": 1849, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI have the following configuration to delete and copy a file from WIX.\n\n &lt;Directory Id='TARGETDIR' Name='SourceDir'&gt;\n &lt;Directory Id=\"AppDataFolder\" Name=\"AppDataFolder\"&gt;\n    &lt;Directory Id=\"GleasonAppData\" Name=\"Gleason\" &gt;\n    &lt;Directory Id=\"GleasonStudioAppData\" Name=\"GleasonStudio\"&gt;\n    &lt;Directory Id=\"DatabaseAppData\" Name =\"Database\"&gt;\n    &lt;Directory Id=\"UserSandboxesAppData\" Name=\"UserSandboxes\" /&gt;\n\n&lt;/Directory&gt;\n&lt;/Directory&gt;\n&lt;/Directory&gt;\n&lt;/Directory&gt;\n\n&lt;/Directory&gt;\n\n&lt;DirectoryRef Id=\"UserSandboxesAppData\"&gt;\n&lt;Component Id=\"comp_deleteBackup\" Guid=\"*\"&gt;\n        &lt;RemoveFile Id=\"RemoveBackup\" Directory=\"UserSandboxesAppData\" \n                                Name=\"DevelopmentBackUp.FDB\" On=\"install\" /&gt;  \n        &lt;RegistryKey Root=\"HKCU\" Key=\"Software\\Gleason\\Database\\RemoveBackup\"&gt;\n            &lt;RegistryValue Value=\"Removed\" Type=\"string\" KeyPath=\"yes\" /&gt;\n        &lt;/RegistryKey&gt;\n\n    &lt;/Component&gt;\n    &lt;Component Id=\"comp_createBackup\" Guid=\"*\"&gt;\n        &lt;CopyFile Id=\"DBBackup\" \n              DestinationDirectory=\"UserSandboxesAppData\" \n              DestinationName=\"DevelopmentBackUp.FDB\" \n              SourceDirectory=\"UserSandboxesAppData\" \n              SourceName=\"Development.FDB\" /&gt;\n\n        &lt;RegistryKey Root=\"HKCU\" Key=\"Software\\Gleason\\Database\\CopyBackup\"&gt;\n            &lt;RegistryValue Value=\"Copied\" Type=\"string\" KeyPath=\"yes\" /&gt;\n        &lt;/RegistryKey&gt;    \n    &lt;/Component&gt;\n&lt;/DirectoryRef&gt; \n\n\nI get 4 errors related to ICE64--The directory 'xxx' is in the user profile but is not listed in the RemoveFile table.\nxxx={UserSandboxesAppData, DatabaseAppData, GleasonStudioAppData, GleasonAppData}\nSomeone else had a very similar problem here: Directory xx is in the user profile but is not listed in the RemoveFile table.. But that solution did not help me.\n\nWhat do I need to change?\n\nThank You,\nVenkat Rao\n    \n\nYou'll want to add some RemoveFolder elements under your components to keep ICE64 happy.\n&lt;RemoveFolder Id=\"RemoveThisFolder\" On=\"uninstall\" /&gt;\n\nFor more details about removing per-user data and managing to pass all the ICE validation tests, check out How to create an uninstall shortcut (and pass all the ICE validation) by Rob Mensching.\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "upgrade python to 2 6 on mac", "id": 1474, "answers": [{"answer_id": 1463, "document_id": 1048, "question_id": 1474, "text": "/bin.\n\n\nDownload python\nUnzip it\n./configure\nmake\nsudo make", "answer_start": 1951, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI'd like to upgrade the default python installation (2.5.1) supplied with OS X Leopard to the latest version. Please let me know how I can achieve this.\n\nThanks\n    \n\nWhen an OS is distributed with some specific Python release and uses it for some OS functionality (as is the case with Mac OS X, as well as many Linux distros &amp;c), you should not tamper in any way with the system-supplied Python (as in, \"upgrading\" it and the like): while Python strives for backwards compatibility within any major release (such as 2.* or 3.*, this can never be 100% guaranted; your OS supplied tested all functionality thoroughly with the specific Python version they distribute; if you manage to alter that version, \"on your head be it\" -- neither your OS supplier nor the PSF accepts any responsibility for whatever damage that might perhaps do to your system.\n\nRather, as other answers already suggested, install any other release you wish \"besides\" the system one -- why tamper with that crucial one, and risk breaking things, when installing others is so easy anyway?!  On typical Mac OS X 10.5 machines (haven't upgraded any of my several macs to 10.6 yet), I have the Apple-supplied 2.5, a 2.4 on the side to support some old projects not worth the bother to upgrate, the latest 2.6 for new stuff, 3.1 as well to get the very newest -- they all live together in peace and quiet, I just type the release number explicitly, i.e. using python2.6 at the prompt, when I want a specific release. What release gets used when at the shell prompt you just say python is up to you (I personally prefer that to mean \"the system-supplied Python\", but it's a matter of taste: by setting paths, or shell aliases, &amp;c, you can make it mean whatever you wish).\n    \n\nDon't upgrade.\n\n\nInstall ActivePython (which co-exists with others). \nOpen Terminal\nType python2.6\n\n    \n\nMay I suggest you leave the \"Default\" be, and install Python in /usr/local/bin.\n\n\nDownload python\nUnzip it\n./configure\nmake\nsudo make install\n\n\ndone.\n\nSince /usr/local/bin comes before /usr/bin in the $PATH, you will invoke 2.6 when you type python, but the OS will remain stable...\n    \n\nYou have a number of options\n\n\nInstall with MacPorts or Fink, e.g.:\n\nsudo port install python2.6\n\nInstall from the disc image from python.org\nInstall from source:\n\ntar xzvf Python-2.6.3.tgz\ncd Python-2.6.3\n./configure &amp;&amp; make &amp;&amp; sudo make install\n\n\n    \n\nThe best is to use macports (just like Adam Rosenfield stated on this thread).\n\nYou can easily switch between Python versions using macports select mechanism:\n\n$ sudo port select --set python pythonXY\n\nTo view the list of available Python versions you can use above and/or confirm which one you're using:\n\n$ sudo port select --list python\n\nTo install a new Python version:\n\n$ sudo port install pythonXY\n    \n\nThe standard http://python.org install for Mac OSX will happily coexist with the \"system python\". If you let the installer change your paths, when you run python from a prompt in terminal, it will find the version at \n\n /Library/Frameworks/Python.framework/Versions/2.6/bin/python\n\n\nHowever, this won't interfere with anything that OSX itself does with python, which correctly hardwires the path to the version that it installs.\n    \n\nI believe there are a few options. The one I like is to install alternative UNIX software on my Macs using MacPorts, http://www.macports.org/ -- this way the new software is installed in a different directory and won't mess up anything depending on the apple-installed version, and macports also has a nice way of keeping their installed software up to date. I believe MacPorts helps take care of dependencies as well. \n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to completely remove Python from a Windows machine?", "id": 653, "answers": [{"answer_id": 658, "document_id": 346, "question_id": 653, "text": "\tOpen Control Panel\n2.\tClick \"Uninstall a Program\"\n3.\tScroll down to Python and click uninstall for each version you don't want anymore.", "answer_start": 757, "answer_category": null}], "is_impossible": false}], "context": "I installed both Python 2.7 and Python 2.6.5. I don't know what went wrong, but nothing related to Python seems to work any more. e.g. \"setup.py install\" for certain packages don't recognize the \"install\" parameter and other odd phenomena...\nI would like to completely remove Python from my system.\nI tried running the 2.7 and 2.6 msi files and choosing remove Python and then running only 2.6 and reinstalling it. Still stuff don't work.\nHow do I completely remove Python - from everything? (!)\nI would not like to reinstall my entire machine just because of the Python install...\nHere's the steps (my non-computer-savvy girlfriend had to figure this one out for me, but unlike all the far more complicated processes one can find online, this one works)\n1.\tOpen Control Panel\n2.\tClick \"Uninstall a Program\"\n3.\tScroll down to Python and click uninstall for each version you don't want anymore.\nThis works on Windows 7 out of the box, no additional programs or scripts required.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Installing Node.js (and npm) on Windows 10", "id": 770, "answers": [{"answer_id": 770, "document_id": 457, "question_id": 770, "text": " I would use the following PATH variables instead:\n%appdata%\\npm\n%ProgramFiles%\\nodejs\nSo your new PATH would look like:\n[existing stuff];%appdata%\\npm;%ProgramFiles%\\nodejs", "answer_start": 48, "answer_category": null}], "is_impossible": false}], "context": "In addition to the answer from @StephanBijzitter I would use the following PATH variables instead:\n%appdata%\\npm\n%ProgramFiles%\\nodejs\nSo your new PATH would look like:\n[existing stuff];%appdata%\\npm;%ProgramFiles%\\nodejs\nThis has the advantage of neiter being user dependent nor 32/64bit dependent.\n I had some issues trying to install Node on Windows 10 and found the solution.\nThe error was as follows:\nC:\\Users\\Stephan>npm\nError: ENOENT, stat 'C:\\Users\\Stephan\\AppData\\Roaming\\npm'\nThe solution is below.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Find a jar file given the class name?\n", "id": 1835, "answers": [{"answer_id": 1821, "document_id": 1406, "question_id": 1835, "text": "If you're on Windows and don't want to install Cygwin, then I suppose you would have to write a batch script to locate the jar files.", "answer_start": 626, "answer_category": null}], "is_impossible": false}], "context": "This must be a very basic question for Java developers, but what is the best way to find the appropriate jar file given a class name?\nFor example, given \"com.ibm.websphere.security.auth.WSSubject\", how do you track down the appropriate jar file? (\"google\" is not the answer I'm looking for!)\nThe java docs do not give any hint of the jar file, and obviously the names of the jar files themselves offer no clue.\nThere must be a 'search local jars', or some sort of 'auto-resolve dependencies', trick in the java world. Ideally, I'm looking for the 'official' way to do this. I happen to be on a windows machine without cygwin.\nIf you're on Windows and don't want to install Cygwin, then I suppose you would have to write a batch script to locate the jar files.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "how to change the default JRE for OS X?", "id": 350, "answers": [{"answer_id": 357, "document_id": 159, "question_id": 350, "text": "Launch Java Preferences. The Java Preferences window contains a list of installed JREs. Java applications and command line tools use the listed order to determine the first compatible version to use. The Vendor column indicates whether the Java release is supplied by Apple or Oracle. The Version column specifies the version of Java that is installed.", "answer_start": 817, "answer_category": null}], "is_impossible": false}], "context": "This page describes the Apple Java Preferences Application.\n\nIf you have not yet installed Apple's Java OS X 2012-006 update, then you are still using a version of Apple Java 6 that includes the plug-in and the Java Preferences app. There is an important difference about the installation of Oracle Java (both JRE and JDK) that you should be aware of.\n\nThe Applications > Utilities > Java Preferences application is part of Apple's implementation of Java. Once Apple no longer distributes Java as part of their release, the Java Preferences application is retired.\n\nUnder Apple's implementation of Java, it was possible to have multiple JREs installed, and the Java Preferences app was used to determine the first compatible version that would be used. The following instructions show how to change the default JRE.\n\nLaunch Java Preferences. The Java Preferences window contains a list of installed JREs. Java applications and command line tools use the listed order to determine the first compatible version to use. The Vendor column indicates whether the Java release is supplied by Apple or Oracle. The Version column specifies the version of Java that is installed.\n\nTo make JRE 8 the default version of Java, re-order the list by dragging Java SE 8 to the top of the list.\n\nNote that if you install only Oracle Java JRE 8, it will not appear in this list. You must install the full JDK in order for JRE 8 to be listed.", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to install Package Control by Command Palette?", "id": 914, "answers": [{"answer_id": 909, "document_id": 571, "question_id": 914, "text": "Win/Linux: ctrl+shift+p, Mac: cmd+shift+p\nType Install Package Control, press enter", "answer_start": 103, "answer_category": null}], "is_impossible": false}, {"question": "How to install Package Control by menu?", "id": 915, "answers": [{"answer_id": 910, "document_id": 571, "question_id": 915, "text": "Open the Tools menu\nSelect Install Package Control\u2026", "answer_start": 192, "answer_category": null}], "is_impossible": false}, {"question": "How to install Package Control manually?", "id": 916, "answers": [{"answer_id": 911, "document_id": 571, "question_id": 916, "text": "Click the Preferences > Browse Packages\u2026 menu\nBrowse up a folder and then into the Installed Packages/ folder\nDownload Package Control.sublime-package and copy it into the Installed Packages/ directory\nRestart Sublime Text", "answer_start": 584, "answer_category": null}], "is_impossible": false}], "context": "Use one of the following methods to install Package Control:\n\nCommand Palette\nOpen the command palette\nWin/Linux: ctrl+shift+p, Mac: cmd+shift+p\nType Install Package Control, press enter\nMenu\nOpen the Tools menu\nSelect Install Package Control\u2026\nThis will download the latest version of Package Control and verify it using public key cryptography. If an error occurs, use the manual method instead.\n\nManual\nIf the command palette/menu method is not possible due to a proxy on your network or using an old version of Sublime Text, the following steps will also install Package Control:\n\nClick the Preferences > Browse Packages\u2026 menu\nBrowse up a folder and then into the Installed Packages/ folder\nDownload Package Control.sublime-package and copy it into the Installed Packages/ directory\nRestart Sublime Text", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Visual Studio publish in debug Vs Release mode", "id": 591, "answers": [{"answer_id": 597, "document_id": 316, "question_id": 591, "text": "If you publish using Debug mode, your generated files have debugging enabled and that will impact the performance.\nIt is always recommended that you publish website in Release Mode\nPlease read this Don\u2019t run production ASP.NET Applications with debug=\u201dtrue\u201d enabled", "answer_start": 189, "answer_category": null}], "is_impossible": false}], "context": "I know the difference between debug and release mode BUILD. But I have a small doubt, does it make any difference on what mode I select while publishing the application thru Visual studio?\nIf you publish using Debug mode, your generated files have debugging enabled and that will impact the performance.\nIt is always recommended that you publish website in Release Mode\nPlease read this Don\u2019t run production ASP.NET Applications with debug=\u201dtrue\u201d enabled\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "MVC3 Deployment Dependency Problems", "id": 552, "answers": [{"answer_id": 554, "document_id": 277, "question_id": 552, "text": "This is because Microsoft.Web.Infrastructure is not in your GAC. You need to add this reference to your project. Right click the reference and go to properties then set copy to local to true.", "answer_start": 209, "answer_category": null}], "is_impossible": false}], "context": "I've just tried deploying an MVC3 application to our IIS7 hosting environment but I got the exception.\nThe app isn't being bin deployed as I have installed ASP.Net Web pages and MVC3 on the web server itself.\nThis is because Microsoft.Web.Infrastructure is not in your GAC. You need to add this reference to your project. Right click the reference and go to properties then set copy to local to true.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "What is the difference between \"pom\" type dependency with scope \"import\" and without \"import\"?", "id": 1896, "answers": [{"answer_id": 1883, "document_id": 1467, "question_id": 1896, "text": "You can only import managed dependencies. This means you can only import other POMs into the dependencyManagement section of your project's POM. i.e.", "answer_start": 533, "answer_category": null}], "is_impossible": false}], "context": "Starting from Maven 2.0.9 there is possibility to include\n<type>pom</type>\n<scope>import</scope>\nin the <dependencyManagement> section.\nAs I understand it, it will be \"replaced\" with dependencies included in this pom as if they were originally defined here.\nWhat is the difference between solution above and simple dependency to this pom without import scope (I saw the latter being called \"dependencies grouping\")? Is the only difference that such \"grouped\" dependencies have lower priority while resolving dependencies precedence?\nYou can only import managed dependencies. This means you can only import other POMs into the dependencyManagement section of your project's POM. i.e.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "visual studio 2015 download getting stuck at applying microsoft asp net", "id": 1487, "answers": [{"answer_id": 1476, "document_id": 1063, "question_id": 1487, "text": ".\n\nGo to Control Panel -&gt; Programs -&gt; Turn Windows features on or off -&gt; Select \"Internet Information Ser", "answer_start": 3032, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI'm currently on my third attempt trying to install Visual Studio 2015 on this computer. I have tried rebooting, hard shut downs, canceling setup and restarting, etc. Each time, it gets stuck at applying Microsoft ASP.net. I have tried leaving it overnight (12 hours). My download speed is 50-60 MBPS according to Ookla Speedtest. I am running Windows 10. I did do a \"custom\" installation and added C++, Python, and the Git extension. If I am not mistaken it said 7 GB size. Why is it doing this? Please help! \n\nI don't understand why this isn't working because I installed VS 2015 for my laptop (a different computer) just a few days ago on a relatives WIFI and their speed was 3-7 MBPS. \n\n\n\n(Note: The installer is not technically frozen since the loading dots on the bottom are moving.) However, bar hasn't filled up at all for like 6 hours. \n\nEDIT:\nDo I need ASP.NET (for C#, Python, and maybe C++)? Can I uncheck certain features for installation so that it wont try to install this? Also, when I cancel installation it never cancels and just stays there so I have to kill the installer with the task manager. This is getting incredibly frustrating. \n    \n\nFixed it...\n\nEasiest way is \"threaten\" to shut down the computer.  Go into Power, restart system.  You will be warned that there is a logging program which will not let the system reset.  Cancel that program and then do NOT restart.\n\nInstaller immediately went on to next part and finished install after having been stuck for 6 hours...\n    \n\nI got it to work, it wasn't perfect, but here is what I did: It was getting stuck at very certain points, most notably ASP.NET. I did a little research and got an idea from something I read (I unfourtunetly can't find the source again). Sometimes windows opens invisible \"confirm\" type windows or installer windows that get stuck. When the installer got stuck I opened up task manager -&gt; details, than checked On the visual studio process by right clicking then clicking analyzing wait chain. This showed me what process the VS install was waiting for. Then, I'd just kill the process. Messy, I know, but better then nothing. I had to do that 2 or 3 times. Afterward it said it installed correctly with 2 components that had warnings. ASP.NET was included. But everything else worked fine (c#, C++) Later I went back and did a repair. That worked pretty smooth. Finally, I installed the Python Tools successfully. (that part is sort of irrelevant but the point was that everything is now working fine). \n\nedit, found another source: Visual Studio 2015 Community Edition Installation Stuck In Windows 10\n    \n\nYes i had this problem too. To solve this open task manager and go to details tab then search for TiWorker.exe, right click on it and choose analyzing wait chain it will display many instances of the process that are in waiting mode , check all those processes and terminate them after doing this the installer will go to the next step.\n    \n\nTo avoid this problem, you have to install IIS first.\n\nGo to Control Panel -&gt; Programs -&gt; Turn Windows features on or off -&gt; Select \"Internet Information Services\" -&gt; OK\n    \n\nI had this problem a couple of hours before writing this answer and what worked for me was:\nI opened other programs at least two or three and went to power and clicked on restart as usual it will warn you about open programs that need too be closed before shutdown or programs that are currently active I then clicked on force close and it closed the first program that I recently opened then I quickly clicked on cancel. Two minutes later instalation was back on track\n    \n\nPress Ctrl+Shift+Esc, then go to Tiworker.exe and terminate all the waiting processes.\n    \n\nI found a solution to this headache of a process to install Visual Studio. If you have tried everything and nothing seams to be working for you even by trying the command line shell then try this. \n\nGo open task manager while running the installer and kill wusa.exe. It will kill the process to search for the update and continue the installation. You may have to kill some other processes as well if they get stuck. Its not great but it seams to be working.\n\nwusa.exe seams to be the task that is the problem that prevents the installer from moving to the next step. This could be because your computer can not reach the update server. \n\nThis seams to be the case for both the Community and Enterprise Version of Visual Studio. \n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to upgrade from VS 2017 to 2019?", "id": 690, "answers": [{"answer_id": 694, "document_id": 382, "question_id": 690, "text": "Please download and install VS 2019 from https://docs.microsoft.com/en-us/visualstudio/releases/2019/release-notes", "answer_start": 397, "answer_category": null}], "is_impossible": false}], "context": "VS 2019 is RTM, and I was wondering what's the proper way to upgrade from VS 2017, is there a dedicated 'upgrade' method, or is it uninstall and install? Maybe install and uninstall?\nWhat's the right way to do it without having to uninstall and reinstall same stuff for nothing? We cannot directly upgrade VS 2017 to VS 2019. They are compatible and work side by side though (like the following). Please download and install VS 2019 from https://docs.microsoft.com/en-us/visualstudio/releases/2019/release-notes\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Split requirements files in pip", "id": 689, "answers": [{"answer_id": 693, "document_id": 381, "question_id": 689, "text": "The -r flag isn't restricted to command-line use only, it can also be used inside requirements files. So running pip install -r req-1-and-2.txt when req-1-and-2.txt contains this:\n-r req-1.txt\n-r req-2.txt\nwill install everything specified in req-1.txt and req-2.txt.", "answer_start": 363, "answer_category": null}], "is_impossible": false}], "context": "None of that works and however much simple it could be, I can't figure out how to do what I want.\nAny suggestion? Just on a note, you can also split the requirements based on your groupings and embed them in a single file ( or again can prepare multiple requirements file based on your environment), that you can execute.\nFor example, the test requirements here:\nThe -r flag isn't restricted to command-line use only, it can also be used inside requirements files. So running pip install -r req-1-and-2.txt when req-1-and-2.txt contains this:\n-r req-1.txt\n-r req-2.txt\nwill install everything specified in req-1.txt and req-2.txt.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How many basic filter types in Apache2.0?", "id": 419, "answers": [{"answer_id": 427, "document_id": 176, "question_id": 419, "text": " three", "answer_start": 511, "answer_category": null}], "is_impossible": false}, {"question": "What does connection filter in Apache2.0 do?", "id": 420, "answers": [{"answer_id": 428, "document_id": 176, "question_id": 420, "text": "Filters of this type are valid for the lifetime of this connection", "answer_start": 632, "answer_category": null}], "is_impossible": false}, {"question": "What does resource filter in Apache2.0 do?", "id": 422, "answers": [{"answer_id": 430, "document_id": 176, "question_id": 422, "text": "Filters of this type are valid for the time that this content is used\nto satisfy a request. ", "answer_start": 1022, "answer_category": null}], "is_impossible": false}, {"question": "What does protocol filter in Apache2.0 do?", "id": 421, "answers": [{"answer_id": 429, "document_id": 176, "question_id": 421, "text": "Filters of this type are valid for the lifetime of this request from\nthe point of view of the client, this means that the request is valid\nfrom the time that the request is sent until the time that the response\nis received.", "answer_start": 749, "answer_category": null}], "is_impossible": false}], "context": "\n\nHow filters work in Apache 2.0\n\n\n\n\n\n\nModules | Directives | FAQ | Glossary | Sitemap\nApache HTTP Server Version 2.4\n\n\n\nApache > HTTP Server > Documentation > Version 2.4 > Developer DocumentationHow filters work in Apache 2.0\nWarning\nThis is a cut 'n paste job from an email\n(<022501c1c529$f63a9550$7f00000a@KOJ>) and only reformatted for\nbetter readability. It's not up to date but may be a good start for\nfurther research.\n\n\nFilter Types\nHow are filters inserted?\nAsis\nExplanations\n\n\n\nFilter Types\nThere are three basic filter types (each of these is actually broken\ndown into two categories, but that comes later).\n\nCONNECTION\nFilters of this type are valid for the lifetime of this connection.\n(AP_FTYPE_CONNECTION, AP_FTYPE_NETWORK)\nPROTOCOL\nFilters of this type are valid for the lifetime of this request from\nthe point of view of the client, this means that the request is valid\nfrom the time that the request is sent until the time that the response\nis received. (AP_FTYPE_PROTOCOL,\nAP_FTYPE_TRANSCODE)\nRESOURCE\nFilters of this type are valid for the time that this content is used\nto satisfy a request.  For simple requests, this is identical to\nPROTOCOL, but internal redirects and sub-requests can change\nthe content without ending the request. (AP_FTYPE_RESOURCE,\nAP_FTYPE_CONTENT_SET)\n\nIt is important to make the distinction between a protocol and a\nresource filter.  A resource filter is tied to a specific resource, it\nmay also be tied to header information, but the main binding is to a\nresource.  If you are writing a filter and you want to know if it is\nresource or protocol, the correct question to ask is:  \"Can this filter\nbe removed if the request is redirected to a different resource?\"  If\nthe answer is yes, then it is a resource filter.  If it is no, then it\nis most likely a protocol or connection filter.  I won't go into\nconnection filters, because they seem to be well understood. With this\ndefinition, a few examples might help:\n\nByterange\nWe have coded it to be inserted for all requests, and it is removed\nif not used.  Because this filter is active at the beginning of all\nrequests, it can not be removed if it is redirected, so this is a\nprotocol filter.\nhttp_header\nThis filter actually writes the headers to the network.  This is\nobviously a required filter (except in the asis case which is special\nand will be dealt with below) and so it is a protocol filter.\nDeflate\nThe administrator configures this filter based on which file has been\nrequested.  If we do an internal redirect from an autoindex page to an\nindex.html page, the deflate filter may be added or removed based on\nconfig, so this is a resource filter.\n\nThe further breakdown of each category into two more filter types is\nstrictly for ordering.  We could remove it, and only allow for one\nfilter type, but the order would tend to be wrong, and we would need to\nhack things to make it work.  Currently, the RESOURCE filters\nonly have one filter type, but that should change.\n\n\nHow are filters inserted?\nThis is actually rather simple in theory, but the code is\ncomplex.  First of all, it is important that everybody realize that\nthere are three filter lists for each request, but they are all\nconcatenated together:\n\nr->output_filters (corresponds to RESOURCE)\nr->proto_output_filters (corresponds to PROTOCOL)\nr->connection->output_filters (corresponds to CONNECTION)\n\nThe problem previously, was that we used a singly linked list to create the filter stack, and we\nstarted from the \"correct\" location.  This means that if I had a\nRESOURCE filter on the stack, and I added a\nCONNECTION filter, the CONNECTION filter would\nbe ignored. This should make sense, because we would insert the connection\nfilter at the top of the c->output_filters list, but the end\nof r->output_filters pointed to the filter that used to be\nat the front of c->output_filters. This is obviously wrong.\nThe new insertion code uses a doubly linked list. This has the advantage\nthat we never lose a filter that has been inserted. Unfortunately, it comes\nwith a separate set of headaches.\nThe problem is that we have two different cases were we use subrequests.\nThe first is to insert more data into a response. The second is to\nreplace the existing response with an internal redirect. These are two\ndifferent cases and need to be treated as such.\nIn the first case, we are creating the subrequest from within a handler\nor filter.  This means that the next filter should be passed to\nmake_sub_request function, and the last resource filter in the\nsub-request will point to the next filter in the main request.  This\nmakes sense, because the sub-request's data needs to flow through the\nsame set of filters as the main request.  A graphical representation\nmight help:\nDefault_handler --> includes_filter --> byterange --> ...\nIf the includes filter creates a sub request, then we don't want the\ndata from that sub-request to go through the includes filter, because it\nmight not be SSI data.  So, the subrequest adds the following:\nDefault_handler --> includes_filter -/-> byterange --> ...\n/\nDefault_handler --> sub_request_core\nWhat happens if the subrequest is SSI data?  Well, that's easy, the\nincludes_filter is a resource filter, so it will be added to\nthe sub request in between the Default_handler and the\nsub_request_core filter.\nThe second case for sub-requests is when one sub-request is going to\nbecome the real request.  This happens whenever a sub-request is created\noutside of a handler or filter, and NULL is passed as the next filter to\nthe make_sub_request function.\nIn this case, the resource filters no longer make sense for the new\nrequest, because the resource has changed.  So, instead of starting from\nscratch, we simply point the front of the resource filters for the\nsub-request to the front of the protocol filters for the old request.\nThis means that we won't lose any of the protocol filters, neither will\nwe try to send this data through a filter that shouldn't see it.\nThe problem is that we are using a doubly-linked list for our filter\nstacks now. But, you should notice that it is possible for two lists to\nintersect in this model.  So, you do you handle the previous pointer?\nThis is a very difficult question to answer, because there is no \"right\"\nanswer, either method is equally valid.  I looked at why we use the\nprevious pointer.  The only reason for it is to allow for easier\naddition of new servers.  With that being said, the solution I chose was\nto make the previous pointer always stay on the original request.\nThis causes some more complex logic, but it works for all cases.  My\nconcern in having it move to the sub-request, is that for the more\ncommon case (where a sub-request is used to add data to a response), the\nmain filter chain would be wrong.  That didn't seem like a good idea to\nme.\n\n\nAsis\nThe final topic.  :-)  Mod_Asis is a bit of a hack, but the\nhandler needs to remove all filters except for connection filters, and\nsend the data.  If you are using mod_asis, all other\nbets are off.\n\n\nExplanations\nThe absolutely last point is that the reason this code was so hard to\nget right, was because we had hacked so much to force it to work.  I\nwrote most of the hacks originally, so I am very much to blame.\nHowever, now that the code is right, I have started to remove some\nhacks.  Most people should have seen that the reset_filters\nand add_required_filters functions are gone.  Those inserted\nprotocol level filters for error conditions, in fact, both functions did\nthe same thing, one after the other, it was really strange. Because we\ndon't lose protocol filters for error cases any more, those hacks went away.\nThe HTTP_HEADER, Content-length, and\nByterange filters are all added in the\ninsert_filters phase, because if they were added earlier, we\nhad some interesting interactions.  Now, those could all be moved to be\ninserted with the HTTP_IN, CORE, and\nCORE_IN filters.  That would make the code easier to\nfollow.\n\n\nCopyright 2021 The Apache Software Foundation.Licensed under the Apache License, Version 2.0.\nModules | Directives | FAQ | Glossary | Sitemap\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "C#'s equivalent of jar files?", "id": 1251, "answers": [{"answer_id": 1244, "document_id": 823, "question_id": 1251, "text": ".NET compiles into dll or exe. You can use ILMerge to merge several dlls/exes into one.", "answer_start": 145, "answer_category": null}], "is_impossible": false}], "context": "Java provides the jar file so that all the class files and jar files are merged into one file. Does C# provide equivalent/similar functionality?\n.NET compiles into dll or exe. You can use ILMerge to merge several dlls/exes into one.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to deploy an ASP.NET Application with zero downtime", "id": 144, "answers": [{"answer_id": 152, "document_id": 87, "question_id": 144, "text": "You can achieve zero downtime deployment on a single server by utilizing Application Request Routing in IIS as a software load balancer between two local IIS sites on different ports. This is known as a blue green deployment strategy where only one of the two sites is available in the load balancer at any given time. Deploy to the site that is \"down\", warm it up, and bring it into the load balancer (usually by passing a Application Request Routing health check), then take the original site that was up, out of the \"pool\" (again by making its health check fail).", "answer_start": 423, "answer_category": null}], "is_impossible": false}], "context": "To deploy a new version of our website we do the following:Zip up the new code, and upload it to the server.On the live server, delete all the live code from the IIS website directory.Extract the new code zipfile into the now empty IIS directory. This process is all scripted, and happens quite quickly, but there can still be a 10-20 second downtime when the old files are being deleted, and the new files being deployed. You can achieve zero downtime deployment on a single server by utilizing Application Request Routing in IIS as a software load balancer between two local IIS sites on different ports. This is known as a blue green deployment strategy where only one of the two sites is available in the load balancer at any given time. Deploy to the site that is \"down\", warm it up, and bring it into the load balancer (usually by passing a Application Request Routing health check), then take the original site that was up, out of the \"pool\" (again by making its health check fail).", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How (in what form) to share (deliver) a Python function?", "id": 1082, "answers": [{"answer_id": 1074, "document_id": 659, "question_id": 1082, "text": "I suggest you checkout Flask to build a microservice. ", "answer_start": 452, "answer_category": null}], "is_impossible": false}], "context": "The final outcome of my work should be a Python function that takes a JSON object as the only input and return another JSON object as output. To keep it more specific, I am a data scientist, and the function that I am speaking about, is derived from data and it delivers predictions (in other words, it is a machine learning model).\nSo, my question is how to deliver this function to the \"tech team\" that is going to incorporate it into a web-service.\nI suggest you checkout Flask to build a microservice. This will let the other team post JSON objects in an HTTP request to your flask application and receive JSON objects back. No knowledge of the underlying system or additional requirements required!\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to specify maven's distributionManagement organisation wide?", "id": 343, "answers": [{"answer_id": 350, "document_id": 155, "question_id": 343, "text": "The best solution specify maven's distributionManagement organisation wide is to create a simple parent pom file project (with packaging 'pom') generically for all projects from your organization.", "answer_start": 133, "answer_category": null}], "is_impossible": false}], "context": "I'm trying to figure out how to organize many (around 50+) maven2 projects, so that they can deploy into a central nexus repository. The best solution specify maven's distributionManagement organisation wide is to create a simple parent pom file project (with packaging 'pom') generically for all projects from your organization.", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to obfuscate Flutter ", "id": 533, "answers": [{"answer_id": 535, "document_id": 258, "question_id": 533, "text": "There's under-tested support for obfuscation. To enable it:\nFor Android:\nAdd to the file [ProjectRoot]/android/gradle.properties :\nextra-gen-snapshot-options=--obfuscate", "answer_start": 271, "answer_category": null}], "is_impossible": false}], "context": "Flutter's wiki mentions obfuscation is an opt-in in release mode.\nAnd yet, the flutter build command has no relevant option - see:\nflutter help -v build apk\nAm I missing something here?\nDid they make obfuscation the default?\nIs obfuscation even relevant for flutter?pps?\nThere's under-tested support for obfuscation. To enable it:\nFor Android:\nAdd to the file [ProjectRoot]/android/gradle.properties :\nextra-gen-snapshot-options=--obfuscate\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "installing Mayavi with pip - no module named vtk", "id": 1195, "answers": [{"answer_id": 1188, "document_id": 771, "question_id": 1195, "text": " I did the following and it worked.\n1.\tinstall python-vtk from repositories systemwide\n2.\tcopy vtk folder from /usr/lib/pymodules/python2.7 to /PATH/TO/YOUR/VIRTUALENV/lib/python2.7/site-packages", "answer_start": 316, "answer_category": null}], "is_impossible": false}], "context": "I've manually installed all it's dependencies, including VTK.\nFor VTK, I followed its installation guide and installed the python wrapper.\nNow if I open a new terminal window and open python, I can import vtk without any error.\nI was stuck with installing vtk into a virtualenv, after hours of search with no results I did the following and it worked.\n1.\tinstall python-vtk from repositories systemwide\n2.\tcopy vtk folder from /usr/lib/pymodules/python2.7 to /PATH/TO/YOUR/VIRTUALENV/lib/python2.7/site-packages\nIn your case the system-wide python packages folder would probably be different but you can easily find it by firing up system python and entering the following:\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "visual studio setup project per user registry settings", "id": 1483, "answers": [{"answer_id": 1472, "document_id": 1059, "question_id": 1483, "text": "ect answer is to just have some default settings in your app, which can write them to the registry if it doesn't find them. It's general good practice that your app should never depend on the registry, and should create things as needed, for any registry entry, not j", "answer_start": 3012, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI'm trying to maintain a Setup Project in Visual Studio 2003 (yes, it's a legacy application). The problem we have at the moment is that we need to write registry entries to HKCU for every user on the computer. They need to be in the HKCU rather than HKLM because they are the default user settings, and they do change per user. My feeling is that\n\n\nThis isn't possible\nThis isn't something the installer should be doing, but something the application should be doing (after all what happens when a user profile is created after the install?).\n\n\nWith that in mind, I still want to change as little as possible in the application, so my question is, is it possible to add registry entries for every user in a Visual Studio 2003 setup project? \n\nAnd, at the moment the project lists five registry root keys (HKEY_CLASSES_ROOT, HKEY_CURRENT_USER, HKEY_LOCAL_MACHINE, HKEY_USERS, and User/Machine Hive). I don't really know anything about the Users root key, and haven't seen User/Machine Hive. Can anyone enlighten me on what they are? Perhaps they could solve my problem above.\n    \n\nFirst: Yes, this is something that belongs in the Application for the exact reson you specified: What happens after new user profiles are created? Sure, if you're using a domain it's possible to have some stuff put in the registry on creation, but this is not really a use case. The Application should check if there are seetings and use the default settings if not.\n\nThat being said, it IS possible to change other users Keys through the HKEY_USERS Hive.\n\nI have no experience with the Visual Studio 2003 Setup Project, so here is a bit of (totally unrelated) VBScript code that might just give you an idea where to look:\n\nconst HKEY_USERS = &amp;H80000003\nstrComputer = \".\"\nSet objReg=GetObject(\"winmgmts:{impersonationLevel=impersonate}!\\\\\" &amp; strComputer &amp; \"\\root\\default:StdRegProv\")\nstrKeyPath = \"\"\nobjReg.EnumKey HKEY_USERS, strKeyPath, arrSubKeys\nstrKeyPath = \"\\Software\\Microsoft\\Windows\\CurrentVersion\\WinTrust\\Trust Providers\\Software Publishing\"\nFor Each subkey In arrSubKeys\n    objReg.SetDWORDValue HKEY_USERS, subkey &amp; strKeyPath, \"State\", 146944\nNext\n\n\n(Code Courtesy of Jeroen Ritmeijer)\n    \n\nI'm guessing that because you want to set it for all users, that you're on some kind of shared computer, which is probably running under a domain?\n\nHERE BE DRAGONS\n\nLet's say Joe and Jane regularly log onto the computer, then they will each have 'registries'.\n\nYou'll then install your app, and the installer will employ giant hacks and disgusting things to set items under HKCU for them.\n\nTHEN, bob will come along and log on (he, and 500 other people have accounts in the domain and so can do this). He's never used this computer before, so he has no registry. The first time he logs in, windows creates him one, but he won't have your setting. \n\nYour app then falls over or behaves incorrectly, and bob complains loudly about those crappy products from raynixon incorporated.\n\nThe correct answer is to just have some default settings in your app, which can write them to the registry if it doesn't find them. It's general good practice that your app should never depend on the registry, and should create things as needed, for any registry entry, not just HKCU, anyway\n    \n\nI'm partway to my solution with this entry on MSDN (don't know how I couldn't find it before).\n\nUser/Machine Hive\nSubkeys and values entered under this hive will be installed under the HKEY_CURRENT_USER hive when a user chooses \"Just Me\" or the HKEY_USERS hive or when a user chooses \"Everyone\" during installation.\n\nRegistry Editor Archive of MSDN Article\n    \n\nDespite what the MSDN article Archive of MSDN Article  says about User/Machine Hive, it doesn't write to HKEY_USERS. Rather it writes to HKCU if you select Just Me and HKLM if you select everyone.\n\nSo my solution is going to be to use the User/Machine Hive, and then in the application it checks if the registry entries are in HKCU and if not, copies them from HKLM. I know this probably isn't the most ideal way of doing it, but it has the least amount of changes.\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "What files do I need to deploy for ASP.NET mvc?", "id": 1126, "answers": [{"answer_id": 1119, "document_id": 703, "question_id": 1126, "text": "If you develop in Microsoft Visual Studio, you can use the Build > Publish option (when you're in the context of an ASP.NET MVC project) and publish it directly.", "answer_start": 326, "answer_category": null}], "is_impossible": false}], "context": "I know I can just copy all of my files from my development environment into my live website directory, but then I know I'm copying several files that aren't necessary (e.g. the .sln file and .csproj files).\nWhat files actually have to be copied for ASP.NET MVC web applications to run? (E.g. Do I need to copy all .CS files?)\nIf you develop in Microsoft Visual Studio, you can use the Build > Publish option (when you're in the context of an ASP.NET MVC project) and publish it directly.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "After uninstalling Java, how do I remove its listing in the Windows Uninstall/Remove Programs using Microsoft utility?", "id": 152, "answers": [{"answer_id": 159, "document_id": 94, "question_id": 152, "text": "Run the Microsoft utility to repair corrupted registry keys that prevents programs from being completely uninstalled or blocking new installations and updates.", "answer_start": 783, "answer_category": null}], "is_impossible": false}, {"question": "After uninstalling Java, how do I remove its listing in the Windows Uninstall/Remove Programs by Manual Registry Edit?", "id": 153, "answers": [{"answer_id": 160, "document_id": 94, "question_id": 153, "text": "Run the Windows Registry Editor by clicking Start->Run and entering the program name regedit in the Open edit field.\nNavigate to the following registry node:\nHKEY_LOCAL_MACHINE/Software/Microsoft/\nWindows/CurrentVersion/Uninstall\n\nUnder this Uninstall node, you will find many registry entry names enclosed in curly braces. e.g. {26A24AE4-039D-4CA4-87B4-2F83216013FB}\nClick on each registry entries in the left pane and that will display its associated data on right pane of Registry Editor.\nor\nClick on Edit->Find tab in Registry Editor window and then enter Java version that you want to find Keys for\nFor example: serching for Java(TM) 6 Update 24 finds this:\nThe Registry Editor\n\nDelete the registry entry found for Java, by right clicking on the registry key name, and selecting Delete.\nClick Yes on the Confirm Key Delete message box.", "answer_start": 1477, "answer_category": null}], "is_impossible": false}], "context": "This article applies to:\nPlatform(s): Windows 10, Windows 7, Windows 8, Windows Vista, Windows XP\nJava version(s): 7.0, 8.0\nSYMPTOMS\n\nAfter Java is uninstalled, Java icon is still visible in the Windows Control Panel\nWindows XP: Add or Remove Programs\nWindows 7, Windows 8, Windows 10: Uninstall Programs\n\nCAUSE\n\nThe Java uninstaller fails to delete registry entries from the Windows registry during the process of uninstalling Java. Since these registry keys are not removed from the Windows registry, the Java icon is still visible in the Windows Control Panel.\n\nSOLUTION\n\nThe following are two methods to cleanup registry entries left behind due to an incomplete uninstall process:\nMicrosoft utility (Recommended Method)\nManual Registry Edit\nRecommended Method: Microsoft utility\nRun the Microsoft utility to repair corrupted registry keys that prevents programs from being completely uninstalled or blocking new installations and updates.\n\nManual Registry Edit (Use this only if the Fix it utility does not work)\nalert iconIncorrectly editing your registry may severely damage your system. You should back up any valued data from your computer before making changes to the registry.\n\nYou can use the File->Export functionality of the registry editor to save the registry key before deleting. Then, in case you deleted the wrong registry key, you can restore the registry from your saved backup file, by using the File->Import functionality in the Windows Registry Editor.\n\nRun the Windows Registry Editor by clicking Start->Run and entering the program name regedit in the Open edit field.\nNavigate to the following registry node:\nHKEY_LOCAL_MACHINE/Software/Microsoft/\nWindows/CurrentVersion/Uninstall\n\nUnder this Uninstall node, you will find many registry entry names enclosed in curly braces. e.g. {26A24AE4-039D-4CA4-87B4-2F83216013FB}\nClick on each registry entries in the left pane and that will display its associated data on right pane of Registry Editor.\nor\nClick on Edit->Find tab in Registry Editor window and then enter Java version that you want to find Keys for\nFor example: serching for Java(TM) 6 Update 24 finds this:\nThe Registry Editor\n\nDelete the registry entry found for Java, by right clicking on the registry key name, and selecting Delete.\nClick Yes on the Confirm Key Delete message box.\nAfter you finish above steps, go back to the Windows Control Panel\nWindows XP: Add or Remove Programs\nWindows 7, Windows 8, Windows 10: Uninstall a program\nThe entry for Java should no longer appear.\nRELATED INFORMATION\n\nRegistry Back up (Microsoft)\n\u00bb Windows Back up the registry", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "how do i install and build against openssl 1 0 0 on ubuntu", "id": 1505, "answers": [{"answer_id": 1494, "document_id": 1083, "question_id": 1505, "text": "tall\n\n\nDo:\n\n    $ sudo ./config --prefix=/usr\n    $ sudo make\n    $ sudo make test\n    $ sudo ", "answer_start": 3842, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nYou can consider this a follow-up question to How do I install the OpenSSL C++ library on Ubuntu?\n\nI'm trying to build some code on Ubuntu 10.04 LTS that requires OpenSSL 1.0.0.\n\nUbuntu 10.04 LTS comes with OpenSSL 0.9.8k:\n\n$ openssl version\nOpenSSL 0.9.8k 25 Mar 2009\n\n\nSo after running sudo apt-get install libssl-dev and building, running ldd confirms I've linked in 0.9.8:\n\n$ ldd foo\n        ...\n        libssl.so.0.9.8 =&gt; /lib/i686/cmov/libssl.so.0.9.8 (0x00110000)\n        ...\n        libcrypto.so.0.9.8 =&gt; /lib/i686/cmov/libcrypto.so.0.9.8 (0x002b0000)\n        ...\n\n\nHow do I install OpenSSL 1.0.0 and the 1.0.0 development package?\n\nUpdate: I'm writing this update after reading SB's answer (but before trying it), because it's clear I need to explain that the obvious solution of downloading and installing OpenSSL 1.0.0 doesn't work:\n\nAfter successfully doing the following (recommended in the INSTALL file):\n\n  $ ./config\n  $ make\n  $ make test\n  $ make install\n\n\n...I still get:\n\nOpenSSL 0.9.8k 25 Mar 2009\n\n\n...and:\n\n$ sudo apt-get install libssl-dev\nReading package lists... Done\nBuilding dependency tree       \nReading state information... Done\nlibssl-dev is already the newest version.\nThe following packages were automatically installed and are no longer required:\n  linux-headers-2.6.32-21 linux-headers-2.6.32-21-generic\nUse 'apt-get autoremove' to remove them.\n0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.\n\n\n...and (just to make sure) after rebuilding my code, ldd still returns the same thing.\n\nUpdate #2: I added the \"-I/usr/local/ssl/include\" and \"-L/usr/local/ssl/lib\" options (suggested by SB) to my makefile, but I'm now getting a bunch of undefine reference compile errors, for example:\n\n/home/dspitzer/foo/foo.cpp:86: undefined reference to `BIO_f_base64'\n/home/dspitzer/foo/foo.cpp:86: undefined reference to `BIO_new'\n\n\n/usr/local/ssl/include/ contains only an openssl directory (which contains numerous .h files), so I also tried \"-I/usr/local/ssl/include/openssl\" but got the same errors.\n\nUpdate #3: I tried changing the OpenSSL includes from (for example):\n\n#include &lt;openssl/bio.h&gt;\n\n\n...to:\n\n#include \"openssl/bio.h\"\n\n\n...in the .cpp source file but still get the same undefined reference errors.\n\nUpdate #4: I now realize those undefined reference errors are linker errors.  If I remove the \"-L/usr/local/ssl/lib\" from my Makefile, I don't get the errors (but it links to OpenSSL 0.9.8).  The contents of /usr/local/ssl/lib/ are:\n\n$ ls /usr/local/ssl/lib/\nengines  libcrypto.a  libssl.a  pkgconfig\n\n\nI added -lcrypto, and the errors went away.\n    \n\nGet the 1.0.0a source from here.\n\n# tar -xf openssl-1.0.0a.tar.gz\n# cd openssl-1.0.0a\n# ./config\n# sudo make install\n\n\nThis puts it in /usr/local/ssl by default\n\nWhen you build, you need to tell gcc to look for the headers in /usr/local/ssl/include and link with libs in /usr/local/ssl/lib.  You can specify this by doing something like:\n\ngcc test.c -o test -I/usr/local/ssl/include -L/usr/local/ssl/lib -lssl -lcrypto\n\n\nEDIT DO NOT overwrite any system libraries.  It's best to keep new libs in /usr/local.  Overwriting Ubuntu defaults can be hazardous to your health and break your system.\n\nAdditionally, I was wrong about the paths as I just tried this in Ubuntu 10.04 VM.  Fixed.\n\nNote, there is no need to change LD_LIBRARY_PATH since the openssl libs you link against by default are static libs (at least by default - there might be a way to configure them as dynamic libs in the ./config step)\n\nYou may need to link against libcrypto because you are using some calls that are built and defined in the libcrypto package.  Openssl 1.0.0 actually builds two libraries, libcrypto and libssl.\n\nEDIT 2 Added -lcrypto to gcc line.\n    \n\nInstead of:\n\n    $ ./config\n    $ make\n    $ make test\n    $ make install\n\n\nDo:\n\n    $ sudo ./config --prefix=/usr\n    $ sudo make\n    $ sudo make test\n    $ sudo make install\n\n\nThis will help you update to openssl 1.0.1g to patch for CVE-2014-0160 (Heartbleed).\n\nOpenSSL Security Advisory [07 Apr 2014]\n\nTLS heartbeat read overrun (CVE-2014-0160)\n\nA missing bounds check in the handling of the TLS heartbeat extension can be\nused to reveal up to 64k of memory to a connected client or server.\n\nOnly 1.0.1 and 1.0.2-beta releases of OpenSSL are affected including\n1.0.1f and 1.0.2-beta1.\n\nThanks for Neel Mehta of Google Security for discovering this bug and to\nAdam Langley  and Bodo Moeller  for\npreparing the fix.\n\nAffected users should upgrade to OpenSSL 1.0.1g. Users unable to immediately\nupgrade can alternatively recompile OpenSSL with -DOPENSSL_NO_HEARTBEATS.\n\n1.0.2 will be fixed in 1.0.2-beta2.\n\nSource: https://www.openssl.org/news/secadv_20140407.txt\n    \n\nHere's what solved it for me:\nUpgrade latest version OpenSSL on Ubuntu\nTranscribing the main information:\nDownload the OpenSSL v1.0.0g source:\n$ wget http://www.openssl.org/source/openssl-1.0.0g.tar.gz\n\nUnpack the archive and install:\n$ tar xzvf openssl-1.0.0g.tar.gz\n$ cd openssl-1.0.0g\n$ ./config\n$ make\n$ make test\n$ sudo make install\n\nAll files, including binaries and man pages are install under the directory /usr/local/ssl. To ensure users use this version of OpenSSL instead of the previous version you must update the paths for man pages and binaries.\nEdit the file /etc/manpath.config adding the following line before the first MANPATH_MAP:\nMANPATH_MAP     /usr/local/ssl/bin      /usr/local/ssl/man\n\nUpdate the man database (I honestly can't remember and don't know for sure if this command was necessary - maybe try without it and at the end when testing if the man pages are still the old versions come back and run mandb):\nsudo mandb\n\nEdit the file /etc/environment and insert the path for OpenSSL binaries (/usr/local/ssl/bin) before the path for Ubuntu's version of OpenSSL (/usr/bin). My environment file looks like this:\nPATH=\"/usr/local/sbin:/usr/local/bin:/usr/local/ssl/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games\"\n\nLogout and login and test:\n$ openssl version\nOpenSSL 1.0.0g 18 Jan 2012\n\nAlso test the man pages by running man openssl and at the very bottom in the left hand corner it should report 1.0.0g.\nNote that although the users will now automatically use the new version of OpenSSL, existing programs (e.g. Apache) may not as they are linked against the libraries from the Ubuntu version.\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "update r version from 3 0 3 to 3 1 2", "id": 1485, "answers": [{"answer_id": 1474, "document_id": 1061, "question_id": 1485, "text": "sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys E084DAB9\nsudo apt-get update\nsudo apt-get upgrade\nsudo apt-get install r-base", "answer_start": 269, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI want to update R with ubuntu from 3.0.3 to 3.1.2. I am working on Ubuntu 14.04LTS, 64bits.\n\nI did the following: \n\ncd /etc/apt/\nsudo gedit sources.list\n\n\nI added the following line :\n\ndeb http://cran.rstudio.com/bin/linux/ubuntu trusty/\n\n\nThen :\n\nsudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys E084DAB9\nsudo apt-get update\nsudo apt-get upgrade\nsudo apt-get install r-base\n\n\nSome files were uploaded.\n\nThen I checked the R version :\n\nsudo R\nversion\n\nplatform       x86_64-unknown-linux-gnu    \narch           x86_64                      \nos             linux-gnu                   \nsystem         x86_64, linux-gnu           \nstatus                                     \nmajor          3                           \nminor          0.3                         \nyear           2014                        \nmonth          03                          \nday            06                          \nsvn rev        65126                       \nlanguage       R                           \nversion.string R version 3.0.3 (2014-03-06)\nnickname       Warm Puppy      \n\n\nI still have the 3.0.3 version, can someone help me to upgrade to 3.1.2?\n\nEDIT for ECII:\n\nsudo apt-get dist-upgrade\n\nReading package lists... Done\nBuilding dependency tree       \nReading state information... Done\nThe following packages were automatically installed and are no longer required:\nblcr-util bwidget gcc-4.8-base:i386 ggobi jags libaudio2:i386 libcr0\nlibexpat1:i386 libffi6:i386 libfontconfig1:i386 libfreetype6:i386\nlibgcrypt11:i386 libgl1-mesa-glx:i386 libglapi-mesa:i386 libglib2.0-0:i386\nlibglpk36 libgpg-error0:i386 libgstreamer-plugins-base1.0-0:i386\nlibgstreamer1.0-0:i386 libhwloc5 libibverbs1 libice6:i386 libjbig0:i386\nlibjpeg-turbo8:i386 libjpeg8:i386 libodbc1 libopenmpi1.6 liborc-0.4-0:i386\nlibqt4-dbus:i386 libqt4-declarative:i386 libqt4-network:i386\nlibqt4-opengl:i386 libqt4-script:i386 libqt4-sql:i386 libqt4-xml:i386\nlibqt4-xmlpatterns:i386 libqtcore4:i386 libqtdbus4:i386 libqtgui4:i386\nlibqtwebkit4:i386 libquantlib-1.2 libsm6:i386 libsprng2 libsqlite3-0:i386\nlibssl1.0.0:i386 libstdc++6:i386 libtiff4 libtiff5:i386 libtk8.5 libtorque2\nlibx11-6:i386 libx11-xcb1:i386 libxau6:i386 libxcb-dri2-0:i386\nlibxcb-dri3-0:i386 libxcb-glx0:i386 libxcb-present0:i386 libxcb-sync1:i386\nlibxcb1:i386 libxdamage1:i386 libxdmcp6:i386 libxext6:i386 libxfixes3:i386\nlibxi6:i386 libxml2:i386 libxrender1:i386 libxshmfence1:i386 libxslt1.1:i386\nlibxss1:i386 libxt6:i386 libxv1:i386 libxxf86vm1:i386\nlinux-headers-3.13.0-35 linux-headers-3.13.0-35-generic\nlinux-image-3.11.0-26-generic linux-image-3.13.0-35-generic\nlinux-image-extra-3.11.0-26-generic linux-image-extra-3.13.0-35-generic lynx\nmpi-default-bin openmpi-bin openmpi-checkpoint openmpi-common skype-bin:i386\ntk-table tk8.5 unixodbc\n\n\nEDIT for cdeterman:\n\nsudo apt-key list\n/etc/apt/trusted.gpg\n--------------------\npub   1024D/437D05B5 2004-09-12\nuid                  Ubuntu Archive Automatic Signing Key &lt;ftpmaster@ubuntu.com&gt;\nsub   2048g/79164387 2004-09-12\n\npub   1024D/FBB75451 2004-12-30\nuid                  Ubuntu CD Image Automatic Signing Key &lt;cdimage@ubuntu.com&gt;\n\npub   4096R/C0B21F32 2012-05-11\nuid                  Ubuntu Archive Automatic Signing Key (2012) &lt;ftpmaster@ubuntu.com&gt;\n\npub   4096R/EFE21092 2012-05-11\nuid                  Ubuntu CD Image Automatic Signing Key (2012) &lt;cdimage@ubuntu.com&gt;\n\npub   1024D/3E5C1192 2010-09-20\nuid                  Ubuntu Extras Archive Automatic Signing Key &lt;ftpmaster@ubuntu.com&gt;\n\npub   1024R/579F147D 2010-04-25\nuid                  Launchpad RKWard Development Archive\n\npub   2048R/E084DAB9 2010-10-19 [expire\u00a0: 2015-10-18]\nuid                  Michael Rutter &lt;marutter@gmail.com&gt;\nsub   2048R/1CFF3E8F 2010-10-19 [expire\u00a0: 2015-10-18]\n\n    \n\nOk, I fixed it.\n\nIn synaptic package manager, I saw that R-3.1.2 was installed in the following directory (and the executable is called R) : /usr/lib/R/bin/R\n\nAnd when I type\n\nwhich R\n\n\nIt returns:\n\n/usr/local/bin/R\n\n\nSo the source of R is not the same path than the one which was updated.\nI've just copy-pasted the new executable (which was in /usr/lib/R/bin/ in the good directory /usr/local/bin/).\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Get referrer after installing app from Android Market", "id": 768, "answers": [{"answer_id": 768, "document_id": 455, "question_id": 768, "text": "Apparently you MUST use the same parameter names as outlined here: http://code.google.com/mobile/analytics/docs/android/#referrals", "answer_start": 65, "answer_category": null}], "is_impossible": false}], "context": "Okay so I found the reason why the Intent wasn't being launched. Apparently you MUST use the same parameter names as outlined here: http://code.google.com/mobile/analytics/docs/android/#referrals\nYou cant use your own parameter names as I was doing :S\n I am trying to register a Broadcast Receiver that catches \"com.android.vending.INSTALL_REFERRER\" intents launched by Android after an app is installed from the Market.\nI am following the details here: http://code.google.com/mobile/analytics/docs/android/#referrals\nHowever, I cannot use Google Analytics so I have created my own solution. I have added the following to my manifest file:\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Cannot install R-forge package using install.packages", "id": 695, "answers": [{"answer_id": 699, "document_id": 387, "question_id": 695, "text": "# First download source file to your working directory\n# As an example use browser to download pkg:partykit from: \n#  http://download.r-forge.r-project.org/src/contrib/partykit_1.1-2.tar.gz\n# Move to working directory\n# Or in the case of returnanalytics (which is a bundle of packages):\n# http://r-forge.r-project.org/R/?group_id=579 and download the tar.gz (source)\n# Then in R:", "answer_start": 817, "answer_category": null}], "is_impossible": false}], "context": "This, question, is, asked, over, and, over, and, over, on the R-sig-finance mailing list, but I do not think it has been asked on stackoverflow.\nIt goes like this:\nWhere can I obtain the latest version of package XYZ that is hosted on R-forge? I tried to install it with install.packages, but this is what happened:\n> install.packages(\"XYZ\",repos=\"http://r-forge.r-project.org\")\nWarning message: package \u2018XYZ\u2019 is not available (for R version 2.15.0)\nLooking on the R-forge website for XYZ, I see that the package failed to build. Therefore, there is no link to download the source. Is there any other way to get the source code? Once I get the source code, how can I turn that into a package that I can load with library(\"XYZ\")? If (and only if) you have the appropriate toolchain for your OS, then this may succeed:\n# First download source file to your working directory\n# As an example use browser to download pkg:partykit from: \n#  http://download.r-forge.r-project.org/src/contrib/partykit_1.1-2.tar.gz\n# Move to working directory\n# Or in the case of returnanalytics (which is a bundle of packages):\n# http://r-forge.r-project.org/R/?group_id=579 and download the tar.gz (source)\n# Then in R:\n\ninstall.packages( \"partykit_1.1-2.tar.gz\", repo=NULL, type=\"source\")\n# for the first of the ReturnAnalytics packages:\ninstall.packages( \"Dowd_0.11.tar.gz\", repo=NULL, type=\"source\")\nThese direction should be \"cross-platform\". I'm not sure the directions in the accepted answer are applicable to Macs (OSX). (I later confirmed that they do \"work\" on a Mac but found the process more involved that what I suggested above. They do result in a directory that do contain the packages in a form that should succeed with R --vanilla CMD INSTALL --build pathToEachPackageSeparately)\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "What is the wix 'KeyPath' attribute?", "id": 1625, "answers": [{"answer_id": 1612, "document_id": 1199, "question_id": 1625, "text": "Windows Installer decides whether to install your component, it will first look whether the keypath resource is already present. If it is, none of the resources in the component are installed.", "answer_start": 71, "answer_category": null}], "is_impossible": false}], "context": "What is the Wix 'KeyPath' attribute? In particular, how does it apply.\nWindows Installer decides whether to install your component, it will first look whether the keypath resource is already present. If it is, none of the resources in the component are installed.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to update ruby on linux (ubuntu)?", "id": 831, "answers": [{"answer_id": 826, "document_id": 513, "question_id": 831, "text": "sudo apt-get install ruby1-9 rubygems1-9\nsudo ln -sf /usr/bin/ruby1-9 /usr/bin/ruby", "answer_start": 347, "answer_category": null}], "is_impossible": false}], "context": "I'm newbie on both ruby and linux, so I'm sure this is trivial but I don't know yet. I currently have ruby 1.8.7 installed and I want to update it to ruby 1.9. How can I do that? There's really no reason to remove ruby1-8, unless someone else knows better. Execute the commands below to install 1.9 and then link ruby to point to the new version.\nsudo apt-get install ruby1-9 rubygems1-9\nsudo ln -sf /usr/bin/ruby1-9 /usr/bin/ruby\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "What's the best way to distribute Java applications?", "id": 1626, "answers": [{"answer_id": 1613, "document_id": 1200, "question_id": 1626, "text": "advanced installer makes it easy to package java apps as windows executables, and it's quite flexible in the way you can set it up.", "answer_start": 463, "answer_category": null}], "is_impossible": false}], "context": "Java is one of my programming languages of choice. I always run into the problem though of distributing my application to end-users.\nGiving a user a JAR is not always as user friendly as I would like and using Java WebStart requires that I maintain a web server.\nWhat's the best way to distribute a Java application? What if the Java application needs to install artifacts to the user's computer? Are there any good Java installation/packaging systems out there?\nadvanced installer makes it easy to package java apps as windows executables, and it's quite flexible in the way you can set it up.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How do I install package.json dependencies in the current directory using npm", "id": 1848, "answers": [{"answer_id": 1834, "document_id": 1419, "question_id": 1848, "text": "You should run:\nnpm install", "answer_start": 512, "answer_category": null}], "is_impossible": false}], "context": "I have a web app: fooapp. I have a package.json in the root. I want to install all the dependencies in a specific node_modules directory. How do I do this?\nnpm makes a copy of my app directory in the node_modules dir and installs the packages inside another node_modules directory.\nI understand this makes sense for installing a package. But I don't require() my web app inside of something else, I run it directly. I'm looking for a simple way to install my dependencies into a specific node_modules directory.\nYou should run:\nnpm install\nfrom inside your app directory (i.e. where package.json is located) will install the dependencies for your app, rather than install it as a module, as described here. These will be placed in ./node_modules relative to your package.json file (it's actually slightly more complex than this, so check the npm docs here).\t\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Install a .NET windows service without InstallUtil.exe", "id": 1657, "answers": [{"answer_id": 1645, "document_id": 1231, "question_id": 1657, "text": "You can install a service using:\nstring[] args;\nManagedInstallerClass.InstallHelper(args);", "answer_start": 237, "answer_category": null}], "is_impossible": false}], "context": "I have a standard .NET windows service written in C#.\nCan it install itself without using InstallUtil? Should I use the service installer class? How should I use it?\nTake a look at the InstallHelper method of the ManagedInstaller class. You can install a service using:\nstring[] args;\nManagedInstallerClass.InstallHelper(args);\nThis is exactly what InstallUtil does. The arguments are the same as for InstallUtil.\nThe benefits of this method are that it involves no messing in the registry, and it uses the same mechanism as InstallUtil.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "linux setup creator", "id": 1932, "answers": [{"answer_id": 1919, "document_id": 1506, "question_id": 1932, "text": "Use InstallBuilder 9", "answer_start": 2028, "answer_category": null}], "is_impossible": false}], "context": "\n\n                    \n\n            \n\n        \n\n            \n\n                \n\n                    \n\n                        Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.\n\n                        \n\n                    \n\n                \n\n            \n\n        \n\n            \n\n        \n\n                \n\n                    \n\n                \n\n            \n\n                \n\n                    Want to improve this question? Update the question so it's on-topic for Stack Overflow.\n\n                \n\n                    Closed 2 years ago.\n\n\n\n            \n\n        \n\n            \n\n                    \n\n                        Improve this question\n\n                    \n\n            \n\n\n\n\n\n    \n\n\n\nI'm looking for a tool that's able to create \"setup\" packages for Linux, just like the Windows install creators do (NSIS, InstallShield, etc.). I want it to be able to present a graphical interface to the user (or ncurses based), where he can select some options and install the package.\n\nAny ideas of such a tool? I'm aware of autopackage, but it's not exactly what I want. It focuses mostly on correctly installing the software, I want something that focuses on creating an installer that's able to copy files, edit, run scripts, etc.\n    \n\nOne option could be loki setup. It supports curses and gtk based setup programs. A few installer generators are cross-platform, relying on the presence of a JVM, like VAInstall. Commercial offerings include InstallAnywhere.\n    \n\nThere isn't one.\n\nInstalling software on linux/bsd is, usually, done by the package management system. What this package management system is depends on the linux distribution or bsd variant.\n\nMaking a package for a distribution is usually done by the distribution themselves. Mostly because they are teo many to for developers to support.\n\nSo you don't package it, let them do it.\n\nUnless the source isn't freely distributed, then pick the distribution(s) your (potential) users are using.\n    \n\nUse InstallBuilder 9. It seems awesome. \n\n\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "how to install pyside on centos", "id": 1986, "answers": [{"answer_id": 1972, "document_id": 1571, "question_id": 1986, "text": "equisites:\n\nsudo yum install epel-release\nsudo yum install cmake qt-devel qt-webkit-devel libxml2-devel libxslt-devel python-devel rpmdevtools gcc gcc-c++ make python-pip\nsudo ln -s /usr/bin/qmake-qt4 /u", "answer_start": 2060, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI want to install ReText on CentOS. Have a problem,\n\n[root@localhost scripts-2.6]# python retext.py \nTraceback (most recent call last):\n  File \"retext.py\", line 23, in &lt;module&gt;\n    from ReText import QtCore, QtWidgets, QtWebKit, datadirs, globalSettings\n  File \"/usr/lib/python2.6/site-packages/ReText/__init__.py\", line 21, in &lt;module&gt;\n    from PySide import QtCore, QtGui, QtWebKit\nImportError: No module named PySide\n\n\nThen I typed yum install PySide and yum install python-pyside to install PySide, and got the message No package available.\n\nAlso I tried yum search pyside and yum search python-, but that did not find the PySide package.\n    \n\nThere is no \"python-pyside\" package in EPEL 7:\nhttp://dl.fedoraproject.org/pub/epel/7/x86_64/repoview/letter_p.group.html\n\nThat is why you can't use the proposed by Nir Ben-Or solution for CentOS 7.\n\nI have solved this with pip install, however you may need to install some prerequisites first.\n\n1) install RPM packages (UPDATED thanks to @fredrik for a comment):\n\nqt-webkit-devel\nlibxml2-devel\nlibxslt-devel\nrpmdevtools\ngcc\ngcc-c++\nqt-devel\ncmake\npython-devel\npython-pip\n\n\nBuild of pyside should tell you if some RPM-s are missing.\n\n2) if you qmake program is not \"/usr/bin/qmake\" you may need to do something like this:\n\nsudo ln -s /usr/bin/qmake-qt4 /usr/bin/qmake\n\n\n3) install pyside via pip (this will take some time for build to complete):\n\nsudo pip install pyside\n\n    \n\nThe CentOS repository does not contain PySide.\nYou can add the EPEL repository to your OS then use: yum install -y python-pyside\nand it will be installed on your system.\n\nIn order to install the EPEL repository for 64bit systems, run the following as root:\n\n\ncd /tmp\nwget http://download.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpm\nrpm -ivh epel-release-6-8.noarch.rpm\n\n\nIf you're using a 32bit system, use the following in step 2:\n\nwget http://download.fedoraproject.org/pub/epel/6/i386/epel-release-6-8.noarch.rpm\n    \n\nThis is for CentOS 7.\n\nFirst, make sure you've got the prerequisites:\n\nsudo yum install epel-release\nsudo yum install cmake qt-devel qt-webkit-devel libxml2-devel libxslt-devel python-devel rpmdevtools gcc gcc-c++ make python-pip\nsudo ln -s /usr/bin/qmake-qt4 /usr/bin/qmake\n\n\nNow you should be able to install PySide:\n\nsudo pip install PySide\n\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "checklist for database schema upgrades", "id": 1488, "answers": [{"answer_id": 1477, "document_id": 1064, "question_id": 1488, "text": "Have a schema dump with static data that is required to be there kept up to date and in version control. \nEvery time you do a schema changing action, ALTER, CREATE, etc. dump it to a file and throw it in version control. \nMake sure you update the original sql db dump. \nWhen doing pushes to live make sure you or your script applies the sql files to the db. \nClean up old sql files that are in version control as they become old.", "answer_start": 3034, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nHaving to upgrade a database schema makes installing a new release of software a lot trickier.  What are the best practices for doing this?\n\nI'm looking for a checklist or timeline of action items, such as\n\n\n8:30 shut down apps\n8:45 modify schema\n9:15 install new apps\n9:30 restart db\n\n\netc, showing how to minimize risk and downtime.  Issues such as\n\n\nbacking out of the upgrade if things go awry\nminimizing impact to existing apps\n\"hot\" updates while the database is running\npromoting from dev to test to production servers\n\n\nare especially of interest.\n    \n\nI have a lot of experience with this.  My application is highly iterative, and schema changes happen frequently.  I do a production release roughly every 2 to 3 weeks, with 50-100 items cleared from my FogBugz list for each one.  Every release we've done over the last few years has required schema changes to support new features.\n\nThe key to this is to practice the changes several times in a test environment before actually making them on the live servers.\n\nI keep a deployment checklist file that is copied from a template and then heavily edited for each release with anything that is out of the ordinary.\n\nI have two scripts that I run on the database, one for schema changes, one for programmability (procedures, views, etc).  The changes script is coded by hand, and the one with the procs is scripted via Powershell.  The change script is run when everything is turned off (you have to pick a time that annoys the least amount of users for this), and it is run command by command, manually, just in case anything goes weird.  The most common problem I have run into is adding a unique constraint that fails due to duplicate rows.\n\nWhen preparing for an integration testing cycle, I go through my checklist on a test server, as if that server was production.  Then, in addition to that, I go get an actual copy of the production database (this is a good time to swap out your offsite backups), and I run the scripts on a restored local version (which is also good because it proves my latest backup is sound).  I'm killing a lot of birds with one stone here.\n\nSo that's 4 databases total:\n\n\nDev: all changes must be made in the change script, never with studio.\nTest: Integration testing happens here\nCopy of production: Last minute deployment practice\nProduction\n\n\nYou really, really need to get it right when you do it on production.  Backing out schema changes is hard.\n\nAs far as hotfixes, I will only ever hotfix procedures, never schema, unless it's a very isolated change and crucial for the business.\n    \n\nI guess you have considered the reads of Scott Ambler?\nhttp://www.agiledata.org/essays/databaseRefactoring.html\n    \n\nThis is a topic that I was just talking about at work. Mainly the problem is that unless database migrations is handled for you nicely by your framework, eg rails and their migration scripts, then it is left up to you. \n\nThe current way that we do it has apparent flaws, and I am open to other suggestions. \n\n\nHave a schema dump with static data that is required to be there kept up to date and in version control. \nEvery time you do a schema changing action, ALTER, CREATE, etc. dump it to a file and throw it in version control. \nMake sure you update the original sql db dump. \nWhen doing pushes to live make sure you or your script applies the sql files to the db. \nClean up old sql files that are in version control as they become old.\n\n\nThis is by no means optimal and is really not intended as a \"backup\" db. It's simply to make pushes to live easy, and to keep developers on the same page. There is probably something cool you could setup with capistrano as far as automating the application of the sql files to the db. \n\nDb specific version control would be pretty awesome. There is probably something that does that and if there isn't there probably should be. \n    \n\nAnd if the Scott Ambler paper whets your appetite I can recommend his book with Pramod J Sadolage called 'Refactoring Databases' - http://www.ambysoft.com/books/refactoringDatabases.html\n\nThere is also a lot of useful advice and information at the Agile Database group at Yahoo - http://tech.groups.yahoo.com/group/agileDatabases/\n    \n\nTwo quick notes:\n\n\nIt goes without saying... So I'll say it twice.\nVerify that you have a valid backup.\nVerify that you have a valid backup.\n@mk.  Check out Jeff's blog post on database version control (if you haven't already)\n\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to install/update/remove APK using \"PackageInstaller\" class in Android L?", "id": 1225, "answers": [{"answer_id": 1218, "document_id": 801, "question_id": 1225, "text": "You either need :\n\u2022\tthe INSTALL_PACKAGES permission. But this permission is not available for third-party application. So even with your profile owner app, you won't have this specific permission.\n\u2022\tRun the process as ROOT_UID. Which means you'll have to root the device.", "answer_start": 526, "answer_category": null}], "is_impossible": false}], "context": "Plz check below classe & give me the suggestion for how to use them https://developer.android.com/reference/android/content/pm/PackageInstaller.html https://developer.android.com/reference/android/content/pm/PackageInstaller.Session.html\nSo please give me an example to install/update/remove app. Can it be possible that the new application will install in device profile owner?\n\nYou cannot silently install a third party application in the newly created user with PackageInstaller.Session.commit() without specific \"rights\".\nYou either need :\n\u2022\tthe INSTALL_PACKAGES permission. But this permission is not available for third-party application. So even with your profile owner app, you won't have this specific permission.\n\u2022\tRun the process as ROOT_UID. Which means you'll have to root the device.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to save and restore Android Studio user interface settings?", "id": 1164, "answers": [{"answer_id": 1157, "document_id": 741, "question_id": 1164, "text": "You can go file > Export settings then you get to choose exactly the settings to export and you get a single settings.jar file", "answer_start": 340, "answer_category": null}], "is_impossible": false}], "context": "Android Studio allows the user to customize certain features, such as editor settings and coding styles. On Windows boxes, the default folder for these settings appears to be C:\\Users\\{username}\\.AndroidStudio{version}\\config. I'm looking for documentation for these files so that we can decide which files should be under version control. You can go file > Export settings then you get to choose exactly the settings to export and you get a single settings.jar file.\nThe goals are: 1) to have consistent UI settings for a group of developers; and 2) to have an easy way to configure a new Android Studio install.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "installation of eclipse php in ubuntu", "id": 1444, "answers": [{"answer_id": 1433, "document_id": 1017, "question_id": 1444, "text": "Install Eclipse:  Open Synaptic Package Manager (System/Administration/Synaptic Package Manager), Find the package \"eclipse\" and mark it for installation, Click Apply.\nInstall PHP Development Tools (PDT): Open Eclipse (you must specify your workspace); ignore the \"Welcome to Eclipse\" screen is displayed, select menu Help/Install New Software...", "answer_start": 1696, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nHow to install  eclipse php in ubuntu platform using apt get install. Any other utilities are required to do that\n    \n\nThe simplest solution, which will allow you to have the latest version, would be to :\n\n\ngo to http://eclipse.org/pdt/\ndownload the lastest version (icon on the right)\nChoose the \"PDT 2.1 All In Ones / Eclipse PHP Package\" package that's OK for you : Linux x86, 32 or 64 bits\ndownload that package\nunzip it\nand voila, you can run Eclipse PDT\n\n\nAdvantages :\n\n\nlast version\ndoesn't require root privileges\n\n\nI'm always using this solution, because new versions of Eclipse PDT generally bring nice enhancements, and are not integrated on the Ubuntu repositories before a long time passes... And I've never had any problem with this solution.\n    \n\nDon't! Don't use apt-get for Eclipse. Instead, follow these steps to install it \"manually\".\n    \n\nI think there are the SIMPLEST one for UBUNTU 10+ ... and I have problems with the other \"handmade methods\" answered here.\n\nSEE https://help.ubuntu.com/community/PHPEclipse  or  https://help.ubuntu.com/community/EclipsePDT\n\n\nUse \"software centre\" or Synaptic to install Eclipse... or apt-get.\nRun Eclipse and use the Help/Add to add the PHP-Eclipse (PDT) package.\n\n\n(Pascal's answer says \"Don't use apt-get\" but I say) UBUNTU supports and I recommend: USE IT, use apt-get! Is faster, simple and secure!\n\nPS: \"PDT - PHP Development Tool\" is the official name for \"Eclipse-PHP\".\n\n\n\nDETAILED INSTALLATION\n\nNOTE: On Ubuntu 10+ this method will install Eclipse 3.5+ (Galileo) and PDT 2.1+. If you have problems or want \"lower risk to re-do\", run, at the second step bellow, the Eclipse with root, \n     $sudo eclipse\n\n\nInstall Eclipse:  Open Synaptic Package Manager (System/Administration/Synaptic Package Manager), Find the package \"eclipse\" and mark it for installation, Click Apply.\nInstall PHP Development Tools (PDT): Open Eclipse (you must specify your workspace); ignore the \"Welcome to Eclipse\" screen is displayed, select menu Help/Install New Software...\n\n\nThe \"Galileo\" site must be added, click \"Add...\" and input:\nName: Galileo \u00a0 Location: http://download.eclipse.org/releases/galileo/\nOnce this is available, select \"Galileo\" in the Work with: drop-down list.\nEnter \"php\" into the filter text, two entries should be displayed for \"PHP Developer Tools (PDT)\"; one under Programming Languages, another under Web, XML, and Java EE Development. Select both and click \"Next\".\nVerify that the items you wish to install are displayed on the \"Install Details\" screen and click \"Finish\".\nEclipse will need restarted for the update to complete.\n\n\n    \n\nhere is the correct &amp; Easy way : http://ubuntulinuxpower.blogspot.com/2013/01/installing-eclipse-pdt-with-php-support.html\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Play Framework application deployment", "id": 1107, "answers": [{"answer_id": 1099, "document_id": 684, "question_id": 1107, "text": "You can look at this direction: Getting Started on Heroku with Scala and Play | Heroku Dev Center.", "answer_start": 178, "answer_category": null}], "is_impossible": false}], "context": "it's been a long time working on a play app & now comes the time to deploye it. that my first time so i'm kind of lost. which hosting compagny is the best & offer good pricing ?\nYou can look at this direction: Getting Started on Heroku with Scala and Play | Heroku Dev Center.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Deployment invalid: spec.template.metadata.labels: Invalid value", "id": 511, "answers": [{"answer_id": 512, "document_id": 236, "question_id": 511, "text": "You need to add selector in spec of Deployment.\nAnd also, these selector should match with labels in PodTemplate.", "answer_start": 55, "answer_category": null}], "is_impossible": false}], "context": "I get the error in deploying my service to production.\nYou need to add selector in spec of Deployment.\nAnd also, these selector should match with labels in PodTemplate.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "bundle install returns could not locate gemfile", "id": 1450, "answers": [{"answer_id": 1439, "document_id": 1023, "question_id": 1450, "text": "n my case this worked:\n\ncd /usr/bin\ntouch Gemfile\nNow you can then run bundle install.\n\n    \n\nSearch for the Gemfile file in your project,  go to that directory and then run \"bundle install\". prior to running this command make sure you have installed the gem \"sudo gem install bundler\"\n    ", "answer_start": 2569, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI'm new to Rails and am currently working through a guide. \nThe guide states:\n\n\n  Use a text editor to update the Gemfile needed by Bundler with the\n  contents of Listing 2.1.\n\nsource 'https://rubygems.org'\n\ngem 'rails', '3.2.3'\n\ngroup :development do\n  gem 'sqlite3', '1.3.5'\nend\n\n\n# Gems used only for assets and not required\n# in production environments by default.\n\ngroup :assets do\n  gem 'sass-rails',   '3.2.4'\n  gem 'coffee-rails', '3.2.2'\n  gem 'uglifier', '1.2.3'\nend\n\ngem 'jquery-rails', '2.0.0'\n\ngroup :production do\n  gem 'pg', '0.12.2'\nend\n\n  \n  We then install and include the gems using the bundle install\n    command:\n\n    $ bundle install --without production \n\n  \n  If Bundler complains about no such file to load -- readline\n  (LoadError) try adding gem \u2019rb-readline\u2019 to your Gemfile.)\n\n\nI followed the steps even adding on gem 'rb-readline' to the Gemfile, but apparently the file can't be found and when I go to my text editor I do see the Gemfile itself. I noticed that they made me put gem 'rails', 3.2.3 and my version of Rails is 3.2.1 so I tried changing it to 3.2.1 but that didn't work either.\n\nAny thoughts or advice would be much appreciated.\n    \n\nYou just need to change directories to your app, THEN run bundle install :)\n    \n\nYou may also indicate the path to the gemfile in the same command e.g. \n\nBUNDLE_GEMFILE=\"MyProject/Gemfile.ios\" bundle install\n\n    \n\nI had this problem as well on an OSX machine. I discovered that rails was not installed... which surprised me as I thought OSX always came with Rails. \nTo install rails\n\n\nsudo gem install rails\nto install jekyll I also needed sudo\nsudo gem install jekyll bundler\ncd ~/Sites\njekyll new &lt;foldername&gt;\ncd &lt;foldername&gt; OR cd !$ (that is magic ;)\nbundle install\nbundle exec jekyll serve\nThen in your browser just go to http://127.0.0.1:4000/ and it really should be running\n\n    \n\nYou must be in the same directory of Gemfile\n    \n\nWhen I had similar problem gem update --system helped me. Run this before bundle install\n    \n\nI had this problem on Ubuntu 18.04. I updated the gem\n\nsudo gem install rails\nsudo gem install jekyll\nsudo gem install jekyll bundler\ncd ~/desiredFolder\njekyll new &lt;foldername&gt;\ncd &lt;foldername&gt; OR \nbundle init\nbundle install\nbundle add jekyll\nbundle exec jekyll serve\n\nAll worked and goto your browser just go to http://127.0.0.1:4000/ and it really should be running\n    \n\n\nMake sure that the file name is Capitalized Gemfile instead of gemfile.\nMake sure you're in the same directory as the Gemfile.\n\n    \n\nIn my case this worked:\n\ncd /usr/bin\ntouch Gemfile\nNow you can then run bundle install.\n\n    \n\nSearch for the Gemfile file in your project,  go to that directory and then run \"bundle install\". prior to running this command make sure you have installed the gem \"sudo gem install bundler\"\n    \n\nsudo gem install rails\n\nmy problem solved, just this code.\n    \n\nThink more about what you are installing and navigate Gemfile folder, then try using sudo bundle install\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "starting with opengl and c proper path", "id": 1518, "answers": [{"answer_id": 1507, "document_id": 1094, "question_id": 1518, "text": "Use GLEW, no argument. Just do it, everyone does\nCode only to the core profile. By default, OpenGL accepts old command (eg fixed function pipeline) that will later disappear, and you don't want to waste your time on that. Specifically: learn about VBO's, texture's, and, most of all, learn about shaders.\n\nForget about glaux and glut. Freeglut is a good and very standard option. A less obvious choice would be qt, but it's QGLWidget allows you to easily make gl calls, and not worry about context creation and all that. And it's dead easy to add gui options, which is always very nice to have when programming graphics.\n\nTo actually learn OpenGL, I would recommend http://duriansoftware.com/joe/An-intro-to-modern-OpenGL.-Table-of-Contents.html. Nehe has that problem where more than half of the stuff is useless to learn, and there's a lot of fluff (window creation et al) around it.", "answer_start": 2061, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI need some specific and some general advice.  I'm a fairly proficient Java programmer and a very experienced web programmer, but I would like to get into software development and I've been tackling C++.  I have a great idea for a game I'd like to do, and I realize it will be a huge undertaking--but I'm looking at it more for an opportunity to learn C++, wrapping, really whatever I run into in the dev process...\n\nBut I can't get my foot in the door conceptually! I can handle the C++ aspect fine, it's just setting up the graphics, the RIGHT way, that's confusing me.  I've run through a bunch of tutorials for OpenGL with C++ that all say the different things, none of which I can really get to work...\n\nSome say to use GLUT and GLEW.  Some say GLUT is dead and that FreeGLUT is the thing now.  Some ignore those entirely and use a bunch of files like \"glaux.h\" that I can't seem to find--and other tutorials devoted to AVOIDING \"glaux.h\"...  Most tutorials I've found come with the caveat in the comments that their version of OpenGL is dated and I should be using newer libraries-- and still others with 3rd party libraries like Ogre and Aurora.\n\nI've been looking through a bunch of books and tutorials that ALL have an almost completely different setup for using OpenGL with C++.  I realized there is probably not one right way of doing it, per se, but I'm looking for the way that is the most current, most popular, and will maximize the usefulness of the project as far as my learning...  Any links to tutorials or advice in general is much appreciated.\n\nBTW, I'm using Visual Studio Express 2010 (good idea?). My game won't be too graphically intense (isometric 2d) but will require a TON of logic and a TON of data, which is why I want to speed things up by using C++.  Any other insights on better ways of doing it than using c++ for login AND graphics (from an industry perspective) are also very valuable to me!  Thanks in advance!\n    \n\nAssuming you're learning OpenGL as a learning experience, I would recommend you this:\n\nUse GLEW, no argument. Just do it, everyone does\nCode only to the core profile. By default, OpenGL accepts old command (eg fixed function pipeline) that will later disappear, and you don't want to waste your time on that. Specifically: learn about VBO's, texture's, and, most of all, learn about shaders.\n\nForget about glaux and glut. Freeglut is a good and very standard option. A less obvious choice would be qt, but it's QGLWidget allows you to easily make gl calls, and not worry about context creation and all that. And it's dead easy to add gui options, which is always very nice to have when programming graphics.\n\nTo actually learn OpenGL, I would recommend http://duriansoftware.com/joe/An-intro-to-modern-OpenGL.-Table-of-Contents.html. Nehe has that problem where more than half of the stuff is useless to learn, and there's a lot of fluff (window creation et al) around it.\n\nBut, I wouldn't really recommend OpenGL as a way to learn real-time graphics programming. Your alternatives are not limited to DirectX. I learned a ton of graphics coding from working with Ogre3D. You still have all the concepts at your disposal that you need to know (working low level with Vertex and Index buffers, Textures, shaders), and implements tons of stuff to make your life easier. You might not learn the ins and outs of a specific API, but you will learn all you need to know conceptually. When I became a graphics programmer, I hadn't written a line of DirectX code, but I got to grips with our engine really swiftly. I learned the actually calls very easy after that. Know the hardware, and the concepts. The actual implementation changes over time anyway.\n\nOh, and just in case I haven't repeated it enough. LEARN SHADERS\n    \n\nThe best tutorial around is (arguably, as anything \"best\") Nehe opengl tutorial. At least, this tutorial has been suggested for many years.\n\nAlso since you come from a Java background you might prefer C# bindings for opengl from frameworks such as Tao, but the actual setup might be harder than say, downloading samples and running them.\n    \n\nIt's easy to see where the variety of choices available for OpenGL with C++ would be a bit bewildering. For the most part, Java gives two a pretty clear-cut choice between two possibilities (do you want a scene graph or not?)\n\nThe situation with C++ is certainly more complex.\n\n\nGlut: This is basically an OpenGL-oriented application framework. It was apparently written primarily to allow examples to be short, but still work. The original implementation has some pretty well-known bugs, and hasn't been updated in over a decade. I would only use it in roughly the originally-intended context: short samples/examples/demos.\nglaux: The story here is sort of similar to GLUT. It has some memory leaks that have been known but unfixed for over a decade now. It was never portable in any case. I'd avoid this one completely.\nGLEW/GLEE: These allow relatively easy use of the functions added in newer versions of OpenGL from OpenGL implementations (e.g., Microsoft's) that haven't been updated to include headers/libraries to provide access to them directly. I prefer Glee by a small margin because it initializes itself automatically, but both work fine.\nFreeGLUT: This has been updated more recently, and some of the bugs expunged. It still hasn't done much to improve the basic design of GLUT though. Probably a better choice for short demos/samples, but still not (IMO) suitable for much serious use.\nOgre3D: This is much bigger than any of the preceding libraries. It's a full-blown scene graph library, with texture loading, defined file format, etc. A solid library, but be aware that if you use it, you won't normally use OpenGL directly at all (e.g., on Windows, it can render via either OpenGL or Direct3D without changing the source code at all).\nOpengSceneGraph: More often used for scientifically-oriented applications, but most closely comparable to Ogre3D.\nFLTK: a small, lightweight GUI library that runs on top of OpenGL.\nGLFW: In the same general spirit as GLUT, but it's been updated much more recently, doesn't have nearly as many bugs, and is (IMO) a nicer design -- minimal but still suitable for \"real\" use.\n\n    \n\nYou should be using OpenGL if you want to write portable 3D applications. Windows, Linux and Mac OS X supports it.\n\nYou might want to take a look at NeHe tutorials. It's one of the best OpenGL tutorials available on the web.\n    \n\nMy understanding is that if you want simple support for a recent version of OpenGL you'll have to leave Windows-land, so try take it in stride when getting started is ... complicated. \n\nI think OpenGL is probably the right choice. You might also want to consider DirectX (which is better supported on Windows), but I'm personally not a big fan of it. You could also learn C# and use XNA, but if you want to learn C++ it just defeats the purpose. (I also can't help mentioning there's a good chance you could make it fast enough without C++)\n\nI have to agree with the others that NeHe's tutorials are pretty much classic. You might also want to consider the OpenGL Red Book, but that costs money.\n\nRegarding the 3rd paragraph, GLUT is old (and for that matter so is GLU), but if you see a good tutorial that uses them, I don't see anything wrong with them. Once you have your feet wet you might want to consider ditching GLUT and using SDL which I believe is a lot more 'alive'. \n\nAs far as GLEW goes, I've used it with success and it's nice if you want to do something fancy like shaders on Windows. However, I would say don't worry about it at first, because it will increase your setup time and keep you from getting to the fun stuff :)\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to add local jar files to a Maven project?", "id": 1858, "answers": [{"answer_id": 1844, "document_id": 1429, "question_id": 1858, "text": "Install the JAR into your local Maven repository (typically .m2 in your home folder) as follows:\nmvn install:install-file \\\n   -Dfile=<path-to-file> \\\n   -DgroupId=<group-id> \\\n   -DartifactId=<artifact-id> \\\n   -Dversion=<version> \\\n   -Dpackaging=<packaging> \\\n   -DgeneratePom=true", "answer_start": 110, "answer_category": null}], "is_impossible": false}], "context": "How do I add local jar files (not yet part of the Maven repository) directly in my project's library sources?\nInstall the JAR into your local Maven repository (typically .m2 in your home folder) as follows:\nmvn install:install-file \\\n   -Dfile=<path-to-file> \\\n   -DgroupId=<group-id> \\\n   -DartifactId=<artifact-id> \\\n   -Dversion=<version> \\\n   -Dpackaging=<packaging> \\\n   -DgeneratePom=true\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Execute command after deploy AWS Beanstalk", "id": 1285, "answers": [{"answer_id": 1277, "document_id": 856, "question_id": 1285, "text": "You can find solution in this site:http://junkheap.net/blog/2013/05/20/elastic-beanstalk-post-deployment-scripts/", "answer_start": 342, "answer_category": null}], "is_impossible": false}], "context": "I have problem with execute command after deploy, i have some node.js project and script, this script use some bin from node_modules, if i write my command for script in .ebextensions/.config he execute before npm install and return error (\"node_modules/.bin/some\": No such file or directory). How i can execute command after deploy. Thanks.\nYou can find solution in this site:http://junkheap.net/blog/2013/05/20/elastic-beanstalk-post-deployment-scripts/\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "The only supported ciphers are AES-128-CBC and AES-256-CBC with the correct key lengths. laravel 5.3", "id": 672, "answers": [{"answer_id": 677, "document_id": 365, "question_id": 672, "text": " You need to have .env on your appication folder then run:\n$ php artisan key:generate\nIf you don't have .env copy from .env.example:\n$ cp .env.example .env", "answer_start": 228, "answer_category": null}], "is_impossible": false}], "context": "I installed a new fresh copy of Laravel 5.3 using composer but I'm getting this error . have this issue and it is totally random... 90% of my requests go through without any errors, then i get one HTTP 500 with this in the logs. You need to have .env on your appication folder then run:\n$ php artisan key:generate\nIf you don't have .env copy from .env.example:\n$ cp .env.example .env\nAnd things will start working! I am also having this exact problem on a fresh install. I've run the key generator, cleared and re-cached. I checked the .env file and the key was successfully inserted.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "changing androidinstalllocation from preferexternal to internalonly", "id": 1389, "answers": [{"answer_id": 1378, "document_id": 961, "question_id": 1389, "text": "Install the application from play store having install Location as preferExternal.\nConfirm that your application is installed in external SD card, you can use Settings-&gt;App to check that.\nNow create your new signed application package.\nInstall using adb install -r myapp.apk\nAgain goto Settings-&gt;App and confirm the location.\nRun functional test to see if nothing is b", "answer_start": 1002, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI have an app on Google Play market and I added android:installLocation=\"preferExternal\" to manifest.xml file and released long time ago. Now I would like to add Android home screen widget, so I need to change it to android:installLocation=\"internalOnly\". If I do that, what happens when a user upgrades? because a user already installed the old app on SD card. What is the best solution for this situation??\n\nIf someone has this kind of experience, please advise me.\n\nThanks in advance :)\n    \n\nWhen you set install location to preferExternal the application specific files are stored in .android_secure directory in external sdcard.\n\nThese type of application are not available when user mounts the SD Card as USB Mass Storage (feature removed from Android Jelly Bean).\n\nComing to the question, I believe in your case the application would be installed onto internal memory with the files from .android_secure moved to internal location.\n\nYou can confirm this by the following:\n\n\nInstall the application from play store having install Location as preferExternal.\nConfirm that your application is installed in external SD card, you can use Settings-&gt;App to check that.\nNow create your new signed application package.\nInstall using adb install -r myapp.apk\nAgain goto Settings-&gt;App and confirm the location.\nRun functional test to see if nothing is broken.\n\n    \n\nOld question, but according to my test, if an app is installed on SD card and you change afterwards the Manifest to android:installLocation=\"internalOnly\" (or omit the android:installLocation attribute, which is the same), when the app is upgraded the package manager will automatically move the app from SD Card to internal storage.\nNo idea what happens if this is not possible (not enough root on internal storage for example).\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Xcode 11.4 compile error 'Missing package product <package name>'", "id": 1903, "answers": [{"answer_id": 1890, "document_id": 1474, "question_id": 1903, "text": "I have two solutions on your question:\nSolution 1: File > Swift Packages > Reset Package Caches\nSolution 2: File > Swift Packages > Update to Latest Package Versions", "answer_start": 474, "answer_category": null}], "is_impossible": false}], "context": "When I integrate a local package in my Xcode project everything is fine initially but when i switch branches and want to run the app Xcode gives me the compile error Missing package product <package name>. When I quit Xcode and re-open it everything is fine again. Any idea what this can be? An Xcode bug?\nWe also integrate external packages via Swift Package Manager which works perfectly fine. No issues there.\nThe issue is also well described in a post by Jesse Squires.\nI have two solutions on your question:\nSolution 1: File > Swift Packages > Reset Package Caches\nSolution 2: File > Swift Packages > Update to Latest Package Versions\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "api for creating installers on windows", "id": 1925, "answers": [{"answer_id": 1912, "document_id": 1499, "question_id": 1925, "text": "WiX is a great tool, but you will have to do a lot of direct coding in order to make things happen", "answer_start": 1206, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nThere are lots of tools for creating installers on Windows (InstallShield, InnoSetup, NSIS, just to name a few). All tools I've seen fall in one or both of these categories\n\n\nPoint-and-click. Nice GUI for creating the installer, but the installer definition/project file can not be manually edited.\nTextfile: No (official) GUI. The installer is compiled from a definition in a text-file which is manually edited.\n\n\nThe installers I'm building are all defined using a DSL (represented as YAML files), so using a GUI is out of the question, and creating is textfile is cumbersome although doable.\n\nWhat I really would want is a tool which exposes a (complete) API, through which I can control the creation of the installer. Are there any such tools out there?\n\nEdit: I'd love to hear about non-MSI based tools as well. MSI is not a requirement (rather the other way around...)\n    \n\nWix 3.0 beta has .NET support included for this purpose. I don't know how well it works but it includes documentation. It's a framework of types for manipulating the installation creation process and all that goodness, so I don't think you even need to write a line of WiX XML if you don't want to.\n    \n\nWiX is a great tool, but you will have to do a lot of direct coding in order to make things happen. Fortunately the documentation is pretty good and there are several GUI tools, such as WixEdit on SourceForge to aid in the process.\n    \n\nWell, there is the Windows Installer API which you could use to create MSI files directly, however I think you'd be better off using WiX.\n\nThe \"direct coding\" will be much less than dealing with the Windows Installer API directly, I'm guessing that's probably going too \"low level\" for what you need. Depending on what you're looking to do, you could use WiX to generate an MSI and then tweak that afterwards using the API.\n\nWhat's wrong with generating XML? That's really going to be your simplest option... you won't need to manually edit it, just write your  own code to generate the required XML from your DSL files and a few templates.\n    ", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "how to download MSI Enterprise Installer for jre?", "id": 399, "answers": [{"answer_id": 406, "document_id": 171, "question_id": 399, "text": "a.\tClick the patch number of the MSI installer update that you need. For example, 32991008 is the patch number of the Oracle JRE 8 Update 291 b35 Enterprise Installer.\nIn the Details table, the updates are grouped by major version (for example, 8 or 11) with the most recent updates appearing at the top of the list.\nb.\tThe Patches & Updates tab opens and contains the patch number, product name, and additional details about the patch. Before downloading the patch, verify that this is the version of the MSI Enterprise JRE Installer that you need.\nc.\tIn the Platform drop-down list, select your operating system and then click the Download button.\nd.\tA new dialog box appears that contains the download link to the .zip file. Click the link and choose Save As from the pop-up menu to begin the download of the file to the directory of your choice.", "answer_start": 2512, "answer_category": null}], "is_impossible": false}, {"question": "how to Install the JRE from the MSI Enterprise Installer after downloading?", "id": 400, "answers": [{"answer_id": 407, "document_id": 171, "question_id": 400, "text": "2.\tExtract the contents of the the .zip file and double click the .msi file to run the installer.\n\u2022\tRun the installer with administrative permissions under the supported Windows Installer environments.\n\u2022\tThe installer notifies you if Java content is disabled in web browsers and provides instructions for enabling it. If you previously hid some of the security prompts for applets and Java Web Start applications, then the installer provides an option for restoring the prompts.\n\u2022\tWhen the JRE is installed on your system, Java Access Bridge is disabled by default. See Enabling and Testing Java Access Bridge on Microsoft Windows in the Java Accessibility Guide for instructions required to enable the Java Access Bridge.\n3.\tAfter you complete the installation of the JRE, you can delete the MSI installer file to recover disk space.", "answer_start": 3362, "answer_category": null}], "is_impossible": false}, {"question": "how to Install the JRE from the Command Line with Basic UI mode?", "id": 401, "answers": [{"answer_id": 408, "document_id": 171, "question_id": 401, "text": "Copymsiexec.exe /i installer.msi [INSTALLCFG=configuration_file_path] [options] /qb", "answer_start": 5801, "answer_category": null}], "is_impossible": false}, {"question": "how to Install the JRE from the Command Line with Silent or unattended mode?", "id": 402, "answers": [{"answer_id": 409, "document_id": 171, "question_id": 402, "text": "Copymsiexec.exe /i installer.msi [INSTALLCFG=configuration_file_path] [options] /qn ", "answer_start": 5914, "answer_category": null}], "is_impossible": false}, {"question": "how to create a log file for msi enterprise Jre installing?", "id": 403, "answers": [{"answer_id": 410, "document_id": 171, "question_id": 403, "text": "append /L C:\\path\\setup.log to the install command and scroll to the end of the log file to verify.\nThe following is an example of creating a log file:\nCopymsiexec.exe /i installer.msi /qn /L C:\\path\\setup.log ", "answer_start": 6627, "answer_category": null}], "is_impossible": false}], "context": "Use the MSI Enterprise JRE Installer to Install the JRE\nYou can use the Microsoft Windows Installer (MSI) Enterprise JRE Installer to install and uninstall the Java Runtime Environment (JRE) for Windows.\nNote:\nThe MSI Enterprise JRE Installer is available as part of Oracle Java SE Subscription and other legacy products (such as Oracle Java SE Advanced or Oracle Java SE Suite), and is only available to customers for download through My Oracle Support (MOS).\nThe Microsoft Windows Installer (MSI) Enterprise JRE Installer enables you to install the JRE across your enterprise. Because it fully supports Windows Installer 3.0, it is fully compatible with system management software, such as Systems Management Server (SMS) and Systems Center Configuration Manager (SCCM). These software management suites enable you to securely deploy software across your enterprise. In addition to the features and options that you can specify with the MSI Enterprise JRE Installer, you can specify a Java Usage Tracker configuration file and a deployment rule set.\nThis section includes the following topics:\n\u2022\tSystem Requirements\n\u2022\tInstalling the JRE from the MSI Enterprise Installer\n\u2022\tInstalling the JRE from the Command Line\n\u2022\tCreating a Log File\n\u2022\tPerforming a Static Installation of the JRE\n\u2022\tUninstalling the JRE with Java Removal and Uninstall Tools\n\u2022\tUninstalling the JRE from the Command Line\nSystem Requirements\nBefore installing the JRE, verify that your system meets the minimum system requirements.\nSee Windows System Requirements for JDK and JRE in the Java Platform, Standard Edition Installation Guide for minimum processor, disk space, and memory requirements.\nIf you have any difficulties, see General Java Troubleshooting in the Java Platform, Standard Edition Troubleshooting Guide or submit a bug report at http://bugreport.java.com/bugreport/.\nInstalling the JRE from the MSI Enterprise Installer\nYou can install JRE 8 by downloading and running the appropriate Oracle JRE 8 MSI Enterprise Installer for your system. The JRE 8 MSI Enterprise Installer is packaged as a \"pure\" .msi installer and runs with minimal dialogs. It supports silent installation of the JRE, and is customizable using command line parameters, a parameter file, or by using third party MSI customization tools.\nYou must have administrative permissions in order to install JRE 8.\n1.\tDownload the required version of the MSI Enterprise JRE Installer from Supported Java SE Downloads on MOS (Doc ID 1439822.1) on My Oracle Support.\na.\tClick the patch number of the MSI installer update that you need. For example, 32991008 is the patch number of the Oracle JRE 8 Update 291 b35 Enterprise Installer.\nIn the Details table, the updates are grouped by major version (for example, 8 or 11) with the most recent updates appearing at the top of the list.\nb.\tThe Patches & Updates tab opens and contains the patch number, product name, and additional details about the patch. Before downloading the patch, verify that this is the version of the MSI Enterprise JRE Installer that you need.\nc.\tIn the Platform drop-down list, select your operating system and then click the Download button.\nd.\tA new dialog box appears that contains the download link to the .zip file. Click the link and choose Save As from the pop-up menu to begin the download of the file to the directory of your choice.\n2.\tExtract the contents of the the .zip file and double click the .msi file to run the installer.\n\u2022\tRun the installer with administrative permissions under the supported Windows Installer environments.\n\u2022\tThe installer notifies you if Java content is disabled in web browsers and provides instructions for enabling it. If you previously hid some of the security prompts for applets and Java Web Start applications, then the installer provides an option for restoring the prompts.\n\u2022\tWhen the JRE is installed on your system, Java Access Bridge is disabled by default. See Enabling and Testing Java Access Bridge on Microsoft Windows in the Java Accessibility Guide for instructions required to enable the Java Access Bridge.\n3.\tAfter you complete the installation of the JRE, you can delete the MSI installer file to recover disk space.\nAfter installation is complete, you can use the Java item in the Windows Start menu to access essential Java information and functions, the help, the Java Control Panel, and to check for updates.\nInstalling the JRE from the Command Line\nYou can install the JRE by downloading the appropriate Oracle JRE MSI Enterprise Installer for your system and running it from the command line.\n1.\tDownload the required version of the MSI Enterprise JRE Installer from Supported Java SE Downloads on MOS (Doc ID 1439822.1) on My Oracle Support.\na.\tClick the patch number of the MSI installer update that you need. For example, 32991008 is the patch number of the Oracle JRE 8 Update 291 b35 Enterprise Installer.\nIn the Details table, the updates are grouped by major version (for example, 8 or 11) with the most recent updates appearing at the top of the list.\nb.\tThe Patches & Updates tab opens and contains the patch number, product name, and additional details about the patch. Before downloading the patch, verify that this is the version of the MSI Enterprise JRE Installer that you need.\nc.\tIn the Platform drop-down list, select your operating system and then click the Download button.\nd.\tA new dialog box appears that contains the download link to the .zip file. Click the link and choose Save As from the pop-up menu to begin the download of the file to the directory of your choice.\n2.\tExtract the contents of the the .zip file and open an MS-DOS prompt with Administrative permissions.\n3.\tRun one of the following commands depending on the type of installation that you want to perform:\no\tBasic UI mode:\nCopymsiexec.exe /i installer.msi [INSTALLCFG=configuration_file_path] [options] /qb\no\tSilent or unattended mode:\nCopymsiexec.exe /i installer.msi [INSTALLCFG=configuration_file_path] [options] /qn \n4.\tThe following items describe the variables used in these commands:\n\u2022\tinstaller.msi: The name of the MSI Enterprise JRE Installer that you obtained in the previous step.\n\u2022\tconfiguration_file_path: The path of the installer configuration file. See Use an Installer Configuration File to Install the JRE.\n\u2022\toptions: Options with specified values, separated by spaces. Use the same options as listed in Installer Configuration File Options. You can also use standard Windows Installer options.\nCreating a Log File\nYou can use a log file to verify that an installation succeeded.\nTo create a log file describing the installation, append /L C:\\path\\setup.log to the install command and scroll to the end of the log file to verify.\nThe following is an example of creating a log file:\nCopymsiexec.exe /i installer.msi /qn /L C:\\path\\setup.log \nThis example causes the log to be written to the C:\\path\\setup.log file.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Powershell in NonInteractive mode", "id": 564, "answers": [{"answer_id": 567, "document_id": 289, "question_id": 564, "text": "You should Run with -Force:\nStop-Process -InputObject $prc -ErrorAction SilentlyContinue -Force", "answer_start": 477, "answer_category": null}], "is_impossible": false}], "context": "I use Octopus for our deployments. I have a problem with one of the Powershell scripts to control the deployment.\nThe programs I try to stop are not the ones you see in the script above, but they represent what I am trying to do. Now the problem I have with it, is that it works well on one server, but not on another. Where it does not work, I get the error message:\nStop-Process : Windows PowerShell is in NonInteractive mode. Read and Prompt functionality is not available.\nYou should Run with -Force:\nStop-Process -InputObject $prc -ErrorAction SilentlyContinue -Force\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "Missing maven .m2 folder", "id": 762, "answers": [{"answer_id": 762, "document_id": 449, "question_id": 762, "text": "Use mvn -X or mvn --debug to find out from which different locations Maven reads settings.xml. This switch activates debug logging. Just check the first lines of mvn --debug | findstr /i /c:using /c:reading.", "answer_start": 359, "answer_category": null}], "is_impossible": false}], "context": "AFAIK maven does not have an installer for Windows, you simply unzip it wherever you like, as explained here.\nHowever in many places there are references to a .m2 folder under the user folder (in Win7 I would guess it to be by default at C:\\Users\\.m2. Alas I do not have that folder. Is there some command to create this folder? Am I missing something basic?\nUse mvn -X or mvn --debug to find out from which different locations Maven reads settings.xml. This switch activates debug logging. Just check the first lines of mvn --debug | findstr /i /c:using /c:reading. Right, Maven uses the Java system property user.home as location for the .m2 folder.\n", "document_id": 0}]}, {"paragraphs": [{"qas": [{"question": "How to specify the required JVM options and set system properties when deploying java applications?", "id": 601, "answers": [{"answer_id": 607, "document_id": 325, "question_id": 601, "text": "You can specify the required JVM options and set system properties using the <fx:jvmarg> and <fx:property> tags in your Ant task.", "answer_start": 147, "answer_category": null}], "is_impossible": false}, {"question": "How to specify JVM options that a user can override when deploying java applications?", "id": 602, "answers": [{"answer_id": 608, "document_id": 325, "question_id": 602, "text": "use the <fx:jvmuserarg> attribute in <fx:platform>, or for self-contained applications, use the -BuserJvmOptions attribute for the Java Packager tool.", "answer_start": 711, "answer_category": null}], "is_impossible": false}, {"question": "How to provide string substitution to the root of the install directory  when deploying java applications?", "id": 603, "answers": [{"answer_id": 609, "document_id": 325, "question_id": 603, "text": "You can provide string substitution to the root of the install directory for parameters passed into the application. In particular, you can use the macro $APPDIR, which is the full path of the directory that contains the self-contained bundles that the bundler creates", "answer_start": 1021, "answer_category": null}], "is_impossible": false}, {"question": "What\u2019s the guideline for packaging complex applications in general when deploying java applications?", "id": 604, "answers": [{"answer_id": 610, "document_id": 325, "question_id": 604, "text": "Mark platform-specific resources accordingly.\n\u2022\tFor JAR file that a user can double-click, consider packaging everything into a single JAR file and loading native libraries and data files from inside that JAR file.\nAlternatively, if you prefer to have multiple files:\no\tEnsure that all dependent JAR files are listed in the <fx:resources> tag in the <fx:jar> task that creates the main JAR file.\no\tList all data files and libraries in filesets with type=\"data\" to copy them into the output folder.\no\tLoad native libraries and resources from locations relative to the main JAR file.\no\tSee the example <fx:jar> Ant Task for a Simple Application in the Ant Task Reference chapter.", "answer_start": 2116, "answer_category": null}], "is_impossible": false}, {"question": "What\u2019s the guideline for packaging complex self-contained applications when deploying java applications?", "id": 605, "answers": [{"answer_id": 611, "document_id": 325, "question_id": 605, "text": "Avoid packaging anything but the main JAR file using <fx:jar>.\no\tList all dependent JAR files in the <fx:resources> section of <fx:deploy> and <fx:jar> for the main application JAR file.\no\tEither use an explicit location relative to the application JAR file to load native libraries, or copy native libraries into the root application folder. Use type=\"data\" to copy native library files.", "answer_start": 2860, "answer_category": null}], "is_impossible": false}, {"question": "What\u2019s the guideline for packaging complex Java Web Start and embedded applications when deploying java applications?", "id": 606, "answers": [{"answer_id": 612, "document_id": 325, "question_id": 606, "text": "List all dependent JAR files in the <fx:resources> section of <fx:deploy>.\no\tWrap native libraries into the JAR files for distribution.\nUse one JAR file per platform. Ensure that the JAR files contain only native libraries and that the libraries are all in the top-level folder of the JAR file.", "answer_start": 3347, "answer_category": null}], "is_impossible": false}], "context": "5.8.2 Customizing JVM Setup\nDoes your application need a larger heap size? Do you want to tune garbage collector behavior, or trace class loading?\nYou can specify the required JVM options and set system properties using the <fx:jvmarg> and <fx:property> tags in your Ant task. These tags are applicable to all execution modes except standalone applications. You always get the default JVM if you double-click the JAR file, but you can tailor the JVM to your requirements if you are running a self-contained application, a Java Web Start application, or an application embedded into a web page.\n5.8.2.1 Specifying User JVM Arguments\nIf you require the capability to specify JVM options that a user can override, use the <fx:jvmuserarg> attribute in <fx:platform>, or for self-contained applications, use the -BuserJvmOptions attribute for the Java Packager tool. These attributes explicitly define an attribute that can be overridden by the user.\n5.8.2.2 Macro Expansion of Application Directory for jvmarg and jvmuserarg\nYou can provide string substitution to the root of the install directory for parameters passed into the application. In particular, you can use the macro $APPDIR, which is the full path of the directory that contains the self-contained bundles that the bundler creates. In the following example, if the program is installed in C:\\Program Files\\myexample, then the macro $APPDIR expands to C:\\Program Files\\myexample\\app (because the app directory is where the application resources are installed in this example).\nExample 5-10 Substituting Parameters Passed to the Application\n<fx:platform>\n  <fx:jvmarg value=\"-Djava.policy.file=$APPDIR/app/whatever.policy\"/>\n  <fx:jvmuserarg name=\"-Xmx\" value=\"768m\" />\n</fx:platform>\n5.8.3 Packaging Complex Applications\nReal-life applications often have more than just a single JAR artifact. They may have third-party libraries, data files, native code, and so on. For a complex application, you might need special packaging tweaks to support different execution modes.\nThe following suggestions are general guidelines for packaging complex applications:\n\u2022\tMark platform-specific resources accordingly.\n\u2022\tFor JAR file that a user can double-click, consider packaging everything into a single JAR file and loading native libraries and data files from inside that JAR file.\nAlternatively, if you prefer to have multiple files:\no\tEnsure that all dependent JAR files are listed in the <fx:resources> tag in the <fx:jar> task that creates the main JAR file.\no\tList all data files and libraries in filesets with type=\"data\" to copy them into the output folder.\no\tLoad native libraries and resources from locations relative to the main JAR file.\no\tSee the example <fx:jar> Ant Task for a Simple Application in the Ant Task Reference chapter.\n\u2022\tThe following guidelines are for self-contained applications:\no\tAvoid packaging anything but the main JAR file using <fx:jar>.\no\tList all dependent JAR files in the <fx:resources> section of <fx:deploy> and <fx:jar> for the main application JAR file.\no\tEither use an explicit location relative to the application JAR file to load native libraries, or copy native libraries into the root application folder. Use type=\"data\" to copy native library files.\no\tSee Example 7-4.\n\u2022\tThe following guidelines are for Java Web Start and embedded applications:\no\tList all dependent JAR files in the <fx:resources> section of <fx:deploy>.\no\tWrap native libraries into the JAR files for distribution.\nUse one JAR file per platform. Ensure that the JAR files contain only native libraries and that the libraries are all in the top-level folder of the JAR file.\n", "document_id": 0}]}]}