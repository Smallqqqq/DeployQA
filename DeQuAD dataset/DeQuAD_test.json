{"data": [{"paragraphs": [{"qas": [{"question": "How should release notes be written?", "id": 1712, "answers": [{"answer_id": 1700, "document_id": 1285, "question_id": 1712, "text": "Public release notes should contain at least:\n\u2022\trelease, buildnumber\n\u2022\tall fixed public bugs\n\u2022\tall added public features\nQA release notes should contain at least:\n\u2022\trelease, buildnumber\n\u2022\tall fixed bugs including bug number\n\u2022\tall added features including links to design docs", "answer_start": 309, "answer_category": null}], "is_impossible": false}], "context": "Is there any sort of guidelines or best practices on how release notes should be written? I guess I am trying to find the proper balance between making the point without being too specific. Also, do developer usually provide a much more release notes for QA team compare to the one submitted for public view? Public release notes should contain at least:\n\u2022\trelease, buildnumber\n\u2022\tall fixed public bugs\n\u2022\tall added public features\nQA release notes should contain at least:\n\u2022\trelease, buildnumber\n\u2022\tall fixed bugs including bug number\n\u2022\tall added features including links to design docs\nConsider your audience and try to think what they need.\nAn other thing to add is new or discontinued support for certain platforms. (For example we quit support for Win3.1 and added Vista 64 bit).\n", "document_id": 1285}]}, {"paragraphs": [{"qas": [{"question": "phpStorm, do not index a folder", "id": 563, "answers": [{"answer_id": 566, "document_id": 288, "question_id": 563, "text": "From their documentation, looks like the only way to achieve this is to Exclude the directory which will hide it from the project tree.", "answer_start": 209, "answer_category": null}], "is_impossible": false}], "context": "\nI can't find the ability of phpStorm 5 to avoid indexing a specific folder. I don't want to exclude it from the Project-View, just don't want phpStorm to index the containing folders, classes, functions etc.\nFrom their documentation, looks like the only way to achieve this is to Exclude the directory which will hide it from the project tree.\n", "document_id": 288}]}, {"paragraphs": [{"qas": [{"question": "Introduction to Erlang/OTP production applications deployment", "id": 1094, "answers": [{"answer_id": 1086, "document_id": 671, "question_id": 1094, "text": "You should take the latter first, an external dependency is some other thing that has to run before your application can run.", "answer_start": 368, "answer_category": null}], "is_impossible": false}], "context": "I would like to develop and deploy an Erlang/OTP application into production on a VPS.\nI am pretty familiar with developing Erlang code on a local machine and my question is about deployment.\nBasically, I would like to know what steps I should take in order to move Erlang code from a local machine to a production server and make it run, i.e. be available for users.\nYou should take the latter first, an external dependency is some other thing that has to run before your application can run.\n", "document_id": 671}]}, {"paragraphs": [{"qas": [{"question": "rbenv shell not command", "id": 940, "answers": [{"answer_id": 935, "document_id": 586, "question_id": 940, "text": "Sets a shell-specific Ruby version by setting the RBENV_VERSION environment variable in your shell. This version overrides application-specific versions and the global version.", "answer_start": 13530, "answer_category": null}], "is_impossible": false}, {"question": "Rbenv stable verions?", "id": 941, "answers": [{"answer_id": 936, "document_id": 586, "question_id": 941, "text": "rbenv install -l", "answer_start": 9831, "answer_category": null}], "is_impossible": false}, {"question": "rbenv local version?", "id": 942, "answers": [{"answer_id": 937, "document_id": 586, "question_id": 942, "text": "rbenv install -L", "answer_start": 9876, "answer_category": null}], "is_impossible": false}, {"question": "install ruby in rbenv?", "id": 943, "answers": [{"answer_id": 938, "document_id": 586, "question_id": 943, "text": "rbenv install 2.0.0-p247", "answer_start": 9920, "answer_category": null}], "is_impossible": false}, {"question": "How to remove rbenv package on Mac OS?", "id": 944, "answers": [{"answer_id": 939, "document_id": 586, "question_id": 944, "text": "Homebrew: brew uninstall rbenv", "answer_start": 12284, "answer_category": null}], "is_impossible": false}, {"question": "How to remove rbenv package on Debian?", "id": 945, "answers": [{"answer_id": 940, "document_id": 586, "question_id": 945, "text": "sudo apt purge rbenv", "answer_start": 12354, "answer_category": null}], "is_impossible": false}, {"question": "How to remove rbenv package on Ubuntu?", "id": 946, "answers": [{"answer_id": 941, "document_id": 586, "question_id": 946, "text": "sudo apt purge rbenv", "answer_start": 12354, "answer_category": null}], "is_impossible": false}, {"question": "How to remove rbenv package on Archlinux?", "id": 947, "answers": [{"answer_id": 942, "document_id": 586, "question_id": 947, "text": " sudo pacman -R rbenv", "answer_start": 12405, "answer_category": null}], "is_impossible": false}, {"question": "What is rbenv version?", "id": 949, "answers": [{"answer_id": 944, "document_id": 586, "question_id": 949, "text": "Displays the currently active Ruby version, along with information on how it was set.", "answer_start": 14411, "answer_category": null}], "is_impossible": false}, {"question": "What is rbenv versions?", "id": 948, "answers": [{"answer_id": 943, "document_id": 586, "question_id": 948, "text": "Lists all Ruby versions known to rbenv, and shows an asterisk next to the currently active version.", "answer_start": 14159, "answer_category": null}], "is_impossible": false}], "context": "Groom your app\u2019s Ruby environment with rbenv.\nUse rbenv to pick a Ruby version for your application and guarantee that your development environment matches production. Put rbenv to work with Bundler for painless Ruby upgrades and bulletproof deployments.\n\nPowerful in development. Specify your app's Ruby version once, in a single file. Keep all your teammates on the same page. No headaches running apps on different versions of Ruby. Just Works\u2122 from the command line and with app servers like Pow. Override the Ruby version anytime: just set an environment variable.\n\nRock-solid in production. Your application's executables are its interface with ops. With rbenv and Bundler binstubs you'll never again need to cd in a cron job or Chef recipe to ensure you've selected the right runtime. The Ruby version dependency lives in one place\u2014your app\u2014so upgrades and rollbacks are atomic, even when you switch versions.\n\nOne thing well. rbenv is concerned solely with switching Ruby versions. It's simple and predictable. A rich plugin ecosystem lets you tailor it to suit your needs. Compile your own Ruby versions, or use the ruby-build plugin to automate the process. Specify per-application environment variables with rbenv-vars. See more plugins on the wiki.\n\nWhy choose rbenv over RVM?\n\nHow It Works\nAt a high level, rbenv intercepts Ruby commands using shim executables injected into your PATH, determines which Ruby version has been specified by your application, and passes your commands along to the correct Ruby installation.\n\nUnderstanding PATH\nWhen you run a command like ruby or rake, your operating system searches through a list of directories to find an executable file with that name. This list of directories lives in an environment variable called PATH, with each directory in the list separated by a colon:\n\n/usr/local/bin:/usr/bin:/bin\nDirectories in PATH are searched from left to right, so a matching executable in a directory at the beginning of the list takes precedence over another one at the end. In this example, the /usr/local/bin directory will be searched first, then /usr/bin, then /bin.\n\nUnderstanding Shims\nrbenv works by inserting a directory of shims at the front of your PATH:\n\n~/.rbenv/shims:/usr/local/bin:/usr/bin:/bin\nThrough a process called rehashing, rbenv maintains shims in that directory to match every Ruby command across every installed version of Ruby\u2014irb, gem, rake, rails, ruby, and so on.\n\nShims are lightweight executables that simply pass your command along to rbenv. So with rbenv installed, when you run, say, rake, your operating system will do the following:\n\nSearch your PATH for an executable file named rake\nFind the rbenv shim named rake at the beginning of your PATH\nRun the shim named rake, which in turn passes the command along to rbenv\nChoosing the Ruby Version\nWhen you execute a shim, rbenv determines which Ruby version to use by reading it from the following sources, in this order:\n\nThe RBENV_VERSION environment variable, if specified. You can use the rbenv shell command to set this environment variable in your current shell session.\n\nThe first .ruby-version file found by searching the directory of the script you are executing and each of its parent directories until reaching the root of your filesystem.\n\nThe first .ruby-version file found by searching the current working directory and each of its parent directories until reaching the root of your filesystem. You can modify the .ruby-version file in the current working directory with the rbenv local command.\n\nThe global ~/.rbenv/version file. You can modify this file using the rbenv global command. If the global version file is not present, rbenv assumes you want to use the \"system\" Ruby\u2014i.e. whatever version would be run if rbenv weren't in your path.\n\nLocating the Ruby Installation\nOnce rbenv has determined which version of Ruby your application has specified, it passes the command along to the corresponding Ruby installation.\n\nEach Ruby version is installed into its own directory under ~/.rbenv/versions. For example, you might have these versions installed:\n\n~/.rbenv/versions/1.8.7-p371/\n~/.rbenv/versions/1.9.3-p327/\n~/.rbenv/versions/jruby-1.7.1/\nVersion names to rbenv are simply the names of the directories in ~/.rbenv/versions.\n\nInstallation\nCompatibility note: rbenv is incompatible with RVM. Please make sure to fully uninstall RVM and remove any references to it from your shell initialization files before installing rbenv.\n\nUsing Package Managers\nInstall rbenv.\nmacOS If you're on macOS, we recommend installing rbenv with Homebrew.\n\nbrew install rbenv\nNote that this also installs ruby-build, so you'll be ready to install other Ruby versions out of the box.\n\nUpgrading with Homebrew\n\nTo upgrade to the latest rbenv and update ruby-build with newly released Ruby versions, upgrade the Homebrew packages:\n\nbrew upgrade rbenv ruby-build\nDebian, Ubuntu and their derivatives\n\nsudo apt install rbenv\nArch Linux and it's derivatives\n\nArchlinux has an AUR Package for rbenv and you can install it from the AUR using the instructions from this wiki page.\n\nSet up rbenv in your shell.\n\nrbenv init\nFollow the printed instructions to set up rbenv shell integration.\n\nClose your Terminal window and open a new one so your changes take effect.\n\nVerify that rbenv is properly set up using this rbenv-doctor script:\n\ncurl -fsSL https://github.com/rbenv/rbenv-installer/raw/main/bin/rbenv-doctor | bash\nChecking for `rbenv' in PATH: /usr/local/bin/rbenv\nChecking for rbenv shims in PATH: OK\nChecking `rbenv install' support: /usr/local/bin/rbenv-install (ruby-build 20170523)\nCounting installed Ruby versions: none\n  There aren't any Ruby versions installed under `~/.rbenv/versions'.\n  You can install Ruby versions like so: rbenv install 2.2.4\nChecking RubyGems settings: OK\nAuditing installed plugins: OK\nThat's it! Installing rbenv includes ruby-build, so now you're ready to install some other Ruby versions using rbenv install.\n\nBasic GitHub Checkout\nFor a more automated install, you can use rbenv-installer. If you prefer a manual approach, follow the steps below.\n\nThis will get you going with the latest version of rbenv without needing a systemwide install.\n\nClone rbenv into ~/.rbenv.\n\ngit clone https://github.com/rbenv/rbenv.git ~/.rbenv\nOptionally, try to compile dynamic bash extension to speed up rbenv. Don't worry if it fails; rbenv will still work normally:\n\ncd ~/.rbenv && src/configure && make -C src\nAdd ~/.rbenv/bin to your $PATH for access to the rbenv command-line utility.\n\nFor bash:\n\nUbuntu Desktop users should configure ~/.bashrc:\n\necho 'export PATH=\"$HOME/.rbenv/bin:$PATH\"' >> ~/.bashrc\nOn other platforms, bash is usually configured via ~/.bash_profile:\n\necho 'export PATH=\"$HOME/.rbenv/bin:$PATH\"' >> ~/.bash_profile\nFor Zsh:\n\necho 'export PATH=\"$HOME/.rbenv/bin:$PATH\"' >> ~/.zshrc\nFor Fish shell:\n\nset -Ux fish_user_paths $HOME/.rbenv/bin $fish_user_paths\nSet up rbenv in your shell.\n\n~/.rbenv/bin/rbenv init\nFollow the printed instructions to set up rbenv shell integration.\n\nRestart your shell so that PATH changes take effect. (Opening a new terminal tab will usually do it.)\n\nVerify that rbenv is properly set up using this rbenv-doctor script:\n\ncurl -fsSL https://github.com/rbenv/rbenv-installer/raw/main/bin/rbenv-doctor | bash\nChecking for `rbenv' in PATH: /usr/local/bin/rbenv\nChecking for rbenv shims in PATH: OK\nChecking `rbenv install' support: /usr/local/bin/rbenv-install (ruby-build 20170523)\nCounting installed Ruby versions: none\n  There aren't any Ruby versions installed under `~/.rbenv/versions'.\n  You can install Ruby versions like so: rbenv install 2.2.4\nChecking RubyGems settings: OK\nAuditing installed plugins: OK\n(Optional) Install ruby-build, which provides the rbenv install command that simplifies the process of installing new Ruby versions.\n\nUpgrading with Git\nIf you've installed rbenv manually using Git, you can upgrade to the latest version by pulling from GitHub:\n\ncd ~/.rbenv\ngit pull\nUpdating the list of available Ruby versions\nIf you're using the rbenv install command, then the list of available Ruby versions is not automatically updated when pulling from the rbenv repo. To do this manually:\n\ncd ~/.rbenv/plugins/ruby-build\ngit pull\nHow rbenv hooks into your shell\nSkip this section unless you must know what every line in your shell profile is doing.\n\nrbenv init is the only command that crosses the line of loading extra commands into your shell. Coming from RVM, some of you might be opposed to this idea. Here's what rbenv init actually does:\n\nSets up your shims path. This is the only requirement for rbenv to function properly. You can do this by hand by prepending ~/.rbenv/shims to your $PATH.\n\nInstalls autocompletion. This is entirely optional but pretty useful. Sourcing ~/.rbenv/completions/rbenv.bash will set that up. There is also a ~/.rbenv/completions/rbenv.zsh for Zsh users.\n\nRehashes shims. From time to time you'll need to rebuild your shim files. Doing this automatically makes sure everything is up to date. You can always run rbenv rehash manually.\n\nInstalls the sh dispatcher. This bit is also optional, but allows rbenv and plugins to change variables in your current shell, making commands like rbenv shell possible. The sh dispatcher doesn't do anything invasive like override cd or hack your shell prompt, but if for some reason you need rbenv to be a real script rather than a shell function, you can safely skip it.\n\nRun rbenv init - for yourself to see exactly what happens under the hood.\n\nInstalling Ruby versions\nThe rbenv install command doesn't ship with rbenv out of the box, but is provided by the ruby-build project. If you installed it either as part of GitHub checkout process outlined above or via Homebrew, you should be able to:\n\n# list latest stable versions:\nrbenv install -l\n\n# list all local versions:\nrbenv install -L\n\n# install a Ruby version:\nrbenv install 2.0.0-p247\nSet a Ruby version to finish installation and start using commands rbenv global 2.0.0-p247 or rbenv local 2.0.0-p247\n\nAlternatively to the install command, you can download and compile Ruby manually as a subdirectory of ~/.rbenv/versions/. An entry in that directory can also be a symlink to a Ruby version installed elsewhere on the filesystem. rbenv doesn't care; it will simply treat any entry in the versions/ directory as a separate Ruby version.\n\nInstalling Ruby gems\nOnce you've installed some Ruby versions, you'll want to install gems. First, ensure that the target version for your project is the one you want by checking rbenv version (see Command Reference). Select another version using rbenv local 2.0.0-p247, for example. Then, proceed to install gems as you normally would:\n\ngem install bundler\nYou don't need sudo to install gems. Typically, the Ruby versions will be installed and writeable by your user. No extra privileges are required to install gems.\n\nCheck the location where gems are being installed with gem env:\n\ngem env home\n# => ~/.rbenv/versions/<ruby-version>/lib/ruby/gems/...\nUninstalling Ruby versions\nAs time goes on, Ruby versions you install will accumulate in your ~/.rbenv/versions directory.\n\nTo remove old Ruby versions, simply rm -rf the directory of the version you want to remove. You can find the directory of a particular Ruby version with the rbenv prefix command, e.g. rbenv prefix 1.8.7-p357.\n\nThe ruby-build plugin provides an rbenv uninstall command to automate the removal process.\n\nUninstalling rbenv\nThe simplicity of rbenv makes it easy to temporarily disable it, or uninstall from the system.\n\nTo disable rbenv managing your Ruby versions, simply remove the rbenv init line from your shell startup configuration. This will remove rbenv shims directory from PATH, and future invocations like ruby will execute the system Ruby version, as before rbenv.\n\nWhile disabled, rbenv will still be accessible on the command line, but your Ruby apps won't be affected by version switching.\n\nTo completely uninstall rbenv, perform step (1) and then remove its root directory. This will delete all Ruby versions that were installed under `rbenv root`/versions/ directory:\n\n rm -rf `rbenv root`\nIf you've installed rbenv using a package manager, as a final step perform the rbenv package removal:\n\nHomebrew: brew uninstall rbenv\nDebian, Ubuntu, and their derivatives: sudo apt purge rbenv\nArchlinux and its derivatives: sudo pacman -R rbenv\nCommand Reference\nLike git, the rbenv command delegates to subcommands based on its first argument. The most common subcommands are:\n\nrbenv local\nSets a local application-specific Ruby version by writing the version name to a .ruby-version file in the current directory. This version overrides the global version, and can be overridden itself by setting the RBENV_VERSION environment variable or with the rbenv shell command.\n\nrbenv local 1.9.3-p327\nWhen run without a version number, rbenv local reports the currently configured local version. You can also unset the local version:\n\nrbenv local --unset\nrbenv global\nSets the global version of Ruby to be used in all shells by writing the version name to the ~/.rbenv/version file. This version can be overridden by an application-specific .ruby-version file, or by setting the RBENV_VERSION environment variable.\n\nrbenv global 1.8.7-p352\nThe special version name system tells rbenv to use the system Ruby (detected by searching your $PATH).\n\nWhen run without a version number, rbenv global reports the currently configured global version.\n\nrbenv shell\nSets a shell-specific Ruby version by setting the RBENV_VERSION environment variable in your shell. This version overrides application-specific versions and the global version.\n\nrbenv shell jruby-1.7.1\nWhen run without a version number, rbenv shell reports the current value of RBENV_VERSION. You can also unset the shell version:\n\nrbenv shell --unset\nNote that you'll need rbenv's shell integration enabled (step 3 of the installation instructions) in order to use this command. If you prefer not to use shell integration, you may simply set the RBENV_VERSION variable yourself:\n\nexport RBENV_VERSION=jruby-1.7.1\nrbenv versions\nLists all Ruby versions known to rbenv, and shows an asterisk next to the currently active version.\n\n$ rbenv versions\n  1.8.7-p352\n  1.9.2-p290\n* 1.9.3-p327 (set by /Users/sam/.rbenv/version)\n  jruby-1.7.1\n  rbx-1.2.4\n  ree-1.8.7-2011.03\nrbenv version\nDisplays the currently active Ruby version, along with information on how it was set.\n\n$ rbenv version\n1.9.3-p327 (set by /Users/sam/.rbenv/version)\nrbenv rehash\nInstalls shims for all Ruby executables known to rbenv (i.e., ~/.rbenv/versions/*/bin/*). Run this command after you install a new version of Ruby, or install a gem that provides commands.\n\n$ rbenv rehash\nrbenv which\nDisplays the full path to the executable that rbenv will invoke when you run the given command.\n\n$ rbenv which irb\n/Users/sam/.rbenv/versions/1.9.3-p327/bin/irb\nrbenv whence\nLists all Ruby versions with the given command installed.\n\n$ rbenv whence rackup\n1.9.3-p327\njruby-1.7.1\nree-1.8.7-2011.03\nEnvironment variables\nYou can affect how rbenv operates with the following settings:\n\nname\tdefault\tdescription\nRBENV_VERSION\t\tSpecifies the Ruby version to be used.\nAlso see rbenv shell\nRBENV_ROOT\t~/.rbenv\tDefines the directory under which Ruby versions and shims reside.\nAlso see rbenv root\nRBENV_DEBUG\t\tOutputs debug information.\nAlso as: rbenv --debug <subcommand>\nRBENV_HOOK_PATH\tsee wiki\tColon-separated list of paths searched for rbenv hooks.\nRBENV_DIR\t$PWD\tDirectory to start searching for .ruby-version files.\nDevelopment\nThe rbenv source code is hosted on GitHub. It's clean, modular, and easy to understand, even if you're not a shell hacker.\n\nTests are executed using Bats:\n\n$ bats test\n$ bats test/<file>.bats\nPlease feel free to submit pull requests and file bugs on the issue tracker.", "document_id": 586}]}, {"paragraphs": [{"qas": [{"question": "npm install g karma does not install executable", "id": 1990, "answers": [{"answer_id": 1976, "document_id": 1574, "question_id": 1990, "text": "\n\n\n    \n\nYou can link it by yourself. As in:\n\n#Assuming that /usr/local/bin is in your PATH\ncd /usr/local/bin\nln -s /usr/local/lib/node_modules/karma/", "answer_start": 1634, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI have what looks like successful karma installation, yet karma is not in /usr/local/bin/ and is not found by bash.\n\nAny idea what is wrong and how to fix it?\n\nHere are the end installation messages:\n\n\n&gt; ws@0.4.31 install /usr/local/lib/node_modules/karma/node_modules/socket.io/node_modules/socket.io-client/node_modules/ws\n&gt; (node-gyp rebuild 2&gt; builderror.log) || (exit 0)\n\n  CXX(target) Release/obj.target/bufferutil/src/bufferutil.o\n  SOLINK_MODULE(target) Release/bufferutil.node\n  SOLINK_MODULE(target) Release/bufferutil.node: Finished\n  CXX(target) Release/obj.target/validation/src/validation.o\n  SOLINK_MODULE(target) Release/validation.node\n  SOLINK_MODULE(target) Release/validation.node: Finished\nkarma@0.11.12 /usr/local/lib/node_modules/karma\n\u251c\u2500\u2500 di@0.0.1\n\u251c\u2500\u2500 graceful-fs@2.0.1\n\u251c\u2500\u2500 rimraf@2.2.5\n\u251c\u2500\u2500 colors@0.6.2\n\u251c\u2500\u2500 mime@1.2.11\n\u251c\u2500\u2500 q@0.9.7\n\u251c\u2500\u2500 coffee-script@1.6.3\n\u251c\u2500\u2500 lodash@2.4.1\n\u251c\u2500\u2500 glob@3.2.7 (inherits@2.0.1)\n\u251c\u2500\u2500 minimatch@0.2.14 (sigmund@1.0.0, lru-cache@2.5.0)\n\u251c\u2500\u2500 optimist@0.6.0 (wordwrap@0.0.2, minimist@0.0.5)\n\u251c\u2500\u2500 source-map@0.1.31 (amdefine@0.1.0)\n\u251c\u2500\u2500 log4js@0.6.9 (semver@1.1.4, async@0.1.15, readable-stream@1.0.17)\n\u251c\u2500\u2500 useragent@2.0.7 (lru-cache@2.2.4)\n\u251c\u2500\u2500 http-proxy@0.10.3 (pkginfo@0.2.3, optimist@0.3.7, utile@0.1.7)\n\u251c\u2500\u2500 chokidar@0.8.1\n\u251c\u2500\u2500 connect@2.12.0 (uid2@0.0.3, methods@0.1.0, pause@0.0.1, cookie-signature@1.0.1, fresh@0.2.0, qs@0.6.6, debug@0.7.4, bytes@0.2.1, raw-body@1.1.2, buffer-crc32@0.2.1, batch@0.5.0, cookie@0.1.0, negotiator@0.3.0, send@0.1.4, multiparty@2.2.0)\n\u2514\u2500\u2500 socket.io@0.9.16 (base64id@0.1.0, policyfile@0.0.4, redis@0.7.3, socket.io-client@0.9.16)\n\n\n    \n\nYou can link it by yourself. As in:\n\n#Assuming that /usr/local/bin is in your PATH\ncd /usr/local/bin\nln -s /usr/local/lib/node_modules/karma/bin/karma\n\n\nClearly, this is not an ideal solution. I think there's a problem in the package.json of karma which prevents npm to create a link to the executable. Might be related to this.\n    \n\nIt seems that npm places all installed binaries in the folder &lt;npm-global-prefix&gt;/bin, so to make these binaries available in the shell you can add the following line to your shell startup script (e.g. ~/.profile, ~/.bash_profile, ~/.bash_aliases):\nexport PATH=`npm prefix -g`/bin:$PATH\n\n    ", "document_id": 1574}]}, {"paragraphs": [{"qas": [{"question": "Can I serve a ClickOnce application with Apache?", "id": 1264, "answers": [{"answer_id": 1256, "document_id": 835, "question_id": 1264, "text": "I found a number of people asking the same question starting around 2005, but here is the first google result - also discusses silverlight.\nhttp://software.clempaul.me.uk/articles/clickonce/", "answer_start": 373, "answer_category": null}], "is_impossible": false}], "context": "We're testing our ClickOnce deployed application internally on IIS (Internet Information Services), but we're wondering if we can deploy it to the wider internet using Apache on Linux so we can make use of our existing external website host.\nIf so, is there anything else I need to consider other than as specifying the correct mime types such as .application and .deploy?\nI found a number of people asking the same question starting around 2005, but here is the first google result - also discusses silverlight.\nhttp://software.clempaul.me.uk/articles/clickonce/\n", "document_id": 835}]}, {"paragraphs": [{"qas": [{"question": "Can't install VS 2015 Update 3", "id": 1177, "answers": [{"answer_id": 1170, "document_id": 754, "question_id": 1177, "text": "Go to control panel->program and features select the Microsoft Visual Studio 2015 Professional with Updates entry and select Modify.", "answer_start": 213, "answer_category": null}], "is_impossible": false}], "context": "So I tried to install VS 2015 Update 3. BTW, I have the professional edition and I keep receiving this screen:\n \nNow, of course, Update 2 is already installed but it will not recognize this.\nWhat do I need to do?\nGo to control panel->program and features select the Microsoft Visual Studio 2015 Professional with Updates entry and select Modify. Now the Visual Studio 2015 installer shows up and here you should see the Update 3.\n", "document_id": 754}]}, {"paragraphs": [{"qas": [{"question": "Fix the upstream dependency conflict installing NPM packages", "id": 1837, "answers": [{"answer_id": 1823, "document_id": 1408, "question_id": 1837, "text": "You should try with npm install --legacy-peer-deps for detail info check this https://blog.npmjs.org/post/626173315965468672/npm-v7-series-beta-release-and-semver-major", "answer_start": 336, "answer_category": null}], "is_impossible": false}], "context": "Trying to npm install vue-mapbox mapbox-gl and I'm getting a dependency tree error.\nI'm running Nuxt SSR with Vuetify, and haven't installed anything related to Mapbox prior to running this install and getting this error.\nLooks like it's a problem with Peer Dependencies in the latest version of npm (v7) which is still a beta version.\nYou should try with npm install --legacy-peer-deps for detail info check this https://blog.npmjs.org/post/626173315965468672/npm-v7-series-beta-release-and-semver-major\n", "document_id": 1408}]}, {"paragraphs": [{"qas": [{"question": "How do I install boto?", "id": 739, "answers": [{"answer_id": 740, "document_id": 427, "question_id": 739, "text": "If necessary, install pip:\nsudo apt-get install python-pip\n2.\tThen install boto:\npip install -U boto", "answer_start": 63, "answer_category": null}], "is_impossible": false}], "context": "1.\tSo that I am able to work with it within my python scripts? If necessary, install pip:\nsudo apt-get install python-pip\n2.\tThen install boto:\npip install -U boto\nThis ensures you'll have the boto glacier code.\nIf you already have boto installed in one python version and then install a higher python version, boto is not found by the new version of python.\n", "document_id": 427}]}, {"paragraphs": [{"qas": [{"question": "sql server 2016 installation freeze", "id": 1949, "answers": [{"answer_id": 1936, "document_id": 1529, "question_id": 1949, "text": "Stop the install (Taskman -&gt; Kill Process)\nOpen regedit\nNavigate to: \n\nHKEY_LOCAL_MACHINE\\SOFTWARE\\WOW6432Node\\Policies\\Microsoft\\Windows\\\n\nAdd New Key Called: AppCompat\n\nYou should have this once added:\n\nHKEY_LOCAL_MACHINE\\SOFTWARE\\WOW6432Node\\Policies\\Microsoft\\Windows\\AppCompat\n\nAdd New DWORD entries as stated below to the AppCompat key: \n\nDWORD: DisableEngine Value = 1\nDWORD: DisablePCA Value = 1\nDWORD: SbEnable Value = 0\n\nCheck the following location as the above key you added should also be here: \n\nHKEY_LOCAL_MACHINE\\SOFTWARE\\Policies\\Microsoft\\Windows\\\n\nRestart the installati", "answer_start": 1543, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nHope someone can help me here . \nI have no further information other than that at a later stage in setup the whole process hangs showing  install_confignonrc_cpu64\nI am unable to cancel out of it even , the only way was to kill it . \nIs there any solution for that  ? \n    \n\nMy  problem was that the installer tries to download Microsoft R Open , if you have a slow connection you will see the installation freeze for a very long time without any progress bar updates or so . \nI fixed the problem by deselecting all R components since i dont need them then the installation went smooth . \n    \n\nJust leave it \"Stuck\" - it is doing a Download in the background, I left the PC for a couple hours while doing some errands, and then..boom- All set.\n    \n\njust had the same problem which is why I found this question.\nFixed the problem by closing the original media install menu window (where I'd selected \"New SQL Server standalone\" which is the first option).\nThe install window then carried on.\n    \n\nI have faced same problem with you, the simple way that i have find is turn on Windows firewall then wait a minute after that it will auto complete setup success. Hope this work in your case .\n\nThanks,\nCuong. \n    \n\nDisable and stop the windows update service and then start the installation. After the installation is complete put the service back on.\n    \n\nSQL Install gets stuck on sqlrsconfigaction_install_confignonrc_cpu64 (SQL Server 2016 using standalone installer, also when installing on Azure VMs)\n\nResolution:\n\n\nStop the install (Taskman -&gt; Kill Process)\nOpen regedit\nNavigate to: \n\nHKEY_LOCAL_MACHINE\\SOFTWARE\\WOW6432Node\\Policies\\Microsoft\\Windows\\\n\nAdd New Key Called: AppCompat\n\nYou should have this once added:\n\nHKEY_LOCAL_MACHINE\\SOFTWARE\\WOW6432Node\\Policies\\Microsoft\\Windows\\AppCompat\n\nAdd New DWORD entries as stated below to the AppCompat key: \n\nDWORD: DisableEngine Value = 1\nDWORD: DisablePCA Value = 1\nDWORD: SbEnable Value = 0\n\nCheck the following location as the above key you added should also be here: \n\nHKEY_LOCAL_MACHINE\\SOFTWARE\\Policies\\Microsoft\\Windows\\\n\nRestart the installation.\n\n    \n\nI closed everything and rebooted.  Not sure if maybe having Visual Studio open or something else prevented it from installing, but it got stuck.\n    ", "document_id": 1529}]}, {"paragraphs": [{"qas": [{"question": "how to Install the Server JRE foe windows?", "id": 358, "answers": [{"answer_id": 365, "document_id": 166, "question_id": 358, "text": "Download the bundle.\n\nBefore the file can be downloaded, you must accept the license agreement.\n\nThe archive binaries can be installed by anyone (not only root users), in any location that you can write to.\n\nThe .tar.gz archive file (also called a tarball) is a file that can be simultaneously uncompressed and extracted in one step.\n\nChange directory to the location where you would like the Server JRE to be installed.\n\nMove the .tar.gz archive binaries to the current directory.\n\nUnpack the tarball and install the Server JRE.\n\nThe Server JRE files are installed in a directory called jdk1.8.0_version in the current directory.\n\nDelete the .tar.gz file if you want to save disk space.", "answer_start": 1281, "answer_category": null}], "is_impossible": false}], "context": "This page describes system requirements and installation instructions for the Server JRE for Windows.\n\nThis page has these topics:\n\n\"System Requirements\"\n\n\"Installation Instructions Notation\"\n\n\"Installation Instructions\"\n\nSee \"JDK 8 and JRE 8 Installation Start Here\" for general information about installing.\n\nSystem Requirements\nSee http://www.oracle.com/technetwork/java/javase/certconfig-2095354.html for information about supported platforms, operating systems, and browsers.\n\nSee \"Windows System Requirements for JDK and JRE\" for minimum processor, disk space, and memory requirements.\n\nIf you have any difficulties, see http://docs.oracle.com/javase/8/docs/technotes/guides/tsgdesktop/index.html, \"Windows Online Installation and Java Update FAQ\", or submit a bug report at http://bugreport.java.com/bugreport/.\n\nInstallation Instructions Notation\nFor any text in this page that contains the following notation, you must substitute the appropriate update version number for the notation.\n\nversion\nFor example, if you are installing Server JRE 8 update release 21, the following string representing the name of the bundle:\n\nserver-jre-8uversion-windows.tar.gz\nbecomes:\n\nserver-jre-8u21-windows.tar.gz\nInstallation Instructions\nInstall the Server JRE by doing the following:\n\nDownload the bundle.\n\nBefore the file can be downloaded, you must accept the license agreement.\n\nThe archive binaries can be installed by anyone (not only root users), in any location that you can write to.\n\nThe .tar.gz archive file (also called a tarball) is a file that can be simultaneously uncompressed and extracted in one step.\n\nChange directory to the location where you would like the Server JRE to be installed.\n\nMove the .tar.gz archive binaries to the current directory.\n\nUnpack the tarball and install the Server JRE.\n\nThe Server JRE files are installed in a directory called jdk1.8.0_version in the current directory.\n\nDelete the .tar.gz file if you want to save disk space.", "document_id": 166}]}, {"paragraphs": [{"qas": [{"question": "Error during Xcode Component Installation", "id": 700, "answers": [{"answer_id": 704, "document_id": 391, "question_id": 700, "text": "You can work around the problem by temporarily changing the system date to before the certificate expiration date", "answer_start": 736, "answer_category": null}], "is_impossible": false}], "context": "I just installed a software update that I was prompted for, presumably affecting Xcode. Now when I start Xcode, I am presented with a dialog box which states that Xcode must install the Mobile Device Framework before continuing. When I try to do this, after providing my password, it fails with \"An unknown error occurred. See the install log for more details.\" I do not see anything useful in install.log that identifies the problem. I've tried re-installing multiple times and rebooting to no avail.I'm currently running Xcode 4.3. Perhaps I need to upgrade to 4.4? Has anyone else run into this problem? This problem has once again appeared in Xcode 11.1 and is caused by an invalid certificate that expired some day in October 2019.You can work around the problem by temporarily changing the system date to before the certificate expiration date.In the case of Xcode 11, set the system date to Oct 3, 2019, run Xcode, then change the date back after the components have installed.", "document_id": 391}]}, {"paragraphs": [{"qas": [{"question": "npm install errors with Error: ENOENT, chmod", "id": 1601, "answers": [{"answer_id": 1588, "document_id": 1175, "question_id": 1601, "text": "Ok it looks like NPM is using your .gitignore as a base for the .npmignore file, and thus ignores /lib. If you add a blank .npmignore file into the root of your application, everything should work.", "answer_start": 389, "answer_category": null}], "is_impossible": false}], "context": "I am trying to globally install an npm module I just published. Every time I try to install, either from npm or the folder, I get this error.\nAfter digging through the npm docs I found an option that would stop npm from making the bin links(--no-bin-links), when I tried the install with it, it worked fine.\nSo what's the deal? Is this some weird fringe case bug that has no solution yet?\nOk it looks like NPM is using your .gitignore as a base for the .npmignore file, and thus ignores /lib. If you add a blank .npmignore file into the root of your application, everything should work.\n", "document_id": 1175}]}, {"paragraphs": [{"qas": [{"question": "App can not be installed via TestFlight: Registered maximum number of devices", "id": 1142, "answers": [{"answer_id": 1135, "document_id": 719, "question_id": 1142, "text": "1. Launch TestFlight on one of the devices you are signed in.\n2. Tap on the \"Apple ID\" button at the bottom.\n3. On the next screen click on the \"Edit\" button on the top right.\n4. Select a device in the \"Other Devices\" section.\n5. Click on the \"Remove(1)\" button on the top left.", "answer_start": 488, "answer_category": null}], "is_impossible": false}], "context": "Will appreciate any help with an error that is displayed when I try to install an app via TestFlight on an iOS device: \" The app can't be installed because you've already registered the maximum number of devices\".\nUsing the same user on another device works fine - app get installed. Both devices are included in the provisioning profile UUIDs.\nThanks in advance. 117\nThere is a maximum number of 10 devices that can be registered to each user.\nTo remove a device from your user account:\n1. Launch TestFlight on one of the devices you are signed in.\n2. Tap on the \"Apple ID\" button at the bottom.\n3. On the next screen click on the \"Edit\" button on the top right.\n4. Select a device in the \"Other Devices\" section.\n5. Click on the \"Remove(1)\" button on the top left.\nNote: You cannot remove more than 5 devices per week.\n", "document_id": 719}]}, {"paragraphs": [{"qas": [{"question": "How do I do a silent install and uninstall with WiX and MSI?", "id": 1188, "answers": [{"answer_id": 1181, "document_id": 764, "question_id": 1188, "text": "Installer .exe's created with WiX can be run from the command line without requiring user input by using one of these command line parameters:\n\u2022\t/quiet - Displays no UI whatsoever\n\u2022\t/passive - Displays a UI but requires no user input. Essentially just displays an install progress bar", "answer_start": 155, "answer_category": null}], "is_impossible": false}], "context": "How can a silent installer be created in WiX that does not display any UI dialogs to the user and installs, upgrades and uninstalls with default settings? Installer .exe's created with WiX can be run from the command line without requiring user input by using one of these command line parameters:\n\u2022\t/quiet - Displays no UI whatsoever\n\u2022\t/passive - Displays a UI but requires no user input. Essentially just displays an install progress bar\nThis answer is based on WiX 3.9.\n", "document_id": 764}]}, {"paragraphs": [{"qas": [{"question": "Install go with brew, and running the gotour", "id": 655, "answers": [{"answer_id": 660, "document_id": 348, "question_id": 655, "text": "I thing you should have set the GOPATH before you go get. Well, at least here in my machine this worked smoothly.\nI set the GOPATH to a folder in my home folder.", "answer_start": 983, "answer_category": null}], "is_impossible": false}], "context": "I was following the http://tour.golang.org/ untill I got to the third step about that tells you that you can install the gotour on your system. After that I've installed the go language with brew by:\nbrew install hg\nbrew install go\nThen I downloaded the gotour by:\ngo get code.google.com/p/go-tour/gotour\nWhen I tried to launch the gotour it didnt recognise the command:\n$ gotour\n-bash: gotour: command not found\nand\n$ go gotour\nand\n$ ./gotour\nSo I tried to see the go path and it was empty,\necho $GOPATH\nso I defined the GOPATH:\nGOPATH=/usr/local/Cellar/go/1.0.2/src/pkg/code.google.com/p/\nexport GOPATH\nNow I can run the gotour by runing\n./gotour\nBut I'm insecure about my go enviroment.. wasn't I suposed to be able to run gotour by\ngo run gotour\nor just by typing (like is described on this website http://www.moncefbelyamani.com/how-to-install-the-go-tour-on-your-mac/):\ngotour\nI would like to know if i'm doing things the right way since I'm new to the go programing language.\nI thing you should have set the GOPATH before you go get. Well, at least here in my machine this worked smoothly.\nI set the GOPATH to a folder in my home folder.\nHope this helps!\n", "document_id": 348}]}, {"paragraphs": [{"qas": [{"question": "List of dependency jar files in Maven", "id": 1871, "answers": [{"answer_id": 1857, "document_id": 1442, "question_id": 1871, "text": "This command will generate the dependencies tree of your maven project:\n$ mvn dependency:tree", "answer_start": 739, "answer_category": null}], "is_impossible": false}], "context": "Using Maven 2, is there a way I can list out the jar dependencies as just the file names?\nmvn dependency:build-classpath \ncan list the jar files, but that will include the full path to their location in my local repository. What I need is essentially just a list of the file names (or the file names that the copy-dependencies goal copied).\nSo the list I need would be something like\nactivation-1.1.jar,antlr-2.7.6.jar,aopalliance-1.0.jar etc...\nideally as a maven property, but I guess, a file such as build-classpath can generate will do.\nWhat I am trying to achieve is writing a Bundle-ClassPath to an otherwise manually maintained MANIFEST.MF file for a OSGi bundle. (You shouldn't need to understand this bit to answer the question.)\nThis command will generate the dependencies tree of your maven project:\n$ mvn dependency:tree\n", "document_id": 1442}]}, {"paragraphs": [{"qas": [{"question": "How to define Context Path in Wildfly?", "id": 1297, "answers": [{"answer_id": 1289, "document_id": 868, "question_id": 1297, "text": "You can do this in the by adding a /WEB-INF/jboss-web.xml file in the application that you deploy:\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<jboss-web xmlns=\"http://www.jboss.com/xml/ns/javaee\"\n   xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n   xsi:schemaLocation=\"\n      http://www.jboss.com/xml/ns/javaee\n      http://www.jboss.org/j2ee/schema/jboss-web_5_1.xsd\">\n   <context-root>/</context-root>\n</jboss-web>", "answer_start": 803, "answer_category": null}], "is_impossible": false}], "context": "I've used the following code to set the Context Path in tomcat where I can access my application directly using localhost:8080 by overriding the tomcat's default path.\nI need to setup the same configuration to wildfly like I've done in tomcat to access my project in localhost:8080 by overriding the wildfly's default welcome page. I tried to do the same in wildfly but I'm stuck where to do that. There are lot of .xml files in wildfly folder (when comparing with tomcat's simple server.xml file) which I get confused on where to start with. I searched using \"How to set Context Path in Wildfly\", but got no success. Can anyone help me on how to do it..? If it's related to coding, then I can do lot of searches and atleast I can get some Ideas, but I'm stuck here at configuration. Thanks in Advance.\nYou can do this in the by adding a /WEB-INF/jboss-web.xml file in the application that you deploy:\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<jboss-web xmlns=\"http://www.jboss.com/xml/ns/javaee\"\n   xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n   xsi:schemaLocation=\"\n      http://www.jboss.com/xml/ns/javaee\n      http://www.jboss.org/j2ee/schema/jboss-web_5_1.xsd\">\n   <context-root>/</context-root>\n</jboss-web>\n", "document_id": 868}]}, {"paragraphs": [{"qas": [{"question": "How many memory is required for running Craft?", "id": 200640, "answers": [{"answer_id": 239392, "document_id": 357718, "question_id": 200640, "text": "A minimum of 256MB ", "answer_start": 704, "answer_category": null}], "is_impossible": false}, {"question": "How many disk space is required for running Craft?", "id": 200639, "answers": [{"answer_id": 239391, "document_id": 357718, "question_id": 200639, "text": "A minimum of 200MB of free disk space", "answer_start": 750, "answer_category": null}], "is_impossible": false}, {"question": "What  PHP Extensions does Craft require?", "id": 200641, "answers": [{"answer_id": 239396, "document_id": 357718, "question_id": 200641, "text": "PCRE\nPDO\nPDO MySQL Driver or PDO PostgreSQL Driver\nGD or ImageMagick. ImageMagick is preferred.\nOpenSSL\nMultibyte String\nJSON\ncURL\nReflection\nSPL\nZip", "answer_start": 861, "answer_category": null}], "is_impossible": false}, {"question": "What privileges does Craft need for MySQL?", "id": 200642, "answers": [{"answer_id": 239400, "document_id": 357718, "question_id": 200642, "text": "SELECT\nINSERT\nDELETE\nUPDATE\nCREATE\nALTER\nINDEX\nDROP\nREFERENCES", "answer_start": 1466, "answer_category": null}], "is_impossible": false}, {"question": "What privileges does the Craft need for MySQL?", "id": 200643, "answers": [{"answer_id": 239401, "document_id": 357718, "question_id": 200643, "text": "SELECT\nINSERT\nDELETE\nUPDATE\nCREATE\nDELETE\nREFERENCES\nCONNECT", "answer_start": 1544, "answer_category": null}], "is_impossible": false}], "context": "Server Requirements\n\n\nCraft 3 Documentation\n\n\nCraft 2 Documentation\nCraft 3 Documentation\nCraft 2 Class Reference\nCraft 3 Class Reference\n\n\n\n\nServer Requirements\nThese are the requirements to successfully install and properly run Craft.\nChecking Your Server #\nBefore you install Craft, it\u2019s important that you check that your server will meet the requirements. Review the requirements below or use the Craft Server Check script to quickly check whether you meet the requirements.\nNot in charge of the server? Send a link to this page to your server administrator.\nServer Requirements #\nCraft requires the following:\n\nPHP 7.0+\nMySQL 5.5+ (with InnoDB) or PostgreSQL 9.5+\nA web server (Apache, Nginx, IIS)\nA minimum of 256MB of memory allocated to PHP\nA minimum of 200MB of free disk space\n\nRequired PHP Extensions #\nCraft requires the following PHP extensions:\n\nPCRE\nPDO\nPDO MySQL Driver or PDO PostgreSQL Driver\nGD or ImageMagick. ImageMagick is preferred.\nOpenSSL\nMultibyte String\nJSON\ncURL\nReflection\nSPL\nZip\n\nOptional PHP Extensions #\n\niconv \u2013\u00a0Adds support for more character encodings than PHP\u2019s built-in mb_convert_encoding() function, which Craft will take advantage of when converting strings to UTF-8.\nIntl \u2013 Adds rich internationalization support.\nDOM - Required for parsing XML feeds as well as yii\\web\\XmlResponseFormatter.\n\nRequired Database User Privileges #\nThe database user you tell Craft to connect with must have the following privileges:\nMySQL #\n\nSELECT\nINSERT\nDELETE\nUPDATE\nCREATE\nALTER\nINDEX\nDROP\nREFERENCES\n\nPostgreSQL #\n\nSELECT\nINSERT\nDELETE\nUPDATE\nCREATE\nDELETE\nREFERENCES\nCONNECT\n\nCP Browser Requirements #\nCraft\u2019s Control Panel requires a modern browser:\nWindows and macOS #\n\nChrome 29 or later\nFirefox 28 or later\nSafari 9.0 or later\nInternet Explorer 11 or later\nMicrosoft Edge\n\nMobile #\n\niOS: Safari 9.1 or later\nAndroid: Chrome 4.4 or later\n\nNote: Craft\u2019s Control Panel browser requirements have nothing to do with your actual website. If you\u2019re a glutton for punishment and want your website to look flawless on IE 6, that\u2019s your choice.\n\n\n\nCraft 3 Documentation\nIntroduction\n\nAbout Craft CMS\nCode of Conduct\nHow to Use the Documentation\n\nInstalling Craft\n\nServer Requirements\nInstallation\n\nUpgrading & Updating Craft\n\nUpgrading from Craft 2\nUpdating Craft 3\nChanges in Craft 3\n\nGetting Started\n\nThe Pieces of Craft\nDirectory Structure\n\nCore Concepts\n\nSections and Entries\nFields\nTemplates\n\nTwig Primer\n\nCategories\nAssets\nUsers\nGlobals\nTags\nRelations\nRouting\nSearching\nSites\nLocalization\nElement Queries\nContent Migrations\nConfiguration\n\nTemplating\n\nGlobal Variables\nFunctions\nFilters\nTags\nQuerying Elements\nElements\nCommon Examples\n\nPlugin Development\n\nIntro to Plugin Dev\nCoding Guidelines\nUpdating Plugins for Craft 3\nChangelogs and Updates\nPlugin Settings\nControl Panel Section\nAsset Bundles\nServices\nExtending Twig\nWidget Types\nField Types\nVolume Types\nUtility Types\nElement Types\nElement Action Types\nPlugin Migrations\nPublishing to the Plugin Store\n\n\n\u00a9 Pixel & Tonic\n\n\n", "document_id": 357718}]}, {"paragraphs": [{"qas": [{"question": "Downloading Java JDK on Linux via wget is shown license page instead", "id": 1596, "answers": [{"answer_id": 1585, "document_id": 1172, "question_id": 1596, "text": "You should updated for JDK 8u171 RPM\nwget --no-check-certificate -c --header \"Cookie: oraclelicense=accept-securebackup-cookie\"", "answer_start": 131, "answer_category": null}], "is_impossible": false}], "context": "When I try to download Java from Oracle I instead end up downloading a page telling me that I need agree to the OTN license terms.\nYou should updated for JDK 8u171 RPM\nwget --no-check-certificate -c --header \"Cookie: oraclelicense=accept-securebackup-cookie\"\n", "document_id": 1172}]}, {"paragraphs": [{"qas": [{"question": "What\u2019s new in Angular 12?", "id": 23, "answers": [{"answer_id": 24, "document_id": 41, "question_id": 23, "text": "Angular version 12 adds a new flag to localize-extract called --migrateMapFile which  generates a JSON file that can be used to map legacy message IDs to canonical ones.\nThere is also a new script called localize-migrate that can use the mapping file which localize-extract generates and migrate all of the IDs in the files that are passed in.", "answer_start": 6408, "answer_category": null}], "is_impossible": false}], "context": "Updating Angular\nCookies concent notice This site uses cookies from Google to deliver its services and to analyze traffic.  Learn more  OK, got it Skip to main content\n\n\nHelp Angular by taking a 1 minute survey!Go to survey\n\n\n\n\ndark_mode\n\n\n\n\n\nAbout Angular\n\nFeaturesResourcesEventsBlogIntroductionGetting Started\n\nWhat is Angular?Try it\n\nGetting startedAdding navigationManaging DataUsing Forms for User InputDeploying an applicationSetupUnderstanding Angular\n\nComponents\n\nOverviewComponent LifecycleView EncapsulationComponent InteractionComponent StylesSharing data between child and parent directives and componentsContent ProjectionDynamic ComponentsAngular ElementsTemplates\n\nIntroductionText interpolationTemplate statementsPipesProperty bindingAttribute, class, and style bindingsEvent bindingTwo-way bindingTemplate reference variablesSVG as templatesDirectives\n\nBuilt-in directivesAttribute DirectivesStructural DirectivesDependency Injection\n\nAngular Dependency InjectionDI ProvidersDeveloper Guides\n\nRouting & Navigation\n\nOverviewCommon routing tasksTutorial: Routing in Single-page ApplicationsTutorial: Creating custom route matchesTutorial: Adding routing to Tour of HeroesRouter referenceForms\n\nIntroductionReactive FormsValidate form inputBuilding Dynamic FormsHTTP ClientTesting\n\nIntro to TestingCode CoverageTesting ServicesBasics of Testing ComponentsComponent Testing ScenariosTesting Attribute DirectivesTesting PipesDebugging TestsTesting Utility APIsInternationalization\n\nOverviewCommon Internationalization tasks\n\nCommon Internationalization tasksAdd the localize packageRefer to locales by IDFormat data based on localePrepare templates for translationsWork with translation filesMerge translations into the appDeploy multiple localesExample Angular applicationOptional Internationalization practices\n\nOptional Internationalization practicesSet the source locale manuallyImport global variants of the locale dataManage marked text with custom IDsAnimations\n\nIntroductionTransition and TriggersComplex SequencesReusable AnimationsRoute Transition AnimationsService Workers & PWA\n\nIntroductionGetting StartedApp ShellService Worker CommunicationService Worker NotificationsService Worker in ProductionService Worker ConfigurationWeb WorkersServer-side RenderingPrerenderingBest Practices\n\nSecurityAccessibilityKeeping Up-to-DateProperty Binding Best PracticesLazy Loading Feature ModulesLightweight Injection Tokens for LibrariesAngular Tools\n\nDev Workflow\n\nDeploying applicationsAOT Compiler\n\nAhead-of-Time CompilationAngular Compiler OptionsAOT Metadata ErrorsTemplate Type-checkingBuilding & ServingCLI BuildersLanguage ServiceDevToolsSchematics\n\nSchematics OverviewAuthoring SchematicsSchematics for LibrariesTutorials\n\nTutorial: Tour of Heroes\n\nIntroductionCreate a Project1. The Hero Editor2. Display a List3. Create a Feature Component4. Add Services5. Add Navigation6. Get Data from a ServerBuilding a Template-driven FormAngular Libraries\n\nLibraries OverviewUsing Published LibrariesCreating LibrariesRelease Information\n\nRelease PracticesRoadmapBrowser SupportUpdating to Version 12\n\nOverviewIvy Compatibility GuideMigration: Legacy Localization IDsDeprecationsAngular IvyUpgrading from AngularJS\n\nUpgrading InstructionsSetup for Upgrading from AngularJSUpgrading for PerformanceAngularJS-Angular ConceptsReference\n\nConceptual Reference\n\nAngular Concepts\n\nIntro to Basic ConceptsIntro to ModulesIntro to ComponentsIntro to Services and DINext StepsBinding syntaxHow event binding worksTemplate variablesWorkspace and project structure\n\nProject File StructureWorkspace Configurationnpm DependenciesTypeScript ConfigurationNgModules\n\nNgModules IntroductionJS Modules vs NgModulesLaunching Apps with a Root ModuleFrequently Used NgModulesTypes of Feature ModulesEntry ComponentsFeature ModulesProviding DependenciesSingleton ServicesSharing NgModulesNgModule APINgModule FAQsObservables & RxJS\n\nObservables OverviewThe RxJS LibraryObservables in AngularPractical UsageCompare to Other TechniquesDependency injection\n\nHierarchical InjectorsDI in ActionCLI Command Reference\n\nOverviewUsage Analyticsng addng analyticsng buildng configng deployng docng e2eng extract-i18nng generateng helpng lintng newng runng serveng testng updateng versionAPI ReferenceError Reference\n\nNG0100: Expression Changed After CheckedNG0200: Circular Dependency in DING0201: No Provider FoundNG0300: Selector CollisionNG0301: Export Not FoundNG0302: Pipe Not FoundNG1001: Argument Not LiteralNG2003: Missing TokenNG2009: Invalid Shadow DOM selectorNG3003: Import Cycle DetectedNG6999: Invalid metadataNG8001: Invalid ElementNG8002: Invalid AttributeNG8003: Missing Reference TargetExample applicationsAngular GlossaryAngular Style and Usage\n\nQuick ReferenceCoding Style GuideContent Contributor's Guide\n\nOverviewReviewing contentUpdating search keywordsUpdating content using GitHub UIDocumentation Style GuideAngular doc localization guidelinesstable (v12.2.8)\n\nmode_edit\n\n\nUpdating Angular Contents\n\nUpdating CLI applicationsBuilding applications with IvyChanges and deprecations in version 12Breaking changes in Angular version 12New deprecations\nThis guide contains information related to updating to Angular version 12.\nUpdating CLI applications\nFor step-by-step instructions on how to update to the latest Angular release and leverage our automated migration tools to do so, use the interactive update guide at update.angular.io.\nBuilding applications with Ivy\nFor libraries, View Engine is deprecated and will be removed in version 13.\nNew libraries created with version 12 or later default to Ivy.\nFor more information about distributing libraries with View Engine and Ivy, see the Building libraries with Ivy section of Creating libraries.\nChanges and deprecations in version 12\n\nFor information about Angular's deprecation and removal practices, see Angular Release Practices.\n\n\nApplications can no longer build with View Engine by setting enableIvy: false.\nSupport for building libraries with View Engine, for backwards compatibility, is deprecated and will be removed in Angular version 13.\nNew libraries created with Angular version 12 will default to building and distributing with Ivy.\nFor more information, see Creating Libraries.\nThe Ivy-based IDE language service is now on by default.\nSee PR #1279.\nAngular's View Engine-based algorithm for generating i18n message IDs is deprecated.\nAngular version 12 adds a new flag to localize-extract called --migrateMapFile which  generates a JSON file that can be used to map legacy message IDs to canonical ones.\nThere is also a new script called localize-migrate that can use the mapping file which localize-extract generates and migrate all of the IDs in the files that are passed in.\nFor better stability, if you are using Angular's i18n, run this migration to move to the new message ID generation algorithm.\nIf you don't run this migration, all your generated message IDs will change when Angular removes the View Engine compiler.\nSee PR #41026.\nThere is now a new build option named inlineStyleLanguage for defining the style sheet language in inline component styles.\nCurrently supported language options are CSS (default), Sass, SCSS, and LESS.\nThe default of CSS enables existing projects to continue to function as expected.\nSee PR #20514.\nFor new applications, strict mode is now the default in the CLI.\nSee PR #20029.\nAdd emitEvent option for AbstractControl class methods.\nSee PR #31031.\nSupport APP_INITIALIZER to work with observables.\nSee PR #31031.\nHttpClient supports specifying request metadata.\nSee PR #25751.\n\n\nBreaking changes in Angular version 12\n\nAdd support for TypeScript 4.2.\nTypeScript <4.2.3 is no longer supported.\nThe supported range of TypeScript versions is 4.2.3 to 4.2.x.\nSee PR #41158.\nAngular CDK and Angular Material internally now use the new Sass module system, which is actively maintained by the Sass team at Google.\nConsequently, applications can no longer consume Angular CDK/Material's Sass with the node-sass npm package.\nnode-sass is unmaintained and does not support newer Sass features. Instead, applications must use the sass npm package, or the sass-embedded npm package for the sass-embedded beta.\nThe Angular tooling now uses Webpack 5 to build applications. Webpack 4 usage and support has been removed.\nYou don't need to make any project level configuration changes to use the upgraded Webpack version when using the official Angular builders.\nCustom builders based on this package that use the experimental programmatic APIs may need to be updated to become compatible with Webpack 5.\nSee PR #20466.\nWebpack 5 generates similar but differently named files for lazy-loaded JavaScript files in development configurations when the namedChunks option is enabled.\nFor the majority of users this change should have no effect on the application or build process.\nProduction builds should also not be affected as the namedChunks option is disabled by default in production configurations.\nHowever, if a project's post-build process makes assumptions as to the file names, then adjustments may need to be made to account for the new naming paradigm.\nSuch post-build processes could include custom file transformations after the build, integration into service-side frameworks, or deployment procedures.\nAn example of a development file name change is lazy-lazy-module.js becoming src_app_lazy_lazy_module_ts.js.\nSee PR #20466.\nWebpack 5 now includes web worker support.\nHowever, the structure of the URL within the worker constructor must be in a specific format that differs from the current requirement.\nTo update web worker usage, where ./app.worker is the actual worker name, change new Worker('./app.worker', ...) to new Worker(new URL('./app.worker', import.meta.url), ...).\nSee PR #20466.\nCritical CSS inlining is now enabled by default.\nTo turn this off, set inlineCritical to false.\nSee PR #20096 and the Style preprocessor options section of Angular workspace configuration.\nng build now produces production bundle by default.\nSee PR #20128.\nPreviously, the Forms module ignored min and max attributes defined on the <input type=\"number\">.\nNow these attributes trigger min and max validation logic in cases where formControl, formControlName, or ngModel directives are also present on a given input.\nSee PR #39063.\n\n\nNew deprecations\n\nSupport for Internet Explorer 11 is deprecated.\nSee Deprecated APIs and features and Microsoft 365 apps say farewell to Internet Explorer 11 and Windows 10 sunsets Microsoft Edge Legacy.\nSass imports from @angular/material/theming are deprecated. There is a new Angular Material Sass API for @use.\nRun the migration script ng g @angular/material:themingApi to switch all your Sass imports for Angular CDK and Angular Material to the new API and @use.\nSupport for publishing libraries with View Engine has been deprecated:\n\nYou can now compile libraries in partial compilation mode to generate Ivy compatible output that will be linked when an application using that library is bundled.\nNew libraries you create with the Angular CLI default to partial compilation mode, and do not support View Engine. You can still build a library with View Engine. See Creating libraries for more information.\nLibraries compiled in partial compilation mode will not contain legacy i18n message IDs.\nIf the library was previously compiled by View Engine, and contained legacy i18n message IDs, then applications may have translation files that you'll need to migrate to the new message ID format. For more information, see Migrating legacy localization IDs.\nFor context, see Issue #38366.\n\n\n\n\nSince version 9, Angular Ivy is the default rendering engine.\nFor more information about Ivy, see Angular Ivy.\n\n\n\nResourcesAboutResource ListingPress KitBlogUsage AnalyticsHelpStack OverflowJoin DiscordGitterReport IssuesCode of ConductCommunityEventsMeetupsTwitterGitHubContributeLanguagesEspa\u00f1ol\u7b80\u4f53\u4e2d\u6587\u7248\u6b63\u9ad4\u4e2d\u6587\u7248\u65e5\u672c\u8a9e\u7248\ud55c\uad6d\uc5b4Complete language list Super-powered by Google \u00a92010-2021.\nCode licensed under an MIT-style License. Documentation licensed under CC BY 4.0.\nVersion 12.2.9-local+sha.f45c692dd9.\n\n\n&amp;amp;lt;div class=\"background-sky hero\"&amp;amp;gt;&amp;amp;lt;/div&amp;amp;gt;\n&amp;amp;lt;section id=\"intro\" style=\"text-shadow: 1px 1px #1976d2;\"&amp;amp;gt;\n&amp;amp;lt;div class=\"hero-logo\"&amp;amp;gt;&amp;amp;lt;/div&amp;amp;gt;\n&amp;amp;lt;div class=\"homepage-container\"&amp;amp;gt;\n&amp;amp;lt;div class=\"hero-headline\"&amp;amp;gt;The modern web&amp;amp;lt;br&amp;amp;gt;developer's platform&amp;amp;lt;/div&amp;amp;gt;\n&amp;amp;lt;/div&amp;amp;gt;\n&amp;amp;lt;/section&amp;amp;gt;\n&amp;amp;lt;h2 style=\"color: red; margin-top: 40px; position: relative; text-align: center; text-shadow: 1px 1px #fafafa; border-top: none;\"&amp;amp;gt;\n&amp;amp;lt;b&amp;amp;gt;&amp;amp;lt;i&amp;amp;gt;This website requires JavaScript.&amp;amp;lt;/i&amp;amp;gt;&amp;amp;lt;/b&amp;amp;gt;\n&amp;amp;lt;/h2&amp;amp;gt;\n\n\n\n\n", "document_id": 41}]}, {"paragraphs": [{"qas": [{"question": "programmatically unplug and replug a usb device to load new driver in os x", "id": 1422, "answers": [{"answer_id": 1411, "document_id": 996, "question_id": 1422, "text": "Look in IOKit/usb/IOUSBLib.h\n    \n\nTake a look at diskutil, and especially the mount and unmount options. Those will softwarematically eject and mount devices. You can use diskutil list to get a list of all the currently mounted devices", "answer_start": 2078, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI'm working on an installer in OS X that installs an IOKit driver for a USB device, and I'm trying to get it to not require a restart at the end.  The installer installs the driver correctly and rebuilds the kext cache, and after it runs, if I unplug and replug the USB device, it correctly loads the new driver and everything works fine.\n\nHowever, I don't want to require the user to physically unplug the device in order for the new driver to load.  There's got to be a way to get OS X to load the new driver programmatically - in effect simulate the device being unplugged and plugged back in again, or something similar.  How would I go about doing this?  So far, hours of Googling has turned up nothing, so any help will be greatly appreciated!\n    \n\nIOUSBDeviceInterface187::USBDeviceReEnumerate() will do what you want. The only hitch is that to find all of the devices of interest and call this on them manually with IOServiceGetMatchingServices().\n\n/*!\n@function USBDeviceReEnumerate\n@abstract   Tells the IOUSBFamily to reenumerate the device.\n@discussion This function will send a terminate message to all clients of the IOUSBDevice (such as \n            IOUSBInterfaces and their drivers, as well as the current User Client), emulating an unplug \n            of the device. The IOUSBFamily will then enumerate the device as if it had just \n            been plugged in. This call should be used by clients wishing to take advantage \n            of the Device Firmware Update Class specification.  The device must be open to use this function. \n@availability This function is only available with IOUSBDeviceInterface187 and above.\n@param      self Pointer to the IOUSBDeviceInterface.\n@param      options A UInt32 reserved for future use. Ignored in current implementation. Set to zero.\n@result     Returns kIOReturnSuccess if successful, kIOReturnNoDevice if there is no connection to an IOService,\n            or kIOReturnNotOpen if the device is not open for exclusive access.\n*/\n\nIOReturn (*USBDeviceReEnumerate)(void *self, UInt32 options);\n\n\nLook in IOKit/usb/IOUSBLib.h\n    \n\nTake a look at diskutil, and especially the mount and unmount options. Those will softwarematically eject and mount devices. You can use diskutil list to get a list of all the currently mounted devices. If you need more info on diskutil, just look at the man page.\n    ", "document_id": 996}]}, {"paragraphs": [{"qas": [{"question": "how do i determine if my r installation on os x has the enable r shlib optio", "id": 1950, "answers": [{"answer_id": 1937, "document_id": 1531, "question_id": 1950, "text": "\n\n\nYou can check for --enable-R-shlib by looking at thethe binary of the resulting R built; on Linux I can do ldd /usr/lib/R/bin/exec/R which reveals that libR.so --- the shared R library --- is loaded by this binary. That is what other projects embedding R (RInside, littler, the R-in-Apache modules, ...) use and presumably what RSRuby would use.\nIf you get an error 'R home directory not defined' I would start by defining the environment variable R_H", "answer_start": 1464, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI've installed R on my OS X machine via the .pkg method. I'm trying to get a Ruby gem called RSRuby to work and though it installed correctly, it's throwing an error when I try to invoke the gem. \n\nFatal error: R home directory is not defined\n\n\nThe RSRuby documentation states that R should be installed with the option --enable-R-shlib. But the R documentation states that the Mac OS X installation does this by default. How can you verify that this option is enabled on an installation if you don't know it was explicitly called during installation?\n\nI'm chasing down why RSRuby can't find R home directory and this seems like the most likely problem. \n\nPer RSRuby documentation, I copied this into my home directory\n\n[~]$ R_HOME=/Library/Frameworks/R.framework/Resource\n\n\nAnd from the R console, this is the output:\n\n&gt; R.home()\n[1] \"/Library/Frameworks/R.framework/Resources\"\n\n\nAnd during gem install, I ran this command:\n\ngem install rsruby -- --with-R-dir=$R_HOME=/Library/Frameworks/R.framework/Resources\n\n\nAlso, these are the contents of my /Library/Frameworks/R.framework/Libraries folder:\n\n[Libraries]$ ls\ni386                    libRblas.dylib.dSYM     libgfortran.2.dylib\nlibR.dylib              libRblas.vecLib.dylib   libreadline.5.2.dylib\nlibR.dylib.dSYM         libRlapack.dylib        libreadline.dylib\nlibRblas.0.dylib        libRlapack.dylib.dSYM   ppc\nlibRblas.dylib          libgcc_s.1.dylib        x86_64\n\n    \n\nBriefly:\n\n\nYou can check for --enable-R-shlib by looking at thethe binary of the resulting R built; on Linux I can do ldd /usr/lib/R/bin/exec/R which reveals that libR.so --- the shared R library --- is loaded by this binary. That is what other projects embedding R (RInside, littler, the R-in-Apache modules, ...) use and presumably what RSRuby would use.\nIf you get an error 'R home directory not defined' I would start by defining the environment variable R_HOME.\n\n\nEdit: Regaring Question 1, on OS X, your dynamic libraries end in .dylib. So the directory listing you show clearly demonstrates that R was built with --enable-R-shlib as a compile-time option.  Regarding Question 2, the very first Google hit I got for rsruby os x clearly describes how to set R_HOME on OS X (as I had told you), see https://github.com/alexgutteridge/rsruby.\n    ", "document_id": 1531}]}, {"paragraphs": [{"qas": [{"question": "Hot deploy with WebLogic server?", "id": 1081, "answers": [{"answer_id": 1073, "document_id": 658, "question_id": 1081, "text": "Yes - there is a feature called FastSwap.\nUsing FastSwap to speed up dev\nUsing FastSwap Deployment to Minimize Redeployment", "answer_start": 306, "answer_category": null}], "is_impossible": false}], "context": "I have an enterprise application running on a WebLogic server.\nWhen I modify any Java class, I have to build the .ear file and deploy to the server every time. Even for a small modification, I need to build the whole application and deploy to the server. The server is taking around 10 minutes to do this?\nYes - there is a feature called FastSwap.\nUsing FastSwap to speed up dev\nUsing FastSwap Deployment to Minimize Redeployment\n", "document_id": 658}]}, {"paragraphs": [{"qas": [{"question": "Enterprise app deployment doesn't install on iOS 8.1.3", "id": 1205, "answers": [{"answer_id": 1198, "document_id": 781, "question_id": 1205, "text": " I would recommend generating it from the provisioning profile using the following script (credit):\n# Create an entitlements file\n# parse provision profile\nsecurity cms -D -i \"provProfile.mobileprovision\" > ProvisionProfile.plist 2>&1\n\n# generate entitilements.plist\n/usr/libexec/PlistBuddy -x -c \"Print Entitlements\" ProvisionProfile.plist > Entitlements.pl", "answer_start": 480, "answer_category": null}], "is_impossible": false}], "context": "After updating iOS 8.1.3, I tried to download, but getting error \"Unable to download app\" and \"could not be installed at this time\" messages appears.\nWhat are changes between 8.1.2 and 8.1.3 which i have to take into consideration?\nI've done quite a few experiments with this. In my experience the bundle identifier in the manifest.plist file isn't actually that critical. The most important thing to do is to get the entitlements.plist correct.\nRather than creating this manually I would recommend generating it from the provisioning profile using the following script (credit):\n# Create an entitlements file\n# parse provision profile\nsecurity cms -D -i \"provProfile.mobileprovision\" > ProvisionProfile.plist 2>&1\n\n# generate entitilements.plist\n/usr/libexec/PlistBuddy -x -c \"Print Entitlements\" ProvisionProfile.plist > Entitlements.pl\n", "document_id": 781}]}, {"paragraphs": [{"qas": [{"question": "Python distutils, how to get a compiler that is going to be used?", "id": 1207, "answers": [{"answer_id": 1200, "document_id": 783, "question_id": 1207, "text": "ou can subclass the distutils.command.build_ext.build_ext command", "answer_start": 535, "answer_category": null}], "is_impossible": false}], "context": "For example, I may use python setup.py build --compiler=msvc or python setup.py build --compiler=mingw32 or just python setup.py build, in which case the default compiler (say, bcpp) will be used. How can I get the compiler name inside my setup.py (e. g. msvc, mingw32 and bcpp, respectively)?\nUPD.: I don't need the default compiler, I need the one that is actually going to be used, which is not necessarily the default one. So far I haven't found a better way than to parse sys.argv to see if there's a --compiler... string there.\nYou can subclass the distutils.command.build_ext.build_ext command.\nOnce build_ext.finalize_options() method has been called, the compiler type is stored in self.compiler.compiler_type as a string (the same as the one passed to the build_ext's --compiler option, e.g. 'mingw32', 'gcc', etc...).\n", "document_id": 783}]}, {"paragraphs": [{"qas": [{"question": "Install npm (Node.js Package Manager) on Windows (w/o using Node.js MSI)", "id": 719, "answers": [{"answer_id": 722, "document_id": 409, "question_id": 719, "text": "https://nodejs.org/download/ . The page has Windows Installer (.msi) as well as other installers and binaries.Download and install for windows.\nNode.js comes with NPM.\nNPM is located in the directory where Node.js is installed.", "answer_start": 269, "answer_category": null}], "is_impossible": false}], "context": "The problem: while using nvm to install Node.js I was able to install the version of Node.js I need, but nvm does not install npm automatically. NPM's page provides no information about installing it. Being not much of a Windows user myself I am completely at a loss..\nhttps://nodejs.org/download/ . The page has Windows Installer (.msi) as well as other installers and binaries.Download and install for windows.\nNode.js comes with NPM.\nNPM is located in the directory where Node.js is installed.\n.\nEvery result in Google seems to use Node.js MSI installer to get that working - which is not an option in my case. So, how do I?\n", "document_id": 409}]}, {"paragraphs": [{"qas": [{"question": "\"Warm Up Cache\" on deployment", "id": 1124, "answers": [{"answer_id": 1117, "document_id": 701, "question_id": 1124, "text": "You may need to use wget or another program to spider the site.", "answer_start": 390, "answer_category": null}], "is_impossible": false}], "context": "I am wondering if anyone has any plugins or capistrano recipes that will \"pre-heat\" the page cache for a rails app by building all of the page cached html at the time the deployment is made, or locally before deployment happens.\nI have some mostly static sites that do not change much, and would run faster if the html was already written, instead of requiring one visitor to hit the site.\nYou may need to use wget or another program to spider the site.\n", "document_id": 701}]}, {"paragraphs": [{"qas": [{"question": "XamlParseException after deploying WPF project", "id": 1129, "answers": [{"answer_id": 1122, "document_id": 706, "question_id": 1129, "text": "This is normally caused by not having all dependencies copied to the output. As you say the error message is not very helpful, but I would check that your application has all the necessary dependencies available to resolve the parsed types.", "answer_start": 336, "answer_category": null}], "is_impossible": false}], "context": "I have been trying to deploy my WPF app, I created a Setup Project using the Setup Wizard.The only Project Output I added was Primary. After building this and installing the program, as soon as i click the exe on my desktop i get a pop up that says \"'My Program' has stopped working\", so i click Debug the Program and i see some error.\nThis is normally caused by not having all dependencies copied to the output. As you say the error message is not very helpful, but I would check that your application has all the necessary dependencies available to resolve the parsed types.\n", "document_id": 706}]}, {"paragraphs": [{"qas": [{"question": "Where is my Django installation?", "id": 646, "answers": [{"answer_id": 651, "document_id": 339, "question_id": 646, "text": "python -c \"import django; print(django.__path__)\"", "answer_start": 294, "answer_category": null}], "is_impossible": false}], "context": "I use Django but I need to find the default templates and applications.\nI don't know where it's installed.\nHow can I find that?\nAs the comments on @olafure's answer https://stackoverflow.com/a/12974642/4515198 rightly say, the sys.path assignment is not required.\nThe following will be enough:\npython -c \"import django; print(django.__path__)\"\nHere the -c option is used to tell python that a \"program is being passed in as string\" (source: command $ python --help on bash)On Microsft-Windows OS: In the Lib/site-packages folder inside your python installation.\n", "document_id": 339}]}, {"paragraphs": [{"qas": [{"question": "what versions of python will work in windows xp", "id": 1999, "answers": [{"answer_id": 1985, "document_id": 1585, "question_id": 1999, "text": "Modify the python.exe header,\nRun a tiny Python script to remap some DLL calls,\nRecalculate some binary hashes,\nCreate a fake kernel32.dll with certain calls forwarded,\nModify python37.dll to point to this fake thing,", "answer_start": 1106, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI would like the most advanced version of Python that still works on Windows XP.  I need both Python 2 and Python 3.\n\nWhat versions of Python will work on Windows XP?\n    \n\nI found that python 2.7.9 and python 3.4.3 are the newest versions of python that work in windows xp.  I found this out through trial and error.\n    \n\nI've found a desperate guy (Daniel Pistelli) who didn't give up at the windows error dialog:\n\nhttps://ntcore.com/?p=458\n\nI like how he approaches the topic:\n\n\n  If we try to start any new application on XP, we\u2019ll get an error\n  message informing us that it is not a valid Win32 application. This\n  happens because of some fields in the Optional Header of the Portable\n  Executable.\n  [...] \n  Fortunately, it\u2019s enough to adjust the fields in the executable we want to start (python.exe), there\u2019s no need to adjust the DLLs as well. If we try run the application now, we\u2019ll get an error message due to a missing API in kernel32. So let\u2019s turn our attention to the imports.\n\n\nAnd then he goes on for another 5-6 screens. To sum it up, apparently you'll have to:\n\n\nModify the python.exe header,\nRun a tiny Python script to remap some DLL calls,\nRecalculate some binary hashes,\nCreate a fake kernel32.dll with certain calls forwarded,\nModify python37.dll to point to this fake thing,\n\n\nand then you're all set. To be honest I couldn't quite follow (let alone verify!) some of the steps but it looks legit and there are links with Daniel's half-baked solutions for the harder parts, also a lot of Python sources explained. I will definitely give this thing a try.\n\nIn fact, here are the final scripts from his GitHub page:\nhttps://github.com/dpistelli/xptmrt\n\nMoral of the story: you can be crazy, but you'll never be dismantle-all-dlls-and-exes-and-open-hexeditors-and-disassemble-hashing-algorithms-and-mock-it-all-together-so-it-works-under-xp-crazy!\n    \n\nSomeone had built Python 3.4.10, which is slightly newer than the officially prescribed Python3 v3.4.3/3.4.4 for Windows XP. Since it comes from the third-party (not the official Python website), use at your own risk,\n\n--&gt;Download&lt;--\n    \n\ni tried 3.3.3 but i came up with an error message use 3.4.3/2.7.9 they are the only versions that work now sadly\n    \n\nAny of them, python is very platform independent. Some features might not work, but that would best be found in the documentation.\n    ", "document_id": 1585}]}, {"paragraphs": [{"qas": [{"question": "$PHP_AUTOCONF errors on mac os x 10.7.3 when trying to install pecl extensions", "id": 648, "answers": [{"answer_id": 653, "document_id": 341, "question_id": 648, "text": "curl -OL http://ftpmirror.gnu.org/autoconf/autoconf-latest.tar.gz\ntar xzf autoconf-latest.tar.gz\ncd autoconf-*\n./configure --prefix=/usr/local\nmake\nsudo make install", "answer_start": 469, "answer_category": null}], "is_impossible": false}], "context": "I am trying to setup my machine with pecl_http and memcache and in both cases, I get similar errors. This is on MAC OS X 10.7.3 (lion) and I also have XCODE installed on it. I also installed Zend Server community edition before running these commands and have CFLAGS='-arch i386 -arch x86_64' environment variables set. So please help with what I need to do. You need to install autoconfig. I usually like to install libraries from source. So you can do the following:\ncurl -OL http://ftpmirror.gnu.org/autoconf/autoconf-latest.tar.gz\ntar xzf autoconf-latest.tar.gz\ncd autoconf-*\n./configure --prefix=/usr/local\nmake\nsudo make install\nI just went through this with Mountain Lion.\nThis does NOT have to be made inside the /usr/bin folder. If you understand the commands given you will notice that you are downloading some files (curl), decompressing them (tar) configuring the installation to your machine needs (./configure), compiling it (make) and installing the library (make install).\n", "document_id": 341}]}, {"paragraphs": [{"qas": [{"question": "installing rubygems in windows?", "id": 868, "answers": [{"answer_id": 863, "document_id": 548, "question_id": 868, "text": "Use chocolatey in PowerShell\n\nchoco install ruby -y\nrefreshenv\ngem install bundler", "answer_start": 4676, "answer_category": null}], "is_impossible": false}], "context": "Stack Overflow\nAbout\nProducts\nFor Teams\nSearch\u2026\n \nHow are we doing? Please help us improve Stack Overflow. Take our short survey\nHome\nPUBLIC\nQuestions\nTags\nUsers\nCOLLECTIVES\nExplore Collectives\nFIND A JOB\nJobs\nCompanies\nTEAMS\nStack Overflow for Teams \u2013 Collaborate and share knowledge with a private group. \nInstalling RubyGems in Windows\nAsked 8 years, 1 month ago\nActive 1 year ago\nViewed 214k times\n\n\n100\n\n\n28\nI'm new to ruby. I tried to install Ruby Gems on my PC by following the steps given in the site http://rubygems.org/pages/download.\n\nI downloaded the package from the mentioned site, changed the directory to the directory in which the setup resides, and tried to run setup using the command setup.rb in command prompt.\n\nBut I get a window pop up that says \"Windows can't open this file\" and prompts me to select a program to open this file.\n\nWhat should I do now? Let me know if I am doing something wrong.\n\nruby\ninstallation\nrubygems\nShare\nImprove this question\nFollow\nedited Oct 22 '20 at 10:19\n\nA876\n41344 silver badges77 bronze badges\nasked Sep 20 '13 at 4:15\n\nuser2797743\n1,00522 gold badges88 silver badges99 bronze badges\nAdd a comment\n7 Answers\n\n133\n\nTo setup you Ruby development environment on Windows:\n\nInstall Ruby via RubyInstaller: http://rubyinstaller.org/downloads/\n\nCheck your ruby version: Start - Run - type in cmd to open a windows console\n\nType in ruby -v\nYou will get something like that: ruby 2.0.0p353 (2013-11-22) [i386-mingw32]\nFor Ruby 2.4 or later, run the extra installation at the end to install the DevelopmentKit. If you forgot to do that, run ridk install in your windows console to install it.\n\nFor earlier versions:\n\nDownload and install DevelopmentKit from the same download page as Ruby Installer. Choose an ?exe file corresponding to your environment (32 bits or 64 bits and working with your version of Ruby).\nFollow the installation instructions for DevelopmentKit described at: https://github.com/oneclick/rubyinstaller/wiki/Development-Kit#installation-instructions. Adapt it for Windows.\nAfter installing DevelopmentKit you can install all needed gems by just running from the command prompt (windows console or terminal): gem install {gem name}. For example, to install rails, just run gem install rails.\nHope this helps.\n\nShare\nImprove this answer\nFollow\nedited Oct 21 '17 at 22:35\n\nPHPirate\n5,42655 gold badges4444 silver badges6868 bronze badges\nanswered Mar 3 '14 at 15:58\n\nbelgoros\n2,83933 gold badges2121 silver badges5757 bronze badges\nThis is the more complete answer. Thanks! \u2013 \nD. Visser\n Mar 9 '16 at 9:49\n4\nDon't forget to add your Ruby install to PATH! (For me it was C:\\Ruby22\\bin) \u2013 \nclarkatron\n May 7 '16 at 18:52\n1\n@clarkatron the installer has an option to do it for you. Also, starting from Ruby 2.4.0, the MSYS2 DevKit is downloaded as the last step of the installation (so no need for steps 5-6). \u2013 \nOhad Schneider\n Aug 23 '17 at 14:15\nAdd a comment\n\n79\n\nI recommend you just use rubyinstaller\n\nIt is recommended by the official Ruby page - see https://www.ruby-lang.org/en/downloads/\n\nWays of Installing Ruby\n\nWe have several tools on each major platform to install Ruby:\n\nOn Linux/UNIX, you can use the package management system of your distribution or third-party tools (rbenv and RVM).\nOn OS X machines, you can use third-party tools (rbenv and RVM).\nOn Windows machines, you can use RubyInstaller.\nShare\nImprove this answer\nFollow\nedited Feb 8 '17 at 12:07\n\nreducing activity\n1,26111 gold badge2525 silver badges5353 bronze badges\nanswered Sep 20 '13 at 5:10\n\nMirage\n1,4391515 silver badges2626 bronze badges\n3\nI downloaded this and now I am stuck with a file that crashes my pc when i try to run, move or delete it, even after rebooting. Never had this before. \u2013 \nDaan Luttik\n May 30 '15 at 12:45\nAdd a comment\n\n24\n\nInstalling Ruby\nGo to http://rubyinstaller.org/downloads/\n\nMake sure that you check \"Add ruby ... to your PATH\". enter image description here\n\nNow you can use \"ruby\" in your \"cmd\".\n\nIf you installed ruby 1.9.3 I expect that the ruby is downloaded in C:\\Ruby193.\n\nInstalling Gem\ninstall Development Kit in rubyinstaller. Make new folder such as C:\\RubyDevKit and unzip.\n\nGo to the devkit directory and type ruby dk.rb init to generate config.yml.\n\nIf you installed devkit for 1.9.3, I expect that the config.yml will be written as C:\\Ruby193.\n\nIf not, please correct path to your ruby folders.\n\nAfter reviewing the config.yml, you can finally type ruby dk.rb install.\n\nNow you can use \"gem\" in your \"cmd\". It's done!\n\nShare\nImprove this answer\nFollow\nedited Oct 5 '15 at 20:49\nanswered Oct 5 '15 at 20:27\n\nMiae Kim\n1,5211818 silver badges2424 bronze badges\nAdd a comment\n\n\n22\n\nUse chocolatey in PowerShell\n\nchoco install ruby -y\nrefreshenv\ngem install bundler\nShare\nImprove this answer\nFollow\nanswered Nov 24 '17 at 1:19\n\nfangxing\n4,29622 gold badges3636 silver badges4444 bronze badges\n2\nIn my case refreshenv did not work: I had to close and reopen PowerShell and then it worked \u2013 \nMarco Lackovic\n Dec 3 '19 at 12:13\nAdd a comment\n\n16\n\nAnother way is to let chocolatey manage your ruby package (and any other package), that way you won't have to put ruby in your path manually:\n\nInstall chocolatey first by opening your favourite command prompt and executing:\n\n@powershell -NoProfile -ExecutionPolicy unrestricted -Command \"iex ((new-object net.webclient).DownloadString('https://chocolatey.org/install.ps1'))\" && SET PATH=%PATH%;%systemdrive%\\chocolatey\\bin\nthen all you need to do is type\n\ncinst ruby\nIn your command prompt and the package installs.\n\nUsing a package manager provides overall more control, I'd recommend this for every package that can be installed via chocolatey.\n\nShare\nImprove this answer\nFollow\nanswered Apr 9 '14 at 10:12\n\nJulian Krispel-Samsel\n6,94233 gold badges3131 silver badges4040 bronze badges\n1\nty @nimrod! My notes; 1.To see what you have installed locally, \"choco list -lo \" ; 2.To install a ruby gem use something like : \"choco gem gemcutter -version 0.7.1\" per github.com/chocolatey/chocolatey/wiki/CommandsGem ; 3.A kickstarted was recently funded so we may see more changes kickstarter.com/projects/ferventcoder/\u2026 \u2013 \nAnneTheAgile\n Jan 20 '15 at 17:46 \n1\nThat's handy thanks! I love chocolatey, so much so that I decided to contribute some design work: github.com/chocolatey/chocolatey/issues/640 \u2013 \nJulian Krispel-Samsel\n Jan 21 '15 at 12:29 \nAdd a comment\n\n3\n\nI use scoop as command-liner installer for Windows... scoop rocks!\nThe quick answer (use PowerShell):\n\nPS C:\\Users\\myuser> scoop install ruby\nLonger answer:\n\nJust searching for ruby:\n\nPS C:\\Users\\myuser> scoop search ruby\n'main' bucket:\n    jruby (9.2.7.0)\n    ruby (2.6.3-1)\n\n'versions' bucket:\n    ruby19 (1.9.3-p551)\n    ruby24 (2.4.6-1)\n    ruby25 (2.5.5-1)\nCheck the installation info :\n\nPS C:\\Users\\myuser> scoop info ruby\nName: ruby\nVersion: 2.6.3-1\nWebsite: https://rubyinstaller.org\nManifest:\n  C:\\Users\\myuser\\scoop\\buckets\\main\\bucket\\ruby.json\nInstalled: No\nEnvironment: (simulated)\n  GEM_HOME=C:\\Users\\myuser\\scoop\\apps\\ruby\\current\\gems\n  GEM_PATH=C:\\Users\\myuser\\scoop\\apps\\ruby\\current\\gems\n  PATH=%PATH%;C:\\Users\\myuser\\scoop\\apps\\ruby\\current\\bin\n  PATH=%PATH%;C:\\Users\\myuser\\scoop\\apps\\ruby\\current\\gems\\bin\nOutput from installation:\n\nPS C:\\Users\\myuser> scoop install ruby\nUpdating Scoop...\nUpdating 'extras' bucket...\nInstalling 'ruby' (2.6.3-1) [64bit]\nrubyinstaller-2.6.3-1-x64.7z (10.3 MB) [============================= ... ===========] 100%\nChecking hash of rubyinstaller-2.6.3-1-x64.7z ... ok.\nExtracting rubyinstaller-2.6.3-1-x64.7z ... done.\nLinking ~\\scoop\\apps\\ruby\\current => ~\\scoop\\apps\\ruby\\2.6.3-1\nPersisting gems\nRunning post-install script...\nFetching rake-12.3.3.gem\nSuccessfully installed rake-12.3.3\nParsing documentation for rake-12.3.3\nInstalling ri documentation for rake-12.3.3\nDone installing documentation for rake after 1 seconds\n1 gem installed\n'ruby' (2.6.3-1) was installed successfully!\nNotes\n-----\nInstall MSYS2 via 'scoop install msys2' and then run 'ridk install' to install the toolchain!\n'ruby' suggests installing 'msys2'.\nPS C:\\Users\\myuser>\nShare\nImprove this answer\nFollow\nanswered Jul 26 '19 at 9:05\n\nnephewtom\n2,48011 gold badge2626 silver badges4646 bronze badges\nAdd a comment\n\n1\n\nCheck that ruby interpreter is already installed and try \"ruby setup.rb\" in command prompt.\n\nShare\nImprove this answer\nFollow\nanswered Sep 20 '13 at 5:09\n\nITemius\n85199 silver badges1919 bronze badges\n1\nor just ruby -v \u2013 \nJay Killeen\n May 3 '17 at 3:18 \nAdd a comment\nHighly active question. Earn 10 reputation (not counting the association bonus) in order to answer this question. The reputation requirement helps protect this question from spam and non-answer activity.\nNot the answer you're looking for? Browse other questions tagged ruby installation rubygems or ask your own question.\nThe Overflow Blog\nDoes ES6 make JavaScript frameworks obsolete?\nPodcast 392: Do polyglots have an edge when it comes to mastering programming...\nFeatured on Meta\nNow live: A fully responsive profile\nPlease welcome Valued Associates #999 - Bella Blue & #1001 - Salmon of Wisdom\nRemote jobs\n\nSenior Software Engineer - Agent Tools\nAssurance  No office location\n$120K - $200KREMOTE\nrubyruby-on-rails\n\nSoftware Engineer - Developer Experience\nCatawiki  No office location\n\u20ac60K - \u20ac85KREMOTE\nrubygo\n\nSenior Ruby on Rails developer\nMarketer Technologies AS  Oslo, Norway\nREMOTERELOCATION\nrubyruby-on-rails\n\nRemote Senior AWS DevOps Engineer\nTheorem, LLC  No office location\nREMOTE\nrubyamazon-web-services\n\nStaff Software Engineer\nRoot Insurance Company  Columbus, OH\nREMOTE\nreactjsruby-on-rails\n\nSenior Backend Developer - Node (Remote)\nBitfinex  No office location\nREMOTE\nrubyjavascript\n\nSenior Backend Engineer\nBlinks Labs GmbH  Berlin, Germany\nREMOTERELOCATION\nrubyruby-on-rails\n\nBack End Engineer (Ruby on Rails)\nModus Create  No office location\nREMOTE\nrubyruby-on-rails\nView more jobs on Stack Overflow\nLinked\n0\nCouldn't install http-cookie gem\n5\nCan\u2019t run \u201dgem install bundle\u2019 in git bash terminal\n1\nInstalling LogStash on windows\n0\nUnable to Setup Ruby Gems on Windows\n1\nRuby Rails Installation and Configuration Issues\n0\ntrying to include sunlight congress gem in my project\n0\nTravis CI is failing but source code works fine?\n0\nHow to install Mongify on Windows\nRelated\n517\nError installing mysql2: Failed to build gem native extension\n2655\nHow can I install pip on Windows?\n5\nHow do I get `gem` working again after upgrading to rubygems 1.8.x?\n178\nThe 'json' native gem requires installed build tools\n1\nRuby windows installer: cannot run gem - required files not found\n0\nHow to install 'savon' gems on Windows machine?\n587\nHow to install a gem or update RubyGems if it fails with a permissions error\n0\nIssue installing Rubygems 2.0.3 (Ruby 2.0.0-p195) - No directory?\n487\nError while installing json gem 'mkmf.rb can't find header files for ruby'\n1006\nNode.js/Windows error: ENOENT, stat 'C:\\Users\\RT\\AppData\\Roaming\\npm'\nHot Network Questions\n\"I'm better at X than you are at Y\" or \"I love X more than I miss Y\"\nWhat kind of substance that existed in the middles ages could, when ignited, potentially destroy everything within several miles?\nWhat is most important for making biking uphill easy?\nCan this water damage to my Canon T6 be repaired?\nQuadratic scaling of axes\nmore hot questions\n Question feed\n\nSTACK OVERFLOW\nQuestions\nJobs\nDeveloper Jobs Directory\nSalary Calculator\nHelp\nMobile\nPRODUCTS\nTeams\nTalent\nAdvertising\nEnterprise\nCOMPANY\nAbout\nPress\nWork Here\nLegal\nPrivacy Policy\nTerms of Service\nContact Us\nCookie Settings\nCookie Policy\nSTACK EXCHANGE\nNETWORK\nTechnology\nLife / Arts\nCulture / Recreation\nScience\nOther\nBlog\nFacebook\nTwitter\nLinkedIn\nInstagram\nsite design / logo \u00a9 2021 Stack Exchange Inc; user contributions licensed under cc by-sa. rev 2021.11.12.40742\n\nYour privacy\n\nBy clicking \u201cAccept all cookies\u201d, you agree Stack Exchange can store cookies on your device and disclose information in accordance with our Cookie Policy.\n\n Stack Overflow requires external JavaScript from another domain, which is blocked or failed to load. Retry using another source.", "document_id": 548}]}, {"paragraphs": [{"qas": [{"question": "How to install Docker-Engine on Fedora", "id": 71, "answers": [{"answer_id": 75, "document_id": 66, "question_id": 71, "text": "ll the latest version of Docker Engine and containerd, or go to the next step to install a specific version:\n$ sudo dnf install docker-ce docker-ce-cli containerd.io\n\nIf prompted to accept the GPG key, verify that the fingerprint matches\n060A 61C5 1B55 8A7F 742B 77AA C52F EB6B 621E 9F35, and if so, accept it.\n\nGot ", "answer_start": 2878, "answer_category": null}], "is_impossible": false}, {"question": "The docker group is created, but no users are added to the group.Cgroups Exception:", "id": 72, "answers": [{"answer_id": 76, "document_id": 66, "question_id": 72, "text": "edora 31 and higher, you need to enable the backward compatibility for Cgroups.\n$ sudo grubby --update-kernel=ALL --args=\"systemd.unified_cgroup_hierarchy=0\"\n\nAft", "answer_start": 4751, "answer_category": null}], "is_impossible": false}], "context": "Install Docker Engine on Fedora\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSearch\n\n\n\n\n\n\n\nToggle navigation\n\n\n\n\n\n\n\nHomeGuidesProduct manualsReferenceSamples\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInstall Docker Engine on FedoraEstimated reading time: 10 minutesTo get started with Docker Engine on Fedora, make sure you\nmeet the prerequisites, then\ninstall Docker.\nPrerequisites\ud83d\udd17\nOS requirements\ud83d\udd17\nTo install Docker Engine, you need the 64-bit version of one of these Fedora versions:\n\nFedora 30\nFedora 31\n\nUninstall old versions\ud83d\udd17\nOlder versions of Docker were called docker or docker-engine. If these are\ninstalled, uninstall them, along with associated dependencies.\n$ sudo dnf remove docker \\\ndocker-client \\\ndocker-client-latest \\\ndocker-common \\\ndocker-latest \\\ndocker-latest-logrotate \\\ndocker-logrotate \\\ndocker-selinux \\\ndocker-engine-selinux \\\ndocker-engine\n\nIt\u2019s OK if dnf reports that none of these packages are installed.\nThe contents of /var/lib/docker/, including images, containers, volumes, and\nnetworks, are preserved. The Docker Engine package is now called docker-ce.\nInstallation methods\ud83d\udd17\nYou can install Docker Engine in different ways, depending on your needs:\n\n\nMost users\nset up Docker\u2019s repositories and install\nfrom them, for ease of installation and upgrade tasks. This is the\nrecommended approach.\n\n\nSome users download the RPM package and\ninstall it manually and manage\nupgrades completely manually. This is useful in situations such as installing\nDocker on air-gapped systems with no access to the internet.\n\n\nIn testing and development environments, some users choose to use automated\nconvenience scripts to install Docker.\n\n\nInstall using the repository\ud83d\udd17\nBefore you install Docker Engine for the first time on a new host machine, you need\nto set up the Docker repository. Afterward, you can install and update Docker\nfrom the repository.\nSet up the repository\nInstall the dnf-plugins-core package (which provides the commands to manage\nyour DNF repositories) and set up the stable repository.\n$ sudo dnf -y install dnf-plugins-core\n\n$ sudo dnf config-manager \\\n--add-repo \\\nhttps://download.docker.com/linux/fedora/docker-ce.repo\n\n\nOptional: Enable the nightly or test repositories.\nThese repositories are included in the docker.repo file above but are disabled\nby default. You can enable them alongside the stable repository.  The following\ncommand enables the nightly repository.\n$ sudo dnf config-manager --set-enabled docker-ce-nightly\n\nTo enable the test channel, run the following command:\n$ sudo dnf config-manager --set-enabled docker-ce-test\n\nYou can disable the nightly or test repository by running the\ndnf config-manager command with the --set-disabled flag. To re-enable it,\nuse the --set-enabled flag. The following command disables the nightly\nrepository.\n$ sudo dnf config-manager --set-disabled docker-ce-nightly\n\nLearn about nightly and test channels.\n\nInstall Docker Engine\n\n\nInstall the latest version of Docker Engine and containerd, or go to the next step to install a specific version:\n$ sudo dnf install docker-ce docker-ce-cli containerd.io\n\nIf prompted to accept the GPG key, verify that the fingerprint matches\n060A 61C5 1B55 8A7F 742B 77AA C52F EB6B 621E 9F35, and if so, accept it.\n\nGot multiple Docker repositories?\nIf you have multiple Docker repositories enabled, installing\nor updating without specifying a version in the dnf install or\ndnf update command always installs the highest possible version,\nwhich may not be appropriate for your stability needs.\n\nDocker is installed but not started. The docker group is created, but no users are added to the group.\n\n\nTo install a specific version of Docker Engine, list the available versions\nin the repo, then select and install:\na. List and sort the versions available in your repo. This example sorts\nresults by version number, highest to lowest, and is truncated:\n$ dnf list docker-ce  --showduplicates | sort -r\n\ndocker-ce.x86_64  3:18.09.1-3.fc28                 docker-ce-stable\ndocker-ce.x86_64  3:18.09.0-3.fc28                 docker-ce-stable\ndocker-ce.x86_64  18.06.1.ce-3.fc28                docker-ce-stable\ndocker-ce.x86_64  18.06.0.ce-3.fc28                docker-ce-stable\n\nThe list returned depends on which repositories are enabled, and is specific\nto your version of Fedora (indicated by the .fc28 suffix in this example).\nb. Install a specific version by its fully qualified package name, which is\nthe package name (docker-ce) plus the version string (2nd column) up to\nthe first hyphen, separated by a hyphen (-), for example,\ndocker-ce-3:18.09.1.\n$ sudo dnf -y install docker-ce-<VERSION_STRING> docker-ce-cli-<VERSION_STRING> containerd.io\n\nDocker is installed but not started. The docker group is created, but no users are added to the group.\n\n\nCgroups Exception:\nFor Fedora 31 and higher, you need to enable the backward compatibility for Cgroups.\n$ sudo grubby --update-kernel=ALL --args=\"systemd.unified_cgroup_hierarchy=0\"\n\nAfter running the command, you must reboot for the changes to take effect.\n\n\nStart Docker.\n$ sudo systemctl start docker\n\n\n\nVerify that Docker Engine is installed correctly by running the hello-world\nimage.\n$ sudo docker run hello-world\n\nThis command downloads a test image and runs it in a container. When the\ncontainer runs, it prints an informational message and exits.\n\n\nDocker Engine is installed and running. You need to use sudo to run Docker\ncommands. Continue to Linux postinstall to allow\nnon-privileged users to run Docker commands and for other optional configuration\nsteps.\nUpgrade Docker Engine\nTo upgrade Docker Engine, follow the installation instructions,\nchoosing the new version you want to install.\nInstall from a package\ud83d\udd17\nIf you cannot use Docker\u2019s repository to install Docker, you can download the\n.rpm file for your release and install it manually. You need to download\na new file each time you want to upgrade Docker Engine.\n\n\nGo to https://download.docker.com/linux/fedora/\nand choose your version of Fedora. Then browse to x86_64/stable/Packages/\nand download the .rpm file for the Docker version you want to install.\n\nNote: To install a nightly or test (pre-release) package,\nchange the word stable in the above URL to nightly or test.\nLearn about nightly and test channels.\n\n\n\nInstall Docker Engine, changing the path below to the path where you downloaded\nthe Docker package.\n$ sudo dnf -y install /path/to/package.rpm\n\nDocker is installed but not started. The docker group is created, but no\nusers are added to the group.\n\n\nStart Docker.\n$ sudo systemctl start docker\n\n\n\nVerify that Docker Engine is installed correctly by running the hello-world\nimage.\n$ sudo docker run hello-world\n\nThis command downloads a test image and runs it in a container. When the\ncontainer runs, it prints an informational message and exits.\n\n\nDocker Engine is installed and running. You need to use sudo to run Docker commands.\nContinue to Post-installation steps for Linux to allow\nnon-privileged users to run Docker commands and for other optional configuration\nsteps.\nUpgrade Docker Engine\nTo upgrade Docker Engine, download the newer package file and repeat the\ninstallation procedure, using dnf -y upgrade\ninstead of dnf -y install, and pointing to the new file.\n\nInstall using the convenience script\ud83d\udd17\nDocker provides convenience scripts at get.docker.com\nand test.docker.com for installing edge and\ntesting versions of Docker Engine - Community into development environments quickly and\nnon-interactively. The source code for the scripts is in the\ndocker-install repository.\nUsing these scripts is not recommended for production\nenvironments, and you should understand the potential risks before you use\nthem:\n\nThe scripts require root or sudo privileges to run. Therefore,\nyou should carefully examine and audit the scripts before running them.\nThe scripts attempt to detect your Linux distribution and version and\nconfigure your package management system for you. In addition, the scripts do\nnot allow you to customize any installation parameters. This may lead to an\nunsupported configuration, either from Docker\u2019s point of view or from your own\norganization\u2019s guidelines and standards.\nThe scripts install all dependencies and recommendations of the package\nmanager without asking for confirmation. This may install a large number of\npackages, depending on the current configuration of your host machine.\nThe script does not provide options to specify which version of Docker to install,\nand installs the latest version that is released in the \u201cedge\u201d channel.\nDo not use the convenience script if Docker has already been installed on the\nhost machine using another mechanism.\n\nThis example uses the script at get.docker.com to\ninstall the latest release of Docker Engine - Community on Linux. To install the latest\ntesting version, use test.docker.com instead. In\neach of the commands below, replace each occurrence of get with test.\n\nWarning:\nAlways examine scripts downloaded from the internet before\nrunning them locally.\n\n$ curl -fsSL https://get.docker.com -o get-docker.sh\n$ sudo sh get-docker.sh\n\n<output truncated>\n\nIf you would like to use Docker as a non-root user, you should now consider\nadding your user to the \u201cdocker\u201d group with something like:\nsudo usermod -aG docker your-user\n\nRemember to log out and back in for this to take effect!\n\nWarning:\nAdding a user to the \u201cdocker\u201d group grants them the ability to run containers\nwhich can be used to obtain root privileges on the Docker host. Refer to\nDocker Daemon Attack Surface\nfor more information.\n\nDocker Engine - Community is installed. It starts automatically on DEB-based distributions. On\nRPM-based distributions, you need to start it manually using the appropriate\nsystemctl or service command. As the message indicates, non-root users can\u2019t\nrun Docker commands by default.\n\nNote:\nTo install Docker without root privileges, see\nRun the Docker daemon as a non-root user (Rootless mode).\nRootless mode is currently available as an experimental feature.\n\nUpgrade Docker after using the convenience script\nIf you installed Docker using the convenience script, you should upgrade Docker\nusing your package manager directly. There is no advantage to re-running the\nconvenience script, and it can cause issues if it attempts to re-add\nrepositories which have already been added to the host machine.\nUninstall Docker Engine\ud83d\udd17\n\n\nUninstall the Docker Engine, CLI, and Containerd packages:\n$ sudo dnf remove docker-ce docker-ce-cli containerd.io\n\n\n\nImages, containers, volumes, or customized configuration files on your host\nare not automatically removed. To delete all images, containers, and\nvolumes:\n$ sudo rm -rf /var/lib/docker\n\n\n\nYou must delete any edited configuration files manually.\nNext steps\ud83d\udd17\n\nContinue to Post-installation steps for Linux.\nReview the topics in Develop with Docker to learn how to build new applications using Docker.\n\nrequirements, apt, installation, fedora, rpm, install, uninstall, upgrade, updateRate this page:\u00a022\u00a026\u00a0i\n\n\n\n\n\nDocker EngineOverviewInstallInstallation per distroInstall on CentOSInstall on DebianInstall on FedoraInstall on UbuntuInstall binariesOptional post-installation stepsRelease notesPrevious versionsEngine 18.09 release notesEngine 18.06 release notesEngine 18.05 release notesEngine 18.04 release notesEngine 18.03 release notesEngine 18.02 release notesEngine 18.01 release notesEngine 17.12 release notesEngine 17.11 release notesEngine 17.10 release notesEngine 17.09 release notesEngine 17.07 release notesEngine 17.06 release notesEngine 17.05 release notesEngine 17.04 release notesEngine 17.03 release notesEngine 1.13 and earlierDeprecated featuresDocker AppDocker BuildxDocker ContextDocker ScanDocker ComposeOverview of Docker ComposeInstall ComposeRelease notesGetting startedEnvironment variables in ComposeEnvironment fileExtend services in ComposeNetworking in ComposeUsing Compose in productionControl startup orderSample apps with ComposeDocker Compose release notesDocker DesktopOverviewMacInstall Docker Desktop for MacUser manualDeploy on KubernetesLeveraging Multi-CPU architecture supportNetworkingDisk utilizationLogs and troubleshootingFAQsStable release notesEdge release notesWindowsInstall Docker Desktop for WindowsInstall Docker Desktop on Windows HomeUser manualDeploy on KubernetesNetworkingLogs and troubleshootingFAQsStable release notesEdge release notesDocker Desktop WSL 2 backendDashboardOpen source licensingDocker HubQuickstartDocker ID accountsSecurity and authenticationManage access tokensTwo-factor authenticationEnable two-factor authenticationDisable two-factor authenticationRecover your Docker Hub accountGenerate a new recovery codeTeams and organizationsRepositoriesVulnerability scanningOfficial imagesAutomated buildsSet up automated buildsTesting in automated buildsAdvanced automated buildsLink to GitHub and BitBucketWebhooksSlack integrationDownload rate limitAdministrationConvert an account into an organizationDeactivate an account or an organizationBillingOverviewUpgrade your planDowngrade your planAdd seatsRemove seatsPublisher & certified contentOverviewSubmit a product for Docker HubUser FAQsPublisher FAQsCertify images & pluginsCertify logging pluginsTrust chainBring Your Own License (BYOL)Release notesOpen-source projectsNotaryGet started with NotaryUse the Notary clientUnderstand the service architectureRun a Notary serviceNotary changelogConfiguration filesServer configurationSigner configurationClient configurationCommon Server and signer configurationsDocker RegistryRegistry overviewUnderstand the RegistryDeploy a registry serverConfigure a registryWork with notificationsRecipesRecipes overviewAuthenticating proxy with apacheAuthenticating proxy with nginxMirroring Docker HubRunning on macOSGarbage collectionTesting an insecure registryDeprecated featuresCompatibilityGetting helpRelease notesSuperseded products and toolsDocker MachineMachine overviewInstall MachineRelease notesGet started with a local VMProvision hosts in the cloudLearn by exampleProvision Digital Ocean DropletsProvision AWS EC2 instancesMachine concepts and helpMachine (docker-machine) CLIMachine CLI overviewMachine command-line completionactiveconfigcreateenvhelpinspectipkilllsmountprovisionregenerate-certsrestartrmscpsshstartstatusstopupgradeurlMachine driversDrivers overviewDriver options and operating system defaultsAmazon Web ServicesDigital OceanExoscaleGenericGoogle Compute EngineIBM SoftlayerMicrosoft AzureMicrosoft Hyper-VOpenStackOracle VirtualBoxRackspaceVMware FusionVMware vCloud AirVMware vSphereMigrate from Boot2Docker to MachineDocker Toolbox (deprecated)KitematicKitematic user guide: intro & overviewSet up an Nginx web serverSet up a Minecraft ServerCreating a local RethinkDB database for developmentFrequently asked questionsKnown issuesTroubleshooting\n\n\n\n\n\n\n\nEdit this page Request docs changes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn this page:\n\nPrerequisites\n\nOS requirements\nUninstall old versions\n\n\nInstallation methods\n\nInstall using the repository\n\nSet up the repository\nInstall Docker Engine\nUpgrade Docker Engine\n\n\nInstall from a package\n\nUpgrade Docker Engine\n\n\nInstall using the convenience script\n\nUpgrade Docker after using the convenience script\n\n\n\n\nUninstall Docker Engine\nNext steps\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhy Docker?\nWhat is a Container?\n\n\n\n\nProducts\nDocker Desktop\nDocker Hub\nFeatures\nContainer Runtime\nDeveloper Tools\nKubernetes\n\n\n\n\nDevelopers\nUse Cases\nPlay with Docker\nCommunity\nOpen Source\nDocker Captains\n\n\n\n\nCompany\nAbout Us\nBlog\nCustomers\nPartners\nNewsroom\nCareers\nContact Us\n\n\n\n\n\n\nStatus\nSecurity\nLegal\nContact\n\n\n\n\n\n\n\nCopyright \u00a9 2013-2020 Docker Inc. All rights reserved.\n\n\n\nTwitter\nYoutube\nGitHub\nLinkedin\nFacebook\nSlideshare\nReddit\n\n\n\n\n\n\n\n", "document_id": 66}]}, {"paragraphs": [{"qas": [{"question": "Passing app secrets in Yesod and Keter", "id": 1325, "answers": [{"answer_id": 1315, "document_id": 894, "question_id": 1325, "text": "Set your secrets as environment variables on your server and \u2018forward\u2019 them to your app using forward-env in config/keter.yml as seen in the following patch: https://github.com/snoyberg/keter/commit/9e9fca314fb78860fb5c9b08cad212d92b0b20d4", "answer_start": 834, "answer_category": null}], "is_impossible": false}], "context": "I'm building a web app with Yesod and am currently passing in secrets such as API keys via environment variables (as per The Twelve-Factor App) to avoid storing these values in version-controlled configuration files. For example, I run my app in dev mode as follows:\nSOME_API_KEY=value yesod devel\nI have a value in my config/settings.yml file that is defined in terms of this environment variable with an empty value as follows:\nmeetup-api-key: \"_env:SOME_API_KEY:\"\nTo deploy using Keter, I'm building the Keter package using the yesod keter command and dropping the resulting file into Keter 'sincoming directory. Since I'm using environment variable configuration, my app's .keter file does not contain the SOME_API_KEY value (which is intentional).\nHow should I pass SOME_API_KEY into the instance of my app running inside Keter?\nSet your secrets as environment variables on your server and \u2018forward\u2019 them to your app using forward-env in config/keter.yml as seen in the following patch: https://github.com/snoyberg/keter/commit/9e9fca314fb78860fb5c9b08cad212d92b0b20d4\n", "document_id": 894}]}, {"paragraphs": [{"qas": [{"question": "Does Android keep the .apk files? if so where?", "id": 1549, "answers": [{"answer_id": 1538, "document_id": 1126, "question_id": 1549, "text": "You can use package manager (pm) over adb shell to list packages:\nadb shell pm list packages | sort", "answer_start": 154, "answer_category": null}], "is_impossible": false}], "context": "After android installs an application from the Marketplace, does it keep the .apk file?\nIs there a standard location where Android would keep such files?\nYou can use package manager (pm) over adb shell to list packages:\nadb shell pm list packages | sort\n", "document_id": 1126}]}, {"paragraphs": [{"qas": [{"question": "What's the foolproof way to tell which version(s) of .NET are installed on a production Windows Server?", "id": 340, "answers": [{"answer_id": 347, "document_id": 152, "question_id": 340, "text": "You should open up IE on the server for which you are looking for this info, and go to this site: http://www.hanselman.com/smallestdotnet/\nThat's all it takes.", "answer_start": 0, "answer_category": null}], "is_impossible": false}], "context": "You should open up IE on the server for which you are looking for this info, and go to this site: http://www.hanselman.com/smallestdotnet/\nThat's all it takes.\nThe site has a script that looks your browser's \"UserAgent\" and figures out what version (if any) of the .NET Framework you have (or don't have) installed, and displays it automatically (then calculates the total size if you chose to download the .NET Framework).\n", "document_id": 152}]}, {"paragraphs": [{"qas": [{"question": "WIX: Howto set the name of the msi output file dynamically", "id": 1220, "answers": [{"answer_id": 1213, "document_id": 796, "question_id": 1220, "text": "\u2022\tset an environment variable with set productversion=1.2.3\n\u2022\tPass -out foo%productversion%.msi to the light.exe linker\n\u2022\tuse the same environment variable in your wix files as $(env.productversion)", "answer_start": 434, "answer_category": null}], "is_impossible": false}], "context": "I want to include some dynamic part in the filename of the msi file my wix projects produce. This dynamic part should be controlled by variables which are part of my wix project and are declared like this.\nThe msi file name is not determined by your wix files, but by the light.exe -out switch. You can use the same value for -out and inside your wix files if you do the following in your build script, assuming it is a batch script:\n\u2022\tset an environment variable with set productversion=1.2.3\n\u2022\tPass -out foo%productversion%.msi to the light.exe linker\n\u2022\tuse the same environment variable in your wix files as $(env.productversion)\n", "document_id": 796}]}, {"paragraphs": [{"qas": [{"question": "Make Maven to copy dependencies into target/lib", "id": 1873, "answers": [{"answer_id": 1859, "document_id": 1444, "question_id": 1873, "text": "You can try this command:\nmvn install dependency:copy-dependencies ", "answer_start": 214, "answer_category": null}], "is_impossible": false}], "context": "How do I get my project's runtime dependencies copied into the target/lib folder?\nAs it is right now, after mvn clean install the target folder contains only my project's jar, but none of the runtime dependencies.\nYou can try this command:\nmvn install dependency:copy-dependencies \n", "document_id": 1444}]}, {"paragraphs": [{"qas": [{"question": "How do YOU deploy your WSGI application? (and why it is the best way)", "id": 598, "answers": [{"answer_id": 604, "document_id": 323, "question_id": 598, "text": " When I don't need any apache features I am going with a pure python webserver like paste", "answer_start": 160, "answer_category": null}], "is_impossible": false}], "context": "Deploying a WSGI application. There are many ways to skin this cat. I am currently using apache2 with mod-wsgi, but I can see some potential problems with this. When I don't need any apache features I am going with a pure python webserver like paste", "document_id": 323}]}, {"paragraphs": [{"qas": [{"question": "Whats best way to package a Java Application with lots of dependencies?", "id": 1246, "answers": [{"answer_id": 1239, "document_id": 818, "question_id": 1246, "text": "In general, most Java developers would use Apache Ant to achieve deployment. Its well documented, so take a look.", "answer_start": 812, "answer_category": null}], "is_impossible": false}], "context": "I'm writing a java app using eclipse which references a few external jars and requires some config files to be user accessable.\nWhat is the best way to package it up for deployment?\nMy understanding is that you cant put Jars inside another jar file, is this correct?\nCan I keep my config files out of the jars and still reference them in the code? Or should the path to the config file be a command line argument?\nAre there any third party plugins for eclipse to help make this easier? I'm using an ant build file at the moment but I'm not sure I know what I'm doing.\nIs there an equivelent of the deployment projects in Visual studio, that will figure out everything you need and just make an installer? I've used install4j before, and it was powerful if no where near as automated as .Net deployment projects.\nIn general, most Java developers would use Apache Ant to achieve deployment. Its well documented, so take a look.\n", "document_id": 818}]}, {"paragraphs": [{"qas": [{"question": "Git push to live server", "id": 1634, "answers": [{"answer_id": 1622, "document_id": 1208, "question_id": 1634, "text": "1.\tAdd remote $ git remote add server ssh://server_hostname:/path/to/git/repo\n2.\tCheckout temp branch on server $ git checkout -b temp\n3.\tPush changes $ git push server\n4.\tCheckout previous branch and delete the temporary one  $ git checkout - # shorthand for previous branch, git checkout @{-1} $ git branch -d temp", "answer_start": 294, "answer_category": null}], "is_impossible": false}], "context": "We have a website that has all its PHP/HTML/JS/CSS/etc files stored in a Git repository. So my question is: is there a way that from my local computer I can push straight to the web server?\n\nLook at the git urls portion of http://www.kernel.org/pub/software/scm/git/docs/v1.6.0.6/git-push.html\n1.\tAdd remote $ git remote add server ssh://server_hostname:/path/to/git/repo\n2.\tCheckout temp branch on server $ git checkout -b temp\n3.\tPush changes $ git push server\n4.\tCheckout previous branch and delete the temporary one  $ git checkout - # shorthand for previous branch, git checkout @{-1} $ git branch -d temp\nI like this approach because the post-receive hook does the work on your server so your local machine can push much faster and free itself up. This also makes it very easy to setup remote tracking for a particular branch. So you could have a branch called production to update your web server, while your master continues to be for development and link to your git repo elsewhere.\n", "document_id": 1208}]}, {"paragraphs": [{"qas": [{"question": "how to create msi installer with wix", "id": 1489, "answers": [{"answer_id": 1478, "document_id": 1065, "question_id": 1489, "text": " to where you need.\n\nTo make an update, make the necessary changes and then change the version number in that project's Properties -&gt; Application -&gt; Assembly Information. Then also change it in Product.wxs &lt;Wix&gt;&lt;Product.Version&gt;. Th", "answer_start": 2053, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nCan someone help me understand how WiX works? I have a directory structure which I would like to create an installer for. I have generated the  for the directory structure with heat.exe and when I build the setup project it generates an .msi file but I don't think it installs anything.\n\nMaybe someone can walk me through the steps of generating a .msi installer.\n\nAny advise is appreciated,\nThank you\n    \n\nIf you're using Visual Studio:\n\nInstall the WiX Toolset V3 Visual Studio plugin.\nInstall the Wax interactive editor.\nBuild your project if you haven't already.\nAdd a new project to the solution containing the project you want to create an installer for.\nChoose the template Setup Project for WiX v3.\nName the installer. A personal convention is the name of the project plus \".Setup\"\nA Product.wxs file will open up. Change the Manufacturer=\"\" value to something like your company name or your name. Keep the file open.\nGo to Tools -&gt; WiX Setup Editor\nOn the left under Root Directory choose InstallFolder\nUnder Projects to install, choose the project you want to install.\nIn the red area to the right, you'll see a list of files. These are the output results of your build from step 3.\nClick the plus sign next to files you want to copy. They should turn white and change to a Resolved state.\nThis might look daunting, but you're just telling it what to copy--which would be your project's executable, configs, dll libraries, and images it's dependent upon.\nYou typically want to copy everything. If there are dll's you know you don't need, it's better to remove them as a dependency from the Visual Studio.\nNotice the Product.wxs has changed. It now has the files you checked off in the Setup Editor GUI added to the &lt;Wix&gt;&lt;Fragment&gt;&lt;ComponentGroup&gt; section. Save that file.\nClose the Setup Editor.\nBuild the setup project you just configured.\nOpen Windows explorer and go to that project's bin/Debug or bin/Release folder, depending on what mode you built in. There you'll see the .msi that you can copy to where you need.\n\nTo make an update, make the necessary changes and then change the version number in that project's Properties -&gt; Application -&gt; Assembly Information. Then also change it in Product.wxs &lt;Wix&gt;&lt;Product.Version&gt;. Then just build your setup project again.\n    \n\nGood tutorial here:\n\nhttp://wix.tramontana.co.hu/ \n\nhttp://www.codeproject.com/Tips/105638/A-quick-introduction-Create-an-MSI-installer-with\n\nThey should get you started. \n\nIf you learn something about the MSI log that will also help - install the MSI with a command line that includes /L*vx  \n\nAnd \"doesn't install anything\" should be easy to check - are there are any files installed, or did it create an entry in Programs&amp;Features? \n    \n\nWiX is a language (XML/XSD) that serves as a way of authoring (compiling) Windows Installer (.MSI) databases.  WiX doesn't install anything, MSI does.\nI maintain an open source project called IsWiX.  The concept is simple. IsWiX provides additional WiX project templates (scaffolding) and graphical designers to assist you in creating installer.  Then as you gain knowledge of WiX and MSI you can make additional tweaks of the WiX XML by hand and go beyond what IsWiX currently knows how to author.\nHere's a video showing how to author, build and test an MSI to deploy an IIS website in a mere 3 minutes.\nUpdate: IsWiX has tutorials now.\n    \n\nAfter a few 'false starts' trying to learn WiX from online tutorials I noticed that on http://wixtoolset.org/ there is a link to the book \"WiX 3.6: A Developer's Guide to Windows Installer XML\".  You can buy it pretty inexpensively in E-book form from Packt, or also Amazon if you like the easy interface with Kindle.\n\nI found this book to be VERY helpful with every little step regarding the .msi creation process.  The book will guide you to create your first .msi in the very 1st chapter!  Granted, you have to continue a little more to have a fully functioning .msi, but given the complexity of Wix, this book is terrific.  It may not be for the gurus among us, but for those of us who need a little more help to understand the material it's wonderful.  I've seen many posts speak to the 'steep learning curve' regarding WiX and it is a complicated process to create a valid .msi, but this book goes a long way toward making that learning curve very bearable.\n    \n\nIf you are using the build system 'cmake', then you can use cpack to generate .msi file by setting the cpack generator to wix.\n    ", "document_id": 1065}]}, {"paragraphs": [{"qas": [{"question": "How to install Xcode with Homebrew?", "id": 908, "answers": [{"answer_id": 903, "document_id": 569, "question_id": 908, "text": "Check and update macOS version.\nCheck if Xcode Command Line Tools was previously installed.\nInstall Homebrew (with an option to install Xcode Command Line Tools).\nUpdate the shell configuration (Mac M1 only).", "answer_start": 950, "answer_category": null}], "is_impossible": false}, {"question": "How to check Xcode with Homebrew?", "id": 909, "answers": [{"answer_id": 904, "document_id": 569, "question_id": 909, "text": "zsh: command not found: brew\nIf Homebrew is not installed, there should be no Homebrew files in /usr/local (for macOS Intel) or /opt/homebrew (for Apple Silicon).", "answer_start": 1524, "answer_category": null}], "is_impossible": false}, {"question": "How to verify Homebrew is installed?", "id": 910, "answers": [{"answer_id": 905, "document_id": 569, "question_id": 910, "text": "$ brew doctor\nYou should see:\n\nYour system is ready to brew.", "answer_start": 4542, "answer_category": null}], "is_impossible": false}], "context": "Install Xcode Command Line Tools with Homebrew\nXcode Command Line Tools are tools for software developers that run on the command line, in the Terminal application. See What are Xcode Command Line Tools?\n\nThe easiest way to install Xcode Command Line Tools is by installing Homebrew, the popular package manager for macOS. When you install Homebrew, you'll be offered the option of installing Xcode Command Line Tools. This is a recent addition to the Homebrew installation process, so older guides or blog posts won't mention it.\n\nWith Homebrew, you can install almost any open source developer tool. Since you'll probably need Homebrew in your local development environment, you might as well let Homebrew install Xcode Command Line Tools for you.\n\nTip: If you did not use a password to log in to your Mac (that is, if your password is blank), you cannot install Homebrew.\n\nSteps\nHere are steps to install Xcode Command Line Tools using Homebrew.\n\nCheck and update macOS version.\nCheck if Xcode Command Line Tools was previously installed.\nInstall Homebrew (with an option to install Xcode Command Line Tools).\nUpdate the shell configuration (Mac M1 only).\nThese instructions are for a terminal running Zsh, the Z shell, on a newer Mac. Older Macs may be running the bash shell and you should upgrade.\n\nCheck for Homebrew\nIf you've already checked the macOS version and checked if Xcode Command Line Tools was previously installed, then check if Homebrew is installed:\n\n$ brew\nIf Homebrew is not installed, you will see:\n\nzsh: command not found: brew\nIf Homebrew is not installed, there should be no Homebrew files in /usr/local (for macOS Intel) or /opt/homebrew (for Apple Silicon).\n\nBrew install\nHomebrew provides an installation script you can download and run with a single command (check that it hasn't changed at the Homebrew site). This is the easiest way to install Homebrew.\n\n$ /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\nThe Homebrew installation script will ask you to enter your Mac user password. This is the password you used to sign into your Mac.\n\nPassword:\nYou won't see the characters as you type. Press enter when you are done.\n\n\n\nYou'll see a list of files and folders that Homebrew will install.\n\nOption to install XCode Command Line Tools\nIf you haven't already installed XCode Command Line Tools, you'll see a message that \"The XCode Command Line Tools will be installed.\" Press return to continue when prompted by the Homebrew installation script. It only takes a minute or two to download and install the Command Line Tools on a 2021 Mac M1 Mini.\n\n\n\nYou\u2019ll see diagnostic and progress messages. Full Homebrew installation takes 2 to 15 minutes on a 2021 Mac M1 Mini, with a 100Mbps Internet connection. It's significantly slower on Mac Intel over a slow Internet connection.\n\n\n\nOn Mac Intel machines, that's all you need to do; Homebrew is ready to use. On Mac Intel, Homebrew installs itself into the /usr/local/bin directory, which is already configured for access by the shell with the macOS default $PATH environment variable (the default is set by the /usr/libexec/path_helper command).\n\nAdd Homebrew shell configuration\nOn Apple Silicon machines, there's one more step. Homebrew files are installed into the /opt/homebrew folder. But the folder is not part of the default $PATH. Follow Homebrew's advice and create a ~/.zprofile file which contains a command which sets up Homebrew. Homebrew shows instructions at the end of the installation process:\n\n- Add Homebrew to your PATH in ~/.zprofile:\necho 'eval \"$(/opt/homebrew/bin/brew shellenv)\"' >> ~/.zprofile\neval \"$(/opt/homebrew/bin/brew shellenv)\"\nThe Homebrew console output will show your user directory name (the example above contains the Unix ~ tilde shortcut instead).\n\nAlternative shell configuration\nAlternatively, you can use a favorite text editor to edit the ~/.zprofile file. Or open ~/.zprofile to use TextEdit to edit the file. You'll need to add the line eval \"$(/opt/homebrew/bin/brew shellenv)\" to the file.\n\nSome developers don't use the ~/.zprofile file, preferring to set the shell configuration in the ~/.zshrc file. That's okay, but the ~/.zshrc file is evaluated every time a shell is launched. The ~/.zprofile file is only evaluated when you login to your mac user account, so setting the shell with the ~/.zprofile file happens only once at login, saving some overhead.\n\nVerify Homebrew installation\nAfter you've installed Homebrew, check that Homebrew is installed properly.\n\n$ brew doctor\nYou should see:\n\nYour system is ready to brew.\n\n\nOn Apple Silicon, if you see zsh: command not found: brew, check that you've created a ~/.zprofile file as described above and restart your terminal application.\n\nIf Homebrew is successfully installed, there will be Homebrew files in /usr/local (for macOS Intel) or /opt/homebrew (for Apple Silicon).\n\nNow you have both Xcode Command Line Tools and Homebrew installed. If you want to learn more about adding Homebrew packages to set up your development environment, see Install a Homebrew Package.", "document_id": 569}]}, {"paragraphs": [{"qas": [{"question": "How to deploy web automatically, but not interrupting the production server", "id": 1298, "answers": [{"answer_id": 1290, "document_id": 869, "question_id": 1298, "text": "You can use a tool that almost solve your problem.\nIt's developed by PHP author Rasmus Lerdorf which named WePloy.\nBut it can deploy PHP website only.", "answer_start": 776, "answer_category": null}], "is_impossible": false}], "context": "I want to deploy a currently running website(php,python,ROR and what ever).\nAnd my code is hosting in git.\nI read about 'deploy your website changes using git\nand stackoverflow answer\nand it's useful. But it has a problem. It'll interrupt the production server.\nAlthought the time is short. But if our server is heavy load. It also interrupt the user.\nSo my question is how to deploy code to the production server without interrupting.\nAssume several situation.\nOnly one production server.\nIs it the only viable method is stop the production server and wait until the deployment is completed?\nMore than two production servers.\nI know we can stop one production server and deploy to it, then deploy to the next server until complete.\nBut is there any method to do this better?\nYou can use a tool that almost solve your problem.\nIt's developed by PHP author Rasmus Lerdorf which named WePloy.\nBut it can deploy PHP website only.\n", "document_id": 869}]}, {"paragraphs": [{"qas": [{"question": "how to fix inno setup error endupdateresource failed 110 on windows?", "id": 1366, "answers": [{"answer_id": 1355, "document_id": 934, "question_id": 1366, "text": "\"Start  -&gt; Settings  -&gt; Updates &amp; Security  -&gt; Windows Security -&gt; Virus &amp; threat protection -&gt; Virus &amp; threat protection settings -&gt; Add or remove exclusions -&gt; ", "answer_start": 936, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nWhen compiling an Inno Setup installer script I get this error\n\n\n  Line: xx Resource update error:\n  EndUpdateResource failed (110)\n\n\nline xx contains\n\n\n  SetupIconFile= pathToIconFile\n\n\nWhat causes the error and how can I fix it?\n    \n\nFound the solution after quite a google session\n\n\n  Win32 error 110 = \"The system cannot\n  open the device or file specified.\"\n  \n  It's probably having trouble writing\n  to the newly generated installer.exe.\n  Check the permissions on the output\n  directory, and disable any anti-virus\n  software.\n\n\nI disabled my anti-virus and the error disappeared.\n    \n\nI got the same problem on Windows 10 with Windows Defender active. I tried first to remove the output folder's \"Read-Only\" attribute as mentioned above but did not work. Since I did not want to disable the antivirus I had to add the Output folder of the Inno Setup Studio to the Exclusion section of Windows Defender: \n\n\"Start  -&gt; Settings  -&gt; Updates &amp; Security  -&gt; Windows Security -&gt; Virus &amp; threat protection -&gt; Virus &amp; threat protection settings -&gt; Add or remove exclusions -&gt; Add an Exclusion of type \"Folder\"\n    \n\nIn McAfee I was able to disable the real time protection and it prevented \"Win32 error 5:EndUpdateResource failed (110)\"\n    ", "document_id": 934}]}, {"paragraphs": [{"qas": [{"question": "How can I find the product GUID of an installed MSI setup?", "id": 751, "answers": [{"answer_id": 752, "document_id": 439, "question_id": 751, "text": "$filter = \"*core*sdk*\"; (Get-ChildItem HKLM:\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Uninstall).Name", "answer_start": 330, "answer_category": null}], "is_impossible": false}], "context": "I need to find the product GUID for an installed MSI file in order to perform maintenance such as patching, uninstall (how-to uninstall) and also for auditing purposes. If you have too many installers to find what you are looking for easily, here is some powershell to provide a filter and narrow it down a little by display name.$filter = \"*core*sdk*\"; (Get-ChildItem HKLM:\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Uninstall).Name", "document_id": 439}]}, {"paragraphs": [{"qas": [{"question": "Parser Error when deploy ASP.NET application", "id": 1106, "answers": [{"answer_id": 1098, "document_id": 683, "question_id": 1106, "text": "You should See Creating a Virtual Directory for Your Application for detailed instructions on creating a virtual directory for your application.", "answer_start": 344, "answer_category": null}], "is_impossible": false}], "context": "I've finished simple asp.net web application project, compiled it, and try to test on local IIS. I've create virtual directory, map it with physical directory, then put all necessary files there, including bin folder with all .dll's In the project settings, build section, output path is bin\\ So when i try to browse my app i got Parser Error.\nYou should See Creating a Virtual Directory for Your Application for detailed instructions on creating a virtual directory for your application.\n", "document_id": 683}]}, {"paragraphs": [{"qas": [{"question": "CMake & CTest : make test doesn't build tests", "id": 1823, "answers": [{"answer_id": 1809, "document_id": 1394, "question_id": 1823, "text": "You need to define the build of the test executable as one of the tests and then add dependencies between the tests. That is:\nADD_TEST(ctest_build_test_code\n         \"${CMAKE_COMMAND}\" --build ${CMAKE_BINARY_DIR} --target test_code)\nADD_TEST(ctest_run_test_code test_code)\nSET_TESTS_PROPERTIES(ctest_run_test_code\n                     PROPERTIES DEPENDS ctest_build_test_code)", "answer_start": 286, "answer_category": null}], "is_impossible": false}], "context": "I'm trying CTest in CMake in order to automatically run some of my tests using make test target. The problem is CMake does not \"understand\" that the test I'm willing to run has to be built since it is part of the project.\nSo I'm looking for a way to explicitly specify this dependency.\nYou need to define the build of the test executable as one of the tests and then add dependencies between the tests. That is:\nADD_TEST(ctest_build_test_code\n         \"${CMAKE_COMMAND}\" --build ${CMAKE_BINARY_DIR} --target test_code)\nADD_TEST(ctest_run_test_code test_code)\nSET_TESTS_PROPERTIES(ctest_run_test_code\n                     PROPERTIES DEPENDS ctest_build_test_code)\n", "document_id": 1394}]}, {"paragraphs": [{"qas": [{"question": "When and when-not to install into the GAC?", "id": 819, "answers": [{"answer_id": 814, "document_id": 501, "question_id": 819, "text": "You could dramatically improve the initial loading time and memory usage of the application on servers which have many multiple instances of the same ASP.NET applications if you put the assemblies in the GAC", "answer_start": 0, "answer_category": null}], "is_impossible": false}], "context": "You could dramatically improve the initial loading time and memory usage of the application on servers which have many multiple instances of the same ASP.NET applications if you put the assemblies in the GAC. At least I saw this on our servers with dozens of installations. It only makes sense to install into the GAC if lots and lots of web applications on the same server will be sharing exactly the same libraries. For example, on a Sharepoint server there can be hundreds of web sites that all need to share the same web part, so in this case it makes sense to deploy the compiled web part to the GAC.", "document_id": 501}]}, {"paragraphs": [{"qas": [{"question": "lxml installation error ubuntu 14.04 (internal compiler error)", "id": 855, "answers": [{"answer_id": 850, "document_id": 535, "question_id": 855, "text": "Possible solution (if you have no ability to increase memory on that machine) is to add swap file\nsudo dd if=/dev/zero of=/swapfile bs=1024 count=524288\nsudo chmod 600 /swapfile\nsudo mkswap /swapfile\nsudo swapon /swapfile", "answer_start": 7094, "answer_category": null}], "is_impossible": false}], "context": "gcc crashed when building pandas #1880\n Closed\ncodeape2 opened this issue on 10 Sep 2012 \u00b7 24 comments\n Closed\ngcc crashed when building pandas\n#1880\ncodeape2 opened this issue on 10 Sep 2012 \u00b7 24 comments\nComments\nAssignees\nNo one assigned\nLabels\nBuild\nProjects\nNone yet\nMilestone\n0.10\nLinked pull requests\nSuccessfully merging a pull request may close this issue.\n\nNone yet\nNotifications\nCustomize\nYou\u2019re not receiving notifications from this thread.\n16 participants\n@mikeatlas\n@mattlangeman\n@codeape2\n@bfroehle\n@sandeepan\n@wesm\n@fission6\n@wuan\n@changhiskhan\n@javpaw\n@lodagro\n@skhalymon\n@amills61\n@fproldan\n@datomnurdin\n@seidapps\n@codeape2\n \ncodeape2 commented on 10 Sep 2012\nI experienced a gcc crash when building the latest pandas.\n\nThis was on Ubuntu 12.04 gcc version 4.6, Cython 0.17.\n\nOn Ubuntu 11.10 gcc version 4.6.1, Cython 0.14 I did not have the issue.\n\n@wesm\n \nMember\nwesm commented on 10 Sep 2012\nWas the crash consistent? Or appear at random?\n\n@lodagro\n \nContributor\nlodagro commented on 10 Sep 2012\nfyi Ubuntu 12.04 + gcc 4.6.3 + cython 0.16 works fine for me (did not try with cython 0.17)\n\n(pandas)[lodagro@ubuntu][515][i]  lsb_release -a\nNo LSB modules are available.\nDistributor ID: Ubuntu\nDescription:    Ubuntu 12.04.1 LTS\nRelease:    12.04\nCodename:   precise\n(pandas)[lodagro@ubuntu][516][i] cython --version\nCython version 0.16\n(pandas)[lodagro@ubuntu][516][i] gcc --version\ngcc (Ubuntu/Linaro 4.6.3-1ubuntu5) 4.6.3\nCopyright (C) 2011 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n@codeape2\n \nAuthor\ncodeape2 commented on 10 Sep 2012\nThe crash was consistent.\n\n@changhiskhan\n \nContributor\nchanghiskhan commented on 11 Sep 2012\nAny specific error messages?\n\n@codeape2\n \nAuthor\ncodeape2 commented on 11 Sep 2012\nSteps to reproduce:\n\nAmazon EC2 t1.micro instance ubuntu/images/ebs/ubuntu-precise-12.04-amd64-server-20120616 (ami-ab9491df)\nFirst try:\nPython, python-dev, build-essentials, python-pip, python-virtualenv installed from Ubuntu packages\nNumpy and Cython installed using pip\npython setup.py build\n\ngives error:\n\nIn file included from pandas/src/tseries.c:260:0:\npandas/src/numpy_helper.h:47:1: warning: function declaration isn\u2019t a prototype [-Wstrict-prototypes]\npandas/src/tseries.c: In function \u2018__pyx_f_6pandas_3lib__bin_search\u2019:\npandas/src/tseries.c:120278:13: warning: \u2018__pyx_v_mid\u2019 may be used uninitialized in this function [-Wuninitialized]\ngcc: internal compiler error: Killed (program cc1)\nPlease submit a full bug report,\nwith preprocessed source if appropriate.\nSee <file:///usr/share/doc/gcc-4.6/README.Bugs> for instructions.\nerror: command 'gcc' failed with exit status 4\nFull console log: http://pastebin.com/eNjwjP8d\n\n$ gcc -dumpversion; cython -V\n4.6\nCython version 0.17\nSecond try:\nCython from Ubuntu package\nNo errors, so it seems Cython is the problem.\n\n$ gcc -dumpversion; cython -V\n4.6\nCython version 0.15.1\nSo it seems that...\nCython 0.17 has a problem that Cython 0.15.1 does not have.\n\n@wesm\n \nMember\nwesm commented on 11 Sep 2012\nBuilds cleanly for me with gcc-4.6.1 and Cython 0.17 =/\n\n@bfroehle\n \nbfroehle commented on 11 Sep 2012\nBuilds fine for me with Cython 0.17 / Ubuntu 12.04.1.\n\n$ uname -a\nLinux highorder 3.2.0-29-generic #46-Ubuntu SMP Fri Jul 27 17:03:23 UTC 2012 x86_64 x86_64 x86_64 GNU/Linux\n$ lsb_release -a\nNo LSB modules are available.\nDistributor ID: Ubuntu\nDescription:    Ubuntu 12.04.1 LTS\nRelease:    12.04\nCodename:   precise\n$ gcc -v\nUsing built-in specs.\nCOLLECT_GCC=gcc\nCOLLECT_LTO_WRAPPER=/usr/lib/gcc/x86_64-linux-gnu/4.6/lto-wrapper\nTarget: x86_64-linux-gnu\nConfigured with: ../src/configure -v --with-pkgversion='Ubuntu/Linaro 4.6.3-1ubuntu5' --with-bugurl=file:///usr/share/doc/gcc-4.6/README.Bugs --enable-languages=c,c++,fortran,objc,obj-c++ --prefix=/usr --program-suffix=-4.6 --enable-shared --enable-linker-build-id --with-system-zlib --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --with-gxx-include-dir=/usr/include/c++/4.6 --libdir=/usr/lib --enable-nls --with-sysroot=/ --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --enable-gnu-unique-object --enable-plugin --enable-objc-gc --disable-werror --with-arch-32=i686 --with-tune=generic --enable-checking=release --build=x86_64-linux-gnu --host=x86_64-linux-gnu --target=x86_64-linux-gnu\nThread model: posix\ngcc version 4.6.3 (Ubuntu/Linaro 4.6.3-1ubuntu5) \nGiven that the compiler crashed this is unfortunately an issue with gcc or your environment, not with pandas. Nonetheless it might be possible that a change here could alleviate your problem.\n\nMaybe you ran out of memory on the t1.micro instance? I don't believe they come with swap space by default. Compiling pandas/src/tseries.c used ~418MB for me.\n\n@wesm\n \nMember\nwesm commented on 11 Sep 2012\nThat might be. that extension is very memory hungry to build\n\n@codeape2\n \nAuthor\ncodeape2 commented on 11 Sep 2012\nYes, looks like memory was the problem. It works on a m1.small instance.\n\n@codeape2 codeape2 closed this on 11 Sep 2012\n@wesm\n \nMember\nwesm commented on 11 Sep 2012\nCool. I guess on t1.micro you would want to compile pandas elsewhere and download the binary; alternately if some brave soul wants to help refactor the Cython code into smaller modules to reduce the memory footprint of gcc...\n\n@amills61\n \namills61 commented on 30 Oct 2012\nI had this same problem trying to build pandas on an Amazon EC2 t1.micro instance using:\n\n$ easy_install pandas\n\n...\npandas/src/tseries.c:120278:13: warning: \u2018__pyx_v_mid\u2019 may be used uninitialized in this function [-Wuninitialized]\ngcc: internal compiler error: Killed (program cc1)\nPlease submit a full bug report,\n...\nTo work around it I followed the instructions for creating an AMI of the current t1.micro instance (following these directions: http://docs.amazonwebservices.com/AWSEC2/latest/UserGuide/Tutorial_CreateImage.html), I then created an new m1.large instance from the AMI. Once I loaded that new instance I again tried:\n\n$ easy_install pandas\nThis time I had no problems now that I had access to more memory in the large instance.\n\nAfter the installation of pandas I again created an AMI of the improved m1.large instance (with pandas now compiled) and used this improved AMI to build a NEW t1.micro instance with pandas.\n\nThe whole process took less than half an hour, but it was a pain to figure out how to deal with the gcc crash in the first place.\n\n@codeape2\n \nAuthor\ncodeape2 commented on 30 Oct 2012\nI did something similar. I installed on a normal instance then moved the site-packages/pandas directory to the micro instance.\n\n@wesm\n \nMember\nwesm commented on 31 Oct 2012\nUnfortunately compiling pandas's C extensions requires a lot of RAM (about half a gig or so). It would be nice if gcc gave a more informative error message\n\n@wuan\n \nContributor\nwuan commented on 31 Oct 2012\nSetting up a temporary swapfile before building pandas helps.\n\nI managed to compile with 512 MB swap created with the following commands:\n163\n\nPossible solution (if you have no ability to increase memory on that machine) is to add swap file\nsudo dd if=/dev/zero of=/swapfile bs=1024 count=524288\nsudo chmod 600 /swapfile\nsudo mkswap /swapfile\nsudo swapon /swapfile\n\n@mattlangeman\n \nmattlangeman commented on 30 Nov 2012\nJust confirming I had this issue and creating a temporary swapfile worked for me. Thanks wuan.\n\n@wesm\n \nMember\nwesm commented on 30 Nov 2012\nFWIW I'm planning some refactoring that should reduce the build memory footprint (hopefully)\n\n@javpaw\n \njavpaw commented on 10 Dec 2012\nThe solution from wuan worked for me too. Thank you.\n\n@kwgoodman kwgoodman mentioned this issue on 8 Jan 2013\nCannot build/install on Ubuntu 12.10 64Bit in Virtual Machine pydata/bottleneck#54\n Closed\n@seidapps\n \nseidapps commented on 12 Mar 2014\nWuan's solution worked for me too! Thank you!!\n\n@fproldan\n \nfproldan commented on 29 May 2014\nThank you Wuan!! Worked for me!\n\n@mikeatlas\n \nmikeatlas commented on 16 Jul 2014\n@wuan thanks for the tip, that helped me here as well.\n\n@fission6\n \nfission6 commented on 12 Aug 2014\nJust a finally command after you install do you pandas build and are using a temp swap as recommended by @wuan, remove the swap with sudo swapoff /swapfile\n\n@sandeepan\n \nsandeepan commented on 8 Sep 2015\nHas a similar issue. Thanks @wuan :) . Saved me :)\n\n@datomnurdin\n \ndatomnurdin commented on 15 Nov 2015\nI got this error message after using @wuan solution.\n\ndd: failed to open '/swapfile': Text file busy\nHow to fix it?\n\n@skhalymon\n \nskhalymon commented on 19 Dec 2015\n@datomnurdin it's seems you already have swapfile, you need unmont current sudo swapoff -a and create new one. Or maybe your swap is fully, you can try to restart machine and rebuild pandas again.", "document_id": 535}]}, {"paragraphs": [{"qas": [{"question": "What does '->' (arrow) mean in gradle's dependency graph?", "id": 1894, "answers": [{"answer_id": 1881, "document_id": 1465, "question_id": 1894, "text": "It means that dependency graph contains multiple dependencies with the same group and module but different versions for e.g. org.hamcrest:hamcrest-core. Gradle tries to resolve conflicted versions automatically - by default the latest version is chosen. On the left side of -> is the requested version, on the right the version that will be picked. Here similar question can be found.", "answer_start": 285, "answer_category": null}], "is_impossible": false}], "context": "I'm trying to run some Android tests, however, the compiler complains that multiple dex files exist.\nMultiple dex files define Lorg/hamcrest/MatcherAssert;\nSo I'm trying to filter duplicated dependencies: $ ./gradlew -q dependencies app:dependencies --configuration androidTestCompile\nIt means that dependency graph contains multiple dependencies with the same group and module but different versions for e.g. org.hamcrest:hamcrest-core. Gradle tries to resolve conflicted versions automatically - by default the latest version is chosen. On the left side of -> is the requested version, on the right the version that will be picked. Here similar question can be found.\n", "document_id": 1465}]}, {"paragraphs": [{"qas": [{"question": "What If Docker Desktop fails to install or start properly on Mac?", "id": 88, "answers": [{"answer_id": 94, "document_id": 73, "question_id": 88, "text": "u quit Docker Desktop before installing a new version of the\napplication ( > Quit Docker Desktop). Otherwise, you get an \u201capplication in use\u201d error when you try to\ncopy the new app from the .dmg to /Applications.\n\n\nRestart your Mac to stop / discard any vestige of the daemon running from\nthe previously installed version.\n\n\nRun the uninstall commands from the menu.\n\n\n\n\nIf dock", "answer_start": 9353, "answer_category": null}], "is_impossible": false}, {"question": "What if docker commands aren\u2019t working properly or as expected?", "id": 89, "answers": [{"answer_id": 95, "document_id": 73, "question_id": 89, "text": "d to\nunset some environment variables, to make sure you are not using the legacy\nDocker Machine environment in your shell or command window. Unset the\nDOCKER_HOST environment variable and related variables. If you use bash, use the following command: unset ${!DOCKER_*}\n\n\nFor the h", "answer_start": 9794, "answer_category": null}], "is_impossible": false}, {"question": "Does Docker Destop support IP v6?", "id": 90, "answers": [{"answer_id": 96, "document_id": 73, "question_id": 90, "text": "yet) supported on Docker Desktop.\n\n\nYou might ", "answer_start": 10885, "answer_category": null}], "is_impossible": false}, {"question": " Docker Desktop ValueError: Extra Data.", "id": 92, "answers": [{"answer_id": 98, "document_id": 73, "question_id": 92, "text": "fied this is likely related to data\nand/or events being passed all at once rather than one by one, so sometimes\nthe data comes back as 2+ objects concatenated and causes an error.\n\n\nForce-ejec", "answer_start": 11035, "answer_category": null}], "is_impossible": false}], "context": "Logs and troubleshooting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSearch\n\n\n\n\n\n\n\nToggle navigation\n\n\n\n\n\n\n\nHomeGuidesProduct manualsReferenceSamples\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLogs and troubleshootingEstimated reading time: 15 minutes\nDid you know that Docker Desktop now offers support for developers subscribed to a Pro or a Team plan? Click here to learn more.\n\nThis page contains information on how to diagnose and troubleshoot Docker Desktop issues, request Docker Desktop support (Pro and Team plan users only), send logs and communicate with the Docker Desktop team, use our forums and Success Center, browse and log issues on GitHub, and find workarounds for known problems.\nTroubleshoot\ud83d\udd17\nChoose  > Troubleshoot\nfrom the menu bar to see the troubleshoot options.\n\nThe Troubleshoot page contains the following options:\n\n\nRestart Docker Desktop: Select to restart Docker Desktop.\n\n\nSupport:  Developers on Pro and Team plans can use this option to send a support request. Other users can use this option to diagnose any issues in Docker Desktop. For more information, see Diagnose and feedback and Support.\n\n\nReset Kubernetes cluster: Select this option to delete all stacks and Kubernetes resources. For more information, see Kubernetes.\n\n\nClean / Purge data: This option resets all Docker data without a\nreset to factory defaults. Selecting this option results in the loss of existing settings.\n\n\nReset to factory defaults: Choose this option to reset all options on\nDocker Desktop to their initial state, the same as when Docker Desktop was first installed.\n\n\nUninstall: Choose this option to remove Docker Desktop from your\nsystem.\n\n\n\nUninstall Docker Desktop from the command line\nTo uninstall Docker Desktop from a terminal, run: <DockerforMacPath>\n--uninstall. If your instance is installed in the default location, this\ncommand provides a clean uninstall:\n$ /Applications/Docker.app/Contents/MacOS/Docker --uninstall\nDocker is running, exiting...\nDocker uninstalled successfully. You can move the Docker application to the trash.\n\nYou might want to use the command-line uninstall if, for example, you find that\nthe app is non-functional, and you cannot uninstall it from the menu.\n\nDiagnose and feedback\ud83d\udd17\nIn-app diagnostics\ud83d\udd17\nIf you encounter problems for which you do not find solutions in this\ndocumentation, on Docker Desktop issues on\nGitHub, or the Docker Desktop forum, we can help you troubleshoot\nthe log data. Before reporting an issue, we recommend that you read the information provided on this page to fix some common known issues.\n\nNote\nDocker Desktop offers support for users subscribed to a Pro or a Team plan. If you are experiencing any issues with Docker Desktop, follow the instructions in this section to send a support request to Docker Support.\n\nBefore you get started, we recommend that you sign into your Docker Desktop application and your Docker Hub account.\n\nChoose  > Troubleshoot.\nSign into Docker Desktop. In addition, ensure you are signed into your Docker account.\n\nClick Get Support. This opens the in-app Diagnose & Support (Diagnose & Feedback for free users) page and starts collecting the diagnostics.\n\n\nWhen the diagnostics collection process is complete, click Upload to upload your diagnostics to Docker Desktop.\nWhen the diagnostics have been uploaded, Docker Desktop prints a Diagnostic ID. Copy this ID.\n\nIf you have subscribed to a Pro or a Team plan, click Get support. This opens the Docker Desktop support form. Fill in the information required and add the ID you copied earlier to the Diagnostics ID field. Click Submit to request Docker Desktop support.\n\nNote\nYou must be signed in to Docker Desktop using your Pro or Team plan credentials to access the support form. For information on what\u2019s covered as part of Docker Desktop support, see Support.\n\n\n\nIf you are not subscribed to a Pro or a team plan, you can click Upgrade your account to upgrade your existing account.\nAlternatively, click Report an issue to open a new Docker Desktop issue on GitHub. This opens Docker Desktop for Mac on GitHub in your web browser in a \u2018New issue\u2019 template. Complete the information required and ensure you add the diagnostic ID you copied earlier. Click submit new issue to create a new issue.\n\n\nDiagnosing from the terminal\ud83d\udd17\nIn some cases, it is useful to run the diagnostics yourself, for instance, if\nDocker Desktop cannot start.\nFirst, locate the com.docker.diagnose tool.  If you have installed Docker Desktop in the Applications directory, then it is located at\n/Applications/Docker.app/Contents/MacOS/com.docker.diagnose.\nTo create and upload diagnostics, run:\n$ /Applications/Docker.app/Contents/MacOS/com.docker.diagnose gather -upload\n\nAfter the diagnostics have finished, you should have the following output,\ncontaining your diagnostics ID:\nDiagnostics Bundle: /tmp/B8CF8400-47B3-4068-ADA4-3BBDCE3985D9/20190726143610.zip\nDiagnostics ID:     B8CF8400-47B3-4068-ADA4-3BBDCE3985D9/20190726143610 (uploaded)\nDiagnostics Bundle: /tmp/BE9AFAAF-F68B-41D0-9D12-84760E6B8740/20190905152051.zip\nDiagnostics ID:     BE9AFAAF-F68B-41D0-9D12-84760E6B8740/20190905152051 (uploaded)\n\nThe diagnostics ID (here BE9AFAAF-F68B-41D0-9D12-84760E6B8740/20190905152051) is\ncomposed of your user ID (BE9AFAAF-F68B-41D0-9D12-84760E6B8740) and a timestamp\n(20190905152051). Ensure you provide the full diagnostics ID, and not just the user ID.\nTo view the contents of the diagnostic file, run:\n$ open /tmp/BE9AFAAF-F68B-41D0-9D12-84760E6B8740/20190905152051.zip\n\n\nCheck the logs\ud83d\udd17\nIn addition to using the diagnose and feedback option to submit logs, you can\nbrowse the logs yourself.\nIn a terminal\nTo watch the live flow of Docker Desktop logs in the command line, run the following script from your favorite shell.\n$ pred='process matches \".*(ocker|vpnkit).*\"\n|| (process in {\"taskgated-helper\", \"launchservicesd\", \"kernel\"} && eventMessage contains[c] \"docker\")'\n$ /usr/bin/log stream --style syslog --level=debug --color=always --predicate \"$pred\"\n\nAlternatively, to collect the last day of logs (1d) in a file, run:\n$ /usr/bin/log show --debug --info --style syslog --last 1d --predicate \"$pred\" >/tmp/logs.txt\n\nIn the Console app\nMacs provide a built-in log viewer, named \u201cConsole\u201d, which you can use to check\nDocker logs.\nThe Console lives in /Applications/Utilities; you can search for it with\nSpotlight Search.\nTo read the Docker app log messages, type docker in the Console window search bar and press Enter. Then select ANY to expand the drop-down list next to your docker search entry, and select Process.\n\nYou can use the Console Log Query to search logs, filter the results in various\nways, and create reports.\n\nTroubleshooting\ud83d\udd17\nSupport for Apple silicon processors\ud83d\udd17\nAt the moment, Docker Desktop is compatible with Intel processors only. You can follow the status of Apple Silicon support in our roadmap.\nMake sure certificates are set up correctly\ud83d\udd17\nDocker Desktop ignores certificates listed under insecure registries, and does\nnot send client certificates to them. Commands like docker run that attempt to\npull from the registry produces error messages on the command line, for example:\nError response from daemon: Get http://192.168.203.139:5858/v2/: malformed HTTP response \"\\x15\\x03\\x01\\x00\\x02\\x02\"\n\nAs well as on the registry. For example:\n2019/06/20 18:15:30 http: TLS handshake error from 192.168.203.139:52882: tls: client didn't provide a certificate\n2019/06/20 18:15:30 http: TLS handshake error from 192.168.203.139:52883: tls: first record does not look like a TLS handshake\n\nFor more about using client and server side certificates, see\nAdding TLS certificates in the Getting Started topic.\nDocker Desktop does not start if Mac user account and home folder are renamed after installing the app\ud83d\udd17\nSee\nDo I need to reinstall Docker Desktop if I change the name of my macOS account?\nin the FAQs.\nVolume mounting requires file sharing for any project directories outside of /Users\ud83d\udd17\nIf you are using mounted volumes and get runtime errors indicating an\napplication file is not found, access to a volume mount is denied, or a service\ncannot start, such as when using Docker Compose,\nyou might need to enable file sharing.\nVolume mounting requires shared drives for projects that live outside of the\n/Users directory. Go to  >\nPreferences > Resources > File sharing and share the drive that contains the Dockerfile and volume.\nIncompatible CPU detected\ud83d\udd17\nDocker Desktop requires a processor (CPU) that supports virtualization and, more\nspecifically, the Apple Hypervisor\nframework.\nDocker Desktop is only compatible with Mac systems that have a CPU that supports the Hypervisor framework. Most Macs built in 2010 and later support it,as described in the Apple Hypervisor Framework documentation about supported hardware:\nGenerally, machines with an Intel VT-x feature set that includes Extended Page\nTables (EPT) and Unrestricted Mode are supported.\nTo check if your Mac supports the Hypervisor framework, run the following command in a terminal window.\nsysctl kern.hv_support\n\nIf your Mac supports the Hypervisor Framework, the command prints\nkern.hv_support: 1.\nIf not, the command prints kern.hv_support: 0.\nSee also, Hypervisor Framework\nReference\nin the Apple documentation, and Docker Desktop Mac system requirements.\nWorkarounds for common problems\ud83d\udd17\n\n\nIf Docker Desktop fails to install or start properly on Mac:\n\n\nMake sure you quit Docker Desktop before installing a new version of the\napplication ( > Quit Docker Desktop). Otherwise, you get an \u201capplication in use\u201d error when you try to\ncopy the new app from the .dmg to /Applications.\n\n\nRestart your Mac to stop / discard any vestige of the daemon running from\nthe previously installed version.\n\n\nRun the uninstall commands from the menu.\n\n\n\n\nIf docker commands aren\u2019t working properly or as expected, you may need to\nunset some environment variables, to make sure you are not using the legacy\nDocker Machine environment in your shell or command window. Unset the\nDOCKER_HOST environment variable and related variables. If you use bash, use the following command: unset ${!DOCKER_*}\n\n\nFor the hello-world-nginx example, Docker Desktop must be running to get to\nthe web server on http://localhost/. Make sure that the Docker icon is\ndisplayed on the menu bar, and that you run the Docker commands in a shell that is connected to the Docker Desktop Engine.\nOtherwise, you might start the webserver container but get a \u201cweb page not\navailable\u201d error when you go to localhost.\n\n\nIf you see errors like Bind for 0.0.0.0:8080 failed: port is already\nallocated or listen tcp:0.0.0.0:8080: bind: address is already in use:\n\n\nThese errors are often caused by some other software on the Mac using those\nports.\n\n\nRun lsof -i tcp:8080 to discover the name and pid of the other process and\ndecide whether to shut the other process down, or to use a different port in\nyour docker app.\n\n\n\n\nKnown issues\ud83d\udd17\n\n\nIPv6 is not (yet) supported on Docker Desktop.\n\n\nYou might encounter errors when using docker-compose up with Docker Desktop\n(ValueError: Extra Data). We\u2019ve identified this is likely related to data\nand/or events being passed all at once rather than one by one, so sometimes\nthe data comes back as 2+ objects concatenated and causes an error.\n\n\nForce-ejecting the .dmg after running Docker.app from it can cause the\nwhale icon to become unresponsive, Docker tasks to show as not responding in\nthe Activity Monitor, and for some processes to consume a large amount of CPU\nresources. Reboot and restart Docker to resolve these issues.\n\n\nDocker does not auto-start on login even when it is enabled in\n> Preferences. This is related to a\nset of issues with Docker helper, registration, and versioning.\n\n\nDocker Desktop uses the HyperKit hypervisor\n(https://github.com/docker/hyperkit) in macOS 10.10 Yosemite and higher. If\nyou are developing with tools that have conflicts with HyperKit, such as\nIntel Hardware Accelerated Execution Manager\n(HAXM),\nthe current workaround is not to run them at the same time. You can pause\nHyperKit by quitting Docker Desktop temporarily while you work with HAXM.\nThis allows you to continue work with the other tools and prevent HyperKit\nfrom interfering.\n\n\nIf you are working with applications like Apache\nMaven that expect settings for DOCKER_HOST and\nDOCKER_CERT_PATH environment variables, specify these to connect to Docker\ninstances through Unix sockets. For example:\nexport DOCKER_HOST=unix:///var/run/docker.sock\n\n\n\nThere are a number of issues with the performance of directories bind-mounted\ninto containers. In particular, writes of small blocks, and traversals of large\ndirectories are currently slow. Additionally, containers that perform large\nnumbers of directory operations, such as repeated scans of large directory\ntrees, may suffer from poor performance. Applications that behave in this way\ninclude:\n\nrake\nember build\nSymfony\nMagento\nZend Framework\nPHP applications that use Composer to install\ndependencies in a vendor folder\n\nAs a workaround for this behavior, you can put vendor or third-party library\ndirectories in Docker volumes, perform temporary file system operations\noutside of bind mounts, and use third-party tools like Unison or rsync to\nsynchronize between container directories and bind-mounted directories. We are\nactively working on performance improvements using a number of different\ntechniques.  To learn more, see the topic on our roadmap.\n\n\nSupport\ud83d\udd17\nDocker Desktop offers support for developers subscribed to a Pro or a Team plan. Click here to upgrade your existing account.\nThis section contains instructions on how to get support, and covers the scope of Docker Desktop support.\nHow do I get Docker Desktop support?\ud83d\udd17\nIf you have subscribed to a Pro and Team account, please raise a ticket through Docker Desktop support.\nDocker Community users can get support through our Github repos for-win and for-mac, where we respond on a best-effort basis.\nWhat support can I get?\ud83d\udd17\nIf you are a Pro or a Team user, you can request for support on the following types of issues:\n\nDesktop upgrade issues\nDesktop installation issues\n\nInstallation crashes\nFailure to launch Docker Desktop on first run\n\n\nUsage issues\n\nCrash closing software\nDocker Desktop not behaving as expected\n\n\nConfiguration issues\nBasic product \u2018how to\u2019 questions\n\nWhat is not supported?\ud83d\udd17\nDocker Desktop excludes support for the following types of issues:\n\nUse on or in conjunction with hardware or software other than that specified in the applicable documentation\nRunning on unsupported operating systems, including beta/preview versions of operating systems\nSupport for the Docker engine, Docker CLI, or other bundled Linux components\nSupport for Kubernetes\nFeatures labeled as experimental\nSystem/Server administration activities\nSupporting Desktop as a production runtime\nScale deployment/multi-machine installation of Desktop\nRoutine product maintenance (data backup, cleaning disk space and configuring log rotation)\nThird-party applications not provided by Docker\nAltered or modified Docker software\nDefects in the Docker software due to hardware malfunction, abuse, or improper use\nAny version of the Docker software other than the latest version\nReimbursing and expenses spent for third-party services not provided by Docker\nDocker Support excludes training, customization, and integration\n\nWhat versions are supported?\ud83d\udd17\nWe currently only offer support for the latest version of Docker Desktop. If you are running an older version, you may be asked to upgrade before we investigate your support request.\nHow many machines can I get support for Docker Desktop on?\ud83d\udd17\nAs a Pro user you can get support for Docker Desktop on a single machine.\nAs a Team, you can get support for Docker Desktop for the number of machines equal to the number of seats as part of your plan.\nWhat OS\u2019s are supported?\ud83d\udd17\nDocker Desktop is available for Mac and Windows. The supported version information can be found on the following pages:\n\nMac system requirements\nWindows system requirements\n\nCan I run Docker Desktop on Virtualized hardware?\ud83d\udd17\nNo, currently this is unsupported and against the terms of use.\nmac, troubleshooting, logs, issuesRate this page:\u00a026\u00a030\u00a0i\n\n\n\n\n\nDocker EngineOverviewInstallInstallation per distroInstall on CentOSInstall on DebianInstall on FedoraInstall on UbuntuInstall binariesOptional post-installation stepsRelease notesPrevious versionsEngine 18.09 release notesEngine 18.06 release notesEngine 18.05 release notesEngine 18.04 release notesEngine 18.03 release notesEngine 18.02 release notesEngine 18.01 release notesEngine 17.12 release notesEngine 17.11 release notesEngine 17.10 release notesEngine 17.09 release notesEngine 17.07 release notesEngine 17.06 release notesEngine 17.05 release notesEngine 17.04 release notesEngine 17.03 release notesEngine 1.13 and earlierDeprecated featuresDocker AppDocker BuildxDocker ContextDocker ScanDocker ComposeOverview of Docker ComposeInstall ComposeRelease notesGetting startedEnvironment variables in ComposeEnvironment fileExtend services in ComposeNetworking in ComposeUsing Compose in productionControl startup orderSample apps with ComposeDocker Compose release notesDocker DesktopOverviewMacInstall Docker Desktop for MacUser manualDeploy on KubernetesLeveraging Multi-CPU architecture supportNetworkingDisk utilizationLogs and troubleshootingFAQsStable release notesEdge release notesWindowsInstall Docker Desktop for WindowsInstall Docker Desktop on Windows HomeUser manualDeploy on KubernetesNetworkingLogs and troubleshootingFAQsStable release notesEdge release notesDocker Desktop WSL 2 backendDashboardOpen source licensingDocker HubQuickstartDocker ID accountsSecurity and authenticationManage access tokensTwo-factor authenticationEnable two-factor authenticationDisable two-factor authenticationRecover your Docker Hub accountGenerate a new recovery codeTeams and organizationsRepositoriesVulnerability scanningOfficial imagesAutomated buildsSet up automated buildsTesting in automated buildsAdvanced automated buildsLink to GitHub and BitBucketWebhooksSlack integrationDownload rate limitAdministrationConvert an account into an organizationDeactivate an account or an organizationBillingOverviewUpgrade your planDowngrade your planAdd seatsRemove seatsPublisher & certified contentOverviewSubmit a product for Docker HubUser FAQsPublisher FAQsCertify images & pluginsCertify logging pluginsTrust chainBring Your Own License (BYOL)Release notesOpen-source projectsNotaryGet started with NotaryUse the Notary clientUnderstand the service architectureRun a Notary serviceNotary changelogConfiguration filesServer configurationSigner configurationClient configurationCommon Server and signer configurationsDocker RegistryRegistry overviewUnderstand the RegistryDeploy a registry serverConfigure a registryWork with notificationsRecipesRecipes overviewAuthenticating proxy with apacheAuthenticating proxy with nginxMirroring Docker HubRunning on macOSGarbage collectionTesting an insecure registryDeprecated featuresCompatibilityGetting helpRelease notesSuperseded products and toolsDocker MachineMachine overviewInstall MachineRelease notesGet started with a local VMProvision hosts in the cloudLearn by exampleProvision Digital Ocean DropletsProvision AWS EC2 instancesMachine concepts and helpMachine (docker-machine) CLIMachine CLI overviewMachine command-line completionactiveconfigcreateenvhelpinspectipkilllsmountprovisionregenerate-certsrestartrmscpsshstartstatusstopupgradeurlMachine driversDrivers overviewDriver options and operating system defaultsAmazon Web ServicesDigital OceanExoscaleGenericGoogle Compute EngineIBM SoftlayerMicrosoft AzureMicrosoft Hyper-VOpenStackOracle VirtualBoxRackspaceVMware FusionVMware vCloud AirVMware vSphereMigrate from Boot2Docker to MachineDocker Toolbox (deprecated)KitematicKitematic user guide: intro & overviewSet up an Nginx web serverSet up a Minecraft ServerCreating a local RethinkDB database for developmentFrequently asked questionsKnown issuesTroubleshooting\n\n\n\n\n\n\n\nEdit this page Request docs changes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn this page:\n\nTroubleshoot\nDiagnose and feedback\nCheck the logs\nTroubleshooting\nKnown issues\nSupport\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhy Docker?\nWhat is a Container?\n\n\n\n\nProducts\nDocker Desktop\nDocker Hub\nFeatures\nContainer Runtime\nDeveloper Tools\nKubernetes\n\n\n\n\nDevelopers\nUse Cases\nPlay with Docker\nCommunity\nOpen Source\nDocker Captains\n\n\n\n\nCompany\nAbout Us\nBlog\nCustomers\nPartners\nNewsroom\nCareers\nContact Us\n\n\n\n\n\n\nStatus\nSecurity\nLegal\nContact\n\n\n\n\n\n\n\nCopyright \u00a9 2013-2020 Docker Inc. All rights reserved.\n\n\n\nTwitter\nYoutube\nGitHub\nLinkedin\nFacebook\nSlideshare\nReddit\n\n\n\n\n\n\n\n", "document_id": 73}]}, {"paragraphs": [{"qas": [{"question": "major.minor.build.revision versioning style vs year.month.day.whatever versioning style", "id": 1311, "answers": [{"answer_id": 1301, "document_id": 880, "question_id": 1311, "text": "You should use time because the added information is useful. The advantage of the other numbers is for marketing only, and for that you can do it anyway.\nFor example, why not have both, a la \"v2.1.20090214.\" Now you have marketing in the major.minor section and utility in the \"build\" section.", "answer_start": 183, "answer_category": null}], "is_impossible": false}], "context": "Is there any reason to use one versioning style over the other for .NET assemblies????\nI'd like to know if there are any advantages/disadvantages in using either style besides taste.\nYou should use time because the added information is useful. The advantage of the other numbers is for marketing only, and for that you can do it anyway.\nFor example, why not have both, a la \"v2.1.20090214.\" Now you have marketing in the major.minor section and utility in the \"build\" section.\n", "document_id": 880}]}, {"paragraphs": [{"qas": [{"question": "How Do I Package and Deploy an Eclipse Java Application?", "id": 1324, "answers": [{"answer_id": 1314, "document_id": 893, "question_id": 1324, "text": "You should have a look at the Fat Jar Plug-In: http://fjep.sourceforge.net/", "answer_start": 447, "answer_category": null}], "is_impossible": false}], "context": "I have built a Java application that has some dependencies (~10). I would like to easily package this application up and deploy it as a single file to a CD or USB drive.\nThere doesn't seem to be any \"nice\" wizard to search the project, grab the dependencies and setup the classpath on the target computer. I have to do this manually.\nIs there a better way? Something simple, easy and straight-forward. A link to a tutorial on this would be great.\nYou should have a look at the Fat Jar Plug-In: http://fjep.sourceforge.net/\n", "document_id": 893}]}, {"paragraphs": [{"qas": [{"question": "Safe way to uninstall old version of python", "id": 1792, "answers": [{"answer_id": 1778, "document_id": 1364, "question_id": 1792, "text": "Yes, your view is right, it's safe.\nThe Mac's system python's are in /System/Library/....\n.dmg's downloaded and installed from python.org are placed in /Library/....", "answer_start": 199, "answer_category": null}], "is_impossible": false}], "context": "I want to update my Python framework on Mac and delete the old versions but I am not sure if is safe to\nrm -fr /Library/Frameworks/Python.framework/Versions/2.4 - 2.5 - 2.6 -3.0 etc.\nAny suggestion?\nYes, your view is right, it's safe.\nThe Mac's system python's are in /System/Library/....\n.dmg's downloaded and installed from python.org are placed in /Library/....\n", "document_id": 1364}]}, {"paragraphs": [{"qas": [{"question": "How to detect whether I need to install VCRedist?", "id": 1755, "answers": [{"answer_id": 1742, "document_id": 1327, "question_id": 1755, "text": "You could take the recommended approach for installing directx: always run the redistributable. Since it's required and you're already shipping it there's no harm in running it even if it's already installed.", "answer_start": 827, "answer_category": null}], "is_impossible": false}], "context": "I have a question very similar to this one but the answer does not work for me.\nSoftware I am maintaining the setup for depends on VC++ 2008 (SP1, precisely), thus I need to find a solution to install VCRedist if not yet installed. I understand the correct way would be to build msi with merge modules, but it's not on my hands.\nThe answer of the duplicate question I am referring to (the accepted one) does not work for me because every tiny release (e.g. 9.0.30729.01 vs 9.0.30729.17) has proper GUIDs, which I am not able to guess or predict for future versions. Furthermore, I reckon that this would not detect Visual Studios and thus unnecessarily install the VCRedist Package when it's already on a developers machine. I do not want to bug anybody with this, certainly not somebody who has already a DevStudio installed.\nYou could take the recommended approach for installing directx: always run the redistributable. Since it's required and you're already shipping it there's no harm in running it even if it's already installed.\n", "document_id": 1327}]}, {"paragraphs": [{"qas": [{"question": "Uninstall Chocolatey package", "id": 834, "answers": [{"answer_id": 829, "document_id": 516, "question_id": 834, "text": " you can use\ncuninst packagename\nto uninstall a package.", "answer_start": 149, "answer_category": null}], "is_impossible": false}], "context": "The Chocolatey package manager supports silent installations via the command below. It doesn't seem to have an uninstall command. As I answered here, you can use\ncuninst packagename\nto uninstall a package.\nDetails can be found at Uninstall Command.\nEdit: For new versions, should use choco uninstall packagename.\n\n", "document_id": 516}]}, {"paragraphs": [{"qas": [{"question": "How should I manage deployments with kubernetes?", "id": 48, "answers": [{"answer_id": 51, "document_id": 59, "question_id": 48, "text": "dition to deploying to Kubernetes, we have also described our application as a Kubernetes YAML file. This simple text file contains everything we need to create our application in a running state. We can check it into version control and share it with our colleagues, allowing us to distribute our applications to other clusters (like the testing and production clusters that probably come after our development environments) easily.\nKube", "answer_start": 5609, "answer_category": null}], "is_impossible": false}], "context": "Deploy to Kubernetes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSearch\n\n\n\n\n\n\n\nToggle navigation\n\n\n\n\n\n\n\nHomeGuidesProduct manualsReferenceSamples\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDeploy to KubernetesEstimated reading time: 5 minutesPrerequisites\ud83d\udd17\n\nDownload and install Docker Desktop as described in Orientation and setup.\nWork through containerizing an application in Part 2.\nMake sure that Kubernetes is enabled on your Docker Desktop:\n\nMac: Click the Docker icon in your menu bar, navigate to Preferences and make sure there\u2019s a green light beside \u2018Kubernetes\u2019.\nWindows: Click the Docker icon in the system tray and navigate to Settings and make sure there\u2019s a green light beside \u2018Kubernetes\u2019.\n\nIf Kubernetes isn\u2019t running, follow the instructions in Orchestration of this tutorial to finish setting it up.\n\n\nIntroduction\ud83d\udd17\nNow that we\u2019ve demonstrated that the individual components of our application run as stand-alone containers, it\u2019s time to arrange for them to be managed by an orchestrator like Kubernetes. Kubernetes provides many tools for scaling, networking, securing and maintaining your containerized applications, above and beyond the abilities of containers themselves.\nIn order to validate that our containerized application works well on Kubernetes, we\u2019ll use Docker Desktop\u2019s built in Kubernetes environment right on our development machine to deploy our application, before handing it off to run on a full Kubernetes cluster in production. The Kubernetes environment created by Docker Desktop is fully featured, meaning it has all the Kubernetes features your app will enjoy on a real cluster, accessible from the convenience of your development machine.\nDescribing apps using Kubernetes YAML\ud83d\udd17\nAll containers in Kubernetes are scheduled as pods, which are groups of co-located containers that share some resources. Furthermore, in a realistic application we almost never create individual pods; instead, most of our workloads are scheduled as deployments, which are scalable groups of pods maintained automatically by Kubernetes. Lastly, all Kubernetes objects can and should be described in manifests called Kubernetes YAML files. These YAML files describe all the components and configurations of your Kubernetes app, and can be used to easily create and destroy your app in any Kubernetes environment.\n\n\nYou already wrote a very basic Kubernetes YAML file in the Orchestration overview part of this tutorial. Now, let\u2019s write a slightly more sophisticated YAML file to run and manage our bulletin board. Place the following in a file called bb.yaml:\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: bb-demo\nnamespace: default\nspec:\nreplicas: 1\nselector:\nmatchLabels:\nbb: web\ntemplate:\nmetadata:\nlabels:\nbb: web\nspec:\ncontainers:\n- name: bb-site\nimage: bulletinboard:1.0\n---\napiVersion: v1\nkind: Service\nmetadata:\nname: bb-entrypoint\nnamespace: default\nspec:\ntype: NodePort\nselector:\nbb: web\nports:\n- port: 8080\ntargetPort: 8080\nnodePort: 30001\n\nIn this Kubernetes YAML file, we have two objects, separated by the ---:\n\nA Deployment, describing a scalable group of identical pods. In this case, you\u2019ll get just one replica, or copy of your pod, and that pod (which is described under the template: key) has just one container in it, based off of your bulletinboard:1.0 image from the previous step in this tutorial.\nA NodePort service, which will route traffic from port 30001 on your host to port 8080 inside the pods it routes to, allowing you to reach your bulletin board from the network.\n\nAlso, notice that while Kubernetes YAML can appear long and complicated at first, it almost always follows the same pattern:\n\nThe apiVersion, which indicates the Kubernetes API that parses this object\nThe kind indicating what sort of object this is\nSome metadata applying things like names to your objects\nThe spec specifying all the parameters and configurations of your object.\n\n\n\nDeploy and check your application\ud83d\udd17\n\n\nIn a terminal, navigate to where you created bb.yaml and deploy your application to Kubernetes:\nkubectl apply -f bb.yaml\n\nyou should see output that looks like the following, indicating your Kubernetes objects were created successfully:\ndeployment.apps/bb-demo created\nservice/bb-entrypoint created\n\n\n\nMake sure everything worked by listing your deployments:\nkubectl get deployments\n\nif all is well, your deployment should be listed as follows:\nNAME      DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\nbb-demo   1         1         1            1           48s\n\nThis indicates all one of the pods you asked for in your YAML are up and running. Do the same check for your services:\nkubectl get services\n\nNAME            TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE\nbb-entrypoint   NodePort    10.106.145.116   <none>        8080:30001/TCP   53s\nkubernetes      ClusterIP   10.96.0.1        <none>        443/TCP          138d\n\nIn addition to the default kubernetes service, we see our bb-entrypoint service, accepting traffic on port 30001/TCP.\n\n\nOpen a browser and visit your bulletin board at localhost:30001; you should see your bulletin board, the same as when we ran it as a stand-alone container in Part 2 of the Quickstart tutorial.\n\n\nOnce satisfied, tear down your application:\nkubectl delete -f bb.yaml\n\n\n\nConclusion\ud83d\udd17\nAt this point, we have successfully used Docker Desktop to deploy our application to a fully-featured Kubernetes environment on our development machine. We haven\u2019t done much with Kubernetes yet, but the door is now open; you can begin adding other components to your app and taking advantage of all the features and power of Kubernetes, right on your own machine.\nIn addition to deploying to Kubernetes, we have also described our application as a Kubernetes YAML file. This simple text file contains everything we need to create our application in a running state. We can check it into version control and share it with our colleagues, allowing us to distribute our applications to other clusters (like the testing and production clusters that probably come after our development environments) easily.\nKubernetes references\ud83d\udd17\nFurther documentation for all new Kubernetes objects used in this article are available here:\n\nKubernetes Pods\nKubernetes Deployments\nKubernetes Services\n\nkubernetes, pods, deployments, kubernetes servicesRate this page:\u00a044\u00a011\u00a0i\n\n\n\n\n\nDocker overviewGet DockerGet startedPart 1: Orientation and setupPart 2: Build and run your imagePart 3: Share images on Docker HubDevelop with DockerOverviewNode.jsBuild imagesRun containersDevelopBest practicesBuild imagesDockerfile best practicesBuild images with BuildKitUse multi-stage buildsManage imagesCreate your own base image (advanced)Set up CI/CDCI/CD Best practicesConfigure GitHub ActionsDeploy your app to the cloudDocker and ACIDocker and ECSRun your app in productionOrchestrationOverviewDeploy to KubernetesDeploy to SwarmConfigure all objectsApply custom metadata to objectsPrune unused objectsFormat command and log outputConfigure the daemonConfigure and run DockerControl Docker with systemdCollect metrics with PrometheusConfigure containersStart containers automaticallyKeep containers alive during daemon downtimeRun multiple services in a containerContainer runtime metricsRuntime options with Memory, CPUs, and GPUsLoggingView a container's logsConfigure logging driversUse docker logs with a logging driverUse a logging driver pluginCustomize log driver outputLogging driver detailsLocal file logging driverLogentries logging driverJSON File logging driverGraylog Extended Format (GELF) logging driverSyslog logging driverAmazon CloudWatch logs logging driverETW logging driverFluentd logging driverGoogle Cloud logging driverJournald logging driverSplunk logging driverSecurityDocker securityDocker security non-eventsProtect the Docker daemon socketUsing certificates for repository client verificationUse trusted imagesOverviewAutomationDelegationsDeploy NotaryManage content trust keysPlay in a content trust sandboxAntivirus softwareAppArmor security profilesSeccomp security profilesIsolate containers with a user namespaceRootless modeScale your appSwarm mode overviewSwarm mode key conceptsGet started with swarm modeSet up for the tutorialCreate a swarmAdd nodes to the swarmDeploy a serviceInspect the serviceScale the serviceDelete the serviceApply rolling updatesDrain a nodeUse swarm mode routing meshHow swarm mode worksHow nodes workHow services workManage swarm security with PKISwarm task statesRun Docker in swarm modeJoin nodes to a swarmManage nodes in a swarmDeploy services to a swarmStore service configuration dataManage sensitive data with Docker secretsLock your swarmSwarm administration guideRaft consensus in swarm modeExtend DockerManaged plugin systemAccess authorization pluginExtending Docker with pluginsDocker network driver pluginsVolume pluginsPlugin configurationPluginsConfigure networkingNetworking overviewUse bridge networksUse overlay networksUse host networkingUse Macvlan networksDisable networking for a containerNetworking tutorialsBridge network tutorialHost networking tutorialOverlay networking tutorialMacvlan network tutorialConfigure the daemon and containersConfigure the daemon for IPv6Docker and iptablesContainer networkingConfigure Docker to use a proxy serverLegacy networking content(Legacy) Container linksManage application dataStorage overviewVolumesBind mountstmpfs mountsTroubleshoot volume problemsStore data within containersAbout storage driversSelect a storage driverUse the AUFS storage driverUse the Btrfs storage driverUse the Device mapper storage driverUse the OverlayFS storage driverUse the ZFS storage driverUse the VFS storage driverEducational resourcesOpen source at DockerContribute to documentationOther ways to contributeDocumentation archive\n\n\n\n\n\n\n\nEdit this page Request docs changes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn this page:\n\nPrerequisites\nIntroduction\nDescribing apps using Kubernetes YAML\nDeploy and check your application\nConclusion\nKubernetes references\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhy Docker?\nWhat is a Container?\n\n\n\n\nProducts\nDocker Desktop\nDocker Hub\nFeatures\nContainer Runtime\nDeveloper Tools\nKubernetes\n\n\n\n\nDevelopers\nUse Cases\nPlay with Docker\nCommunity\nOpen Source\nDocker Captains\n\n\n\n\nCompany\nAbout Us\nBlog\nCustomers\nPartners\nNewsroom\nCareers\nContact Us\n\n\n\n\n\n\nStatus\nSecurity\nLegal\nContact\n\n\n\n\n\n\n\nCopyright \u00a9 2013-2020 Docker Inc. All rights reserved.\n\n\n\nTwitter\nYoutube\nGitHub\nLinkedin\nFacebook\nSlideshare\nReddit\n\n\n\n\n\n\n\n", "document_id": 59}]}, {"paragraphs": [{"qas": [{"question": "heroku buildpack installing texlive binary package", "id": 1961, "answers": [{"answer_id": 1947, "document_id": 1542, "question_id": 1961, "text": "This buildpack worked for me:\nhttps://github.com/holiture/heroku-buildpack-tex\n    \n\nThat error code tends to be returned because of problems with your path (https://tex.stackexchange.com/questions/21692/latex-compilation-failure-on-mac-os-x-from-python-script) Follow the last step of the tutorial here https://devcenter.heroku.com/articles/buildpack-binaries and add the directory that your build pack works on to your path.", "answer_start": 1894, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI'm trying to install the texlive-full package on heroku through making a custom buildpack. I'm working on django application, so I'm currently using version of heroku-buildpack-python (https://github.com/heroku/heroku-buildpack-python).\n\nAccording to this tutorial: \nhttps://devcenter.heroku.com/articles/buildpack-binaries\nI found a binary package of texlive on http://packages.ubuntu.com/lucid/texlive-binaries (Links in the right column - Download Source Package) and I added these lines of code in python default heroku buildpack in the section #Build time in order to extract and install texlive-full on heroku.\n\n# Build Time\n\n# Switch to the repo's context.\ncd $BUILD_DIR\n\nTEXLIVE_BINARY=\"http://archive.ubuntu.com/ubuntu/pool/main/t/texlive-base/texlive-base_2012.20120611-5.debian.tar.gz\"\nTEXLIVE_VENDOR=\"vendor/texlive\"\n\n# vendor awesome-vm\nmkdir -p $1/$TEXLIVE_VENDOR\ncurl $TEXLIVE_BINARY -o - | tar -xz -C $1/$TEXLIVE_VENDOR -f -\n\n\nAfter pushing the django application to heroku I can see that slug is 58.0 MB big (before it was just 10.0 MB), so it might have added texlive-full binary package to it (which is about 44 MB). However the latex equation on the site is still not showing and the same error appears; (That appears when texlive is not installed)\n\nValueError at / latex returned code 32512 for formula:\n\nIs there some easier way to install texlive-full on heroku? Or what is the correct notation for installing texlive-full in buildpack or any other kind of debian package?\n    \n\nBased on this buildpack I built my own version: https://github.com/syphar/heroku-buildpack-tex\n\nIt installs a small version of TeX-Live 2013 in your slug, and you can extend it by adding your own packages (collections or single packages from CTAN). \n\nSince your (compressed) slug-size is limited to 300 MB on Heroku you can't texlive-full inside your application. \n    \n\nThis buildpack worked for me:\nhttps://github.com/holiture/heroku-buildpack-tex\n    \n\nThat error code tends to be returned because of problems with your path (https://tex.stackexchange.com/questions/21692/latex-compilation-failure-on-mac-os-x-from-python-script) Follow the last step of the tutorial here https://devcenter.heroku.com/articles/buildpack-binaries and add the directory that your build pack works on to your path.\n    ", "document_id": 1542}]}, {"paragraphs": [{"qas": [{"question": "multiple mysql instances on a single machine", "id": 1432, "answers": [{"answer_id": 1421, "document_id": 1006, "question_id": 1432, "text": "download + install mysql essential package (mysql-essential-5.1.30-win32.msi)\nrun configuration wizard\nrun configuration wizard again (modifying db path, port no)\ntake a look at http://dev.mysql.com/doc/refman/5.1/en/multiple-windows-services.html\ntake a look at my.ini (and the backup)\nfigure out that I need to make a copy of the data dir and merge ini with backup ini file and didn't need to run the config wizard twice. Should have just edited the INI to begin with (but that would require reading the manual.. too much work! Ini file is pretty well documented anyway though)\nmerge ini files, make service name modifications\nrun commands: mysqld --install mysql1, mysqld --install mysql2, net start mysql1, net start mysql2", "answer_start": 1570, "answer_category": null}], "is_impossible": false}], "context": "\n\n                    \n\n            \n\n        \n\n            \n\n                \n\n                    \n\n                        Closed. This question is off-topic. It is not currently accepting answers.\n\n                        \n\n                    \n\n                \n\n            \n\n        \n\n            \n\n        \n\n                \n\n                    \n\n                \n\n            \n\n                \n\n                    Want to improve this question? Update the question so it's on-topic for Stack Overflow.\n\n                \n\n                    Closed 10 years ago.\n\n\n\n            \n\n        \n\n            \n\n                    \n\n                        Improve this question\n\n                    \n\n            \n\n\n\n\n\n    \n\n\n\nHow to configure multiple MySQL instances on a single machine? There are a lot of information on the web, but they are not very useful.\n\nInstead of generic information that can only be understood by a mind with years of MySQL administrative experience, I am looking for a tutorial that guides me from begin to end. I would appreciate answers or pointers that are self-sufficient, i.e., don't require a lot of knowledge from the user's part, instead of obscure references. \n    \n\nOK, this question should be closed as not-programming-related, this is really a question for the upcoming \"sister\" site but I'm going to try and answer it anyway. Now I've never used MySQL and someone can probably do a better answer.\n\nLets start with a google search\n\n(5 mins later... decided to take the plunge after only skim reading a couple of results)\n\n\ndownload + install mysql essential package (mysql-essential-5.1.30-win32.msi)\nrun configuration wizard\nrun configuration wizard again (modifying db path, port no)\ntake a look at http://dev.mysql.com/doc/refman/5.1/en/multiple-windows-services.html\ntake a look at my.ini (and the backup)\nfigure out that I need to make a copy of the data dir and merge ini with backup ini file and didn't need to run the config wizard twice. Should have just edited the INI to begin with (but that would require reading the manual.. too much work! Ini file is pretty well documented anyway though)\nmerge ini files, make service name modifications\nrun commands: mysqld --install mysql1, mysqld --install mysql2, net start mysql1, net start mysql2\n\n\n20 mins, done. \n\nNow we have step by step instructions for running multiple MySQL instances on a single machine, which will probably turn up as the first result in google next time anyone searches for it ;)\n    ", "document_id": 1006}]}, {"paragraphs": [{"qas": [{"question": "javafx self installer with inno setup 5 allow user to change install directory", "id": 1443, "answers": [{"answer_id": 1432, "document_id": 1016, "question_id": 1443, "text": "1.8.0u60 or so).\n\nJust add &lt;installdirChooser&gt; as some &lt;bundleArg", "answer_start": 1989, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI am using Ant to build a self deploying EXE for a JavaFX application.\n\nCurrently Inno Setup places the EXE here: C:\\Users\\username\\AppData\\Local\\application name\n\nI would like to place this in a different location, and provide the user the option to override this.  However I can't seem to find the ant settings to change this.\n\nIs this possible?\n\nThanks!\n    \n\nActually you can't change this using ANT. However, as you already know the deploy mechanism uses Inno Setup and you can modify its behaviour. \n\nDuring the fx:deploy ANT task a default ApplicationName.iss file is created. This default file contains e.g. the setting, which is responsible for the install directory. This default file is only created, if you don't provide any customized on your own. So, I would recommend to run the ANT script, copy the default file and modify it. If you enable the verbose flag of the fx:deploy task you can use the console output to find out, where the default file is created and where the ANT task searches for your customized file before creating the default one:\n\n&lt;fx:deploy\n    ...\n    verbose=\"true\"&gt;\n\n    &lt;fx:info title=\"${appname}\" vendor=\"${vendor}\"/&gt;\n    ...\n&lt;/fx:deploy&gt;\n\n\nIn my case I found the default file in \n\nC:\\Users\\gfkri\\AppData\\Local\\Temp\\fxbundler3627681647438085792\\windows\n\n\nand had to put the customized file to \n\npackage/windows/ApplicationName.iss\n\n\nrelative to the ANT build script.\n\nIf you got so far, you'll find the line DisableDirPage=Yes in your ApplicationName.iss file. Change it to DisableDirPage=No and the user gets the possibility to change the install directory.\n\nFurther you will find the parameter DefaultDirName. If you want to install your Application to C:\\Program File\\ApplicationName by default you can use the constant {pf} e.g.: DefaultDirName={pf}\\ApplicationName. \n    \n\nThe original answer is not true anymore, because that feature got added to the JDK (just dont know when, but it was there when using 1.8.0u60 or so).\n\nJust add &lt;installdirChooser&gt; as some &lt;bundleArguments&gt; and set it to true:\n\n&lt;plugin&gt;\n    &lt;groupId&gt;com.zenjava&lt;/groupId&gt;\n    &lt;artifactId&gt;javafx-maven-plugin&lt;/artifactId&gt;\n    &lt;version&gt;8.4.0&lt;/version&gt;\n    &lt;configuration&gt;\n        &lt;mainClass&gt;your.mainclass&lt;/mainClass&gt;\n        &lt;verbose&gt;true&lt;/verbose&gt;\n        &lt;bundleArguments&gt;\n            &lt;identifier&gt;SOME-GUID-USED-FOR-UPDATE-DETECTION&lt;/identifier&gt;\n            &lt;installdirChooser&gt;true&lt;/installdirChooser&gt;\n        &lt;/bundleArguments&gt;\n    &lt;/configuration&gt;\n&lt;/plugin&gt;\n\n\nDisclaimer: I'm the maintainer of the javafx-maven-plugin\n    ", "document_id": 1016}]}, {"paragraphs": [{"qas": [{"question": "How To Integrate Clojure Web Applications in Apache", "id": 532, "answers": [{"answer_id": 534, "document_id": 257, "question_id": 532, "text": "People are deploying Compojure apps to non-Jetty servlet containers.Check out:https://github.com/weavejester/lein-ring ,https://github.com/weavejester/lein-beanstalk.", "answer_start": 474, "answer_category": null}], "is_impossible": false}], "context": "Given this OP was written about two years ago, rather than ask the same question again, I am wondering if step-by-step instructions exist, so that I can integrate a Noir or other Clojure web application into Apache, whether it's Jetty, Tomcat, or something else. Similar instructions exist for Django, and I think I understand that Python is being run in Django's case as an engine rather than a ring framework, so things are more complicated with Clojure web applications. People are deploying Compojure apps to non-Jetty servlet containers.Check out:https://github.com/weavejester/lein-ring ,https://github.com/weavejester/lein-beanstalk.", "document_id": 257}]}, {"paragraphs": [{"qas": [{"question": "How to include dependent DLLs?", "id": 1303, "answers": [{"answer_id": 1294, "document_id": 873, "question_id": 1303, "text": "You need to better understand the third-party library and how it uses its own dependencies. If the installation of the API solves the problem, but copying the files manually does not, then you're missing something. There's either a missing file, or some environment variable or registry entry that's required. Two things that will really help you in this is the depends tool (which is part of the C++ installation) and procmon, which will tell you all the registry keys and files that get used at runtime.", "answer_start": 411, "answer_category": null}], "is_impossible": false}], "context": "I am using a 3rd party API which is defined in 2 DLLs. I have included those DLLs in my project and set references to them. So far so good.\nHowever, these DLLs have at least one dependent DLL which cannot be found at runtime. I copied the missing DLL into the project and set the 'Copy to output' flag but without success.\nWhat should I be doing here to tell the project where it should find the dependent DLL?\nYou need to better understand the third-party library and how it uses its own dependencies. If the installation of the API solves the problem, but copying the files manually does not, then you're missing something. There's either a missing file, or some environment variable or registry entry that's required. Two things that will really help you in this is the depends tool (which is part of the C++ installation) and procmon, which will tell you all the registry keys and files that get used at runtime.\n", "document_id": 873}]}, {"paragraphs": [{"qas": [{"question": "Automated heroku deploy from subfolder", "id": 1641, "answers": [{"answer_id": 1629, "document_id": 1215, "question_id": 1641, "text": "I made a symlink using ln -s server/requirements.txt requirements.txt.\nMy Procfile looks like this: web: gunicorn --pythonpath server/api app:app.", "answer_start": 389, "answer_category": null}], "is_impossible": false}], "context": "I was able to make it work. I have a server subfolder with Python Flask app and I wanted to deploy it automatically using GitHub integration.\nHeroku uses buildpacks to detect the language & framework of your project. More about that here.\nI found the source code for my buildpack here. Then you just need to look at the detection script. For python it checks the requirements.txt file, so I made a symlink using ln -s server/requirements.txt requirements.txt.\nMy Procfile looks like this: web: gunicorn --pythonpath server/api app:app.\nEverything works now!\n", "document_id": 1215}]}, {"paragraphs": [{"qas": [{"question": "installing in program files vs appdata", "id": 1979, "answers": [{"answer_id": 1965, "document_id": 1564, "question_id": 1979, "text": "You should use AppData for any configuration, or program files that will change with the program.", "answer_start": 1090, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nWhat are the benefits and downsides of installing an application in the user's AppData directory?\n\nI assume that installing in AppData will let users in restricted environments be able to install and use the application.\n    \n\nOne of the benefits of installing to Program Files is that is exactly where the user expects it to go. Usually users go here first to look where the program is installed. The biggest downside is that the user needs administrative privileges, which isn't always required by the program.\n\nYou are correct in assuming installing to AppData will let restricted users install and use the application. It also will follow the user around on network systems, allowing them to have access to the program from any machine they log into with the same account. This is also a downside however since it means 2 or 3 users on a machine will have 2 or 3 versions of it installed.\n\nGoogle installs its products to AppData, which is great since it can then be installed by any user (including at schools where the computers are stuck with IE6 or something).\n\nYou should use AppData for any configuration, or program files that will change with the program. I personally prefer installing programs to Program Files, because that's where users assume it will be installed, and it requires admin permissions, which is usually set for a reason, and abiding by those permissions is just nice for an app to do. Ultimately it's up to you, but the default should probably just be Program Files.\n    \n\nGenerally appears that the \"Appdata\" directory equates to the user software directory someone might notice using Linux systems; hadn't paid too much attention to the shift from Program Files directory installations toward the increasingly popular Appdata folder, but it slowly dawns on me.\n\nThread is a bit old, but relevant considering that Windows is still pretty active; Program Files installations are mostly for your touchy apps --the ones that you will want a bit more access control over like maintenance and security apps. Your Appdata folder is pretty much any other \"user\" specific soft like media players, text editors; etc. The \"administrator\" of a Windows installation will also benefit from using appdata for \"normal\" software along the same lines as not installing software in the root directory while administering a Linux, Mac or BSD system.\n    ", "document_id": 1564}]}, {"paragraphs": [{"qas": [{"question": "How to install PIL with pip on Mac OS?", "id": 1586, "answers": [{"answer_id": 1575, "document_id": 1163, "question_id": 1586, "text": "On Mac OS X, you can use this command:\nsudo pip install https://effbot.org/media/downloads/Imaging-1.1.7.", "answer_start": 125, "answer_category": null}], "is_impossible": false}], "context": "I am trying to install PIL (the Python Imaging Library) using the command:\nsudo pip install pil\nbut I get the wrong message:\nOn Mac OS X, you can use this command:\nsudo pip install https://effbot.org/media/downloads/Imaging-1.1.7.\n", "document_id": 1163}]}, {"paragraphs": [{"qas": [{"question": "Visual studio installation path grayed out", "id": 1187, "answers": [{"answer_id": 1180, "document_id": 763, "question_id": 1187, "text": "1.\tDownload it\n2.\tExtract it\n3.\tRun it with Setup.ForcedUninstall.exe in an administrator command prompt", "answer_start": 650, "answer_category": null}], "is_impossible": false}], "context": "I had to reformat one of my drives (T:) and change its purpose. I had Visual studio 2015 installed on it, uninstalled it before formatting and now the drive has a different letter (can't change it, other things installed on it). I want to install visual studio 2015 again, but on the C: drive. When I run the installation, I get this:\n \nThe T: drive doesn't exist anymore, and I can't change the installation path to another drive.\n16\nI had the same problem. I had an installed Visual Studio on a crashed harddisk. I tried everything above, nothing worked. You should use this method as ultima ratio:\nThere is a VisualStudioUninstaller by Microsoft.\n1.\tDownload it\n2.\tExtract it\n3.\tRun it with Setup.ForcedUninstall.exe in an administrator command prompt\n\nI tried some solutions where I had to delete registry keys, but didn't succeed since most of the solutions were for older versions of visual studio. Is there a way to change the path?\n", "document_id": 763}]}, {"paragraphs": [{"qas": [{"question": "Deny access to .svn folders on Apache", "id": 1708, "answers": [{"answer_id": 1696, "document_id": 1281, "question_id": 1708, "text": "The best option is to use Apache configuration.\nUsing htaccess or global configuration depends mainly on if you control your server.\nIf you do, you can use something like\n<DirectoryMatch .*\\.svn/.*>\n    Deny From All\n</DirectoryMatch>\nIf you don't, you can do something similar in .htaccess files with FilesMatch", "answer_start": 500, "answer_category": null}], "is_impossible": false}], "context": "We have a rails application in subversion that we deploy with Capistrano but have noticed that we can access the files in '/.svn', which presents a security concern.\nI wanted to know what the best way to do this. A few ideas:\n\u2022\tGlobal Apache configuration to deny access\n\u2022\tAdding .htaccess files in the public folder and all subfolders\n\u2022\tCap task that changes the permissions\nI don't really like the idea of deleting the folders or using svn export, since I would like to keep the 'svn info' around.\nThe best option is to use Apache configuration.\nUsing htaccess or global configuration depends mainly on if you control your server.\nIf you do, you can use something like\n<DirectoryMatch .*\\.svn/.*>\n    Deny From All\n</DirectoryMatch>\nIf you don't, you can do something similar in .htaccess files with FilesMatch\n", "document_id": 1281}]}, {"paragraphs": [{"qas": [{"question": "installing a lower ios version than the latest one to a device", "id": 1944, "answers": [{"answer_id": 1931, "document_id": 1524, "question_id": 1944, "text": "Apple wants us to use the latest version, and develop for the latest version, there is no way of downgrading anymore", "answer_start": 1254, "answer_category": null}], "is_impossible": false}], "context": "\n\n                    \n\n            \n\n        \n\n            \n\n                \n\n                    \n\n                        Closed. This question is off-topic. It is not currently accepting answers.\n\n                        \n\n                    \n\n                \n\n            \n\n        \n\n            \n\n        \n\n                \n\n                    \n\n                \n\n            \n\n                \n\n                    Want to improve this question? Update the question so it's on-topic for Stack Overflow.\n\n                \n\n                    Closed 9 years ago.\n\n\n\n            \n\n        \n\n            \n\n                    \n\n                        Improve this question\n\n                    \n\n            \n\n\n\n\n\n    \n\n\n\nI know that it is possible to install lower versions of IOS to jailbroken devices over various tools by creating custom firmware.The problem is we have some test devices in the company and we want to have one device eactly with IOS 4.3. It will not be downgrade since the device itself has IOs 4.2.1 installed.Is there a documented way to active it and the most importantly where can i find older IOS versions?\n    \n\nEDIT:\nEditing because this no longer works and it gets downvoted by people who didn't read the comments.\n\nApple wants us to use the latest version, and develop for the latest version, there is no way of downgrading anymore. \n\nOLD ANSWER:\nYou can download any firmware for any iOS device here\n\niOS Downloads\n\nTo install it, you have to go to iTunes and click on update with the alt key pressed, then you can choose the .ipsw file with the firmware you want.\n\nFor the  \"This device isn't elegible...\" message try this\n    \n\nApple doesn't want you to install anything but the latest version available of iOS, so every install of a firmware will get checked with Apple's servers.\n\nThere is a way to save the authorization for other firmwares with tools like TinyUmbrella, but you can only save the authorization of the currently allowed version; so, if you did not use TinyUmbrella 2 years ago when iOS 4 was the newest version, there is no way to downgrade your device.\n    ", "document_id": 1524}]}, {"paragraphs": [{"qas": [{"question": "What are the specific differences between .msi and setup.exe file?", "id": 1623, "answers": [{"answer_id": 1610, "document_id": 1197, "question_id": 1623, "text": "You can use MSI. MSI is an installer file which installs your program on the executing system.", "answer_start": 81, "answer_category": null}], "is_impossible": false}], "context": "I searched a lot, but all are guessed answers. Help me to find the exact answer.\nYou can use MSI. MSI is an installer file which installs your program on the executing system.\n", "document_id": 1197}]}, {"paragraphs": [{"qas": [{"question": "which file shall I download for installing JRE 8 release for a Linux platform?\n", "id": 221, "answers": [{"answer_id": 229, "document_id": 118, "question_id": 221, "text": "Download File and Instructions\tArchitecture\tWho Can Install\njre-8uversion-linux-x64.tar.gz\n\"Installation of the 64-bit JRE on Linux Platforms\"\t64-bit\tanyone\njre-8uversion-linux-i586.tar.gz\n\"Installation of the 32-bit JRE on Linux Platforms\"\t32-bit\tanyone\njre-8uversion-linux-x64.rpm\n\"Installation of the 64-bit JRE on RPM-based Linux Platforms\"\t64-bit RPM-based Linux\troot\njre-8uversion-linux-i586.rpm\n\"Installation of the 32-bit JRE on RPM-based Linux Platforms\"\t32-bit RPM-based Linux\troot", "answer_start": 2485, "answer_category": null}], "is_impossible": false}, {"question": "how to install JRE for 64-bit Linux, using an archive binary file?", "id": 222, "answers": [{"answer_id": 230, "document_id": 118, "question_id": 222, "text": "Download the file.\n\nBefore the file can be downloaded, you must accept the license agreement. The archive binary can be installed by anyone (not only root users), in any location that you can write to. However, only the root user can install the JDK into the system location.\n\nChange directory to the location where you would like the JDK to be installed, then move the .tar.gz archive binary to the current directory.\n\nUnpack the tarball and install the JRE.\n\n% tar zxvf jre-8uversion-linux-x64.tar.gz\nThe Java Development Kit files are installed in a directory called jdk1.8.0_version in the current directory.\n\nDelete the .tar.gz file if you want to save disk space.", "answer_start": 3505, "answer_category": null}], "is_impossible": false}, {"question": "how to install  64-bit JRE on RPM-based Linux Platforms, using an archive binary file?", "id": 223, "answers": [{"answer_id": 231, "document_id": 118, "question_id": 223, "text": "Download the file.\n\nBefore the file can be downloaded, you must accept the license agreement.\n\nBecome root by running su and entering the super-user password.\n\nUninstall any earlier installations of the JDK packages.\n\n# rpm -e package_name\nInstall the package.\n\n# rpm -ivh jre-8uversion-linux-x64.rpm\nTo upgrade a package:\n\n# rpm -Uvh jre-8uversion-linux-x64.rpm\nDelete the .rpm file if you want to save disk space.\n\nExit the root shell.\n", "answer_start": 4535, "answer_category": null}], "is_impossible": false}, {"question": "how to install JRE for 32-bit Linux, using an archive binary file?", "id": 224, "answers": [{"answer_id": 232, "document_id": 118, "question_id": 224, "text": "Download the file.\n\nBefore the file can be downloaded, you must accept the license agreement. The archive binary can be installed by anyone (not only root users), in any location that you can write to. However, only the root user can install the JRE into the system location.\n\nChange directory to the location where you would like the JRE to be installed, then move the .tar.gz archive binary to the current directory.\n\nUnpack the tarball and install the JRE.\n\n% tar zxvf jre-8uversion-linux-i586.tar.gz\nThe JRE files are installed in a directory called jre1.8.0_version in the current directory.\n\nDelete the .tar.gz file if you want to save disk space.", "answer_start": 5247, "answer_category": null}], "is_impossible": false}, {"question": "how to install  32-bit JRE on RPM-based Linux Platforms, using an archive binary file?", "id": 225, "answers": [{"answer_id": 233, "document_id": 118, "question_id": 225, "text": "Download the file.\n\nBefore the file can be downloaded, you must accept the license agreement.\n\nBecome root by running su and entering the super-user password.\n\nUninstall any earlier installations of the JRE packages.\n\n# rpm -e package_name\nInstall the package.\n\n# rpm -ivh jre-8uversion-linux-i586.rpm\nTo upgrade a package:\n\n# rpm -Uvh jre-8uversion-linux-i586.rpm\nExit the root shell.\n\nThere is no need to reboot.\n\nDelete the .rpm file if you want to save disk space.", "answer_start": 6262, "answer_category": null}], "is_impossible": false}, {"question": "I meet jdk-8u281-ea-bin-b03-linux-amd64-20_oct_2020.rpm: Header V3 RSA/SHA256 Signature, key ID ec551f03: NOKEY, how to install Public Key on RPM-based Linux Platforms?", "id": 226, "answers": [{"answer_id": 234, "document_id": 118, "question_id": 226, "text": "Download the key file from https://yum.oracle.com/RPM-GPG-KEY-oracle-ol7 using the following command:\n\nwget https://yum.oracle.com/RPM-GPG-KEY-oracle-ol7\n\nInstall the key using the following command:\n\nsudo rpm --import RPM-GPG-KEY-oracle-ol7", "answer_start": 7314, "answer_category": null}], "is_impossible": false}], "context": "Skip to Content\nHome PageJava SoftwareJava SE DownloadsJava SE 8 DocumentationSearch\nJava Platform, Standard Edition Installation Guide\nContents    Previous    Next\n7 JRE Installation for Linux Platforms\nThis page describes JRE for Linux system requirements and gives installation instructions for several JRE-Linux combinations.\n\nThis page contains these topics:\n\n\"System Requirements\"\n\n\"JRE 8 Installation Instructions\"\n\n\"General Installation Notes\"\n\nSee \"JDK 8 and JRE 8 Installation Start Here\" for general information about installing JDK 8 and JRE 8.\n\nFor information on enhancements to JDK 8 that relate to the installer, see \"Installer Enhancements in JDK 8\".\n\nSystem Requirements\nSee http://www.oracle.com/technetwork/java/javase/certconfig-2095354.html for information about supported platforms, operating systems, and browsers.\n\nOn a 64-bit system, you can download either the 64-bit or the 32-bit version of the Java platform. However, if you are using a 32-bit browser and you want to use the plugin, then you need to install the 32-bit version of the Java platform. To determine which version of Firefox you are running, launch the application, and select the menu item Help -> About Mozilla Firefox. At the bottom of the window is a version string line that contains either \"Linux i686\" (32-bit) or \"Linux x86_64\" (64-bit). To setup the Java plugin, see \"Manual Installation and Registration of Java Plugin for Linux\".\n\nJRE 8 Installation Instructions\nThis topic contains these topics:\n\n\"Installation Instruction Notation and Files\"\n\n\"Installation of the 64-bit JRE on Linux Platforms\"\n\n\"Installation of the 64-bit JRE on RPM-based Linux Platforms\"\n\n\"Installation of the 32-bit JRE on Linux Platforms\"\n\n\"Installation of the 32-bit JRE on RPM-based Linux Platforms\"\n\n\"Installation of Public Key on RPM-based Linux Platforms\"\n\nInstallation Instruction Notation and Files\nFor instructions containing the notation version, substitute the appropriate JRE update version number. For example, if you are installing update JRE 8 update release 2, the following string representing the name of the bundle:\n\njre-8uversion-linux-i586.tar.gz\nbecomes:\n\njre-8u2-linux-i586.tar.gz\nNote that, as in the preceding example, the version number is sometimes preceded with the letter u, for example, 8u2, and sometimes it is preceded with an underbar, for example, jre1.8.0_02.\n\nThe following table lists the options and instructions for downloading the JRE 8 release for a Linux platform.\n\nDownload File and Instructions\tArchitecture\tWho Can Install\njre-8uversion-linux-x64.tar.gz\n\"Installation of the 64-bit JRE on Linux Platforms\"\t64-bit\tanyone\njre-8uversion-linux-i586.tar.gz\n\"Installation of the 32-bit JRE on Linux Platforms\"\t32-bit\tanyone\njre-8uversion-linux-x64.rpm\n\"Installation of the 64-bit JRE on RPM-based Linux Platforms\"\t64-bit RPM-based Linux\troot\njre-8uversion-linux-i586.rpm\n\"Installation of the 32-bit JRE on RPM-based Linux Platforms\"\t32-bit RPM-based Linux\troot\nJDK 7u6 and later releases include JavaFX SDK (version 2.2 or later). The JavaFX SDK and Runtime are installed and integrated into the standard JDK directory structure.\n\nFor information about how to work with JavaFX, see http://docs.oracle.com/javase/8/javase-clienttechnologies.htm.\n\nInstallation of the 64-bit JRE on Linux Platforms\nThis procedure installs the Java Runtime Environment (JRE) for 64-bit Linux, using an archive binary file (.tar.gz).\n\nThese instructions use the following file:\n\njre-8uversion-linux-x64.tar.gz\nDownload the file.\n\nBefore the file can be downloaded, you must accept the license agreement. The archive binary can be installed by anyone (not only root users), in any location that you can write to. However, only the root user can install the JDK into the system location.\n\nChange directory to the location where you would like the JDK to be installed, then move the .tar.gz archive binary to the current directory.\n\nUnpack the tarball and install the JRE.\n\n% tar zxvf jre-8uversion-linux-x64.tar.gz\nThe Java Development Kit files are installed in a directory called jdk1.8.0_version in the current directory.\n\nDelete the .tar.gz file if you want to save disk space.\n\nInstallation of the 64-bit JRE on RPM-based Linux Platforms\nThis procedure installs the Java Runtime Environment (JRE) for 64-bit RPM-based Linux platforms, such as Red Hat and SuSE, using an RPM binary file (.rpm) in the system location. You must be root to perform this installation.\n\nThese instructions use the following file:\n\njre-8uversion-linux-x64.rpm\nDownload the file.\n\nBefore the file can be downloaded, you must accept the license agreement.\n\nBecome root by running su and entering the super-user password.\n\nUninstall any earlier installations of the JDK packages.\n\n# rpm -e package_name\nInstall the package.\n\n# rpm -ivh jre-8uversion-linux-x64.rpm\nTo upgrade a package:\n\n# rpm -Uvh jre-8uversion-linux-x64.rpm\nDelete the .rpm file if you want to save disk space.\n\nExit the root shell.\n\nThere is no need to reboot.\n\nInstallation of the 32-bit JRE on Linux Platforms\nThis procedure installs the Java Runtime Environment (JRE) for 32-bit Linux, using an archive binary file (.tar.gz).\n\nThese instructions use the following file:\n\njre-8uversion-linux-i586.tar.gz\nDownload the file.\n\nBefore the file can be downloaded, you must accept the license agreement. The archive binary can be installed by anyone (not only root users), in any location that you can write to. However, only the root user can install the JRE into the system location.\n\nChange directory to the location where you would like the JRE to be installed, then move the .tar.gz archive binary to the current directory.\n\nUnpack the tarball and install the JRE.\n\n% tar zxvf jre-8uversion-linux-i586.tar.gz\nThe JRE files are installed in a directory called jre1.8.0_version in the current directory.\n\nDelete the .tar.gz file if you want to save disk space.\n\nInstallation of the 32-bit JRE on RPM-based Linux Platforms\nThis procedure installs the Java Runtime Environment (JRE) for 32-bit RPM-based Linux platforms, such as Red Hat and SuSE, using an RPM binary file (.rpm) in the system location. You must be root to perform this installation.\n\nThese instructions use the following file:\n\njre-8uversion-linux-i586.rpm\nDownload the file.\n\nBefore the file can be downloaded, you must accept the license agreement.\n\nBecome root by running su and entering the super-user password.\n\nUninstall any earlier installations of the JRE packages.\n\n# rpm -e package_name\nInstall the package.\n\n# rpm -ivh jre-8uversion-linux-i586.rpm\nTo upgrade a package:\n\n# rpm -Uvh jre-8uversion-linux-i586.rpm\nExit the root shell.\n\nThere is no need to reboot.\n\nDelete the .rpm file if you want to save disk space.\n\nInstallation of Public Key on RPM-based Linux Platforms\nSince JDK version 8, the JRE RPMs are signed with OL keys. Installation of these RPMs on Linux distributions other than OL gives a warning message indicating that security validation of the package fails. This indicates that the public key used to sign this RPM needs to be installed in the system. A sample warning message and the public key installation steps are as follows:\n\nSample Warning Message\njdk-8u281-ea-bin-b03-linux-amd64-20_oct_2020.rpm: Header V3 RSA/SHA256 Signature, key ID ec551f03: NOKEY\nInstallation Steps\nDownload the key file from https://yum.oracle.com/RPM-GPG-KEY-oracle-ol7 using the following command:\n\nwget https://yum.oracle.com/RPM-GPG-KEY-oracle-ol7\n\nInstall the key using the following command:\n\nsudo rpm --import RPM-GPG-KEY-oracle-ol7\n\n\nNote:\n\nPublic key installation needs to be done only once. The key installation is persistent across reboots.\nGeneral Installation Notes\nThis topic describes general installation topics.\n\nRoot Access\nInstalling the software automatically creates a directory called jre1.8.0_version. Note that if you choose to install the Java SE Runtime Environment into system-wide location such as /usr/jre, you must first become root to gain the necessary permissions. If you do not have root access, simply install the Java SE Runtime Environment into your home directory, or a subdirectory that you have permission to write to.\n\nOverwriting Files\nIf you install the software in a directory that contains a subdirectory named jre1.8.0_version, the new software overwrites files of the same name in that jre1.8.0_version directory. Rename the old directory if it contains files you want to keep.\n\nSystem Preferences\nBy default, the installation script configures the system such that the backing store for system preferences is created inside the JRE's installation directory. If the JRE is installed on a network-mounted drive, it and the system preferences can be exported for sharing with Java runtime environments on other machines.\n\nSee http://docs.oracle.com/javase/8/docs/technotes/guides/preferences/index.html for more information about preferences in the Java platform.\n\nContents    Previous    Next\nCopyright \u00a9 1993, 2021, Oracle and/or its affiliates. All rights reserved. | Cookie \u559c\u597d\u8bbe\u7f6e | Ad Choices.Contact Us", "document_id": 118}]}, {"paragraphs": [{"qas": [{"question": "Difference between compile and runtime configurations in Gradle", "id": 1828, "answers": [{"answer_id": 1814, "document_id": 1399, "question_id": 1828, "text": "In the most common case, the artifacts needed at compile time are a subset of those needed at runtime. For example, let's say that a program called app uses library foo, and library foo internally uses library bar. Then only foo is needed to compile app, but both foo and bar are needed to run it. This is why by default, everything that you put on Gradle's compile configuration is also visible on its runtime configuration, but the opposite isn't true.", "answer_start": 377, "answer_category": null}], "is_impossible": false}], "context": "My question is a little bit common, but it is linked with Gradle too.\nWhy we need compile and runtime configuration?\nWhen I compile something I need artifacts to convert my java classes in bytecode so I need compile configuration, but why is needed runtime configuration do I need something else to run my application in JVM?\nSorry if it sounds stupid, but I don't understand.\nIn the most common case, the artifacts needed at compile time are a subset of those needed at runtime. For example, let's say that a program called app uses library foo, and library foo internally uses library bar. Then only foo is needed to compile app, but both foo and bar are needed to run it. This is why by default, everything that you put on Gradle's compile configuration is also visible on its runtime configuration, but the opposite isn't true.\n", "document_id": 1399}]}, {"paragraphs": [{"qas": [{"question": "What's the hardware requirements for Chef's frontend servers?", "id": 58, "answers": [{"answer_id": 61, "document_id": 64, "question_id": 58, "text": "4 cores (physical or virtual)\n4GB RAM\n20 GB of free disk space (SSD if on premises, Premium Storage in Microsoft Azure, EBS-Optimized GP2 in AWS)", "answer_start": 13911, "answer_category": null}], "is_impossible": false}, {"question": "What's the hardware requirements for Chef's backend servers?", "id": 59, "answers": [{"answer_id": 62, "document_id": 64, "question_id": 59, "text": "2 cores (physical or virtual)\n8GB RAM\n50 GB/backend server (SSD if on premises, Premium Storage in Microsoft Azure, EBS-Optimized GP2 in AWS)\n", "answer_start": 14080, "answer_category": null}], "is_impossible": false}, {"question": "How to allocate Chef's node?", "id": 60, "answers": [{"answer_id": 63, "document_id": 64, "question_id": 60, "text": " A good rule to follow is to allocate 2 MB per node", "answer_start": 13687, "answer_category": null}], "is_impossible": false}, {"question": "How many cluster nodes does HA installation need?", "id": 61, "answers": [{"answer_id": 64, "document_id": 64, "question_id": 61, "text": "The HA backend installation requires three cluster nodes.", "answer_start": 13120, "answer_category": null}], "is_impossible": false}, {"question": "What is Runit flags Chef's backend service?", "id": 62, "answers": [{"answer_id": 65, "document_id": 64, "question_id": 62, "text": "See https://github.com/chef-cookbooks/runit for details. Many of the flags are repeated across the various backend services - they are only documented once at the top here. The same defaults are used unless specified below.", "answer_start": 29508, "answer_category": null}], "is_impossible": false}], "context": "\n\n\n\n\n\n\nHigh Availability: Chef Backend\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLearn Chef\n\n\n\n\n\n\n\n\n\nTutorials\n\n\nSkills Library\n\n\nDocs\n\n\nTraining\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOverview\n\n\n\n\n\nPlatform Overview\n\n\n\n\n\nCommunity\n\n\n\n\n\nAbout the Community\n\n\n\n\nContributing\n\n\n\n\nGuidelines\n\n\n\n\nDocs Style Guide\n\n\n\n\nSend Feedback\n\n\n\n\n\n\nPackages & Platforms\n\n\n\n\n\nPackages\n\n\n\n\nPlatforms\n\n\n\n\nSupported Versions\n\n\n\n\nOmnitruck API\n\n\n\n\nLicensing\n\n\n\n\n\nAbout Licensing\n\n\n\n\nAccepting License\n\n\n\n\n\n\n\n\n\n\nChef Infra\n\n\n\n\n\nGetting Started\n\n\n\n\n\nChef Infra Overview\n\n\n\n\nInstall ChefDK\n\n\n\n\nConfigure ChefDK\n\n\n\n\nQuick Start\n\n\n\n\nSystem Requirements\n\n\n\n\nChef on Azure Guide\n\n\n\n\n\nInstalling Chef on Windows\n\n\n\n\nMicrosoft Azure\n\n\n\n\nChef Workstation on Azure Cloud Shell\n\n\n\n\nAzure Powershell_Cmdlets\n\n\n\n\nKnife Azure\n\n\n\n\nKnife Azurerm\n\n\n\n\n\n\nChef on Windows Guide\n\n\n\n\n\nChef for Microsoft Windows\n\n\n\n\nChef Workstation and ChefDK on Windows\n\n\n\n\nChef Infra Client on Windows\n\n\n\n\nKnife Windows\n\n\n\n\n\n\nGlossary\n\n\n\n\nUninstall\n\n\n\n\n\n\nConcepts\n\n\n\n\n\nChefDK\n\n\n\n\nChef Infra Client Overview\n\n\n\n\nChef Infra Server Overview\n\n\n\n\nchef-repo\n\n\n\n\nCookbooks\n\n\n\n\nCustom Resources\n\n\n\n\nNodes\n\n\n\n\nPolicy\n\n\n\n\n\nAbout Policy\n\n\n\n\nData Bags\n\n\n\n\nRun-lists\n\n\n\n\nEnvironments\n\n\n\n\nRoles\n\n\n\n\n\n\nSecrets\n\n\n\n\nAuthentication\n\n\n\n\nAuthorization\n\n\n\n\nEnvironment Variables\n\n\n\n\nSupermarket\n\n\n\n\n\nSupermarket\n\n\n\n\nPublic Supermarket\n\n\n\n\nPrivate Supermarket\n\n\n\n\nShare Cookbooks\n\n\n\n\n\n\n\n\nFeatures\n\n\n\n\n\nFIPS\n\n\n\n\nHandlers\n\n\n\n\nManagement Console\n\n\n\n\n\nAbout the Management Console\n\n\n\n\nConfigure SAML\n\n\n\n\nClients\n\n\n\n\nCookbooks\n\n\n\n\nData Bags\n\n\n\n\nEnvironments\n\n\n\n\nNodes\n\n\n\n\nRoles\n\n\n\n\nUsers\n\n\n\n\nmanage.rb\n\n\n\n\nchef-manage-ctl\n\n\n\n\n\n\nPush Jobs\n\n\n\n\nSearch\n\n\n\n\n\n\nTroubleshooting\n\n\n\n\n\nSetup\n\n\n\n\n\nChefDK\n\n\n\n\nNodes\n\n\n\n\n\nInstall via Bootstrap\n\n\n\n\nInstall via Install Script\n\n\n\n\nchef-client (executable)\n\n\n\n\nclient.rb\n\n\n\n\nUpgrades\n\n\n\n\nSecurity\n\n\n\n\n\n\nChef Infra Server\n\n\n\n\n\nHosted Chef Server\n\n\n\n\nInstall Chef Infra Server\n\n\n\n\nInstall Standalone\n\n\n\n\nChef Infra Server Prerequisites\n\n\n\n\nTiered Installation\n\n\n\n\nInstall High Availability\n\n\n\n\n\n\nWorking with Proxies\n\n\n\n\nAir-gapped Installation\n\n\n\n\nFIPS-mode\n\n\n\n\nIntegrations\n\n\n\n\n\nAWS Marketplace\n\n\n\n\nGoogle Cloud Platform\n\n\n\n\nVMware\n\n\n\n\n\n\nSupermarket\n\n\n\n\n\nPublic Supermarket\n\n\n\n\nInstall Private Supermarket\n\n\n\n\nCustomize Supermarket\n\n\n\n\nsupermarket.rb Settings\n\n\n\n\nBackup and Restore\n\n\n\n\nLog Files\n\n\n\n\nMonitoring\n\n\n\n\nknife supermarket\n\n\n\n\nsupermarket-ctl\n\n\n\n\nSupermarket API\n\n\n\n\n\n\nManagement Console\n\n\n\n\nPush Jobs\n\n\n\n\n\n\nCookbook Reference\n\n\n\n\n\nAbout Cookbooks\n\n\n\n\nAttributes\n\n\n\n\nFiles\n\n\n\n\nLibraries\n\n\n\n\nRecipes\n\n\n\n\n\nAbout Recipes\n\n\n\n\nDebug Recipes, Client Runs\n\n\n\n\n\n\nRecipe DSL\n\n\n\n\n\nDSL Overview\n\n\n\n\nattribute?\n\n\n\n\ncookbook_name\n\n\n\n\ndata_bag\n\n\n\n\ndata_bag_item\n\n\n\n\ndeclare_resource\n\n\n\n\ndelete_resource\n\n\n\n\ndelete_resource!\n\n\n\n\nedit_resource\n\n\n\n\nedit_resource!\n\n\n\n\nfind_resource\n\n\n\n\nfind_resource!\n\n\n\n\nplatform?\n\n\n\n\nplatform_family?\n\n\n\n\nreboot_pending?\n\n\n\n\nrecipe_name\n\n\n\n\nresources\n\n\n\n\nsearch\n\n\n\n\nshell_out\n\n\n\n\nshell_out!\n\n\n\n\ntag, tagged?, untag\n\n\n\n\nvalue_for_platform\n\n\n\n\nvalue_for_platform_family\n\n\n\n\nwith_run_context\n\n\n\n\nWindows Platform\n\n\n\n\nregistry_data_exists?\n\n\n\n\nregistry_get_subkeys\n\n\n\n\nregistry_get_values\n\n\n\n\nregistry_has_subkeys?\n\n\n\n\nregistry_key_exists?\n\n\n\n\nregistry_value_exists?\n\n\n\n\nWindows Platform Helpers\n\n\n\n\nLog Entries\n\n\n\n\n\n\nCustom Resources DSL\n\n\n\n\nResources\n\n\n\n\n\nAbout Resources\n\n\n\n\nCommon Functionality\n\n\n\n\nMigrating from Definitions\n\n\n\n\nCustom Resources\n\n\n\n\nAll Resources (Single Page)\n\n\n\n\napt_package\n\n\n\n\napt_preference\n\n\n\n\napt_repository\n\n\n\n\napt_update\n\n\n\n\narchive_file\n\n\n\n\nbash\n\n\n\n\nbatch\n\n\n\n\nbff_package\n\n\n\n\nbreakpoint\n\n\n\n\nbuild_essential\n\n\n\n\ncab_package\n\n\n\n\nchef_gem\n\n\n\n\nchef_handler\n\n\n\n\nchef_sleep\n\n\n\n\nchocolatey_config\n\n\n\n\nchocolatey_feature\n\n\n\n\nchocolatey_package\n\n\n\n\nchocolatey_source\n\n\n\n\ncookbook_file\n\n\n\n\ncron\n\n\n\n\ncron_d\n\n\n\n\ncron_access\n\n\n\n\ncsh\n\n\n\n\ndirectory\n\n\n\n\ndmg_package\n\n\n\n\ndnf_package\n\n\n\n\ndpkg_package\n\n\n\n\ndsc_resource\n\n\n\n\ndsc_script\n\n\n\n\nexecute\n\n\n\n\nfile\n\n\n\n\nfreebsd_package\n\n\n\n\ngem_package\n\n\n\n\ngit\n\n\n\n\ngroup\n\n\n\n\nhomebrew_cask\n\n\n\n\nhomebrew_package\n\n\n\n\nhomebrew_tap\n\n\n\n\nhostname\n\n\n\n\nhttp_request\n\n\n\n\nifconfig\n\n\n\n\nips_package\n\n\n\n\nkernel_module\n\n\n\n\nksh\n\n\n\n\nlaunchd\n\n\n\n\nlink\n\n\n\n\nlocale\n\n\n\n\nlog\n\n\n\n\nmacos_userdefaults\n\n\n\n\nmacports_package\n\n\n\n\nmdadm\n\n\n\n\nmount\n\n\n\n\nmsu_package\n\n\n\n\nohai_hint\n\n\n\n\nohai\n\n\n\n\nopenbsd_package\n\n\n\n\nopenssl_dhparam\n\n\n\n\nopenssl_ec_private_key\n\n\n\n\nopenssl_ec_public_key\n\n\n\n\nopenssl_rsa_private_key\n\n\n\n\nopenssl_rsa_public_key\n\n\n\n\nopenssl_x509_certificate\n\n\n\n\nopenssl_x509_crl\n\n\n\n\nopenssl_x509_request\n\n\n\n\nosx_profile\n\n\n\n\npackage\n\n\n\n\npacman_package\n\n\n\n\npaludis_package\n\n\n\n\nperl\n\n\n\n\nportage_package\n\n\n\n\npowershell_package\n\n\n\n\npowershell_package_source\n\n\n\n\npowershell_script\n\n\n\n\npython\n\n\n\n\nreboot\n\n\n\n\nreference\n\n\n\n\nregistry_key\n\n\n\n\nremote_directory\n\n\n\n\nremote_file\n\n\n\n\nrhsm_errata_level\n\n\n\n\nrhsm_errata\n\n\n\n\nrhsm_register\n\n\n\n\nrhsm_repo\n\n\n\n\nrhsm_subscription\n\n\n\n\nroute\n\n\n\n\nrpm_package\n\n\n\n\nruby\n\n\n\n\nruby_block\n\n\n\n\nscript\n\n\n\n\nservice\n\n\n\n\nsmartos_package\n\n\n\n\nsnap_package\n\n\n\n\nsolaris_package\n\n\n\n\nssh_known_hosts_entry\n\n\n\n\nsubversion\n\n\n\n\nsudo\n\n\n\n\nswap_file\n\n\n\n\nsysctl\n\n\n\n\nsystemd_unit\n\n\n\n\ntemplate\n\n\n\n\ntimezone\n\n\n\n\nuser\n\n\n\n\nwindows_ad_join\n\n\n\n\nwindows_auto_run\n\n\n\n\nwindows_certificate\n\n\n\n\nwindows_dfs_folder\n\n\n\n\nwindows_dfs_namespace\n\n\n\n\nwindows_dfs_server\n\n\n\n\nwindows_dns_record\n\n\n\n\nwindows_dns_zone\n\n\n\n\nwindows_env\n\n\n\n\nwindows_feature\n\n\n\n\nwindows_feature_dism\n\n\n\n\nwindows_feature_powershell\n\n\n\n\nwindows_firewall_rule\n\n\n\n\nwindows_font\n\n\n\n\nwindows_package\n\n\n\n\nwindows_pagefile\n\n\n\n\nwindows_path\n\n\n\n\nwindows_printer\n\n\n\n\nwindows_printer_port\n\n\n\n\nwindows_service\n\n\n\n\nwindows_share\n\n\n\n\nwindows_shortcut\n\n\n\n\nwindows_task\n\n\n\n\nwindows_uac\n\n\n\n\nwindows_workgroup\n\n\n\n\nyum_package\n\n\n\n\nyum_repository\n\n\n\n\nzypper_package\n\n\n\n\nzypper_repository\n\n\n\n\n\n\nTemplates\n\n\n\n\nCookbook Repo\n\n\n\n\nmetadata.rb\n\n\n\n\nCookbook Versioning\n\n\n\n\nRuby Guide\n\n\n\n\n\n\nChefDK\n\n\n\n\n\nAbout ChefDK\n\n\n\n\nBerkshelf\n\n\n\n\nchef-shell (executable)\n\n\n\n\nchef (executable)\n\n\n\n\n\nchef env\n\n\n\n\nchef exec\n\n\n\n\nchef gem\n\n\n\n\nchef generate attribute\n\n\n\n\nchef generate cookbook\n\n\n\n\nchef generate file\n\n\n\n\nchef generate recipe\n\n\n\n\nchef generate repo\n\n\n\n\nchef generate resource\n\n\n\n\nchef generate template\n\n\n\n\nchef shell-init\n\n\n\n\n\n\nchef-apply (executable)\n\n\n\n\nChef Solo\n\n\n\n\n\nAbout Chef Solo\n\n\n\n\nchef-solo (executable)\n\n\n\n\nsolo.rb\n\n\n\n\n\n\nchef-vault\n\n\n\n\nChefSpec\n\n\n\n\nconfig.rb (knife.rb)\n\n\n\n\nOptional config.rb Settings\n\n\n\n\ncookstyle\n\n\n\n\nDelivery CLI\n\n\n\n\nFoodcritic\n\n\n\n\nTest Kitchen\n\n\n\n\n\nAbout Test Kitchen\n\n\n\n\nkitchen (executable)\n\n\n\n\nkitchen.yml\n\n\n\n\nkitchen-vagrant\n\n\n\n\n\n\nKnife\n\n\n\n\n\nAbout Knife\n\n\n\n\nSetting up Knife\n\n\n\n\nKnife Common Options\n\n\n\n\nconfig.rb (knife.rb)\n\n\n\n\nknife bootstrap\n\n\n\n\nknife client\n\n\n\n\nknife configure\n\n\n\n\nknife cookbook\n\n\n\n\nknife cookbook site\n\n\n\n\nknife data bag\n\n\n\n\nknife delete\n\n\n\n\nknife deps\n\n\n\n\nknife diff\n\n\n\n\nknife download\n\n\n\n\nknife edit\n\n\n\n\nknife environment\n\n\n\n\nknife exec\n\n\n\n\nknife list\n\n\n\n\nknife node\n\n\n\n\nknife raw\n\n\n\n\nknife recipe list\n\n\n\n\nknife role\n\n\n\n\nknife search\n\n\n\n\nknife serve\n\n\n\n\nknife show\n\n\n\n\nknife ssh\n\n\n\n\nknife ssl_check\n\n\n\n\nknife ssl_fetch\n\n\n\n\nknife status\n\n\n\n\nknife supermarket\n\n\n\n\nknife tag\n\n\n\n\nknife upload\n\n\n\n\nknife user\n\n\n\n\nknife xargs\n\n\n\n\nknife opc\n\n\n\n\n\n\nOhai\n\n\n\n\n\nAbout Ohai\n\n\n\n\nohai (executable)\n\n\n\n\n\n\nPolicyfile\n\n\n\n\n\nAbout Policyfile\n\n\n\n\nPolicyfile.rb\n\n\n\n\n\n\npush-jobs-client (executable)\n\n\n\n\n\n\nChef Infra Server\n\n\n\n\n\nRunbook (Single Page)\n\n\n\n\nBackup & Restore\n\n\n\n\nBackend Failure Recovery\n\n\n\n\nFirewalls & Ports\n\n\n\n\nActive Directory & LDAP\n\n\n\n\nLog Files\n\n\n\n\nMonitor\n\n\n\n\nOrganizations & Groups\n\n\n\n\nSecurity\n\n\n\n\nServices\n\n\n\n\nTuning\n\n\n\n\nUpgrades\n\n\n\n\nUpgrade HA Cluster\n\n\n\n\nUsers\n\n\n\n\nchef-server-ctl\n\n\n\n\nchef-backend-ctl\n\n\n\n\nchef-server.rb\n\n\n\n\nChef Infra Server Optional Settings\n\n\n\n\nopscode-expander-ctl\n\n\n\n\nChef Infra Server API\n\n\n\n\nPush Jobs\n\n\n\n\n\nknife push jobs\n\n\n\n\npush-jobs-client\n\n\n\n\npush-jobs-client.rb\n\n\n\n\npush-jobs-server.rb\n\n\n\n\nPush Jobs API\n\n\n\n\nChef Infra Server Sent Events\n\n\n\n\n\n\n\n\nRelease Notes\n\n\n\n\n\nChef Infra Client\n\n\n\n\nChefDK\n\n\n\n\nChef Infra Server\n\n\n\n\nChef Push Jobs\n\n\n\n\n\n\nDeprecations\n\n\n\n\n\n\n\nChef Workstation\n\n\n\n\n\nChef Workstation\n\n\n\n\n\nDocumentation\n\n\n\n\n\n\n\nChef Habitat\n\n\n\n\n\nDocumentation\n\n\n\n\n\nTutorials\n\n\n\n\n\n\n\nChef InSpec\n\n\n\n\n\nDocumentation\n\n\n\n\n\nTutorials\n\n\n\n\n\n\n\nChef Automate\n\n\n\n\n\nChef Automate Documentation\n\n\n\n\n\n\n\nLegacy\n\n\n\n\n\nWorkflow\n\n\n\n\n\nWorkflow Basics\n\n\n\n\n\nWorkflow Overview\n\n\n\n\nConfigure a Pipeline\n\n\n\n\nConfigure a Project\n\n\n\n\nConfigure Data Collection\n\n\n\n\nData Collection with Chef HA\n\n\n\n\nData Collection without Chef Infra Server\n\n\n\n\nAudit Cookbook\n\n\n\n\n\n\nManaging Workflow\n\n\n\n\n\nbuild-cookbook (cookbook)\n\n\n\n\ndelivery-truck (cookbook)\n\n\n\n\nManage Dependencies\n\n\n\n\nManage Secrets\n\n\n\n\nPublish to Multiple Chef Infra Servers\n\n\n\n\nRunners\n\n\n\n\nWorkflow w/Bitbucket\n\n\n\n\nWorkflow w/Email (SMTP)\n\n\n\n\nWorkflow w/GitHub\n\n\n\n\nWorkflow w/Slack\n\n\n\n\nUsers and Roles\n\n\n\n\nAuthentication w/LDAP\n\n\n\n\nAuthentication w/SAML\n\n\n\n\nElasticsearch and Kibana Auth\n\n\n\n\nTuning\n\n\n\n\n\n\nReference\n\n\n\n\n\nDelivery CLI\n\n\n\n\ndelivery.rb\n\n\n\n\nWorkflow DSL\n\n\n\n\n\n\nAWS OpsWorks for Chef Automate\n\n\n\n\nChef Automate for Microsoft Azure\n\n\n\n\n\n\n\n\nExtension APIs\n\n\n\n\n\nCompliance DSL\n\n\n\n\n\nHandlers\n\n\n\n\n\nCustom Handlers\n\n\n\n\nHandler DSL\n\n\n\n\nCommunity Handlers\n\n\n\n\n\n\nKnife Plugins\n\n\n\n\n\nCloud Plugins\n\n\n\n\nWriting Custom Plugins\n\n\n\n\n\n\nOhai Plugins\n\n\n\n\n\nCustom Plugins\n\n\n\n\nCommunity Plugins\n\n\n\n\n\n\n\n\n\nAvailable on GitHub\n\n\n\n\n\nGet Chef\n\n\n\n\n\nSend Feedback\n\n\n\n\n\nSupport\n\n\n\n\n\nSite Map\n\n\n\n\n\nArchive\n\n\n\n\n\n\n\n\n\n\n\n\nTable Of Contents\n\nHigh Availability: Chef Backend\nOverview\nKey Differences From Standalone Chef server\n\n\nRecommended Cluster Topology\nNodes\nHardware Requirements\n\n\nNetwork Services\nNetwork Port Requirements\nInbound from load balancer to frontend group\nInbound from frontend group to backend cluster\nPeer communication, backend cluster\n\n\n\n\nInstallation\nStep 1: Create Cluster\nStep 2: Shared Credentials\nStep 3: Install and Configure Remaining Backend Nodes\nStep 4: Generate Chef Infra Server Configuration\nStep 5: Install and Configure First Frontend\nStep 6: Adding More Frontend Nodes\nUpgrading Chef Infra Server on the Frontend Machines\nConfiguring Frontend and Backend Members on Different Networks\n\n\nCluster Security Considerations\nCommunication Between Nodes\nCommunication Between Frontend Group & Backend Cluster\nSecuring Communication\nServices and Secrets\nChef Infra Server frontend\n\n\nSoftware Versions\n\n\nchef-backend.rb Options\nCommon \u2018Runit\u2019 flags for any backend service\nPostgreSQL settings\nPostgreSQL settings given to postgresql.conf\netcd settings\nElastic Search JVM settings\nElastic Search configuration\nChef HA backend leader management service settings\nChef HA backend leader health status settings\nChef HA backend leader connection pool settings\nSSL settings\n\n\nchef-backend-ctl\n\n\n\n\n\n\n\n\n\nHigh Availability: Chef Backend\u00b6\n[edit on GitHub]\nThis topic introduces the underlying concepts behind the architecture\nof the high availability Chef Infra Server cluster. The topic then\ndescribes the setup and installation process for a high availability\nChef Infra Server cluster comprised of five total nodes (two frontend and three backend).\n\nOverview\u00b6\nThe Chef Infra Server can operate in a high availability configuration\nthat provides automated load balancing and failover for stateful\ncomponents in the system architecture. This type of configuration\ntypically splits the servers into two segments: The backend cluster,\nand the frontend group.\n\nThe frontend group, comprised of one (or more) nodes running the\nChef Infra Server. Nodes in the frontend group handle requests to the\nChef Infra Server API and access to the Chef management console. Frontend group\nnodes should be load balanced, and may be scaled horizontally by\nincreasing the number of nodes available to handle requests.\n\nThe backend cluster, comprised of three nodes working\ntogether, provides high availability data persistence for the\nfrontend group.\n\nNote\nAt this time, backend clusters can only have three nodes.\n\n\n\n\nImportant\nWhen doing cloud deployments, Chef HA clusters are not meant to be geographically dispersed across multiple regions or datacenters; however, in cloud providers such as AWS, you can deploy HA clusters across multiple Availability Zones within the same region.\n\n\nKey Differences From Standalone Chef server\u00b6\nThere are several key differences between the high availability Chef Infra Server cluster and a standalone Chef Infra Server instance.\n\nWhile Apache Solr is used in standalone Chef Infra Server instances,\nin the high availability Chef Infra Server cluster it is replaced with\nElasticsearch. Elasticsearch provides more flexible clustering\noptions while maintaining search API compatibility with Apache Solr.\nWrites to the search engine and the database are handled\nasynchronously via RabbitMQ and chef-expander in standalone\nChef Infra Server instances. However, the high availability Chef server\ncluster writes to the search engine and the database\nsimultaneously. As such the RabbitMQ and chef-expander services\nare no longer present in the high availability Chef Infra Server cluster.\nStandalone Chef Infra Server instances write Bookshelf data to\nthe filesystem. In a high availability Chef Infra Server cluster, Bookshelf data is written to the database.\n\n\n\n\nRecommended Cluster Topology\u00b6\n\nNodes\u00b6\n\nThe HA backend installation requires three cluster nodes. Chef has not tested and does not support installations with other numbers of backend cluster nodes.\nOne or more frontend group nodes\n\n\nHardware Requirements\u00b6\nThe following are a list of general hardware requirements for both frontend and backend servers. The important guideline you should follow are that frontend servers tend to be more CPU bound and backend servers are more disk and memory bound. Also, disk space for backend servers should scale up with the number of nodes that the servers are managing. A good rule to follow is to allocate 2 MB per node. The disk values listed below should be a good default value that you will want to modify later if/when your node count grows.\n\n64-bit architecture\n\nFrontend Requirements\n\n4 cores (physical or virtual)\n4GB RAM\n20 GB of free disk space (SSD if on premises, Premium Storage in Microsoft Azure, EBS-Optimized GP2 in AWS)\n\nBackend Requirements\n\n2 cores (physical or virtual)\n8GB RAM\n50 GB/backend server (SSD if on premises, Premium Storage in Microsoft Azure, EBS-Optimized GP2 in AWS)\n\n\nWarning\nThe Chef Infra Server MUST NOT use a network file system of any type\u2014virtual or physical\u2014for backend storage. The Chef Infra Server database operates quickly. The behavior of operations, such as the writing of log files, will be unpredictable when run over a network file system.\n\n\n\n\nNetwork Services\u00b6\n\nA load balancer between the rest of the network, and the frontend\ngroup (Not provided). Because management console session data is\nstored on each node in the frontend group individually, the load\nbalancer should be configured with sticky sessions.\n\n\n\nNetwork Port Requirements\u00b6\n\nInbound from load balancer to frontend group\u00b6\n\nTCP 80 (HTTP)\nTCP 443 (HTTPS)\n\n\n\nInbound from frontend group to backend cluster\u00b6\n\nTCP 2379 (etcd)\nTCP 5432 (PostgreSQL)\nTCP 7331 (leaderl)\nTCP 9200-9300 (Elasticsearch)\n\n\n\nPeer communication, backend cluster\u00b6\n\n2379 (etcd)\n2380 (etcd)\n5432 (PostgreSQL)\n9200-9400 (Elasticsearch)\n\n\n\n\n\nInstallation\u00b6\nThese instructions assume you are using the minimum versions:\n\nChef Server  : 12.5.0\nChef Backend : 0.8.0\n\nDownload Chef Infra Server and Chef Backend (chef-backend) if you do not have them already.\nBefore creating the backend HA cluster and building at least one Chef Infra Server to be part of the frontend group, verify:\n\nThe user who will install and build the backend HA cluster and\nfrontend group has root access to all nodes.\nThe number of backend and frontend nodes that are desired. It is\nrequired to have three backend nodes, but the number of frontend nodes\nmay vary from a single node to a load-balanced tiered configuration.\nSSH access to all boxes that will belong to the backend HA cluster\nfrom the node that will be the initial bootstrap.\nA time synchronization policy is in place, such as Network Time Protocol (NTP). Drift of\nless than 1.5 seconds must exist across all nodes in the backend HA\ncluster.\n\n\nStep 1: Create Cluster\u00b6\nThe first node must be bootstrapped to initialize the cluster. The\nnode used to bootstrap the cluster will be the cluster leader when the\ncluster comes online. After bootstrap completes this node is no\ndifferent from any other back-end node.\n\nInstall the Chef Backend package on the first backend node as root.\n\nDownload Chef Backend (chef-backend)\nIn RedHat/CentOS: yum install PATH_TO_RPM\nIn Debian/Ubuntu: dpkg -i PATH_TO_DEB\n\n\nUpdate /etc/chef-backend/chef-backend.rb with the following\ncontent:\npublish_address 'external_IP_address_of_this_box' # External ip address of this backend box\n\n\n\nIf any of the backends or frontends are in different networks from each other then\nadd a postgresql.md5_auth_cidr_addresses line to /etc/chef-backend/chef-backend.rb with\nthe following content where , \"<NET-1_IN_CIDR>\", ..., \"<NET-N_IN_CIDR>\" is the list\nof all of the networks that your backends and frontends are in.\nSee the Configuring Frontend and Backend Members on Different Networks\nsection for more information:\npublish_address 'external_IP_address_of_this_box' # External ip address of this backend box\npostgresql.md5_auth_cidr_addresses = [\"samehost\", \"samenet\", \"<NET-1_IN_CIDR>\", ..., \"<NET-N_IN_CIDR>\"]\n\n\n\nRun chef-backend-ctl create-cluster.\n\n\n\n\nStep 2: Shared Credentials\u00b6\nThe credentials file /etc/chef-backend/chef-backend-secrets.json\ngenerated by bootstrapping must be shared with the other nodes. You\nmay copy them directly, or expose them via a common mounted location.\nFor example, to copy using ssh:\nscp /etc/chef-backend/chef-backend-secrets.json <USER>@<IP_BE2>:/home/<USER>\nscp /etc/chef-backend/chef-backend-secrets.json <USER>@<IP_BE3>:/home/<USER>\n\n\nDelete this file from the destination after Step 4 has been completed\nfor each backend being joined to the cluster.\n\n\nStep 3: Install and Configure Remaining Backend Nodes\u00b6\nFor each additional node do the following in sequence (if you attempt\nto join nodes in parallel the cluster may fail to become available):\n\nInstall the Chef Backend package on the node.\n\nDownload Chef Backend (chef-backend)\nIn RedHat/CentOS: yum install PATH_TO_RPM\nIn Debian/Ubuntu: dpkg -i PATH_TO_DEB\n\n\nIf you added a postgresql.md5_auth_cidr_addresses line to the leader\u2019s /etc/chef-backend/chef-backend.rb in Step 1: Create Cluster then update this node\u2019s /etc/chef-backend/chef-backend.rb with the following content where postgresql.md5_auth_cidr_addresses is set to the same value used in the leader\u2019s chef-backend.rb. If all of the backend and frontend clusters are in the same network then you don\u2019t need to modify this node\u2019s /etc/chef-backend/chef-backend.rb at all.\npublish_address 'external_IP_address_of_this_box' # External ip address of this backend box\npostgresql.md5_auth_cidr_addresses = [\"samehost\", \"samenet\", \"<NET-1_IN_CIDR>\", ..., \"<NET-N_IN_CIDR>\"]\n\n\n\nAs root or with sudo:\nchef-backend-ctl join-cluster <IP_BE1> -s /home/<USER>/chef-backend-secrets.json\n\n\n\nAnswer the prompts regarding which public IP to use. As an alternative, you may specify them on\nthe chef-backend join-cluster command line. See chef-backend-ctl join-cluster --help for\nmore information.\nIf you manually added the publish_address line to /etc/chef-backend/chef-backend.rb then\nyou will not be prompted for the public IP and you should not use the --publish-address option\nto specify the the public IP on the chef-backend join-cluster command line.\n\nIf you copied the shared chef-backend-secrets.json file to a user HOME directory on this host, remove it now.\n\nRepeat these steps for each follower node, after which the cluster is online and available. From any node in the backend HA cluster, run the following command:\nchef-backend-ctl status\n\n\nshould return something like:\nService        Local Status        Time in State  Distributed Node Status\nelasticsearch  running (pid 6661)  1d 5h 59m 41s  state: green; nodes online: 3/3\netcd           running (pid 6742)  1d 5h 59m 39s  health: green; healthy nodes: 3/3\nleaderl        running (pid 6788)  1d 5h 59m 35s  leader: 1; waiting: 0; follower: 2; total: 3\npostgresql     running (pid 6640)  1d 5h 59m 43s  leader: 1; offline: 0; syncing: 0; synced: 2\n\n\n\n\n\n\nStep 4: Generate Chef Infra Server Configuration\u00b6\nLog into the node from Step 1, and we will generate our chef-server frontend node configuration:\nchef-backend-ctl gen-server-config <FE1-FQDN> -f chef-server.rb.FE1\nscp chef-server.rb.FE1 USER@<IP_FE1>:/home/<USER>\n\n\n\nNote\n/etc/chef-backend/chef-backend-secrets.json is not made available to Chef Infra Server frontend nodes.\n\n\n\nStep 5: Install and Configure First Frontend\u00b6\nOn the first frontend node, assuming that the generated configuration was copied as detailed in Step 4:\n\nInstall the current chef-server-core package\nRun cp /home/<USER>/chef-server.rb.<FE1> /etc/opscode/chef-server.rb\nAs the root user, run chef-server-ctl reconfigure\n\n\n\nStep 6: Adding More Frontend Nodes\u00b6\nFor each additional frontend node you wish to add to your cluster:\n\nInstall the current chef-server-core package.\n\nGenerate a new /etc/opscode/chef-server.rb from any of the backend nodes via\nchef-backend-ctl gen-server-config <FE_NAME-FQDN> > chef-server.rb.<FE_NAME>\n\n\n\nCopy it to /etc/opscode on the new frontend node.\n\nFrom the first frontend node configured in Step 5, copy the\nfollowing files from the first frontend to /etc/opscode on the\nnew frontend node:\n\n/etc/opscode/private-chef-secrets.json\n\n\nNote\nFor Chef Server versions prior to 12.14, you will also need to copy the key files:\n\n/etc/opscode/webui_priv.pem\n/etc/opscode/webui_pub.pem\n/etc/opscode/pivotal.pem\n\n\n\nOn the new frontend node run mkdir -p /var/opt/opscode/upgrades/.\n\nFrom the first frontend node, copy /var/opt/opscode/upgrades/migration-level to the same location on the new node.\n\nOn the new frontend run touch /var/opt/opscode/bootstrapped.\n\nOn the new frontend run chef-server-ctl reconfigure as root.\n\n\n\n\nUpgrading Chef Infra Server on the Frontend Machines\u00b6\n\nOn one frontend server, follow the standalone upgrade process.\nCopy /var/opt/opscode/upgrades/migration-level from the first upgraded frontend to /var/opt/opscode/upgrades/migration-level on each of the remaining frontends.\nOnce the updated file has been copied to each of the remaining frontends, perform the standalone upgrade process on each of the frontend servers.\n\n\n\nConfiguring Frontend and Backend Members on Different Networks\u00b6\nBy default, PostgreSQL only allows systems on its local network to connect to the database server that runs it and the pg_hba.conf used by PostgreSQL controls network access to the server. The default pg_hba.conf has the following four entries:\nhost    all         all         samehost               md5\nhostssl replication replicator  samehost               md5\nhost    all         all         samenet                md5\nhostssl replication replicator  samenet                md5\n\n\nTo allow other systems to connect, such as members of a frontend group that might exist on a different network, you will need to authorize that usage by adding the following line to the /etc/chef-backend/chef-backend.rb file on all of the backend members.\npostgresql.md5_auth_cidr_addresses = [\"samehost\", \"samenet\", \"<YOURNET IN CIDR>\"]\n\n\nAfter setting the md5_auth_cidr_addresses value and reconfiguring the server, two entries will be created in pg_hba.conf for each value in the md5_auth_cidr_addresses array. Existing values in pg_hba.conf will be overwritten by the values in the array, so we must also specify \u201csamehost\u201d and \u201csamenet\u201d, which will continue to allow systems on a local network to connect to PostgreSQL.\nFor example, if a frontend host at 192.168.1.3 can reach a backend member over the network, but the backend\u2019s local network is 192.168.2.x, you would add the following line to /etc/chef-backend/chef-backend.rb\npostgresql.md5_auth_cidr_addresses = [\"samehost\", \"samenet\", \"192.168.1.3/24\"]\n\n\nwhich would result in the following two entries being added to the pg_hba.conf file.\nhost    all         all         samehost               md5\nhostssl replication replicator  samehost               md5\nhost    all         all         samenet                md5\nhostssl replication replicator  samenet                md5\nhost    all         all         192.168.1.3/24         md5\nhostssl replication replicator  192.168.1.3/24         md5\n\n\nRunning chef-backend-ctl reconfigure on all the backends will allow that frontend to complete its connection.\n\nImportant\nThe postgresql.md5_auth_cidr_addresses subnet settings must be identical for all members of the backend cluster. In the case where the subnet settings of the frontend cluster are different from the subnet settings of the backend cluster, the values set on the members of the backend cluster should contain the subnet of the frontend cluster. This guarantees that all members of a cluster can still communicate with each other after a cluster change of state occurs.  For example, if the frontend subnet setting is \u201c192.168.1.0/24\u201d and the backend subnet setting is \u201c192.168.2.0/24\u201d, then the postgresql.md5_auth_cidr_addresses subnet settings must be postgresql.md5_auth_cidr_addresses = [\"samehost\", \"samenet\", \"192.168.1.0/24\", 192.168.2.0/24]\n\n\n\n\nCluster Security Considerations\u00b6\nA backend cluster is expected to run in a trusted environment. This means that untrusted users that communicate with and/or eavesdrop on services provided by the backend cluster can potentially view sensitive data.\n\nCommunication Between Nodes\u00b6\nPostgreSQL communication between nodes in the backend cluster is encrypted, and uses password authentication. All other communication in the backend cluster is unauthenticated and happens in the clear (without encryption).\n\n\nCommunication Between Frontend Group & Backend Cluster\u00b6\nPostgreSQL communication from nodes in the frontend group to the leader of the backend cluster uses password authentication, but communication happens in the clear (without encryption).\nElasticsearch communication is unauthenticated and happens in the clear (without encryption).\n\n\nSecuring Communication\u00b6\nBecause most of the peer communication between nodes in the backend cluster happens in the clear, the backend cluster is vulnerable to passive monitoring of network traffic between nodes. To help prevent an active attacker from intercepting or changing cluster data, Chef recommends using iptables or an equivalent network ACL tool to restrict access to PostgreSQL, Elasticsearch and etcd to only hosts that need access.\nBy service role, access requirements are as follows:\n\n\n\n\n\n\nService\nAccess Requirements\n\n\n\nPostgreSQL\nAll backend cluster members and all Chef Infra Server frontend group nodes.\n\nElasticsearch\nAll backend cluster members and all Chef Infra Server frontend group nodes.\n\netcd\nAll backend cluster members and all Chef Infra Server frontend group nodes.\n\n\n\n\n\nServices and Secrets\u00b6\nCommunication with PostgreSQL requires password authentication. The backend cluster generates PostgreSQL users and passwords during the initial cluster-create. These passwords are present in the following files on disk:\n\n\n\n\n\n\n\n\nSecret\nOwner\nGroup\nMode\n\n\n\n/etc/chef-backend/secrets.json\nroot\nchef_pgsql\n0640\n\n/var/opt/chef-backend/leaderl/data/sys.config\nchef_pgsql\nchef_pgsql\n0600\n\n/var/opt/chef-backend/PostgreSQL/9.5/recovery.conf\nchef_pgsql\nchef_pgsql\n0600\n\n\n\nThe following services run on each node in the backend cluster. The user account under which the service runs as listed the second column:\n\n\n\n\n\n\nService\nProcess Owner\n\n\n\npostgresql\nchef_pgsql\n\nelasticsearch\nchef-backend\n\netcd\nchef-backend\n\nleaderl\nchef_pgsql\n\nepmd\nchef_pgsql (or first user launching an erlang process)\n\n\n\n\nChef Infra Server frontend\u00b6\nThe chef-backend-ctl gen-server-config command, which can be run as root from any node in the backend cluster, will automatically generate a configuration file containing the superuser database access credentials for the backend cluster PostgreSQL instance.\n\n\n\nSoftware Versions\u00b6\nThe backend HA cluster uses the Chef installer to package all of the software\nnecessary to run the services included in the backend cluster. For a full list of the software packages included (and their versions), see the file located at /opt/chef-backend/version-manifest.json.\nDo not attempt to upgrade individual components of the Chef package. Due to the way Chef packages are built, modifying any of the individual components in the package will lead to cluster instability. If the latest version of the backend cluster is providing an out-of-date package, please bring it to the attention of Chef by filling out a ticket with support@chef.io.\n\n\n\nchef-backend.rb Options\u00b6\nThe chef-backend.rb file is generated using chef-backend-ctl gen-sample-backend-config and controls most of the various feature and configuration flags going into a Chef HA backend node. A number of these options control the reliability, stability and uptime of the backend PostgreSQL databases, the elastic search index, and the leader election system. Please refrain from changing them unless you have been advised to do so.\n\nfqdn Host name of this node.\nhide_sensitive Set to false if you wish to print deltas of sensitive files and templates during chef-backend-ctl reconfigure runs. true by default.\nip_version Set to either 'ipv4' or 'ipv6'. 'ipv4' by default.\npublish_address Externally resolvable IP address of this back-end node.\n\n\nCommon \u2018Runit\u2019 flags for any backend service\u00b6\nSee https://github.com/chef-cookbooks/runit for details. Many of the flags are repeated across the various backend services - they are only documented once at the top here. The same defaults are used unless specified below.\n\npostgresql.enable Sets up and runs this service. true by default.\npostgresql.environment A hash of environment variables with their values as content used in the service\u2019s env directory.\npostgresql.log_directory The directory where the svlogd log service will run. '/var/log/chef-backend/postgresql/<version>' by default.\npostgresql.log_rotation.file_maxbytes The maximum size a log file can grow to before it is automatically rotated. 104857600 by default (100MB).\npostgresql.log_rotation.num_to_keep The maximum number of log files that will be retained after rotation. 10 by default.\netcd.enable\netcd.log_directory '/var/log/chef-backend/etcd' by default\netcd.log_rotation.file_maxbytes\netcd.log_rotation.num_to_keep\nelasticsearch.enable\nelasticsearch.log_directory '/var/log/chef-backend/elasticsearch' by default. Also affects path.logs in the elastic search configuration yml.\nelasticsearch.log_rotation.file_maxbytes\nelasticsearch.log_rotation.num_to_keep\nleaderl.enable\nleaderl.log_directory '/var/log/chef-backend/leaderl' by default.\nleaderl.start_down Set the default state of the runit service to \u2018down\u2019 by creating <sv_dir>/down file. true by default.\nleaderl.log_rotation.file_maxbytes\nleaderl.log_rotation.num_to_keep\n\n\n\nPostgreSQL settings\u00b6\n\npostgresql.db_superuser Super user account to create. Password is in chef-backend-secrets.json. 'chef_pgsql' by default.\npostgresql.md5_auth_cidr_addresses A list of authorized addresses from which other backend nodes can connect to perform streaming replication. samehost and samenet are special symbols to allow connections from the this node\u2019s IP address and its subnet. You may also use all to match any IP address. You may specify a hostname or IP address in CIDR format (172.20.143.89/32 for a single host, or 172.20.143.0/24 for a small network. See https://www.postgresql.org/docs/9.5/static/auth-pg-hba-conf.html for alternative formats. [\"samehost\", \"samenet\"] by default.\npostgresql.replication_user Username used by postgres streaming replicator when accessing this node. 'replicator' by default.\npostgresql.username 'chef_pgsql' by default.\n\n\n\nPostgreSQL settings given to postgresql.conf\u00b6\nSee https://www.postgresql.org/docs/9.5/static/runtime-config.html for details. Some defaults are provided:\n\npostgresql.archive_command ''\npostgresql.archive_mode 'off'\npostgresql.archive_timeout 0\npostgresql.checkpoint_completion_target 0.5\npostgresql.checkpoint_timeout '5min'\npostgresql.checkpoint_warning '30s'\npostgresql.effective_cache_size Automatically calculated based on available memory.\npostgresql.hot_standby 'on'\npostgresql.keepalives_count 2 Sets tcp_keepalives_count\npostgresql.keepalives_idle 60 Sets tcp_keepalives_idle\npostgresql.keepalives_interval 15 Sets tcp_keepalives_interval\npostgresql.log_checkpoints true\npostgresql.log_min_duration_statement -1\npostgresql.max_connections 350\npostgresql.max_replication_slots 12\npostgresql.max_wal_senders 12\npostgresql.max_wal_size 64\npostgresql.min_wal_size 5\npostgresql.port 5432\npostgresql.shared_buffers Automatically calculated based on available memory.\npostgresql.wal_keep_segments 32\npostgresql.wal_level 'hot_standby'\npostgresql.wal_log_hints on\npostgresql.work_mem '8MB'\n\n\n\netcd settings\u00b6\n\netcd.client_port 2379 Port to use for ETCD_LISTEN_CLIENT_URLS\nand ETCD_ADVERTISE_CLIENT_URLS.\netcd.peer_port 2380 Port to use for ETCD_LISTEN_PEER_URLS and\nETCD_ADVERTISE_PEER_URLS.\n\nThe following settings relate to etcd\u2019s consensus protocol. Chef\nBackend builds its own leader election on top of etcd\u2019s consensus\nprotocol. Updating these settings may be advisable if you are seeing\nfrequent failover events as a result of spurious etcd connection\ntimeouts. The current defaults assume a high-latency environment, such\nthose you might find if deploying Chef Backend to various cloud\nproviders.\n\netcd.heartbeat_interval 500 ETCD_HEARTBEAT_INTERVAL in\nmilliseconds. This is the frequency at which the leader will send\nheartbeats to followers. Etcd\u2019s documentation recommends that this\nis set roughly to the round-trip times between members. (The default\nbefore 1.2 was 100)\netcd.election_timeout 5000 ETCD_ELECTION_TIMEOUT in\nmilliseconds. This controls how long an etcd node will wait for\nheartbeat before triggering an election. Per Etcd\u2019s documentation,\nthis should be 5 to 10 times larger than the\netcd.heartbeat_interval. Increasing etcd.election_timeout\nincreases the time it will take for etcd to detect a\nfailure. (The default value before 1.2 was 1000)\netcd.snapshot_count 5000 ETCD_SNAPSHOT_COUNT which is the number\nof committed transactions to trigger a snapshot to disk.\n\n\nNote\nEven though the defaults assume a high-latency environment, cloud deployments should be restricted to the same datacenter, or in AWS, in the same region. This means that geographically-dispersed cluster deployments are not supported. Multiple Availability Zones are supported as long as they are in the same region.\n\nFor additional information on the etcd tunables, see\nhttps://coreos.com/etcd/docs/latest/tuning.html.\n\n\nElastic Search JVM settings\u00b6\n\nelasticsearch.heap_size Automatically computed by elastic search based on available memory. Specify in MB if you wish to override.\nelasticsearch.java_opts Flags to directly pass to the JVM when launching elastic search. If you override a heap flag here, the setting here takes precedence.\nelasticsearch.new_size Java heap\u2019s new generation size.\n\n\n\nElastic Search configuration\u00b6\nSee https://www.elastic.co/guide/en/elasticsearch/reference/current/settings.html for details.\n\nelasticsearch.plugins_directory '/var/opt/chef-backend/elasticsearch/plugins' Sets path.plugins.\nelasticsearch.port 9200 Sets http.port.\nelasticsearch.scripts_directory '/var/opt/chef-backend/elasticsearch/scripts' Sets path.scripts.\n\n\n\nChef HA backend leader management service settings\u00b6\n\nleaderl.db_timeout Socket timeout when connecting to PostgreSQL\nin milliseconds. 2000 by default.\nleaderl.http_acceptors Http threads that responds to monitoring\nand leadership status requests from HAProxy. 10 by default.\nleaderl.http_address The address that leaderl listens on. This\naddress should not be 127.0.0.1. It should be reachable from\nany front-end node. '0.0.0.0' by default.\nleaderl.http_port 7331 by default.\nleaderl.leader_ttl_seconds The number of seconds it takes the\nleader key to expire. Increasing this value will increase the\namount of time the cluster will take to recognize a failed leader.\nLowering this value may lead to frequent leadership changes and\nthrashing. 30 by default (10 by default before 1.2).\nleaderl.required_active_followers The number of followers that\nmust be syncing via a PostgreSQL replication slot before a new\nleader will return 200 to /leader HTTP requests. If an existing\nleader fails to maintain this quorum of followers, the /leader\nendpoint will return 503 but active connections will still be able\nto complete their writes to the database. 0 by default.\nleaderl.runsv_group The group that sensitive password files will\nbelong to. This is used internally for test purposes and should\nnever be modified otherwise. 'chef_pgsql' by default.\nleaderl.status_internal_update_interval_seconds How often we\ncheck for a change in the leader service\u2019s status. 5 seconds by\ndefault.\nleaderl.status_post_update_interval_seconds How often etcd is\nupdated with the leader service\u2019s current status. 10 seconds by\ndefault.\nleaderl.username 'chef_pgsql'\nleaderl.log_rotation.max_messages_per_second Rate limit for the\nnumber of messages that the Erlang error_logger will output.\n1000 by default.\nleaderl.etcd_pool.ibrowse_options Internal options to affect how\nrequests to etcd are made (see\nhttps://github.com/cmullaparthi/ibrowse/blob/master/doc/ibrowse.html).\nleaderl.epmd_monitor.check_interval How often (in milliseconds)\nto check that leaderl is registered with the Erlang Port Mapping\nDaemon (epmd).  60000 by default.\n\n\n\nChef HA backend leader health status settings\u00b6\n\nleaderl.health_check.interval_seconds How frequently, in\nseconds, to poll the service for health status. We recommend\nsetting this to at least 5 times the value of\nleaderl.leader_ttl_seconds. 5 by default (2 by default before\nversion 1.2)\nleaderl.health_check.max_bytes_behind_leader Limit on maximum different between elected leader and current node in bytes. 52428800 (50MB) by default.\nleaderl.health_check.max_elasticsearch_failures Number of Elastic Search API failures allowed before health check fails. 5 by default.\nleaderl.health_check.max_etcd_failures Number of etcd failures allowed before health check fails. 5 by default.\nleaderl.health_check.max_pgsql_failures Number of PostgreSQL connection failures allowed before health check fails. 5 by default.\nleaderl.health_check.fatal_system_checks Whether or not system check failures (such as disk space failures) will result in the node being marked ineligible for leadership. false by default. Added in Chef Backend 1.4.\nleaderl.health_check.disk_paths An array containing the paths to check for sufficient disk space. [/var/log/chef-backend, /var/opt/chef-backend] by default. Added in Chef Backend 1.4.\nleaderl.health_check.disk_min_space_mb The minimum amount of disk space (in megabytes) required for a disk health check to pass. 250 by default. Added in Chef Backend 1.4.\n\n\n\nChef HA backend leader connection pool settings\u00b6\nSee https://github.com/seth/pooler/blob/master/README.org for details. These are internal settings that affect the responsiveness, uptime and reliability of the backend cluster. They should not be modified unless you are advised to do so by Support.\n\nleaderl.etcd_pool.cull_interval_seconds 60\nleaderl.etcd_pool.http_timeout_ms 5000\nleaderl.etcd_pool.init_count 10\nleaderl.etcd_pool.max_age_seconds 60\nleaderl.etcd_pool.max_connection_duration_seconds 300\nleaderl.etcd_pool.max_count 10\n\n\n\nSSL settings\u00b6\nIf certificate and certificate_key are nil, the SSL Certificate will be auto-generated using the other parameters provided. Otherwise, they are on-disk locations to user-provided certificate.\n\nssl.certificate Provide this path if you have a pre-generated SSL cert.\nssl.certificate_key Provide this path if you have a pre-generated SSL cert.\nssl.ciphers Ordered list of allowed SSL ciphers. This will be updated based on security considerations and the version of OpenSSL being shipped.\nssl.company_name\nssl.country_name\nssl.data_dir Where certificates will be stored. '/var/opt/chef-backend/ssl/' by default\nssl.duration 3650 days by default (10 years).\nssl.key_length 2048 by default.\nssl.organizational_unit_name\n\n\n\n\nchef-backend-ctl\u00b6\nThe Chef Infra Server backend HA cluster includes a command-line utility named chef-backend-ctl. This command-line tool is used to manage the Chef Infra Server backend HA cluster, start and stop individual services, and tail Chef Infra Server log files. For more information, see the chef-backend-ctl documentation.\n\n\n\n\n\u00a9 Copyright: 2019 Chef Software, Inc.\nThis page is about: Current version of Chef.\nProvide feedback on Chef documentation.\nPrivacy policy.\n\n\n\n\n\n\n\n\n", "document_id": 64}]}, {"paragraphs": [{"qas": [{"question": "error: could not create '/Library/Python/2.7/site-packages/xlrd': Permission denied", "id": 1140, "answers": [{"answer_id": 1133, "document_id": 717, "question_id": 1140, "text": "try sudo python setup.py install", "answer_start": 183, "answer_category": null}], "is_impossible": false}], "context": "I'm trying to install xlrd on mac 10.8.4 to be able to read excel files through python.\nI have followed the instructions on http://www.simplistix.co.uk/presentations/python-excel.pdf\ntry sudo python setup.py install\nthe /Library folder needs root permission to be accessed.\n", "document_id": 717}]}, {"paragraphs": [{"qas": [{"question": "where is windows 4 5 bootstrapper package", "id": 1927, "answers": [{"answer_id": 1914, "document_id": 1501, "question_id": 1927, "text": "uninstalling the existing package (via Add or Remove Programs) then rebuilding the installer", "answer_start": 1646, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nJust got the source files for a setup project that I'm trying to run and I get this warning:\n\n\n  WARNING: Could not find prerequisite 'Windows Installer 4.5' in path\n  'c:\\Program Files\\Microsoft SDKs\\Windows\\v6.0A\\Bootstrapper\\'\n\n\nI check and of course there is no folder for the 4.5 installer as it says in the warning. I google and find out that I can't download it separately but it's allegedly included in the Visual Studio distribution. \n\nWell... I'm running VS 2008 and it's not there.  I also looked on another computer that I recently installed VS 2008 express on and it's not there either.\n\nAnyone know where I can get it?\n    \n\nOdd problem, that's a Windows SDK version 7 bootstrapper.  Right-click your Setup project, Properties, Prerequisites button. Does it actually appear in the list? Tick 3.1 instead of 4.5 if it does.\n    \n\nGoogling \"Windows Installer 4.5\" turned up http://www.microsoft.com/download/en/details.aspx?id=8483. This seems to be a common problem that affects other redistributables that specify that directory.\n    \n\nThe 6.0 SDK is for Vista which comes with Windows Installer 4.0. The 4.5 version is distributed by Microsoft as an update.\n\nYou can try installing the 7.0 SDK. It's for Windows 7 which comes with Windows Installer 5.0. So it includes a Windows Installer 4.5 prerequisite package.\n\nBasically, when the 6.0 SDK was released Windows Installer 4.5 didn't exist.\n    \n\nI was having the same issue (x86 path though):\n\n\n  WARNING: Could not find prerequisite 'Windows Installer 4.5' in path 'C:\\Program Files (x86)\\Microsoft SDKs\\ClickOnce Bootstrapper\\'\n\n\nWhat solved it for me was uninstalling the existing package (via Add or Remove Programs) then rebuilding the installer.\n    \n\nI have the same issue, but I can't see the package in Add/Remove Programs. I tried removing the ClickOnce package using the VS Installer and reinstalling, but it didn't fix the issue. I'm removing VS2019 altoghether and will reinstall it to see if that fixes it.\n\nQuite annoying since everything was working perfectly with VS2017. \n    ", "document_id": 1501}]}, {"paragraphs": [{"qas": [{"question": "Pip build option to use multicore", "id": 697, "answers": [{"answer_id": 701, "document_id": 389, "question_id": 697, "text": "1.\tBackup your original make command:\nsudo cp /usr/bin/make /usr/bin/make.bak\n2.\twrite a \"fake\" make command, which will append --jobs=6 to its parameter list and pass them to the original make command make.bak:\nmake.bak --jobs=6 $@", "answer_start": 457, "answer_category": null}], "is_impossible": false}], "context": "\nI found that pip only use single core when it compiles packages. Since some python packages takes some time to build using pip, I'd like to utilize multicore on the machine. How can I achieve same thing for pip?\nThe Ultimate Way to Resolve This Problem\nBecause all the c / cpp files would be compiled by using make commend, and make has an option which specify how many cpu cores shoule be used to compile the source code, we could do some tricks on make.\n1.\tBackup your original make command:\nsudo cp /usr/bin/make /usr/bin/make.bak\n2.\twrite a \"fake\" make command, which will append --jobs=6 to its parameter list and pass them to the original make command make.bak:\nmake.bak --jobs=6 $@\nSo after that, not even compile python with c libs, but also others contain c libs would speed up on compilation by 6 cores. Actually all files compiled by using make command will speed up.\n", "document_id": 389}]}, {"paragraphs": [{"qas": [{"question": "How to get a dependency tree for an artifact?", "id": 1862, "answers": [{"answer_id": 1848, "document_id": 1433, "question_id": 1862, "text": "You should:\n1) Use maven dependency plugin\n2) Find pom.xml of your artifact in maven central repository\n3) Use maven dependency plugin against your artifact", "answer_start": 267, "answer_category": null}], "is_impossible": false}], "context": "dependency:tree can be used to see the dependency tree for a given project. But what I need is to see the dependency tree for a 3rd party artifact.\nI guess I can create an empty project, but I'm looking for something easier (I need to do this for several artifacts).\nYou should:\n1) Use maven dependency plugin\n2) Find pom.xml of your artifact in maven central repository\n3) Use maven dependency plugin against your artifact\n", "document_id": 1433}]}, {"paragraphs": [{"qas": [{"question": "installing graphviz for use with python 3 on windows 10", "id": 1468, "answers": [{"answer_id": 1457, "document_id": 1040, "question_id": 1468, "text": "ork:\n\nimport os\nos.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/", "answer_start": 2713, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nSo I've had Python 3.6 on my Windows 10 computer for a while now, and today I just downloaded and installed the graphviz 0.8.2 (https://pypi.python.org/pypi/graphviz) package via the admin commandline with:\n\npip3 install graphviz\n\nIt was only after this point that I downloaded the Graphviz 2.38 MSI installer file and installed the program at:\n\nC:\\Program Files (x86)\\Graphviz2.38\n\nSo then I tried to run this simple Python program:\n\nfrom graphviz import Digraph\n\ndot = Digraph(comment=\"The round table\")\ndot.node('A', 'King Arthur')\ndot.node('B', 'Sir Bedevere the Wise')\ndot.node('L', 'Sir Lancelot the Brave')\ndot.render('round-table.gv', view=True)\n\n\nBut unfortunately, I received the following error when I try to run my Python program from commandline:\n\nTraceback (most recent call last):\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\graphviz\\backend.py\", line 124, in render\n    subprocess.check_call(args, startupinfo=STARTUPINFO, stderr=stderr)\n  File \"C:\\Program Files\\Python36\\lib\\subprocess.py\", line 286, in check_call\n    retcode = call(*popenargs, **kwargs)\n  File \"C:\\Program Files\\Python36\\lib\\subprocess.py\", line 267, in call\n    with Popen(*popenargs, **kwargs) as p:\n  File \"C:\\Program Files\\Python36\\lib\\subprocess.py\", line 709, in __init__\n    restore_signals, start_new_session)\n  File \"C:\\Program Files\\Python36\\lib\\subprocess.py\", line 997, in _execute_child\n    startupinfo)\nFileNotFoundError: [WinError 2] The system cannot find the file specified\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"D:\\foldername\\testing.py\", line 11, in &lt;module&gt;\n    dot.render('round-table.gv', view=True)\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\graphviz\\files.py\", line 176, in render\n    rendered = backend.render(self._engine, self._format, filepath)\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\graphviz\\backend.py\", line 127, in render\n    raise ExecutableNotFound(args)\ngraphviz.backend.ExecutableNotFound: failed to execute ['dot', '-Tpdf', '-O', 'round-table.gv'], make sure the Graphviz executables are on your systems' PATH\n\n\nNotice how what I've asked seems VERY similar to this question asked here:\n\"RuntimeError: Make sure the Graphviz executables are on your system's path\" after installing Graphviz 2.38\n\nBut for some reason, adding those paths (suggested in the solutions at the link above) to the system variables isn't working, and I don't know why!  I tried restarting the computer after adding the paths as well, still to no success.  See the image below:\n\n\n\nAlthough the other suggested solution, which was to add these few lines in front of my Python code, did work:\n\nimport os\nos.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'\n\n\nBut here's the issue: I don't understand why adding to the environment variables didn't work, and this is my primary concern.  So my question is this: why did adding those lines of code in front of the Python script work but changing the environment variables didn't?  What do I need to do to get my script to run without adding those lines of code in front?\n    \n\nCan you please post the output you get when you type SET in a cmd window after setting the PATH environment variable?\n\nDoes it contain C:/Program Files (x86)/Graphviz2.38/bin/ ?\n\nA cmd window must be restarted before updated environment variables become effective!\n    ", "document_id": 1040}]}, {"paragraphs": [{"qas": [{"question": "free mulitplatform installer", "id": 1378, "answers": [{"answer_id": 1367, "document_id": 948, "question_id": 1378, "text": "If your project is opensource, you can use BitRock's Install Builder for free", "answer_start": 1051, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nExpanding on Free install wizard software I am looking for a cross platform installer.  We support something like 27 platform variations and most of the installers mentioned in the other question are windows only.  We are looking for something portable, possibly java based but I suppose any scripting language would also work.  Suggestions? \n\nEdit:\nIn order of number of platforms under each OS: linux, solaris, windows, hpux, aix, zlinux, bsd, OSX, OS/360 (I don't care about this one).  So about as much variation as you can get without VMS or mainframe.  A lot of the linux platforms are similar varying only in version of libc.  But we do support 32 and 64 bit on each processor on which the OS runs.  As an example we support 4 solaris platforms: \n\n\nsolaris sparc 32\nsolaris sparc 64\nsolaris x86 32 bit\nsolaris x86 64 bit\n\n    \n\nIzPack ?\n    \n\nI don't know of any free cross-platform ones, sorry. I do know that several commercial companies that I know swear by BitRock. There's quite a few Linux stacks built with it.\n    \n\nIf your project is opensource, you can use BitRock's Install Builder for free. And this page on the wxWidgets site lists alot of different platform installers, describing various issues with each.\n    \n\nYou should take a look at InstallJammer.  Free, open source, and very easy to use while being powerful enough to make just about anything you need.  Supports most platforms out-of-the-box.\n    \n\nI also suggest\n\nhttp://packjacket.sourceforge.net/\n    \n\nHere's a list of some: Open Source Installer Generators in Java.\n    ", "document_id": 948}]}, {"paragraphs": [{"qas": [{"question": "How to create plugin\u2019s scaffolding?", "id": 200634, "answers": [{"answer_id": 239121, "document_id": 357717, "question_id": 200634, "text": "Use [pluginfactory.io] to create your plugin\u2019s scaffolding with just a few clicks.\n", "answer_start": 1181, "answer_category": null}], "is_impossible": false}, {"question": "How to release plugin?", "id": 200637, "answers": [{"answer_id": 239378, "document_id": 357717, "question_id": 200637, "text": "If you\u2019re ready to publicly release your plugin, register it as a new Composer package on Packagist. Then you can install it like any other package, by just passing its package name to Composer\u2019s require command.\n> cd ~/dev/my-craft-project\n> composer require package/name", "answer_start": 8127, "answer_category": null}], "is_impossible": false}, {"question": "Which file is the Primary Plugin Class?", "id": 200638, "answers": [{"answer_id": 239386, "document_id": 357717, "question_id": 200638, "text": "The src/Plugin.php file ", "answer_start": 6026, "answer_category": null}], "is_impossible": false}, {"question": "How to see plugin icon?", "id": 200636, "answers": [{"answer_id": 239128, "document_id": 357717, "question_id": 200636, "text": "Plugins can provide an icon, which will be visible on the Settings \u2192 Plugins page.\n", "answer_start": 8416, "answer_category": null}], "is_impossible": false}, {"question": "What is plugin handle?", "id": 200635, "answers": [{"answer_id": 239126, "document_id": 357717, "question_id": 200635, "text": "Something that uniquely identifies your plugin within the Craft ecosystem. (Plugin handles must begin with a letter and contain only lowercase letters, numbers, and dashes. They should be kebab-cased.)", "answer_start": 1921, "answer_category": null}], "is_impossible": false}], "context": "Intro to Plugin Dev\n\n\nCraft 3 Documentation\n\n\nCraft 2 Documentation\nCraft 3 Documentation\nCraft 2 Class Reference\nCraft 3 Class Reference\n\n\n\n\nIntro to Plugin Dev\n\nWhat are Plugins?\nGetting Started\n\nPreparation\nSetting up the basic file structure\ncomposer.json\nPrimary Plugin Class\nLoading your plugin into a Craft project\n\nPlugin Icons\n\nWhat are Plugins? #\nPlugins are mini applications that run alongside Craft\u2019s core code. They can be simple, serving a single purpose like providing a new Dashboard widget type, or they can be complex, introducing entirely new concepts to the system, like an e-commerce application. Craft\u2019s plugin architecture provides a solid foundation for building just about anything.\nTechnically, plugins are a superset of Yii Modules, which means they can have models, active record classes, controllers, application components, and other things. It wouldn\u2019t hurt to take some time to read up on those concepts if you are new to Yii.\nThe main benefits of Craft Plugins over Yii Modules are:\n\nPlugins can be installed and uninstalled.\nPlugins can have their own migration track.\nPlugins can have their own section in the Control Panel.\n\nGetting Started #\n\nUse [pluginfactory.io] to create your plugin\u2019s scaffolding with just a few clicks.\n\nPreparation #\nBefore you begin working on a plugin, you need to decide on a few things:\n\nPackage name \u2013 Used to name your Composer package. It\u2019s required even if you don\u2019t want to distribute your plugin via Composer. (See Composer\u2019s documentation for details.) We recommend prefixing the second segment (after the /) with craft-, to help identify that this is a Craft plugin. For example, pixelandtonic/craft-recipes.\nNamespace prefix \u2013 Your plugin\u2019s class namespaces will begin with this. (See the PSR-4 autoloading specification for details.) Note that this should not begin with craft\\; use something that identifies you, the developer.\nPlugin handle \u2013 Something that uniquely identifies your plugin within the Craft ecosystem. (Plugin handles must begin with a letter and contain only lowercase letters, numbers, and dashes. They should be kebab-cased.)\nPlugin name \u2013 What your plugin will be called within the Control Panel.\n\nNaming things is one of the two hardest things in computer science, so if you can make a decision on those things, the rest of the plugin should practically write itself.\nSetting up the basic file structure #\nTo create a plugin, create a new directory for it somewhere on your computer. A common approach is to store them in a ~/dev/ folder alongside your Craft projects:\n~/dev/\nmy-craft-project.dev/\nmy-plugin/\ncomposer.json\nsrc/\nPlugin.php\n\n\nThe name of your plugin directory doesn\u2019t matter. Just choose something that is easy to identify.\n\ncomposer.json #\nWhether or not you wish to make your plugin available as a Composer dependency (you probably should), your plugin must have a composer.json file. Craft will check this file to get basic information about the plugin.\nUse this template as a starting point for your composer.json file:\n{\n\"name\": \"package/name\",\n\"description\": \"Your plugin\u2019s package description\",\n\"version\": \"1.0.0\",\n\"type\": \"craft-plugin\",\n\"minimum-stability\": \"dev\",\n\"require\": {\n\"craftcms/cms\": \"^3.0.0-RC1\"\n},\n\"autoload\": {\n\"psr-4\": {\n\"ns\\\\prefix\\\\\": \"src/\"\n}\n},\n\"support\": {\n\"email\": \"you@example.com\"\n},\n\"extra\": {\n\"handle\": \"plugin-handle\",\n\"name\": \"Plugin Name\",\n\"developer\": \"Developer Name\",\n\"developerUrl\": \"https://developer-url.com\"\n}\n}\n\nReplace:\n\npackage/name with your package name.\nns\\\\prefix\\\\ with your namespace prefix. (Use double-backslashes because JSON, and note this must end with \\\\.)\nyou@example.com with your support email.\nplugin-handle with your plugin handle.\nPlugin Name with your plugin name.\nDeveloper Name with your name, or the organization name that the plugin should be attributed to.\nhttps://developer-url.com with the URL to the website the developer name should link to in the Control Panel.\n\nHere\u2019s a full list of the properties that can go in that extra object:\n\nhandle \u2013 The plugin handle (required).\nclass \u2013 The primary Plugin class name. If not set, the installer will look for a Plugin.php file at each of the autoload path roots.\nbasePath \u2013 The base path to your plugin\u2019s source files. This can begin with one of your autoload namespaces, formatted as a Yii alias (e.g. @vendorname/foo). If not set, the directory that contains your primary Plugin class will be used.\nname \u2013 The plugin name. If not set, the package name (sans vendor prefix) will be used.\nversion - The plugin version. If not set, the current package version will be used.\nschemaVersion \u2013 The plugin schema version.\ndescription \u2013 The plugin description. If not set, the main description property will be used.\ndeveloper \u2013 The developer name. If not set, the first author\u2019s name will be used (via the authors property).\ndeveloperUrl \u2013 The developer URL. If not set, the homepage property will be used, or the first author\u2019s homepage (via the authors property).\ndeveloperEmail \u2013 The support email. If not set, the support.email property will be used.\ndocumentationUrl \u2013 The plugin\u2019s documentation URL. If not set, the support.docs property will be used.\nchangelogUrl \u2013 The plugin\u2019s changelog URL (used to show pending plugin updates and their release notes).\ndownloadUrl \u2013 The plugin\u2019s download URL (used to update manual installations of the plugin).\nsourceLanguage \u2013 The plugin\u2019s source language (defaults to en-US).\nhasSettings \u2013 Whether the plugin has settings (should be true or false).\nhasCpSection \u2013 Whether the plugin has its own section in the Control Panel (should be true or false).\ncomponents \u2013 Object defining any component configs that should be present on the plugin.\n\n\nDon\u2019t include composer/installers as a Composer dependency.\n\n\n\nWhile not strictly required by Composer, we recommend you explicitly set the version in your composer.json because it makes a couple things easier on your when developing the plugin. Don\u2019t forget to keep it updated though!\n\nPrimary Plugin Class #\nThe src/Plugin.php file is your plugin\u2019s primary class. It will get instantiated at the beginning of every request. Its init() method is the best place to register event listeners, and any other steps it needs to take to initialize itself.\nUse this template as a starting point for your Plugin.php file:\n<?php\nnamespace ns\\prefix;\n\nclass Plugin extends \\craft\\base\\Plugin\n{\npublic function init()\n{\nparent::init();\n\n// Custom initialization code goes here...\n}\n}\n\nReplace ns\\prefix with your plugin\u2019s namespace prefix.\nLoading your plugin into a Craft project #\nTo get Craft to see your plugin, you will need to install it as a Composer dependency of your Craft project. There are multiple ways to do that:\nPath Repository #\nDuring development, the easiest way to work on your plugin is with a path repository, which will tell Composer to symlink your plugin into the vendor/ folder right alongside other dependencies.\nTo set it up, open your Craft project\u2019s composer.json file and make the following changes:\n\nSet minimum-stability to \"dev\"\nSet prefer-stable to true\nAdd a new path repository record, pointed at your plugin\u2019s root directory.\n\n{\n\"minimum-stability\": \"dev\",\n\"prefer-stable\": true,\n\"repositories\": [\n{\n\"type\": \"path\",\n\"url\": \"../my-plugin\"\n}\n]\n}\n\n\nSet the url value to the absolute or relative path to your plugin\u2019s source directory. (The ../my-plugin example value assumes that the plugin lives in a folder alongside the project\u2019s folder.)\n\nIn your terminal, go to your Craft project and tell Composer to require your plugin. (Use the same package name you gave your plugin in its composer.json file.)\n> cd ~/dev/my-craft-project\n> composer require package/name\n\nComposer\u2019s installation log should indicate that the package was installed via a symlink:\n- Installing package/name (X.Y.Z): Symlinking from ../my-plugin\n\n\nOne caveat of path Composer repositories is that Composer is not too smart about keeping their dependencies updated when calling composer update. You may need to remove and re-require your plugin in your Craft project each time its dependencies change.\n\nPackagist #\nIf you\u2019re ready to publicly release your plugin, register it as a new Composer package on Packagist. Then you can install it like any other package, by just passing its package name to Composer\u2019s require command.\n> cd ~/dev/my-craft-project\n> composer require package/name\n\nPlugin Icons #\nPlugins can provide an icon, which will be visible on the Settings \u2192 Plugins page.\n\nPlugin icons must be square SVG files, saved as icon.svg at the root of your plugin\u2019s source directory (e.g src/).\nIf your plugin has a Control Panel section, you can also give its global nav item a custom icon by saving an icon-mask.svg file in the root of your plugin\u2019s source directory. Note that this icon cannot contain strokes, and will always be displayed in a solid color (respecting alpha transparency).\n\n\n\nCraft 3 Documentation\nIntroduction\n\nAbout Craft CMS\nCode of Conduct\nHow to Use the Documentation\n\nInstalling Craft\n\nServer Requirements\nInstallation\n\nUpgrading & Updating Craft\n\nUpgrading from Craft 2\nUpdating Craft 3\nChanges in Craft 3\n\nGetting Started\n\nThe Pieces of Craft\nDirectory Structure\n\nCore Concepts\n\nSections and Entries\nFields\nTemplates\n\nTwig Primer\n\nCategories\nAssets\nUsers\nGlobals\nTags\nRelations\nRouting\nSearching\nSites\nLocalization\nElement Queries\nContent Migrations\nConfiguration\n\nTemplating\n\nGlobal Variables\nFunctions\nFilters\nTags\nQuerying Elements\nElements\nCommon Examples\n\nPlugin Development\n\nIntro to Plugin Dev\nCoding Guidelines\nUpdating Plugins for Craft 3\nChangelogs and Updates\nPlugin Settings\nControl Panel Section\nAsset Bundles\nServices\nExtending Twig\nWidget Types\nField Types\nVolume Types\nUtility Types\nElement Types\nElement Action Types\nPlugin Migrations\nPublishing to the Plugin Store\n\n\n\u00a9 Pixel & Tonic\n\n\n", "document_id": 357717}]}, {"paragraphs": [{"qas": [{"question": "How to install RVM system requirements without giving sudo access for RVM user", "id": 1692, "answers": [{"answer_id": 1680, "document_id": 1265, "question_id": 1692, "text": "Try this:\nrvm get 1.18.8\nrvm install <whichever-version-you-want>", "answer_start": 719, "answer_category": null}], "is_impossible": false}], "context": "On my Debian server I have a user called \"deployer\" that does not have sudo access, and has RVM installed.\nWhen installing Ruby using \"deployer\", like 1.9.3, it triggers a task to install dependencies\n\"Installing requirements for debian, might require sudo password.\"\nwhich fails and stops installation because \"deployer\" can not sudo.\nI don't want to add \"deployer\" into the sudoers list, and don't want to install RVM for some other user just for a one-time use for installing dependencies.\nWhat is the correct way to install that dependencies? Or how do I list them to install manually?\nThe problem was introduced somewhere in the latest RVM versions. Don't know exactly when, but definitely in the past 3-4 months.\nTry this:\nrvm get 1.18.8\nrvm install <whichever-version-you-want>\nI don't know exactly when on the path between 1.18.8 and 1.20.12 that problem got introduced, but for me the installation works with RVM v1.18.8 and fails with v1.20.12.\n", "document_id": 1265}]}, {"paragraphs": [{"qas": [{"question": "MSVC 2015 Universal CRT for app-local deployment", "id": 1319, "answers": [{"answer_id": 1309, "document_id": 888, "question_id": 1319, "text": "You can download UCRT from Microsoft Donwload Center: https://www.microsoft.com/en-us/download/confirmation.aspx?id=48234", "answer_start": 781, "answer_category": null}], "is_impossible": false}], "context": "It was announced that the Universal CRT would be a re-distributable DLL such that app-local deployment would still be possible.\nI have installed the Visual Studio 2015 Express Edition and I was looking for ucrtbase.dll in the SDK directories, but I could not find anything. The directory that I looked into was \"C:\\Program Files (x86)\\Microsoft SDKs\\Windows Kits\\10\", but that only has the debug version of the CRT under \"Microsoft.UniversalCRT.Debug\".\nIs there a package that I can download that contains the ucrtbase.dll, or perhaps a Visual Studio or SDK update?\nAlso I am not sure at this point whether just the ucrtbase.dll will be sufficient, as the applications seem to be linked to the stub DLLs api-*.dll, and I'm not sure whether those need to be re-distributed as well.\nYou can download UCRT from Microsoft Donwload Center: https://www.microsoft.com/en-us/download/confirmation.aspx?id=48234\n", "document_id": 888}]}, {"paragraphs": [{"qas": [{"question": "How do I install pip on macOS or OS X?", "id": 140, "answers": [{"answer_id": 148, "document_id": 86, "question_id": 140, "text": "You have to run the following command \u201csudo easy_install pip\u201d to install pip on MacOS.", "answer_start": 191, "answer_category": null}], "is_impossible": false}], "context": "It is generally recommended to avoid installing pip on the OS-provided python commands, and to install Python via the https://python.org installers or using something like Homebrew or pyenv. You have to run the following command \u201csudo easy_install pip\u201d to install pip on MacOS.", "document_id": 86}]}, {"paragraphs": [{"qas": [{"question": "programmatically determine what jdk jres are installed on my box", "id": 1374, "answers": [{"answer_id": 1363, "document_id": 943, "question_id": 1374, "text": " bundle a tiny Java application which prints various details such as the running JVM", "answer_start": 1458, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nIs there a standard way to do this?  I realize this can be somewhat platform dependent.  Our product right now is only supported on Windows - so I suppose that's what I'm interested in right now.  The only things I can think of are to either scan the registry or crawl the file system.  Scanning the file system seems like it can take a really long time - and the registry can be unreliable.  Should I do both?  Any other suggestions?  I tried to look for an API to do this with no luck.\n    \n\nSystem.out.println(System.getProperty(\"java.version\"));\n\n\nOther properties here\n    \n\nI would firstly start by looking for the JAVA_HOME environment variable (and possibly JDK_HOME although thats far less common) and then determining what version that is and whether it's a JDK or JRE.\n\nAfter that check for common instead locations. Find out the system's program files directory (don't just assume it's C:\\Program Files even though it is 99.5% of the time) and look for common install locations under that (eg Java).\n\nI wouldn't do an exhaustive search.\n\nIt's worth asking: do you really need to find JDKs this way? Can't you just ask the user what JDK he or she wishes to use, possibly suggesting any easy ones you've found already?\n    \n\nI'd probably go for a combination of looking for the Java installed registry keys and crawling the default locations for installation (which shouldn't take too long).\n\nAn alternative approach would be to bundle a tiny Java application which prints various details such as the running JVM.\n    ", "document_id": 943}]}, {"paragraphs": [{"qas": [{"question": "How to sign an installer to prevent Vista's UAC warning", "id": 1338, "answers": [{"answer_id": 1328, "document_id": 907, "question_id": 1338, "text": "Windows contains a list of Trusted Root Certificate Authorities.(MMC.EXE > Add snap-in > Certificates > Computer Account). Some of those are intended for \"Code Signing\", which means those Certificate Authorities can counter-sign your signature. Verisign is in that list. If you're working on a corporate internet, the companies signature might/can be in there too.", "answer_start": 417, "answer_category": null}], "is_impossible": false}], "context": "I am using visual studio to create the installer but the app was not written in .net. Do I need to sign both the .msi and the .exe contained in the .msi? What kind of key do I need and where should I get it? Currently I am thinking of verisign although they are expensive I need this to be trusted. Once I get the key how do I sign? If I undestand right al.exe and signtool.exe are for .net assemblies right? Thanks!\nWindows contains a list of Trusted Root Certificate Authorities.(MMC.EXE > Add snap-in > Certificates > Computer Account). Some of those are intended for \"Code Signing\", which means those Certificate Authorities can counter-sign your signature. Verisign is in that list. If you're working on a corporate internet, the companies signature might/can be in there too.\n", "document_id": 907}]}, {"paragraphs": [{"qas": [{"question": "Is there a canonical definition of the staging environment? ", "id": 1038, "answers": [{"answer_id": 1033, "document_id": 619, "question_id": 1038, "text": "The book on continuous delivery has become a de facto canonical text on the topic. It is in agreement with your understanding of staging.", "answer_start": 321, "answer_category": null}], "is_impossible": false}], "context": "It is my understanding that the (legitimate) purpose for a staging server is for testing the deployment process, rather than your code's acceptance or function. However, I almost never find people who share this understanding, with the most common alternative being that a staging server is synonymous with a UAT server.\nThe book on continuous delivery has become a de facto canonical text on the topic. It is in agreement with your understanding of staging.\n", "document_id": 619}]}, {"paragraphs": [{"qas": [{"question": "Hot deploy on JBoss - how do I make JBoss \"see\" the change?", "id": 1710, "answers": [{"answer_id": 1698, "document_id": 1283, "question_id": 1710, "text": "you should go to JBos administrative panel (by default localhost:9990), there in profile settings open Core - Deployment Scanners. Turn on Autodeploy-Exploded (set to true), and by your wishes you can set scanner time (by default 5000 ms) to appropriate for your (I set to 2000, for more fast incremental publish in Eclipse when I make changes to projects).", "answer_start": 688, "answer_category": null}], "is_impossible": false}], "context": "I am developing a Java EE application that I deploy over and over again on a local JBoss installation during development. I want to speed up the build by hot deploying my application straight into [JBOSS]/server/default/deploy/myApp\nIt seems to work - but there also seems to be a somewhat arbitrary delay between the hard deploy and when JBoss starts using the new classes. I am not very familiar with JBoss, but I assume it caches classes, and that this is what causes the problem.\nAm I correct, and if so, how do I make JBoss flush it's cache?\nI had the same problem in my bundle: (Eclipse IDE + JBoss server adapter) + JBoss AS 7.0.1 (community project).\nMy solution is very simple - you should go to JBos administrative panel (by default localhost:9990), there in profile settings open Core - Deployment Scanners. Turn on Autodeploy-Exploded (set to true), and by your wishes you can set scanner time (by default 5000 ms) to appropriate for your (I set to 2000, for more fast incremental publish in Eclipse when I make changes to projects). That's it. Now JBoss makes HOT deploy not only for HTML (JSF, XHTML and so on) files, but also takes care of POJO classes (beans and so on) files.\n", "document_id": 1283}]}, {"paragraphs": [{"qas": [{"question": "Installing Ant on Cygwin", "id": 1778, "answers": [{"answer_id": 1764, "document_id": 1349, "question_id": 1778, "text": "Assuming you have a JDK already installed, you can do this:\n$ export ANT_HOME=/cygdrive/c/apache-ant-1.7.1", "answer_start": 334, "answer_category": null}], "is_impossible": false}], "context": "I'm having some trouble figuring out how to install Ant on Cygwin. I want to use Ant to build Nutch. I've looked through a bunch of tutorials but I can't find anything that is low level enough for me to understand. I need something like...\nDownload ant, put it here\nOpen Cygwin\ntype \"export ANT_HOME=...\"\nCan anyone help me out here?\nAssuming you have a JDK already installed, you can do this:\n$ export ANT_HOME=/cygdrive/c/apache-ant-1.7.1\n", "document_id": 1349}]}, {"paragraphs": [{"qas": [{"question": "Gradle: Make a 3rd party jar available to local gradle repository", "id": 836, "answers": [{"answer_id": 831, "document_id": 518, "question_id": 836, "text": "you can include your file system JAR dependencies as:\ndependencies {\n    runtime files('libs/a.jar', 'libs/b.jar')\n    runtime fileTree(dir: 'libs', include: '*.jar')\n}\nyou may change runtime for compile/testCompile/etc..", "answer_start": 606, "answer_category": null}], "is_impossible": false}], "context": "currently, I'm testing Gradle as an alternative to Maven. In my projects, there are some 3rd party jars, which aren't available in any (Maven) repositories. My problem is now, how could I manage it to install these jars into my local .gradle repository. (If it's possible, I don't want to use the local Maven repository, because Gradle should run independently.) At the moment, I get a lot of exceptions because of missing jars. In Maven, it's quite simple by running the install command. However, my Google search for something similar to the Maven install command wasn't successful. Has anybody an idea? you can include your file system JAR dependencies as:\ndependencies {\n    runtime files('libs/a.jar', 'libs/b.jar')\n    runtime fileTree(dir: 'libs', include: '*.jar')\n}\nyou may change runtime for compile/testCompile/etc.. As of October 2014, this is still the case--because gradle does an md5 checksum of your jarfile, you can't simply download it and put it into a directory under .gradle/caches, and gradle doesn't, as far as I can tell, have any tasks which let you take a local file and push that file to its cache.\n", "document_id": 518}]}, {"paragraphs": [{"qas": [{"question": "The engine \"node\" is incompatible with this module", "id": 1889, "answers": [{"answer_id": 1876, "document_id": 1460, "question_id": 1889, "text": "You can try to ignore the engines :\n$ yarn install --ignore-engines", "answer_start": 313, "answer_category": null}], "is_impossible": false}], "context": "I am getting below yarn error when deploying to AWS\nerror fs-extra@7.0.1: The engine \"node\" is incompatible with this module. Expected version \">=6 <7 || >=8\". Got \"7.0.0\"\nAny idea how will this be resolved?\nWill this work out if I specify engine in package.json\n{ \n  \"engines\" : { \n    \"node\" : \">=8.0.0\" \n  }\n}\nYou can try to ignore the engines :\n$ yarn install --ignore-engines\n", "document_id": 1460}]}, {"paragraphs": [{"qas": [{"question": "npm run build not including all the files", "id": 1321, "answers": [{"answer_id": 1311, "document_id": 890, "question_id": 1321, "text": "This usually occurs because of your hosting service provider is not supporting node.js. Please check with your hosting service provider.\nIf they are not supporting node.js then you can try with heroku", "answer_start": 393, "answer_category": null}], "is_impossible": false}], "context": "I am new in ReactJs. This is my first try to develop and deploy on server.\nI have done few pages and wanted to display this project on subdomain so my client can have a track. To deploy on a server I built it using npm run build. This works successfully. I deploy a code on a server. But It is displaying just a index or home page. I guess it is not building inside pages. How can we do that?\nThis usually occurs because of your hosting service provider is not supporting node.js. Please check with your hosting service provider.\nIf they are not supporting node.js then you can try with heroku\n", "document_id": 890}]}, {"paragraphs": [{"qas": [{"question": "node_modules/rxjs/internal/types.d.ts(81,44): error TS1005: ';' expected error after installation of Angular 6", "id": 1599, "answers": [{"answer_id": 1587, "document_id": 1174, "question_id": 1599, "text": "This problem might arise due to version mismatch. To solve your problem you need to do following changes in your package.json file.\nStep 1 : Go to package.json and modify \"rxjs\": \"^6.0.0\" to \"rxjs\": \"6.0.0\"\nStep 2 Run npm install in your project.\nThere is no need to change the typescript version. (Mine: \"typescript\": \"~2.7.2\")\nEdit: If you are using rxjs-compat then you also need to do following in order to fixed the issue. change the rxjs-compat version from \"rxjs-compat\": \"^6.2.2\" to \"rxjs-compat\": \"6.2.2\"", "answer_start": 129, "answer_category": null}], "is_impossible": false}], "context": "I got an error of\nnode_modules/rxjs/internal/types.d.ts(81,44): error TS1005: ';' expected.\nafter the installation of Angular 6.\nThis problem might arise due to version mismatch. To solve your problem you need to do following changes in your package.json file.\nStep 1 : Go to package.json and modify \"rxjs\": \"^6.0.0\" to \"rxjs\": \"6.0.0\"\nStep 2 Run npm install in your project.\nThere is no need to change the typescript version. (Mine: \"typescript\": \"~2.7.2\")\nEdit: If you are using rxjs-compat then you also need to do following in order to fixed the issue. change the rxjs-compat version from \"rxjs-compat\": \"^6.2.2\" to \"rxjs-compat\": \"6.2.2\"\n", "document_id": 1174}]}, {"paragraphs": [{"qas": [{"question": "Uninstall ReSharper 4.5", "id": 874, "answers": [{"answer_id": 869, "document_id": 555, "question_id": 874, "text": "Control Panel\n--> Add Remove Programs\n--> JetBrains ReSharper 4.5\n--> Uninstall", "answer_start": 614, "answer_category": null}], "is_impossible": false}], "context": "109\n\n\n6\nI have ReSharper 4.5 in Visual Studio 2008. Now I want to install ReSharper 5, but I can't do it before I uninstall ReSharper 4.5.\n\nHow can I uninstall ReSharper 4.5?\n\nvisual-studio\nvisual-studio-2008\ninstallation\nresharper\nfailed-installation\nShare\nImprove this question\nFollow\nedited Nov 23 '16 at 8:16\n\nSimon D.\n4,38122 gold badges2626 silver badges5454 bronze badges\nasked Dec 15 '09 at 8:24\n\nAndreyAkinshin\n17.4k2626 gold badges9292 silver badges152152 bronze badges\nis there a simple upgrade feature? or do you need do install from scratch? \u2013 \naron\n Dec 15 '09 at 17:45\nAdd a comment\n5 Answers\n\n219\n\nControl Panel\n--> Add Remove Programs\n--> JetBrains ReSharper 4.5\n--> Uninstall\n\n??\n\nShare\nImprove this answer\nFollow\nanswered Dec 15 '09 at 8:27\n\nstiank81\n24.7k4141 gold badges126126 silver badges201201 bronze badges\n74\nDamn I was looking for everything but JetBrains....why to confuse users? it should be straight forward named \"Resharper\" or \"ReSharper by jetbrains\"or something :D ReSharper is nice but too expensive for a beginner or hobbyst... \u2013 \nSaeid Yazdani\n Nov 10 '11 at 8:02\n1\nAgreed that it might be confusing as you'd typically look for ReSharper - not Jetbrains.. Not too expensive though. I'd gladly buy this on my own if me company didn't buy it for me. VS without R# is not the same.. \u2013 \nstiank81\n Nov 10 '11 at 8:55\n10\nYes, it's ironic that the answer is so simple. But, being able to uninstall from Add Remove programs... that's insane. This is not a standalone program, this is a VS plugin. Why don't they offer some Uninstall plugin menu on Visual Studio. Or even the ReSharper guys themselves. \u2013 \nerandros\n Nov 1 '13 at 17:48\n12\nNow it's called \"JetBrains Products in Visual Studio 20XX\" \u2013 \nLuca Trazzi\n Jan 29 '15 at 12:56\n3\n@Erandros to add insult to injury if your license server is (temporarily) down the plugin still sits there (you have to manually suspend it via Tools > Options) interfering with regular VS usage. Or maybe I just don't remember how VS works without R# ;) \u2013 \ndrzaus\n Apr 20 '15 at 16:42\nShow 1 more comment\n\n4\n\nCan't you just install Resharper 5 directly? It will override the Resharper 4.5\n\nShare\nImprove this answer\nFollow\nanswered Dec 16 '09 at 11:42\n\nGraviton\n78.5k138138 gold badges405405 silver badges582582 bronze badges\nAdd a comment\n\n2\n\nAfter uninstalling Resharper from Control Panel, you might still see it in your visual studio.\n\nSo in order to remove it completely, follow these 2 steps :-\n\nClick on the Resharper 4.5 installer (exe file) and instead of \"Install\", select \"Remove\". It works just like how the Visual Studio installer works, same exe file is used to install/uninstall the software.\n\nRemove the Resharper files from \"JetBrains\" folder in AppData.\n\nAfter doing this, visual studio will ask you to modify some settings. Restore it to default and then you're good to go.\n\nShare\nImprove this answer\nFollow\nanswered Jul 5 '19 at 4:55\n\nAnshul Dahiya\n57044 silver badges99 bronze badges\nAdd a comment\n\n\n2\n\nYou can do it in the Toolbox.\n\nOpen toolbox\nClick on hexagon on a Resharper Tools\nConfigure Resharper Components\nChoose remove Resharper\nClick next, accept and etc.\nShare\nImprove this answer\nFollow\nanswered Nov 25 '20 at 8:42\n\nWootiae\n51711 gold badge55 silver badges1414 bronze badges\nAdd a comment\n\n0\n\nI know this is an old post, but in VS 2017 you have to go to the Extensions panel and try to reinstall the program. When the standalone installer runs, it will give you an option to uninstall. You cannot uninstall it directly from the Extensions panel, nor does it show up in Control Panels > Programs and Features.", "document_id": 555}]}, {"paragraphs": [{"qas": [{"question": "How to install Docker for Mac?", "id": 85, "answers": [{"answer_id": 89, "document_id": 72, "question_id": 85, "text": "le-click Docker.dmg to open the installer, then drag the Docker icon to\nthe Applications folder.\n\n\n\nDouble-click Docker.app in the Applications folder to start Docker. (In the example below, the Applications folder is in \u201cgrid\u201d view mode.)\n\nThe Docker menu in the top status bar indicates that Docker Desktop is running, and accessible from a terminal.\n\nIf you\u2019ve just installed the app, Docker Desktop launches the onboarding tutorial. The tutorial includes a simple exercise to build an example Docker image, run it as a container, push and save the image to Docker Hub.\n\n\n\nClick the Docker menu () to see\nPreferences and other options.\n\n\nSelect About Docker to verify that you have the latest version.\n\n\nC", "answer_start": 2462, "answer_category": null}], "is_impossible": false}, {"question": "What are requirements for Docker on Mac?", "id": 84, "answers": [{"answer_id": 91, "document_id": 72, "question_id": 84, "text": " you experience any issues after upgrading your macOS to version 10.15, you must install the latest version of Docker Desktop to be compatible with this version of macOS.\nNote: Docker supports Docker Desktop on the most recent versions of macOS. Docker Desktop currently supports macOS Mojave and macOS Catalina.\nAs new major versions of macOS are made generally available, Docker stops supporting the oldest version and support the newest version of macOS.\n\n\nAt least 4 GB of RAM.\n\n\nVirtualBox prior to version 4.3.30 must not be installed as it is not compatible with Docker Desktop.\n\n", "answer_start": 1661, "answer_category": null}], "is_impossible": false}, {"question": "How to uninstall Docker Desktop on Mac.", "id": 86, "answers": [{"answer_id": 92, "document_id": 72, "question_id": 86, "text": "the Docker menu, select Troubleshoot and then select Uninstall.\nClick Uninstall to confirm your selection.\n\n\nNo", "answer_start": 3398, "answer_category": null}], "is_impossible": false}], "context": "Install Docker Desktop on Mac\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSearch\n\n\n\n\n\n\n\nToggle navigation\n\n\n\n\n\n\n\nHomeGuidesProduct manualsReferenceSamples\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInstall Docker Desktop on MacEstimated reading time: 6 minutesDocker Desktop for Mac is the Community version of Docker for Mac.\nYou can download Docker Desktop for Mac from Docker Hub.\nDownload from Docker Hub\nBy downloading Docker Desktop, you agree to the terms of the Docker Software End User License Agreement and the Docker Data Processing Agreement.\nWhat to know before you install\ud83d\udd17\nRelationship to Docker Machine: Installing Docker Desktop on Mac does not affect machines you created with Docker Machine. You have the option to copy containers and images from your local default machine (if one exists) to the Docker Desktop HyperKit VM. When\nyou are running Docker Desktop, you do not need Docker Machine nodes running locally (or anywhere else). With Docker Desktop, you have a new, native\nvirtualization system running (HyperKit) which takes the place of the\nVirtualBox system.\nSystem requirements\ud83d\udd17\nYour Mac must meet the following requirements to successfully install Docker Desktop:\n\n\nMac hardware must be a 2010 or a newer model, with Intel\u2019s hardware support for memory management unit (MMU) virtualization, including Extended Page Tables (EPT) and Unrestricted Mode. You can check to see if your machine has this support by running the following command in a terminal: sysctl kern.hv_support\nIf your Mac supports the Hypervisor framework, the command prints kern.hv_support: 1.\n\n\nmacOS must be version 10.14 or newer. That is, Mojave or Catalina. We recommend upgrading to the latest version of macOS.\nIf you experience any issues after upgrading your macOS to version 10.15, you must install the latest version of Docker Desktop to be compatible with this version of macOS.\nNote: Docker supports Docker Desktop on the most recent versions of macOS. Docker Desktop currently supports macOS Mojave and macOS Catalina.\nAs new major versions of macOS are made generally available, Docker stops supporting the oldest version and support the newest version of macOS.\n\n\nAt least 4 GB of RAM.\n\n\nVirtualBox prior to version 4.3.30 must not be installed as it is not compatible with Docker Desktop.\n\n\nWhat\u2019s included in the installer\ud83d\udd17\nThe Docker Desktop installation includes\nDocker Engine, Docker CLI client,\nDocker Compose, Notary, Kubernetes, and Credential Helper.\nInstall and run Docker Desktop on Mac\ud83d\udd17\n\n\nDouble-click Docker.dmg to open the installer, then drag the Docker icon to\nthe Applications folder.\n\n\n\nDouble-click Docker.app in the Applications folder to start Docker. (In the example below, the Applications folder is in \u201cgrid\u201d view mode.)\n\nThe Docker menu in the top status bar indicates that Docker Desktop is running, and accessible from a terminal.\n\nIf you\u2019ve just installed the app, Docker Desktop launches the onboarding tutorial. The tutorial includes a simple exercise to build an example Docker image, run it as a container, push and save the image to Docker Hub.\n\n\n\nClick the Docker menu () to see\nPreferences and other options.\n\n\nSelect About Docker to verify that you have the latest version.\n\n\nCongratulations! You are now successfully running Docker Desktop.\nIf you would like to rerun the tutorial, go to the Docker Desktop menu\nand select Learn.\nUninstall Docker Desktop\ud83d\udd17\nTo unistall Docker Desktop from your Mac:\n\nFrom the Docker menu, select Troubleshoot and then select Uninstall.\nClick Uninstall to confirm your selection.\n\n\nNote: Uninstalling Docker Desktop will destroy Docker containers and images local to the machine and remove the files generated by the application.\n\nSwitch between Stable and Edge versions\ud83d\udd17\nDocker Desktop allows you to switch between Stable and Edge releases. However, you can only have one version of Docker Desktop installed at a time. Switching between Stable and Edge versions can destabilize your development environment, particularly in cases where you switch from a newer (Edge) channel to an older (Stable) channel.\nFor example, containers created with a newer Edge version of Docker Desktop may\nnot work after you switch back to Stable because they may have been created\nusing Edge features that aren\u2019t in Stable yet. Keep this in mind as\nyou create and work with Edge containers, perhaps in the spirit of a playground\nspace where you are prepared to troubleshoot or start over.\nExperimental features are turned on by default on Edge releases. However, when you switch from a Stable to an Edge release, you must turn on the experimental features flag to access experimental features. From the Docker Desktop menu, click Preferences > Command Line and then turn on the Enable experimental features toggle. Click Apply & Restart for the changes to take effect.\nTo safely switch between Edge and Stable versions, ensure you save images and export the containers you need, then uninstall the current version before installing another. For more information, see the section Save and Restore data below.\nSave and restore data\ud83d\udd17\nYou can use the following procedure to save and restore images and container data. For example, if you want to switch between Edge and Stable, or to reset your VM disk:\n\n\nUse docker save -o images.tar image1 [image2 ...] to save any images you\nwant to keep. See save in the Docker\nEngine command line reference.\n\n\nUse docker export -o myContainner1.tar container1 to export containers you\nwant to keep. See export in the\nDocker Engine command line reference.\n\n\nUninstall the current version of Docker Desktop and install a different version (Stable or Edge), or reset your VM disk.\n\n\nUse docker load -i images.tar to reload previously saved images. See\nload in the Docker Engine.\n\n\nUse docker import -i myContainer1.tar to create a filesystem image\ncorresponding to the previously exported containers. See\nimport in the Docker Engine.\n\n\nFor information on how to back up and restore data volumes, see Backup, restore, or migrate data volumes.\nWhere to go next\ud83d\udd17\n\nGetting started provides an overview of Docker Desktop on Mac, basic Docker command examples, how to get help or give feedback, and links to other topics about Docker Desktop on Mac.\nTroubleshooting describes common problems, workarounds, how\nto run and submit diagnostics, and submit issues.\nFAQs provide answers to frequently asked questions.\nRelease notes lists component updates, new features, and\nimprovements associated with Stable releases. For information about Edge releases, see\nEdge release notes.\nGet started with Docker provides a general Docker tutorial.\n\nmac, install, download, run, docker, localRate this page:\u00a0828\u00a0189\u00a0i\n\n\n\n\n\nDocker EngineOverviewInstallInstallation per distroInstall on CentOSInstall on DebianInstall on FedoraInstall on UbuntuInstall binariesOptional post-installation stepsRelease notesPrevious versionsEngine 18.09 release notesEngine 18.06 release notesEngine 18.05 release notesEngine 18.04 release notesEngine 18.03 release notesEngine 18.02 release notesEngine 18.01 release notesEngine 17.12 release notesEngine 17.11 release notesEngine 17.10 release notesEngine 17.09 release notesEngine 17.07 release notesEngine 17.06 release notesEngine 17.05 release notesEngine 17.04 release notesEngine 17.03 release notesEngine 1.13 and earlierDeprecated featuresDocker AppDocker BuildxDocker ContextDocker ScanDocker ComposeOverview of Docker ComposeInstall ComposeRelease notesGetting startedEnvironment variables in ComposeEnvironment fileExtend services in ComposeNetworking in ComposeUsing Compose in productionControl startup orderSample apps with ComposeDocker Compose release notesDocker DesktopOverviewMacInstall Docker Desktop for MacUser manualDeploy on KubernetesLeveraging Multi-CPU architecture supportNetworkingDisk utilizationLogs and troubleshootingFAQsStable release notesEdge release notesWindowsInstall Docker Desktop for WindowsInstall Docker Desktop on Windows HomeUser manualDeploy on KubernetesNetworkingLogs and troubleshootingFAQsStable release notesEdge release notesDocker Desktop WSL 2 backendDashboardOpen source licensingDocker HubQuickstartDocker ID accountsSecurity and authenticationManage access tokensTwo-factor authenticationEnable two-factor authenticationDisable two-factor authenticationRecover your Docker Hub accountGenerate a new recovery codeTeams and organizationsRepositoriesVulnerability scanningOfficial imagesAutomated buildsSet up automated buildsTesting in automated buildsAdvanced automated buildsLink to GitHub and BitBucketWebhooksSlack integrationDownload rate limitAdministrationConvert an account into an organizationDeactivate an account or an organizationBillingOverviewUpgrade your planDowngrade your planAdd seatsRemove seatsPublisher & certified contentOverviewSubmit a product for Docker HubUser FAQsPublisher FAQsCertify images & pluginsCertify logging pluginsTrust chainBring Your Own License (BYOL)Release notesOpen-source projectsNotaryGet started with NotaryUse the Notary clientUnderstand the service architectureRun a Notary serviceNotary changelogConfiguration filesServer configurationSigner configurationClient configurationCommon Server and signer configurationsDocker RegistryRegistry overviewUnderstand the RegistryDeploy a registry serverConfigure a registryWork with notificationsRecipesRecipes overviewAuthenticating proxy with apacheAuthenticating proxy with nginxMirroring Docker HubRunning on macOSGarbage collectionTesting an insecure registryDeprecated featuresCompatibilityGetting helpRelease notesSuperseded products and toolsDocker MachineMachine overviewInstall MachineRelease notesGet started with a local VMProvision hosts in the cloudLearn by exampleProvision Digital Ocean DropletsProvision AWS EC2 instancesMachine concepts and helpMachine (docker-machine) CLIMachine CLI overviewMachine command-line completionactiveconfigcreateenvhelpinspectipkilllsmountprovisionregenerate-certsrestartrmscpsshstartstatusstopupgradeurlMachine driversDrivers overviewDriver options and operating system defaultsAmazon Web ServicesDigital OceanExoscaleGenericGoogle Compute EngineIBM SoftlayerMicrosoft AzureMicrosoft Hyper-VOpenStackOracle VirtualBoxRackspaceVMware FusionVMware vCloud AirVMware vSphereMigrate from Boot2Docker to MachineDocker Toolbox (deprecated)KitematicKitematic user guide: intro & overviewSet up an Nginx web serverSet up a Minecraft ServerCreating a local RethinkDB database for developmentFrequently asked questionsKnown issuesTroubleshooting\n\n\n\n\n\n\n\nEdit this page Request docs changes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn this page:\n\nWhat to know before you install\nSystem requirements\nWhat\u2019s included in the installer\nInstall and run Docker Desktop on Mac\nUninstall Docker Desktop\nSwitch between Stable and Edge versions\n\nSave and restore data\n\n\nWhere to go next\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhy Docker?\nWhat is a Container?\n\n\n\n\nProducts\nDocker Desktop\nDocker Hub\nFeatures\nContainer Runtime\nDeveloper Tools\nKubernetes\n\n\n\n\nDevelopers\nUse Cases\nPlay with Docker\nCommunity\nOpen Source\nDocker Captains\n\n\n\n\nCompany\nAbout Us\nBlog\nCustomers\nPartners\nNewsroom\nCareers\nContact Us\n\n\n\n\n\n\nStatus\nSecurity\nLegal\nContact\n\n\n\n\n\n\n\nCopyright \u00a9 2013-2020 Docker Inc. All rights reserved.\n\n\n\nTwitter\nYoutube\nGitHub\nLinkedin\nFacebook\nSlideshare\nReddit\n\n\n\n\n\n\n\n", "document_id": 72}]}, {"paragraphs": [{"qas": [{"question": "why i dont have permissions to remove six while installing a pip package", "id": 1991, "answers": [{"answer_id": 1977, "document_id": 1575, "question_id": 1991, "text": "hen:\n\nsudo pip install https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.5.0-cp27-none-linux_x86_", "answer_start": 2253, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI've run this command to pip install TensorFlow:\n\npip install https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.5.0-cp27-none-linux_x86_64.whl\n\n\nBut I'm having trouble removing six (to reinstall it anyway?)\n\nDoes anyone have any insight into this problem. pip uninstall six doesn't work either\n\nmy terminal is spitting out:\n\nCollecting tensorflow==0.5.0 from https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.5.0-cp27-none-linux_x86_64.whl\n  Using cached https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.5.0-cp27-none-linux_x86_64.whl\nCollecting six&gt;=1.10.0 (from tensorflow==0.5.0)\n  Using cached six-1.10.0-py2.py3-none-any.whl\nCollecting numpy&gt;=1.9.2 (from tensorflow==0.5.0)\nInstalling collected packages: six, numpy, tensorflow\n  Found existing installation: six 1.9.0\n    Uninstalling six-1.9.0:\nException:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/site-packages/pip/basecommand.py\", line 211, in main\n    status = self.run(options, args)\n  File \"/usr/local/lib/python2.7/site-packages/pip/commands/install.py\", line 311, in run\n    root=options.root_path,\n  File \"/usr/local/lib/python2.7/site-packages/pip/req/req_set.py\", line 640, in install\n    requirement.uninstall(auto_confirm=True)\n  File \"/usr/local/lib/python2.7/site-packages/pip/req/req_install.py\", line 716, in uninstall\n    paths_to_remove.remove(auto_confirm)\n  File \"/usr/local/lib/python2.7/site-packages/pip/req/req_uninstall.py\", line 125, in remove\n    renames(path, new_path)\n  File \"/usr/local/lib/python2.7/site-packages/pip/utils/__init__.py\", line 315, in renames\n    shutil.move(old, new)\n  File \"/usr/local/lib/python2.7/shutil.py\", line 303, in move\n    os.unlink(src)\nOSError: [Errno 13] Permission denied: '/usr/local/lib/python2.7/site-packages/six-1.9.0.dist-info/DESCRIPTION.rst'\n\n    \n\nEnsure that your pip is python2.x for the location field:\n\n$ pip show pip\n---\nMetadata-Version: 2.0\nName: pip\nVersion: 7.1.2\nSummary: The PyPA recommended tool for installing Python packages.\nHome-page: https://pip.pypa.io/\nAuthor: The pip developers\nAuthor-email: python-virtualenv@groups.google.com\nLicense: MIT\nLocation: /usr/local/lib/python2.7/dist-packages\nRequires: \n\n\nThen:\n\nsudo pip install https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.5.0-cp27-none-linux_x86_64.whl\n\n    ", "document_id": 1575}]}, {"paragraphs": [{"qas": [{"question": "Do I have to install sql server on each client if my software uses it?", "id": 1322, "answers": [{"answer_id": 1312, "document_id": 891, "question_id": 1322, "text": "If you use a SQL Server database to persist your data in your application you need\na. client-server infrastructure where your client connect to a central SQL Server via LAN\nb. or use a client installed sql server edition (which is what you want I guess). For this you can use SQL Server Express edition or just a MSDE (Desktop Edition).", "answer_start": 615, "answer_category": null}], "is_impossible": false}], "context": "If we develop some software in c# (or basically .Net), we don't install visual studio to any client. The client just have to have required .Net framework (1.0, 1.1, 2.0, 3.0 etc) installed and we are good to go.\nSame way, if we are making making an app in VC2008, he just needs to have Visual C++ 2008 runtime (available from MS site free, about 4-5Mb). So, basically, we just need runtime environment. but there isn't any SQL server 2008 runtime (or I am not aware of it?).\nSo, my question is if my software is using SQL Server 2008, what runtime (or anything else) will be required on client side for it to work?\nIf you use a SQL Server database to persist your data in your application you need\na. client-server infrastructure where your client connect to a central SQL Server via LAN\nb. or use a client installed sql server edition (which is what you want I guess). For this you can use SQL Server Express edition or just a MSDE (Desktop Edition).\n", "document_id": 891}]}, {"paragraphs": [{"qas": [{"question": "Downloading all maven dependencies to a directory NOT in repository?", "id": 1832, "answers": [{"answer_id": 1818, "document_id": 1403, "question_id": 1832, "text": "The maven dependency plugin can potentially solve your problem.\nIf you have a pom with all your project dependencies specified, all you would need to do is run\nmvn dependency:copy-dependencies", "answer_start": 494, "answer_category": null}], "is_impossible": false}], "context": "I started to convert my project to maven because I needed to use a library that was distributed in binary form over maven only, but after banging my head against the wall on it for far too long I've decided to stop hurting myself and just use Ant. I'd like to just have maven download the jar and all of its transitive dependencies into a directory of my choosing so I can just check them into my SCM as I normally enjoy and be a blissful developer once again.\nAny ideas how to do that easily?\nThe maven dependency plugin can potentially solve your problem.\nIf you have a pom with all your project dependencies specified, all you would need to do is run\nmvn dependency:copy-dependencies\n", "document_id": 1403}]}, {"paragraphs": [{"qas": [{"question": "missing single page application template in mvc4 rc", "id": 1382, "answers": [{"answer_id": 1371, "document_id": 952, "question_id": 1382, "text": "You can get the SPA template in the Fall 2012 update. More information on the template here and here. \n    \n\nIt`s very risky now", "answer_start": 1103, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nWhen I installed MVC4 beta on a development PC, it had a template for SPA (Single Page Application). Today on a different PC with the same setup (VS2010 SP1, win7), I installed MVC4 RC but no longer is the SPA template available (see image).  Any one else having this issue? or is this a documented change that I could not find?\n\n\n    \n\nChanges from ASP.NET MVC 4 Beta\n\nThe major changes from ASP.NET MVC 4 Beta in this release are summarized below:\n\nRemoved ASP.NET Single Page Application: ASP.NET Single Page Application (SPA) shipped with ASP.NET MVC 4 Beta as an early preview of the experience for building applications that include significant client-side interactions using JavaScript. SPA won\u2019t ship with the final MVC 4 release, but will continue to evolve outside of the MVC 4 release. Check out the ASP.NET SPA home page for details.\n\n...\n\nhttp://www.asp.net/whitepapers/mvc4-release-notes\n\nP.S., Wow I am glad that I didn't use it for an app I build right now. I was considering it 3 months ago, but decided not to risk it because MS said that it is experimental.\n    \n\nYou can get the SPA template in the Fall 2012 update. More information on the template here and here. \n    \n\nIt`s very risky now\n\nFor example if you try to run it with System.json for serialization it runs well with WebApi and DBDATAContext methods but it`s impossible to use it with Entities relationship entity circular errors and so on. (There is a solution to change private access of method get, but would be too much work to have to change in all the access methos of all your entities.\n    ", "document_id": 952}]}, {"paragraphs": [{"qas": [{"question": "How to install python3-devel on red hat 7", "id": 1179, "answers": [{"answer_id": 1172, "document_id": 756, "question_id": 1179, "text": "# yum install python3-devel.x86_64 --enablerepo=rhel-7-server-optional-rpms", "answer_start": 955, "answer_category": null}], "is_impossible": false}], "context": "I am trying to install something in my virtual environment, which uses anaconda python 3.6. I get the gcc failed with exit status 1, hinting on the absence of the right python3-devel package, as described in error: command 'gcc' failed with exit status 1 while installing eventlet.\nTo fix the error, I tried to install the python3-devel package on my server running RHEL 7.3. I did yum install python3-devel, but got a 'package not found' error. Then I found https://serverfault.com/questions/710354/repository-for-python3-devel-on-centos-7, which hints to the python34-devel package in the EPEL repository. I installed it using yum, but upon trying to install something in my virtual environment, I still get the gcc failed with exit status 1 error. I thought I might update this for 2020. As of RHEL 7.7, python-devel is not available in EPEL, it has been retired by Fedora Project. All I wanted for today was the python h files, and this got me there:\n# yum install python3-devel.x86_64 --enablerepo=rhel-7-server-optional-rpms\nWe do have one of the Redhat No-Cost Developer licenses, but I am not sure that is required for the optional-rpms.\n", "document_id": 756}]}, {"paragraphs": [{"qas": [{"question": "putpkt: write failed, broken pipe", "id": 566, "answers": [{"answer_id": 569, "document_id": 291, "question_id": 566, "text": "To fix this, there's no need to delete the app, clean or rebuild. Just need to restart the device.", "answer_start": 294, "answer_category": null}], "is_impossible": false}], "context": "I'm using AdHoc deploy to deploy my app on an iPad, and I get this error. I've checked out a few questions in SO that say the same, but the solution has always been restart XCode, Restart iPad, Restart Mac. I've done all three and this error does not go away. Any other tips I need to look at?\nTo fix this, there's no need to delete the app, clean or rebuild. Just need to restart the device.\n", "document_id": 291}]}, {"paragraphs": [{"qas": [{"question": "Installed a package with Anaconda, can't import in Python", "id": 1777, "answers": [{"answer_id": 1763, "document_id": 1348, "question_id": 1777, "text": "You want to use the Python that came when you installed Anaconda. Just add Anaconda path to the beginning of your $PATH. (In order to do this you probably need to edit your ~/.bashrc file (or the equivalent file for your shell) then source ~/.bashrc.", "answer_start": 568, "answer_category": null}], "is_impossible": false}], "context": "Forgive me but I'm new to python. I've installed a package (theano) using conda install theano, and when I type conda list, the package exists\nHowever, when I enter the python interpreter by running python, and try to import it with import theano, I get an error: \"no module named theano\", and when I list all python modules, theano doesn't exist. \nWhat am I missing?\nProbably due to the fact you have multiply python envs installed in your computer. when you do which python you will probably get the native python installed in your computer. that is /usr/bin/python\nYou want to use the Python that came when you installed Anaconda. Just add Anaconda path to the beginning of your $PATH. (In order to do this you probably need to edit your ~/.bashrc file (or the equivalent file for your shell) then source ~/.bashrc.\n", "document_id": 1348}]}, {"paragraphs": [{"qas": [{"question": "Android: Understanding the APK installation process", "id": 721, "answers": [{"answer_id": 724, "document_id": 411, "question_id": 721, "text": "Everything that you want to know, I think, is in the Android develeper website http://developer.android.com/tools/building/index.html and to understand packaging of the app itself, here is the image better (it's not the APK installation process, but it can help you understand the apk structure and instalation)", "answer_start": 1081, "answer_category": null}], "is_impossible": false}], "context": "I am trying to understand the process of how an apk is installed on Android, specifically the Android SDK emulator via adb install (where i am testing).\nIn searching I have found no satisfactory answer outside of \"the apk is simply copied to /data/app and is installed when you run it. This is not satisfactory to me as it does not explain how the apk's icon appears on the menu amongst other issues.\nComing from a Windows background, running an .exe or installer to install a program modifies registries, files, services, etc.... I need to understand if this or something similar occurrs when an apk is installed on Android.\nSo if anyone can explain to me what occurrs specifically when an apk is installed on Android I would greatly appreciate it.\nOn a side note I would also like to know if the Dalvik VM \"zygote\" is involved in the installation or does it occur at the lower linux kernel level?\nMy ultimate goal here is to use strace to caputer the installation process of an apk to document system modifications, file creations, network activity and other events of interest.\nEverything that you want to know, I think, is in the Android develeper website http://developer.android.com/tools/building/index.html and to understand packaging of the app itself, here is the image better (it's not the APK installation process, but it can help you understand the apk structure and instalation). You can also google about aapt tool and .dex to understand more because classes.dex is the substruction of your application - contains the java and classes compiled. One dex file contains multiple classes as opposed to java class file which contain only that one class. dex file is java bytecode converted with DX tool which is integral part of Android SDK. As a result of that, it allows every application to run as its own process with its own instance of the Dalvik virtual machine.\n", "document_id": 411}]}, {"paragraphs": [{"qas": [{"question": "How to deploy correctly when using Composer's develop / production switch?", "id": 1654, "answers": [{"answer_id": 1642, "document_id": 1228, "question_id": 1654, "text": "You can do this with\ncomposer update --no-dev", "answer_start": 1071, "answer_category": null}], "is_impossible": false}], "context": "Composer has the option to load several dependencies only while being in development, so the tools will not be installed in production (on the live server). This is (in theory) very handy for scripts that only make sense in development, like tests, fake-data-tools, debugger, etc. Composer has changed the behaviour of install and update dramatically in 2013, require-dev-dependencies are now installed by default (!), feel free to create a composer.json with a require-dev block and perform an composer install to reproduce.\nAs the most accepted way to deploy is to push the composer.lock (that holds your current composer setup) and then do an composer install on the production server, this will also install the development stuff.\nWhat's the correct way to deploy this without installing the -dev dependencies ?\nNote: I'm trying to create a canonical Q/A here to clarify the weird Composer deployment. Feel free to edit this question.\nWhen you want to deploy to production, you'll need to make sure composer.lock doesn't have any packages that came from require-dev.\nYou can do this with\ncomposer update --no-dev\nOnce you've tested locally with --no-dev you can deploy everything to production and install based on the composer.lock. \n", "document_id": 1228}]}, {"paragraphs": [{"qas": [{"question": "how to create complete installer for asp net web application", "id": 1957, "answers": [{"answer_id": 1943, "document_id": 1539, "question_id": 1957, "text": " /&gt;\n\n    \n\nYou can use the Professional edition of Adva", "answer_start": 1612, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI'm programmed a web application on visual studio by ASP.Net C# and local database. then i want to create installer for this web application to do these actions on a target PC:\n\n\nInstall requirements for database\nInstall IIS on PC\nPublish my Web Application as an website on IIS\n\n\nNOTE\n\nActually, this web application should be run on a LAN that consist 3 PC and one router. one of that PC's should be a server and the web application must be installed on that system, then the other PC's getting to use from web application on a LAN.\n\nUpdate\n\nI'm used from Advanced Installer and setup works fine. but when i start the website in client system, always get this error:\n\nA network-related or instance-specific error occurred while establishing a connection to SQL Server. The server was not found or was not accessible. Verify that the instance name is correct and that SQL Server is configured to allow remote connections. (provider: SQL Network Interfaces, error: 50 - Local Database Runtime error occurred. Cannot create an automatic instance. See the Windows Application event log for error details.\n\nand this is my connection string in web.config:\n\n&lt;add \n  name=\"SetupTestEntities\" \n  connectionString=\"metadata=\n  res://*/Model.csdl|\n  res://*/Model.ssdl|\n  res://*/Model.msl;\n  provider=System.Data.SqlClient;\n  provider connection string=&amp;quot;\n  data source=(LocalDB)\\v11.0;\n  attachdbfilename=|DataDirectory|\\SetupTest.mdf;\n  integrated security=True;\n  connect timeout=30;\n  MultipleActiveResultSets=True;App=EntityFramework&amp;quot;\" providerName=\"System.Data.EntityClient\" /&gt;\n\n    \n\nYou can use the Professional edition of Advanced Installer. This edition has support to install:\n\n\nIIS web sites, app pools, web apps and virtual directories\ninstall windows features without writing any scripting\ninstall prerequisites, thus your requirements for your database\n\n\nAdvanced Installer also supports running SQL scripts, so you could create on the fly your databases and populate them with your defaults, but this feature is available in the Enterprise edition. \n\nIn the end Advanced Installer will build and MSI or EXE installer as output (depending on what you want), that you can use to deliver your application to any machine.\n\n(disclaimer: I work on this product)\n    ", "document_id": 1539}]}, {"paragraphs": [{"qas": [{"question": "iOS App deployment without AppStore", "id": 541, "answers": [{"answer_id": 543, "document_id": 266, "question_id": 541, "text": "If you intend on developing an application for some other company and their employees, then your only viable option is to sign the final build with a signing certificate attached to said company's development account. The enterprise signing route is a really great approach, if you can get the company to sign all the paperwork to get their own developer account, owned by them.", "answer_start": 444, "answer_category": null}], "is_impossible": false}], "context": "I'm developping an App in my company. We want to distribute this App to our customers but without using the AppStore from Apple, is it possible?\nI heard about MDM (mobile device manager) but I'm not really sure if it will cover this need?\nI heard also about Enterprise developer license for in house deployment but if I'm understanding correctly it means the App can be deployed only inside my company and not to our customers, is it correct? \nIf you intend on developing an application for some other company and their employees, then your only viable option is to sign the final build with a signing certificate attached to said company's development account. The enterprise signing route is a really great approach, if you can get the company to sign all the paperwork to get their own developer account, owned by them.\n", "document_id": 266}]}, {"paragraphs": [{"qas": [{"question": "'Desynchronize' a PHP config file with Git", "id": 1277, "answers": [{"answer_id": 1269, "document_id": 848, "question_id": 1277, "text": "First you should remove config.php from the repository, then create a file named \".gitignore\" and add the filename \"config.php\" to it.", "answer_start": 1139, "answer_category": null}], "is_impossible": false}], "context": "I'm currently working on a PHP site, and am using Git both locally (development) and on the production site (a repo on my web server uses a post-recieve hook to deploy to the web root), the details of which are outlined at toroid.org's 'Using Git to manage a web site' article.\nIssue is that I also have a config.php file (of sorts) that I use for connecting to the database that differs between my local development environment (a local install of MySQL with test data) and my remote 'production' web server (which has it's own 'live' database). Whenever I push my changes to the website however the config file goes with it and replaces my 'live' config with settings connecting to the (nonexistent) development one! I then have to manually SSH to my web server and replace the config.php file, kind of defeating the purpose!\nIs there any way I can get Git to... kind of 'desynchronize' the config.php file? The 'development' config.php should never get saved to the git repo, and when the production server deploys the PHP scripts etc to the web root directory, it should also leave the existing 'production' config.php file untouched.\nFirst you should remove config.php from the repository, then create a file named \".gitignore\" and add the filename \"config.php\" to it.\n", "document_id": 848}]}, {"paragraphs": [{"qas": [{"question": "What is the wix 'KeyPath' attribute?", "id": 897, "answers": [{"answer_id": 892, "document_id": 564, "question_id": 897, "text": "The KeyPath for a Component is a single resource that the Windows Installer uses to determine if a Component \"exists\" on a machine. ", "answer_start": 4028, "answer_category": null}], "is_impossible": false}], "context": "Windows Installer Components Introduction.\n# by Rob Mensching on Saturday, October 4, 2003\n\nIt's late but C89.5's Electrobox is still kicking so I thought I'd sit down and write about one of the topics at the root of my blog \"thought tree\". This thought tree is a text file I've been editing with notepad that captures random questions that I see go by (usually through email at work) whose answers deserve to be written down. The interesting thing is that many answers to seemingly innocuous questions require a bit of depth of understanding. For example, Siew Moi (pronounced \"Sue May\" in American and Australian English for those of you who haven't been formally introduced) asked for more detail about Merge Modules. I thought, sure Merge Modules are a great thing to talk about (and I happen to know a little bit about them) but the first thing I'd want to explain is why Merge Modules were created. To understand why Merge Modules were created you need to know about the Component Rules. To understand the Component Rules, I probably need to explain what Components are with respect to the Windows Installer. Fortunately, Components are one of the \"root thoughts\" of my tree so let's go dig into those and we'll take another step down the path to more advanced topics.\n\nNot too long ago, I talked about resources and how they were the building blocks of a program. At that time, I also indicated that most programs of consequence are made up of a significant number of resources. For example, Microsoft Office Professional 2003 is composed of over 20,000 resources. Granted Office is a suite of several programs that are rather intricate themselves but even smaller line of business applications can have 50 or a 100 resources.\n\nTo help organize all those resources most installation technologies provide some sort of abstraction to group resources together. The Windows Installer calls its abstraction a Component, as if the word \"component\" wasn't overloaded enough in the world of computing. Thus it is important to note that these Windows Installer Components have nothing to do with the Component Object Model (COM) or any other definition of component you may have heard somewhere else. I wish I could say that the System Definition Model (the stuff Microsoft pays me to work on most days) didn't add another definition for component into the mix, but we do. Anyway, my point is that a Windows Installer Component, which I will refer to as just a Component from here out, is an entity all to itself with its own semantics independent from everything else that might share the name \"component\". If we're clear on that, I'll continue defining the Windows Installer Component.\n\nIn the Windows installer, Components are not only a way of collecting a set of resources to help with the organization; Components are the \"atomic units of setup\" in the Windows Installer. Let me come back to that very key statement after providing a few more facts. Components are identified by a globally unique identifier or GUID (pronounced \"goo id\" if you're from Office or \"gwid\" [sounds a lot like squid] if you're from VisualStudio). You'll find I often refer to the GUID of a Component as the ComponentId. Now since the ComponentId is globally unique it is possible to walk up to any machine and ask it, \"Hey, do you have Component X installed?\" This ability is key for more advanced tricks we can play with the Windows Installer that I'll cover in a future blog entry. See, I told you this Component stuff was the \"root\" of a lot of other ideas. <grin/>\n\nAnother important facet of Components is that they are tied to a single directory (a resource). This means that if any files (also resources) are contained in a Component they must all install to the same directory. Not all install engines have this restriction but requiring a Component to install all of its files to a single directory does simplify installation a little bit. This is especially true if you consider how the Windows Installer handles Components' KeyPaths.\n\nThe KeyPath for a Component is a single resource that the Windows Installer uses to determine if a Component \"exists\" on a machine. The KeyPath may be the directory of the Component, or a file contained in the Component, or a registry key contained in the Component, or an ODBC data source contained in the Component. Note that there can only be one KeyPath for a Component. If your Component has two files and three registry keys you have to pick one of those five resources or even the directory of the Component as the KeyPath. There are a lot of other implications to consider when picking the KeyPath of a Component but let me get back to my statement about Components being the \"atomic units of setup\".\n\nWhen the Windows Installer decides it is going to install a Component it installs all of the resources in the Component together. There is one exception to that statement. When installing files that have versions, if a file already exists on the machine and that existing file has a higher version than the file in our Component the Windows Installer will skip over our Component's file. The Installer does this because one of the rules of setup is that files with higher versions should provide all of the functionality of files with lower versions and usually more. Installing a lower versioned file over the top of a higher versioned file very likely will decrease the functionality of the file and break any programs depending on that newer functionality. Of course, you can override that rule (the second axiom of setup at work) and force all the files in a Component to get installed even if some of the files exist with newer versions (note, this downgrading of files is dangerous and definitely should be done only as a last resort to get your program running again).\n\nSo how does the Windows Installer decide it's going to install a Component? Well, by checking for the existence of the KeyPath, of course. By default, the Windows Installer takes the resource that you marked to be the KeyPath for the Component and checks to see if that resource already exists on the machine. If the resource already exists then the Installer decides the Component does not need to be installed and moves on to the next Component's KeyPath. Again, with versioned files the file is determined to exist if a file of equal or higher version is already on the machine.\n\nBut what if your Component has a bunch of resources in it and only the KeyPath resource already exists on the machine. In that case, none of your resources will be installed because the Installer has decided your Component doesn't need to be installed. This further explains why I said that Components are the \"atomic units of setup\". It is very much the case of all or nothing with Components. Therefore it is very important to pick the KeyPaths for your Components carefully.\n\nAnother option is to put only one resource in each Component. This approach will (for the most part) ensure every resource is appropriately tracked. However, Components are also what I like to call the \"primary moving parts\" of the install engine. More Components means more code to churn which translates into slower processing. There are also cases that I'll cover in the future where you cannot split the resources across Components. Ultimately, there is a balance to strike for your setup.\n\nI've talked a lot about install but you are probably wondering about uninstall. Well, uninstall is where it really becomes clear that Components are the \"atomic units of setup\" in the Windows Installer. Remember the ComponentId? Well, the Windows Installer basically counts the number of times it has installed that ComponentId. In other words, every time a program is installed all of the Components that make up the program's setup have the count on their ComponentIds increased by one. This process is called \"reference counting\". Of course, when a program is uninstalled all of the Components have their ComponentIds decreased by one. When the count on a ComponentId reaches zero the Component must no longer be needed and the Windows Installer deletes all of the resources in the Component from the machine.\n\nNow if you are really sharp you'll see a flaw in here. I'm going to save that discussion for later but suffice it to say this flaw provided one of the primary driving forces for Merge Modules.\n\nThere is more to say about Components but I think that's enough for one blog entry. Electrobox ended over two hours ago and in another couple hours the sun will start coming up. Until next time, keep coding, you know I will. <smile/>", "document_id": 564}]}, {"paragraphs": [{"qas": [{"question": "gem install hangs indefinitely", "id": 1959, "answers": [{"answer_id": 1945, "document_id": 1541, "question_id": 1959, "text": ".\n\nWhat I did was put this on my .zshrc file:\n\nrvm use 1.9.3 --defa", "answer_start": 1493, "answer_category": null}], "is_impossible": false}, {"question": "ruby gem install hangs indefinitely", "id": 1960, "answers": [{"answer_id": 1946, "document_id": 1541, "question_id": 1960, "text": "to\nsudo gem update --syst", "answer_start": 2253, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nBackground: I'm a designer that works well with HTML, CSS, and JS. But when it comes to setting up my environment is where I fall short.\n\nI recently purchased a home computer. I want to set up Middleman to use in a project. I already installed rvm and all its requirements. I am on ruby-2.0.0-p0, which from what I understand is the latest stable release.\n\nWhen I attempt to install Middleman, or any other gem for that matter, nothing happens. The cursor just moves to the next line. \n\nSome guidance, or troubleshooting steps, would be greatly appreciated!\n\nThank you, \n\nRicardo\n    \n\nIf gem install is hanging, it's most likely a network, proxy, or firewall issue on your end.\n\nYou can investigate by issuing your gem install command in verbose mode with -V.  It'll show you what URLs it's communicating with to download the gem, and you can hopefully see what it's doing and where it's hanging:\n\n&gt; gem install -V middleman\nHEAD https://rubygems.org/latest_specs.4.8.gz\n302 Moved Temporarily\nHEAD https://s3.amazonaws.com/production.s3.rubygems.org/latest_specs.4.8.gz\n200 OK\nGET https://rubygems.org/latest_specs.4.8.gz\n302 Moved Temporarily\nGET https://s3.amazonaws.com/production.s3.rubygems.org/latest_specs.4.8.gz\n...\n\n\nYou can also check status.rubygems.org where they'll alert you in case the gem/spec servers do have problems (see screenshot below):\n\n\n    \n\nFound my problem! I was runnning ruby 1.8.7. I needed to update my .zshrc file to use 1.9.3 as default.\n\nWhat I did was put this on my .zshrc file:\n\nrvm use 1.9.3 --default\n\n    \n\nYou seem to be on Mac. Have you Xcode installed?\n\nThe cite from MiddleMan official:\n\n\n  Mac OS X comes prepackaged with both Ruby and Rubygems, however, some\n  of the Middleman's dependencies need to be compiled during\n  installation and on OS X that requires Xcode. Xcode can be installed\n  via the Mac App\n  Store.\n  Alternately, if you have a free Apple Developer account, you can just\n  install Command Line Tools for Xcode from their downloads\n  page.\n\n\nBTW, just out of curiousity, is your connection OK? Try to run ping google.com in the sibling terminal during gem install \u2026.\n    \n\nIn case this helps someone, my terminal was hanging on\ngem update --system\n\nand changing it to\nsudo gem update --system\n\nfixed it.\n    ", "document_id": 1541}]}, {"paragraphs": [{"qas": [{"question": "Staging instance on Heroku", "id": 335, "answers": [{"answer_id": 343, "document_id": 147, "question_id": 335, "text": "Your interface to Heroku is essentially a Git branch. The Heroku gem does some work through their API, but within your Git repository, it's just a new remote branch.", "answer_start": 137, "answer_category": null}], "is_impossible": false}], "context": "I'd like to be able to push code to dev.myapp.com for testing and then to www.myapp.com for production use. Is this possible with Heroku?Your interface to Heroku is essentially a Git branch. The Heroku gem does some work through their API, but within your Git repository, it's just a new remote branch.", "document_id": 147}]}, {"paragraphs": [{"qas": [{"question": "PyCharm: Anaconda installation is not found", "id": 1203, "answers": [{"answer_id": 1196, "document_id": 779, "question_id": 1203, "text": "mklink /D \"%HOMEPATH%\\anaconda\" \"C:\\Dev\\Anaconda3\"\n\"C:\\Dev\\Anaconda3\" should be the anaconda installation folder on your PC", "answer_start": 487, "answer_category": null}], "is_impossible": false}], "context": "I had Anaconda on Windows 10 installed in C:\\ProgramData\\Anaconda3 before using PyCharm. Now PyCharm displays: \"Anaconda installation is not found\" when I try using a conda env.\nI also added Anaconda to PATH.\nIs there a way to show PyCharm where Anaconda is installed?\nIn @Ahti Kitsik's answer above, the following line did not work, and resulted in a an error: mklink /D %HOMEPATH%\\anaconda C:\\ProgramData\\Anaconda3\nBecause of a different install location, the following worked for me:\nmklink /D \"%HOMEPATH%\\anaconda\" \"C:\\Dev\\Anaconda3\"\n\"C:\\Dev\\Anaconda3\" should be the anaconda installation folder on your PC.\nAlso, be sure to run the cmd with administrator privilege, otherwise you will get a permission error when trying to create the symlink.\n", "document_id": 779}]}, {"paragraphs": [{"qas": [{"question": "How to not make install step when building external project with cmake?", "id": 1769, "answers": [{"answer_id": 1755, "document_id": 1340, "question_id": 1769, "text": "You almost had it: Instead of INSTALL_COMMAND \"\" put something like\nINSTALL_COMMAND cmake -E echo \"Skipping install step.\"", "answer_start": 162, "answer_category": null}], "is_impossible": false}], "context": "I'm building dependency project with cmake ExternalProject_Add command:\nbut i get error:\nI don't need to make install step, so my question is: how to disable it?\nYou almost had it: Instead of INSTALL_COMMAND \"\" put something like\nINSTALL_COMMAND cmake -E echo \"Skipping install step.\"\n", "document_id": 1340}]}, {"paragraphs": [{"qas": [{"question": "how to Convert style sheets to binary form using command when deploying java applications?", "id": 440, "answers": [{"answer_id": 449, "document_id": 182, "question_id": 440, "text": "javapackager -createbss", "answer_start": 1389, "answer_category": null}], "is_impossible": false}, {"question": "how to Create JAR when deploying java applications?", "id": 441, "answers": [{"answer_id": 450, "document_id": 182, "question_id": 441, "text": "javapackager -createjar", "answer_start": 1598, "answer_category": null}], "is_impossible": false}, {"question": "how to Sign the JAR files when deploying java applications?", "id": 442, "answers": [{"answer_id": 451, "document_id": 182, "question_id": 442, "text": "javapackager -signJar", "answer_start": 1808, "answer_category": null}], "is_impossible": false}], "context": "5.3.1 Java Packaging Tools\nThe recommended way to package Java applications is to use a collection of Ant tasks (ant-javafx.jar), which are provided with the JRE.\n\nNetBeans IDE uses these Ant tasks to package JavaFX and Java SE projects. Embedded packaging support in NetBeans IDE covers most of the typical use cases. However, if you need something special, you can tune packaging by adding custom packaging hooks to the build.xml file, for example, as a -post-jar target.\n\nMost of the other popular IDEs can easily use custom Ant build scripts. Other popular build frameworks, for example Maven or Gradle, support integration with Ant also.\n\nThe JRE includes a command-line packaging utility, javapackager, which can be used for simple packaging tasks. Note that javapackager is a convenience utility and does not provide as much flexibility or as many options as Ant tasks.\n\nTable 5-1 summarizes how to accomplish the build steps using the various packaging tools available. Note that javapackager also provides a -makeall macro command to create a complete application package for simple applications. For more information, see Chapter 9, \"The Java Packager Tool.\"\n\nTable 5-1 Java Packaging Tasks and Tools\n\nTask\tJava Packager Library Ant Task\tJava Packager Tool Command\tNetBeans IDE\nConvert any CSS files to binary format (See Section 5.4, \"Style Sheet Conversion.\")\n\n<fx:csstobin>\n\n\njavapackager -createbss\n\nPackaging category in Project Properties\n\nSelect Binary Encode JavaFX CSS Files check box.\n\nCreate a JAR archive (See Section 5.5, \"Create the Main Application JAR File.\")\n\n<fx:jar>\n\n\njavapackager -createjar\n\nOccurs by default with a Build command, using the configuration in Project Properties.\n\nSign a JAR archive as one binary object (See Section 5.6, \"Sign the JAR Files.\")\n\n<fx:signjar>\n\n\njavapackager -signJar\n\nDeployment category in Project Properties\n\nRequest unrestricted access check box.\n\nTo attach a certificate, click Edit.\n\nAssemble application package for deployment (See Section 5.7, \"Run the Deploy Task or Command\") and Chapter 7, \"Self-Contained Application Packaging.\"", "document_id": 182}]}, {"paragraphs": [{"qas": [{"question": "how to completely remove python from a windows machine?", "id": 871, "answers": [{"answer_id": 866, "document_id": 552, "question_id": 871, "text": "Your problems probably started because your python path is pointing to the wrong one.", "answer_start": 5865, "answer_category": null}], "is_impossible": false}], "context": "How to set the path and environment variables in Windows\nUpdated: 11/06/2021 by Computer Hope\nSetting the path and environment variables differs depending on the version of the Windows operating system you have on your computer. Choose a link below for your version of Windows.\n\nNote\nAdministrator privileges are usually required to modify the path and environment variables.\n\nWindows 10.\nWindows 8.\nWindows Vista and Windows 7.\nWindows 2000 and Windows XP.\nWhat is the default Windows %PATH%?\nSetting %PATH% from the command line.\nSetting the path and variables in Windows 10\nFrom the desktop, right-click the very bottom-left corner of the screen to access the Power User Task Menu.\nIn the Power User Task Menu, select the System option.\nIn the Settings window, scroll down to the Related settings section and click the System info link.\nIn the System window, click the Advanced system settings link in the left navigation pane.\nIn the System Properties window, click the Advanced tab, then click the Environment Variables button near the bottom of that tab.\nIn the Environment Variables window (pictured below), highlight the Path variable in the System variables section and click the Edit button. Add or modify the path lines with the paths you want the computer to access. Each different directory is separated with a semicolon, as shown below.\nC:\\Program Files;C:\\Winnt;C:\\Winnt\\System32\nWindows 10 environmental path settings\n\nNote\nYou can edit other environment variables by highlighting the variable in the System variables section and clicking Edit. If you need to create a new environment variable, click New and enter the variable name and variable value.\n\nTo view and set the path in the Windows command line, use the path command.\n\nSetting the path and variables in Windows 8\nFrom the desktop, right-click the very bottom-left corner of the screen to access the Power User Task Menu.\nIn the Power User Task Menu, select the System option.\nClick the Advanced System Settings link in the left column.\nIn the System Properties window, click the Advanced tab, then click the Environment Variables button near the bottom of that tab.\nIn the Environment Variables window (pictured below), highlight the Path variable in the System variables section and click the Edit button. Add or modify the path lines with the paths you want the computer to access. Each different directory is separated with a semicolon, as shown below.\nC:\\Program Files;C:\\Winnt;C:\\Winnt\\System32\nWindows environmental path settings\n\nNote\nYou can edit other environment variables by highlighting the variable in the System variables section and clicking Edit. If you need to create a new environment variable, click New and enter the variable name and variable value.\n\nTo view and set the path in the Windows command line, use the path command.\n\nSetting the path and variables in Windows Vista and Windows 7\nOn the desktop, right-click the Computer icon and select Properties. If you don't have a Computer icon on your desktop, click Start, right-click the Computer option in the Start menu, and select Properties.\nClick the Advanced System Settings link in the left column.\nIn the System Properties window, click the Advanced tab, then click the Environment Variables button near the bottom of that tab.\nIn the Environment Variables window (pictured below), highlight the Path variable in the System variables section and click the Edit button. Add or modify the path lines with the paths you want the computer to access. Each different directory is separated with a semicolon, as shown below.\nC:\\Program Files;C:\\Winnt;C:\\Winnt\\System32\nWindows environmental path settings\n\nNote\nYou can edit other environment variables by highlighting the variable in the System variables section and clicking Edit. If you need to create a new environment variable, click New and enter the Variable name and Variable value.\n\nTo view and set the path in the Windows command line, use the path command.\n\nSetting the path and variables in Windows 2000 and Windows XP\nThe path is now managed by Windows 2000 and Windows XP and not the autoexec.bat or autoexec.nt files, as was done with earlier versions of Windows. To change the system environment variables, follow the steps below.\n\nFrom the desktop, right-click My Computer and click Properties. If you don't have a My Computer icon on your desktop, click Start, right-click the My Computer option in the Start menu, and select Properties.\nIn the System Properties window, click the Advanced tab.\nIn the Advanced section, click the Environment Variables button.\nIn the Environment Variables window (as shown below), highlight the Path variable in the System Variable section and click the Edit button. Add or modify the path lines with the paths you want the computer to access. Each different directory is separated with a semicolon, as shown below.\nC:\\Program Files;C:\\Winnt;C:\\Winnt\\System32\nWindows environmental path settings\n\nNote\nYou can edit other environment variables by highlighting the variable in the System variables section and clicking Edit. If you need to create a new environment variable, click New and enter the Variable name and Variable value.\n\nTo view and set the path in the Windows command line, use the path command.\n\nWhat is the default Windows %PATH%?\nThe path is based on programs installed on the computer, so there is no \"default path.\" However, the Windows minimum path is often the path below.\n\n%SystemRoot%\\system32;%SystemRoot%;%SystemRoot%\\System32\\Wbem\nNote\nKeep in mind that as you install programs, the path is updated with the paths for the newly installed programs. So, if you have erased your path after installing other programs, those programs may be affected.\n\nSetting path in the MS-DOS and Windows command line\nTo view and set the path in MS-DOS and in the Windows command line, use the path command.\n\nYour problems probably started because your python path is pointing to the wrong one.", "document_id": 552}]}, {"paragraphs": [{"qas": [{"question": "Using Git with shared hosting plan", "id": 588, "answers": [{"answer_id": 594, "document_id": 313, "question_id": 588, "text": "If you have no SSH access to the remote server and the hosting provider doesn't offer a git deploy feature, you won't be able to deploy directly using git.\nA simple search for git-ftp reveals that there are already two projects that seems to do what I suggested:ezyang/git-ftp.", "answer_start": 280, "answer_category": null}], "is_impossible": false}], "context": "I would like to deploy the website and any updates to that webserver, but I was wondering if that was possible with Git with only FTP access? And if not, what would be an advisable way to update my website? Manually drag an dropping files through Filezilla can get a bit tedious.\nIf you have no SSH access to the remote server and the hosting provider doesn't offer a git deploy feature, you won't be able to deploy directly using git.\nA simple search for git-ftp reveals that there are already two projects that seems to do what I suggested:ezyang/git-ftp.\n", "document_id": 313}]}, {"paragraphs": [{"qas": [{"question": "How to add man and zip to \"git bash\" installation on Windows", "id": 761, "answers": [{"answer_id": 761, "document_id": 448, "question_id": 761, "text": "Install 7-zip on windows.\n2.\tadd 7-zip folder (C:\\Program Files\\7-Zip) to PATH\nOn gitbash exp: export PATH=$PATH:\"C:\\Program Files\\7-Zip\" (temporary)\nOn Windows, adding PATH like image below (permanent)\n \n3.\tduplicate a copy of 7z.exe to be zip.exe\n4.\treopen gitbash again. done!", "answer_start": 589, "answer_category": null}], "is_impossible": false}], "context": "I am using git bash on Windows - that is git for Windows via the integrated bash. Apparently it uses the MINGW/MSYS underpinning. (Update from @VonC: It now uses msys2 since msysgit is obsolete since Q4 2015.)\nSo there are already a lot of MSYS tools installed - from awk to zcat. However I miss the man command and zip to compress multiple files into a zip file (unzip exists!).\nWhere from can I install them? I do not want to install another copy of the MINGW system! Any way just to add some pre-compiled tools to the git bash installation? 7-zip can be added to gitbash as follows:\n1.\tInstall 7-zip on windows.\n2.\tadd 7-zip folder (C:\\Program Files\\7-Zip) to PATH\nOn gitbash exp: export PATH=$PATH:\"C:\\Program Files\\7-Zip\" (temporary)\nOn Windows, adding PATH like image below (permanent)\n \n3.\tduplicate a copy of 7z.exe to be zip.exe\n4.\treopen gitbash again. done!\nThis way, it works on my laptop.\nIf you skip step 3. you still can call zip command as 7z instead of zip\nConclusion: Gitbash is running base on windows Path, I think you can run any command that you have added to your Windows PATH.\n", "document_id": 448}]}, {"paragraphs": [{"qas": [{"question": "cannot build r package png fedora 20", "id": 1946, "answers": [{"answer_id": 1933, "document_id": 1526, "question_id": 1946, "text": "t using\n\nsudo apt-get install l", "answer_start": 2037, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI am trying to build the R package png, the repo says that libpng needs to be available.\n\nI run a Linux Fedora 20 distro\n\nLooks like I have it...\n\n[root@localhost bin]# yum install libpng\nLoaded plugins: langpacks, refresh-packagekit\nPackage 2:libpng-1.6.3-3.fc20.x86_64 already installed and latest version\nNothing to do\n\n\nBut when I try to install it:\n\n&gt; install.packages(\"png\")\nInstalling package into \u2018/home/statquant/R/x86_64-redhat-linux-gnu-library/3.0\u2019\n(as \u2018lib\u2019 is unspecified)\ntrying URL 'http://cran.rstudio.com/src/contrib/png_0.1-7.tar.gz'\nContent type 'application/x-gzip' length 24990 bytes (24 Kb)\nopened URL\n==================================================\ndownloaded 24 Kb\n\n* installing *source* package \u2018png\u2019 ...\n** package \u2018png\u2019 successfully unpacked and MD5 sums checked\n** libs\ngcc -m64 -std=gnu99 -I/usr/include/R -DNDEBUG  -I/usr/local/include    `libpng-config --cflags` -fpic  -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches  -m64 -mtune=generic  -c read.c -o read.o\n/bin/sh: libpng-config: command not found\nread.c:3:17: fatal error: png.h: No such file or directory\n #include &lt;png.h&gt;\n                 ^\ncompilation terminated.\nmake: *** [read.o] Error 1\nERROR: compilation failed for package \u2018png\u2019\n* removing \u2018/home/statquant/R/x86_64-redhat-linux-gnu-library/3.0/png\u2019\n\nThe downloaded source packages are in\n        \u2018/tmp/RtmpG5MjG9/downloaded_packages\u2019\nWarning message:\nIn install.packages(\"png\") :\n  installation of package \u2018png\u2019 had non-zero exit status\n\n    \n\nFor some R packages you need the corresponding development library to successfully install the R package.  In your case this should do what you need\n\n# Do the following in your terminal (not in an R session)\nyum install libpng-devel\n\n\nafterwards you should be able to install the R package\n\n# Do the following in the R console (during an R session)\ninstall.packages(\"png\")\n\n    \n\nI had the same problem on Ubuntu (16.04) and solved it using\n\nsudo apt-get install libpng-dev\n\n    ", "document_id": 1526}]}, {"paragraphs": [{"qas": [{"question": "Azure cloud deployment fails : Certificate with thumbprint was not found", "id": 468, "answers": [{"answer_id": 477, "document_id": 201, "question_id": 468, "text": "If you are using Visual Studio then you can fix this error as follows:\nRight click your Web Role / Worker Role (under Roles folder in the cloud project) \u2192 Properties \u2192 Certificates\nClick on the ellipsis button under Thumbprint which will point to your certificate.\nUpload the certificate which shown here to Windows Azure environment (Production or Staging)", "answer_start": 337, "answer_category": null}], "is_impossible": false}], "context": "I am developing a Web API based web service to be hosted on Azure. I am using Azure 1.8 SDK. When I try to deploy my cloud service, it takes a very long time to upload.\nThe certificate used in your project doesn't exist on the cloud environment. Make sure the same certificate used by your project is uploaded to the cloud environment. \nIf you are using Visual Studio then you can fix this error as follows:\nRight click your Web Role / Worker Role (under Roles folder in the cloud project) \u2192 Properties \u2192 Certificates\nClick on the ellipsis button under Thumbprint which will point to your certificate.\nUpload the certificate which shown here to Windows Azure environment (Production or Staging)\n", "document_id": 201}]}, {"paragraphs": [{"qas": [{"question": "cannot download and install scikit learn", "id": 1368, "answers": [{"answer_id": 1357, "document_id": 937, "question_id": 1368, "text": "e commands you should issue:\n\npip install numpy\npip install ", "answer_start": 1125, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI am pretty new to python. I want to use KMean code, and I want to install scikit-learn or sklearn. \n\nI used this code to attempt install these packages:\n\npip install -U sklearn\npip install -U scikit-learn\n\n\nBut I got this error:\n\nCommand /usr/bin/python -c \"import setuptools, tokenize;__file__='/tmp/pip_build_reihaneh/sklearn/setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace('\\r\\n', '\\n'), __file__, 'exec'))\" install --record /tmp/pip-89YQB7-record/install-record.txt --single-version-externally-managed --compile failed with error code 1 in /tmp/pip_build_reihaneh/sklearn\nStoring debug log for failure in /home/reihaneh/.pip/pip.log\n\n\nWhat is the  cause of the problem? \n    \n\npip install -U &lt;package&gt;, short for pip install --upgrade &lt;package&gt;, will upgrade &lt;package&gt; to the most recent stable version in the pip repo.\n\npip install &lt;package&gt; will install the most recent stable version of &lt;package&gt; in the pip repo.\n\nThe difference is upgrading vs. installing. You want the latter. \n\nscikit-learn requires scipy and numpy, so here are the commands you should issue:\n\npip install numpy\npip install scipy\npip install scikit-learn\n\n\nIf you already have any of the dependencies, just plug in a -U between pip install and the package name.\n\nIf you're using Python 3.x, replace pip with pip3.\n    ", "document_id": 937}]}, {"paragraphs": [{"qas": [{"question": "Detecting superfluous #includes in C/C++?", "id": 1827, "answers": [{"answer_id": 1813, "document_id": 1398, "question_id": 1827, "text": "You can use Google's cppclean (links to: download, documentation) it can find several categories of C++ problems, and it can now find superfluous #includes.", "answer_start": 597, "answer_category": null}], "is_impossible": false}], "context": "I often find that the headers section of a file get larger and larger all the time but it never gets smaller. Throughout the life of a source file classes may have moved and been refactored and it's very possible that there are quite a few #includes that don't need to be there and anymore. Leaving them there only prolong the compile time and adds unnecessary compilation dependencies. Trying to figure out which are still needed can be quite tedious.\nIs there some kind of tool that can detect superfluous #include directives and suggest which ones I can safely remove?\nDoes lint do this maybe?\nYou can use Google's cppclean (links to: download, documentation) it can find several categories of C++ problems, and it can now find superfluous #includes.\n", "document_id": 1398}]}, {"paragraphs": [{"qas": [{"question": "can't brew install node", "id": 759, "answers": [{"answer_id": 759, "document_id": 446, "question_id": 759, "text": "Run brew install node\n2.\tMake note of the module that is causing the issue and delete it. In your case, this would be /usr/local/include/node\n3.\tUninstall - brew uninstall node\n4.\tRe-install - brew install node", "answer_start": 613, "answer_category": null}], "is_impossible": false}], "context": "I am trying to upgrade my node via homebrew but ran into a problem because I think I tried to download through the website. I am very new to terminal. I've tried to search for answers but many people say you can just delete the unbrewed header files. I am not sure how to do that. but for some reason when I try to brew install node, it says the brew link step did not complete successfully.\nI also tried to brew link node but that doesn't work either. I ran out of options so I came here. I ran into this same issue under the same circumstances. I was able to get the brew version to install successfully by:\n1.\tRun brew install node\n2.\tMake note of the module that is causing the issue and delete it. In your case, this would be /usr/local/include/node\n3.\tUninstall - brew uninstall node\n4.\tRe-install - brew install node\nI had to repeat this process a few times for each remaining problematic directory (about 3 different ones in actuality), and the install eventually succeeded without any issues.\n", "document_id": 446}]}, {"paragraphs": [{"qas": [{"question": "How to implement a limited feature rollout (language agnostic) to your users?", "id": 1091, "answers": [{"answer_id": 1083, "document_id": 668, "question_id": 1091, "text": "For the people who are getting the new feature, the site they are on should be as close as possible to the site that everyone will be on once the feature is public.", "answer_start": 394, "answer_category": null}], "is_impossible": false}], "context": "I would like to know some common or best practices of rolling out a new website feature to a select group of the userbase.\nThe users could be, for example, based solely on a percentage of your overall user base (10%). The rollout should be customizable (configurable) and support any number of features. It would also be useful to associate rollouts to specific user roles or privileges (ACL).\nFor the people who are getting the new feature, the site they are on should be as close as possible to the site that everyone will be on once the feature is public.\n", "document_id": 668}]}, {"paragraphs": [{"qas": [{"question": "Installing SciPy with pip", "id": 1594, "answers": [{"answer_id": 1583, "document_id": 1170, "question_id": 1594, "text": "In Ubuntu 10.04 (Lucid), I could successfully pip install scipy (within a virtualenv) after installing some of its dependencies, in particular:\n$ sudo apt-get install libamd2.2.0 libblas3gf libc6 libgc", "answer_start": 150, "answer_category": null}], "is_impossible": false}], "context": "It is possible to install NumPy with pip using pip install numpy.\nIs there a similar possibility with SciPy? (Doing pip install scipy does not work.)\nIn Ubuntu 10.04 (Lucid), I could successfully pip install scipy (within a virtualenv) after installing some of its dependencies, in particular:\n$ sudo apt-get install libamd2.2.0 libblas3gf libc6 libgc\n", "document_id": 1170}]}, {"paragraphs": [{"qas": [{"question": "installer custom action problem cant write to register key", "id": 1967, "answers": [{"answer_id": 1953, "document_id": 1552, "question_id": 1967, "text": "Try to change:\nRegistryKey key = Registry.LocalMachine.OpenSubKey(key_path);  \n\nTo:\nRegistryKey key = Registry.LocalMachine.OpenSubKey(key_path, Microsoft.Win32.RegistryKeyPermissionCheck.ReadWriteSubTree);", "answer_start": 2087, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nIn the Custom Actions editor I've added the custom action to Install and Uninstall stages of the process. In the properties window I've marked the CustomActionData property as :\n\n/TARGETDIR = \"[TARGETDIR]\"\n\n\nI'm hoping that the above passes the installation directory info into the custom action.\n\nThe custom action seems to be firing, but I'm getting the following error message :\n\n\"Error 1001. Can't write to register's key\" (or something like that, I'm translating it from my local language).\n\nWhat am I doing wrong?\n\nusing System;\nusing System.Collections;\nusing System.Collections.Generic;\nusing System.ComponentModel;\nusing System.Configuration.Install;\nusing System.Linq;\n//using System.Windows.Forms;\nusing Microsoft.Win32;\n\nnamespace CustomActions\n{\n    [RunInstaller(true)]\n    public partial class Installer1 : Installer\n    {\n        public override void Install(IDictionary stateSaver)\n        {\n            base.Install(stateSaver);\n\n            const string key_path = \"SOFTWARE\\\\VendorName\\\\MyAppName\";\n            const string key_value_name = \"InstallationDirectory\";\n\n            RegistryKey key = Registry.LocalMachine.OpenSubKey(key_path);\n\n            if (key == null)\n            {\n                key = Registry.LocalMachine.CreateSubKey(key_path);\n            }\n\n            string tgt_dir = Context.Parameters[\"TARGETDIR\"];\n\n            key.SetValue(key_value_name, tgt_dir);\n\n        }\n\n        public override void Uninstall(IDictionary savedState)\n        {\n            base.Uninstall(savedState);\n\n            const string key_path = \"SOFTWARE\\\\VendorName\";\n            const string key_name = \"MyAppName\";\n\n            RegistryKey key = Registry.LocalMachine.OpenSubKey(key_path);\n\n            if (key.OpenSubKey(key_name) != null)\n            {\n                key.DeleteSubKey(key_name);\n            }\n        }\n\n        public override void Rollback(IDictionary savedState)\n        {\n            base.Rollback(savedState);\n        }\n\n\n        public Installer1()\n        {\n            InitializeComponent();\n        }\n    }\n}\n\n    \n\nTry to change:\nRegistryKey key = Registry.LocalMachine.OpenSubKey(key_path);  \n\nTo:\nRegistryKey key = Registry.LocalMachine.OpenSubKey(key_path, Microsoft.Win32.RegistryKeyPermissionCheck.ReadWriteSubTree);\n    ", "document_id": 1552}]}, {"paragraphs": [{"qas": [{"question": "How to install Visual C++ Build tools?", "id": 766, "answers": [{"answer_id": 766, "document_id": 453, "question_id": 766, "text": "you need to include at least the individual components:\n\u2022\tVC++ 2017 version xx.x tools\n\u2022\tWindows SDK to use standard libraries.", "answer_start": 359, "answer_category": null}], "is_impossible": false}], "context": "I need to install Visual C++ Build Tools. When I've download installer, I've tried to install it, however it's telling me I need to uninstall VS 2015!\nHow can I solve it? Why is Visual C++ Build tools telling me it needs to remove current VS 2015 installation? The current version (2019/03/07) is Build Tools for Visual Studio 2017. It's an online installer, you need to include at least the individual components:\n\u2022\tVC++ 2017 version xx.x tools\n\u2022\tWindows SDK to use standard libraries.\nI had the same issue too, the problem is exacerbated with the download link now only working for Visual Studio 2017, and installing the package from the download link did nothing for VS2015, although it took up 5gB of space.\n", "document_id": 453}]}, {"paragraphs": [{"qas": [{"question": "how to install mingw 64 on windows 10", "id": 1406, "answers": [{"answer_id": 1395, "document_id": 979, "question_id": 1406, "text": "You can get a MinGW-w64 build that requires no installation from http://winlibs.com/, just extract the archive and start using it. The site also explains how to use the compiler from Code::Blocks IDE.\n    \n\nTry the MSYS install:\nhttps://www.msys2.org/", "answer_start": 1776, "answer_category": null}], "is_impossible": false}], "context": "\n\n                    \n\n            \n\n        \n\n            \n\n                \n\n                    \n\n                        Closed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.\n\n                        \n\n                    \n\n                \n\n            \n\n        \n\n            \n\n        \n\n                \n\n                    \n\n                \n\n            \n\n                \n\n                    Want to improve this question? Update the question so it's on-topic for Stack Overflow.\n\n                \n\n                    Closed 6 months ago.\n\n            The community is reviewing whether to reopen this question as of 25 days ago.\n\n\n\n            \n\n        \n\n            \n\n                    \n\n                        Improve this question\n\n                    \n\n            \n\n\n\n\n\n    \n\n\n\nI simply want to install MinGW64 to get the 64 bits C compiler. I know there are tons of tutorials on the web but they do not work for me.\n\nI went to: https://sourceforge.net/projects/mingw-w64/ and clicked the download button.\n\nAccording to the tutorials i saw there should be an installer but all i got is a folder named 'mingw-w64-v7.0.0'\n\n\ninside of it there are those folders:\nbuild-aux\nCOPYING.MinGW-w64\nCOPYING.MinGW-w64-runtime\nmingw-w64-crt\nmingw-w64-doc\nmingw-w64-headers\nmingw-w64-libraries\nmingw-w64-tools\nbut i found nowhere any kind of executables or installer, what do i do next ?\nNote I already have the 32 bits version installed in c:\\MinGW,  thank you very much for any help as i am getting really frustrated.\n    \n\nDownload web installer from here:\nhttp://sourceforge.net/projects/mingw-w64/files/Toolchains%20targetting%20Win32/Personal%20Builds/mingw-builds/installer/mingw-w64-install.exe/download\n    \n\nYou can get a MinGW-w64 build that requires no installation from http://winlibs.com/, just extract the archive and start using it. The site also explains how to use the compiler from Code::Blocks IDE.\n    \n\nTry the MSYS install:\nhttps://www.msys2.org/\nThe page shows how to install mingw.\n    ", "document_id": 979}]}, {"paragraphs": [{"qas": [{"question": "Application Installation Failed in Android Studio", "id": 1535, "answers": [{"answer_id": 1524, "document_id": 1112, "question_id": 1535, "text": "You can follow these steps to disable Instant Run from Android Studio:\nFile > Settings > Build,Execution,Deployment > Instant Run > Un-check (Enable Instant Run to hot swap code)", "answer_start": 301, "answer_category": null}], "is_impossible": false}], "context": "Yesterday my app was running perfect from Android Studio but today when I started working on my app and running it i am getting error message continuously\nInstallation failed with message Failed to establish session.\nAnd App is not exist(already Uninstalled) in device. Please suggest me what to do ?\nYou can follow these steps to disable Instant Run from Android Studio:\nFile > Settings > Build,Execution,Deployment > Instant Run > Un-check (Enable Instant Run to hot swap code)\n", "document_id": 1112}]}, {"paragraphs": [{"qas": [{"question": "problem in running java application in non jdk non jre installed system", "id": 1376, "answers": [{"answer_id": 1365, "document_id": 946, "question_id": 1376, "text": "For this to work correctly you must either explicitly invoke the java.exe file in the destination library, or the Java installer must be run to register properly with Windows.\n\nGet the MSI version (for Windows) and tell your installation program to install it.", "answer_start": 1291, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI am working in a java application. It is desktop application for Windows Operating System. I am creating a installer for my application. My application works fine for the system where jdk is insalled. I also want to run my application where jdk/jre is not present.\n\nFor this i am providing jre1.7 along with the installer. I am using advanced installer for creating exe form my application's jar file and providing JRE 1.7 along with all the lib files which requires in my application.\n\nWhen i install application in non jdk/jre system. It gets installed properly but when i try to run the application, it shows \"Missing JRE\",\"Java Runtime Environment Not Found.\" \n\nwhen i install application it gets successfully installed along with JRE 1.7 lib files.\n\nI want my application to recognize JRE 1.7 automatically, which i am providing along with the installer.\n\nPlease guide me that what can be to recognize JRE for my application that i am providing along with the installer.\n\nI thanks to your all valuable suggestions.\n    \n\nIt should work out of the box. Maybe check if you didn't mistakenly bundle an x64 JRE with your app? \n    \n\nYou may want to enclose a Java 6 runtime instead.  Java 7 has not been released yet.\n\nIt sounds like you are just unpacking the files.  For this to work correctly you must either explicitly invoke the java.exe file in the destination library, or the Java installer must be run to register properly with Windows.\n\nGet the MSI version (for Windows) and tell your installation program to install it.\n    ", "document_id": 946}]}, {"paragraphs": [{"qas": [{"question": "Xcode on Mac App Store can't install , show disk space not enough", "id": 1630, "answers": [{"answer_id": 1618, "document_id": 1204, "question_id": 1630, "text": "What you can also do is manually download Xcode and install it. (Without using the app store update) Therefor do the following steps:\ngo to https://developer.apple.com/download/more/\nsearch for Xcode (or latest version e.g Xcode 10.2) and manually download and install it.", "answer_start": 332, "answer_category": null}], "is_impossible": false}], "context": "I'm using the Mac OS to install the XCode10.1.\nI have 18.43GB free disk space in the mac,\nbut when I click the install button on the Xcode from the app store,\nit's always show the \"Not enough storage disk space, you can't install the product\" alert message.\nHow to fix the problem in the MacOS Mojave(10.14.1)?\nThank you very much.\nWhat you can also do is manually download Xcode and install it. (Without using the app store update) Therefor do the following steps:\ngo to https://developer.apple.com/download/more/\nsearch for Xcode (or latest version e.g Xcode 10.2) and manually download and install it.\n", "document_id": 1204}]}, {"paragraphs": [{"qas": [{"question": "wix specify licence shows nothing", "id": 1914, "answers": [{"answer_id": 1901, "document_id": 1486, "question_id": 1914, "text": "ion\n\n\nOpen WordPad\nWrite your text\nSave by default(rtf)\nRebuild your msi\nP", "answer_start": 1730, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI'm trying to specify the licence for my wix setup project.\n\nI have created a rtf with a few dummy lines in wordpad/notepad/vs tried a few different ways as I read there was an issue with ones created in word but I dont think that should apply here, in any case I also opened it up in notepad++ and verified there is no funky characters hidden in it.\n\nI am specifying the file like so\n\n&lt;WixVariable Id=\"WixUILicenseRtf\" Value=\"$(var.ProjectDir)\\Resources\\test.rtf\" /&gt;\n\n\nAnd the file exists under the project\\Resources directory.\n\nWhen I run the installer all that is shown in the licence area is a blank text box with no scroll bars etc.\n\nIs there something else I should be doing?\n    \n\nSave the license.rtf from WordPad.  See this webpage (http://wixtoolset.org/documentation/manual/v3/wixui/wixui_customizations.html), specifically this section:\n\n\n  There is a known issue with the rich\n  text control used to display the text\n  of the license file that can cause the\n  text to appear blank until the user\n  scrolls down in the control. This is\n  typically caused by complex RTF\n  content (such as the RTF generated\n  when saving an RTF file in Microsoft\n  Word). If you run into this behavior\n  in your setup UI, one of the following\n  workarounds will fix it in most cases:\n  \n  Open your RTF file in WordPad and save\n  it from there in order to remove the\n  complex RTF content from the file.\n  After saving it, rebuild your MSI. Use\n  a dialog set other than the\n  WixUI_Minimal set. This problem\n  typically only occurs when the license\n  agreement screen is the first one\n  displayed during setup, which only\n  happens with the WixUI_Minimal dialog\n  set.\n\n    \n\nThanks to @Daniel Powell's decision\n\n\nOpen WordPad\nWrite your text\nSave by default(rtf)\nRebuild your msi\nProfit.\n\n    \n\nOpen trf file in wordpad instead of md-word, It will solve the scroll issue\n    \n\n&lt;WixVariable Id=\"WixUILicenseRtf\" Value=\"test.rtf\" /&gt;\n\n\nand include your test.rtf in the Setup project.\n    ", "document_id": 1486}]}, {"paragraphs": [{"qas": [{"question": "How to run a \".bat\" file during installation?", "id": 1175, "answers": [{"answer_id": 1168, "document_id": 752, "question_id": 1175, "text": "1.\tCreate a setup project and configure as you normally would\n2.\tIn the \"File System\" view add the batch file you want to execute and cmd.exe (C:\\Windows\\System32\\cmd.exe)\n3.\tOpen the \"Custom Actions\" view \n4.\tRight-click on \"Commit\" and choose \"Add Custom Action\"\n5.\tNavigate to and select cmd.exe.\n6.\tOpen the properties panel for the newly created custom action.\n7.\tDelete /Commit from the Arguments property.\n8.\tEnter: /c \"[TARGETDIR]subdirectoryname\\batchfile.bat\" in the Arguments property", "answer_start": 538, "answer_category": null}], "is_impossible": false}], "context": "In a Setup project the executable files such as \".exe , .dll , .js , .vbs\" are acceptable but there is no way to run a .bat file in a Custom Action.\nThe question is how to run the *.bat files during installation?\nHere's the scenario: I have an application I would like to deploy via a Visual Studio Setup project. In addition to my application files, I would like to create a subdirectory in the target directory that contains a batch (.bat) file. I would like this file to run at the end of the installation process.\nHere's what you do:\n1.\tCreate a setup project and configure as you normally would\n2.\tIn the \"File System\" view add the batch file you want to execute and cmd.exe (C:\\Windows\\System32\\cmd.exe)\n3.\tOpen the \"Custom Actions\" view \n4.\tRight-click on \"Commit\" and choose \"Add Custom Action\"\n5.\tNavigate to and select cmd.exe.\n6.\tOpen the properties panel for the newly created custom action.\n7.\tDelete /Commit from the Arguments property.\n8.\tEnter: /c \"[TARGETDIR]subdirectoryname\\batchfile.bat\" in the Arguments property That's it. The batch file will now be executed once the rest of the installation process is completed.\n", "document_id": 752}]}, {"paragraphs": [{"qas": [{"question": "python easy_install fails with \"assembler for architecture ppc not installed\" on Mac OS X", "id": 1182, "answers": [{"answer_id": 1175, "document_id": 758, "question_id": 1182, "text": "sudo env ARCHFLAGS=\"-arch i386\" easy_install whatever", "answer_start": 558, "answer_category": null}], "is_impossible": false}], "context": "I'm a pretty big noob at this stuff (I've learned to use python and unix a bit, but I've never had to deal with installation.) Earlier I was getting an error related to gcc-4.2 not being found, and I found some posts that recommended reinstalling XCode. I went with 4.0 (bad choice?) and now I get this. I've got no idea what to do at this point. This happened for me after having upgraded to XCode 4; I haven't had time to figure out what went wrong during the upgrade (or whether this is the intended behaviour), but the following workaround works for me:\nsudo env ARCHFLAGS=\"-arch i386\" easy_install whatever\n\n", "document_id": 758}]}, {"paragraphs": [{"qas": [{"question": "How do I uninstall a Windows service if the files do not exist anymore?", "id": 849, "answers": [{"answer_id": 844, "document_id": 529, "question_id": 849, "text": "Simply run Autoruns and it shows you the currently configured auto-start applications as well as the full list of Registry and file system locations available for auto-start configuration. ", "answer_start": 1262, "answer_category": null}], "is_impossible": false}], "context": "Autoruns for Windows v14.06\n2021/10/27\n3 \u5206\u949f\u53ef\u770b\u5b8c\n\n\n\n\n\n\n+2\nBy Mark Russinovich\n\nPublished: October 26, 2021\n\nDownload Download Autoruns and Autorunsc (3.7 MB)\nRun now from Sysinternals Live.\n\nIntroduction\nThis utility, which has the most comprehensive knowledge of auto-starting locations of any startup monitor, shows you what programs are configured to run during system bootup or login, and when you start various built-in Windows applications like Internet Explorer, Explorer and media players. These programs and drivers include ones in your startup folder, Run, RunOnce, and other Registry keys. Autoruns reports Explorer shell extensions, toolbars, browser helper objects, Winlogon notifications, auto-start services, and much more. Autoruns goes way beyond other autostart utilities.\n\nAutoruns' Hide Signed Microsoft Entries option helps you to zoom in on third-party auto-starting images that have been added to your system and it has support for looking at the auto-starting images configured for other accounts configured on a system. Also included in the download package is a command-line equivalent that can output in CSV format, Autorunsc.\n\nYou'll probably be surprised at how many executables are launched automatically!\n\nScreenshot\nAutoruns\n\nUsage\nSimply run Autoruns and it shows you the currently configured auto-start applications as well as the full list of Registry and file system locations available for auto-start configuration. Autostart locations displayed by Autoruns include logon entries, Explorer add-ons, Internet Explorer add-ons including Browser Helper Objects (BHOs), Appinit DLLs, image hijacks, boot execute images, Winlogon notification DLLs, Windows Services and Winsock Layered Service Providers, media codecs, and more. Switch tabs to view autostarts from different categories.\n\nTo view the properties of an executable configured to run automatically, select it and use the Properties menu item or toolbar button. If Process Explorer is running and there is an active process executing the selected executable then the Process Explorer menu item in the Entry menu will open the process properties dialog box for the process executing the selected image.\n\nNavigate to the Registry or file system location displayed or the configuration of an auto-start item by selecting the item and using the Jump to Entry menu item or toolbar button, and navigate to the location of an autostart image.\n\nTo disable an auto-start entry uncheck its check box. To delete an auto-start configuration entry use the Delete menu item or toolbar button.\n\nThe Options menu includes several display filtering options, such as only showing non-Windows entries, as well as access to a scan options dialog from where you can enable signature verification and Virus Total hash and file submission.\n\nSelect entries in the User menu to view auto-starting images for different user accounts.\n\nMore information on display options and additional information is available in the on-line help.\n\nAutorunsc Usage\nAutorunsc is the command-line version of Autoruns. Its usage syntax is:\n\nUsage: autorunsc [-a <*|bdeghiklmoprsw>] [-c|-ct] [-h] [-m] [-s] [-u] [-vt] [[-z ] | [user]]]\n\nAUTORUNSC USAGE\nParameter\tDescription\n-a\tAutostart entry selection:\n*\tAll.\nb\tBoot execute.\nd\tAppinit DLLs.\ne\tExplorer addons.\ng\tSidebar gadgets (Vista and higher)\nh\tImage hijacks.\ni\tInternet Explorer addons.\nk\tKnown DLLs.\nl\tLogon startups (this is the default).\nm\tWMI entries.\nn\tWinsock protocol and network providers.\no\tCodecs.\np\tPrinter monitor DLLs.\nr\tLSA security providers.\ns\tAutostart services and non-disabled drivers.\nt\tScheduled tasks.\nw\tWinlogon entries.\n-c\tPrint output as CSV.\n-ct\tPrint output as tab-delimited values.\n-h\tShow file hashes.\n-m\tHide Microsoft entries (signed entries if used with -v).\n-s\tVerify digital signatures.\n-t\tShow timestamps in normalized UTC (YYYYMMDD-hhmmss).\n-u\tIf VirusTotal check is enabled, show files that are unknown by VirusTotal or have non-zero detection, otherwise show only unsigned files.\n-x\tPrint output as XML.\n-v[rs]\tQuery VirusTotal for malware based on file hash. Add 'r' to open reports for files with non-zero detection. Files reported as not previously scanned will be uploaded to VirusTotal if the 's' option is specified. Note scan results may not be available for five or more minutes.\n-vt\tBefore using VirusTotal features, you must accept the VirusTotal terms of service. If you haven't accepted the terms and you omit this option, you will be interactively prompted.\n-z\tSpecifies the offline Windows system to scan.\nuser\tSpecifies the name of the user account for which autorun items will be shown. Specify '*' to scan all user profiles.\nRelated Links\nWindows Internals Book The official updates and errata page for the definitive book on Windows internals, by Mark Russinovich and David Solomon.\nWindows Sysinternals Administrator's Reference The official guide to the Sysinternals utilities by Mark Russinovich and Aaron Margosis, including descriptions of all the tools, their features, how to use them for troubleshooting, and example real-world cases of their use.\nDownload\nDownload Download Autoruns and Autorunsc (3.7 MB)\nRun now from Sysinternals Live.", "document_id": 529}]}, {"paragraphs": [{"qas": [{"question": "The located assembly's manifest definition does not match the assembly reference", "id": 1890, "answers": [{"answer_id": 1877, "document_id": 1461, "question_id": 1890, "text": "You can see https://docs.microsoft.com/archive/blogs/junfeng/the-located-assemblys-manifest-definition-with-name-xxx-dll-does-not-match-the-assembly-reference.", "answer_start": 449, "answer_category": null}], "is_impossible": false}], "context": "I am trying to run some unit tests in a C# Windows Forms application (Visual Studio 2005), and I get the error.\nI look in my references, and I only have a reference to Utility version 1.2.0.203 (the other one is old).\nAny suggestions on how I figure out what is trying to reference this old version of this DLL file?\nBesides, I don't think I even have this old assembly on my hard drive. Is there any tool to search for this old versioned assembly?\nYou can see https://docs.microsoft.com/archive/blogs/junfeng/the-located-assemblys-manifest-definition-with-name-xxx-dll-does-not-match-the-assembly-reference.\n", "document_id": 1461}]}, {"paragraphs": [{"qas": [{"question": "Overwriting an existing Heroku app", "id": 1104, "answers": [{"answer_id": 1096, "document_id": 681, "question_id": 1104, "text": "You would replace the 'project' with your Heroku app name. Secondly you would need to force a push to Heroku.\ngit push -f heroku master", "answer_start": 514, "answer_category": null}], "is_impossible": false}], "context": "I have a Sinatra app, hosted on Heroku. Lately, I've been developing that same app from a different folder. It's not a branch, it's just a parallel app / directory with identical contents but different code. I want to push this new app to Heroku, overwriting the app that's currently there. I don't want to merge the two locally, just continue from the new one while keeping the old. What's the proper command sequence for this? I have doubts about running heroku create, as that will result in a new app. Thanks!\nYou would replace the 'project' with your Heroku app name. Secondly you would need to force a push to Heroku.\ngit push -f heroku master\n", "document_id": 681}]}, {"paragraphs": [{"qas": [{"question": "Where to deploy node.js apps on a Linux server?", "id": 502, "answers": [{"answer_id": 504, "document_id": 228, "question_id": 502, "text": "You cannot deploy nodejs app on a shared server directly, because the shared server is configured with server name not with the port, so instead, you can use PHP to run nodejs app.", "answer_start": 128, "answer_category": null}], "is_impossible": false}], "context": "I have implemented app on adionisjs framework and I want to deploy it on godaddy shared server, is there is any way to do that?\nYou cannot deploy nodejs app on a shared server directly, because the shared server is configured with server name not with the port, so instead, you can use PHP to run nodejs app.\n", "document_id": 228}]}, {"paragraphs": [{"qas": [{"question": "Glassfish DeploymentException: Error in linking security policy for", "id": 525, "answers": [{"answer_id": 527, "document_id": 250, "question_id": 525, "text": "The same thing was happening to me.\nHere is what I did:Stoped the Glassfish server.Deleted all the content from glassfishhome/glassfish/domains/yourdomainname/generated.Started Glassfish", "answer_start": 242, "answer_category": null}], "is_impossible": false}], "context": "I have been trying to deploy my web application (war) from Glassfish AdminConsole but I keep getting the following error message -\nException while loading the app : Error in linking security policy for MyApp-war -- Inconsistent Module State. The same thing was happening to me.\nHere is what I did:Stoped the Glassfish server.Deleted all the content from glassfishhome/glassfish/domains/yourdomainname/generated.Started Glassfish\n", "document_id": 250}]}, {"paragraphs": [{"qas": [{"question": "Most of my assets suddenly return 404 after a push to heroku", "id": 1100, "answers": [{"answer_id": 1092, "document_id": 677, "question_id": 1100, "text": "You should put this line in config/environments/production.rb:\nconfig.assets.compile = true", "answer_start": 187, "answer_category": null}], "is_impossible": false}], "context": "I have deployed this app (rails 3.2.11) a million times, I haven't messed with any settings. After deploying to heroku this morning, it seems that it loads nothing that's inside /assets/\nYou should put this line in config/environments/production.rb:\nconfig.assets.compile = true\n", "document_id": 677}]}, {"paragraphs": [{"qas": [{"question": "How to install PyQt on Ubuntu 12.04 with python 2.7?", "id": 1763, "answers": [{"answer_id": 1750, "document_id": 1335, "question_id": 1763, "text": "You should usually use the Distribution-provided packages - the Ubuntu-provided packages are perfectly adequate for your needs.", "answer_start": 269, "answer_category": null}], "is_impossible": false}], "context": "There may be some similar questions like this one, but I didn't find any answer for me.\nI tried some ways to do it like:\nsudo aptitude install python-qt4-dev python-sip4 python-sip4-dev\nMy question is: What is proper and effective way to install PyQt4 on Ubuntu 12.04?\nYou should usually use the Distribution-provided packages - the Ubuntu-provided packages are perfectly adequate for your needs.\n", "document_id": 1335}]}, {"paragraphs": [{"qas": [{"question": "Installing a windows service from a Visual Studio Installer project", "id": 1157, "answers": [{"answer_id": 1150, "document_id": 734, "question_id": 1157, "text": "To find these right click project->\"view\"->\"custom actions\" under there it needs the primary output added to the folders.", "answer_start": 817, "answer_category": null}], "is_impossible": false}], "context": "A colleague has written a Windows Application and left me to do the installers. I have created the installer project through Visual Studio and added the primary output of the service project to the new project.\nWhen I run the installer it creates the correct folders and copies the dlls, exe and config file in, but it doesn't do the actual install of the service.\nThe service isn't listed in the Services window, and if I double click on the exe I'm told I need to run installutil to install the service.\nHow do I make the installer do this bit for me? I found this article:\nhttp://www.codeproject.com/KB/install/InstallService.aspx\nbut that seems overly complex for what I would expect to be pretty basic.\nI had this issue in my case the problem was I neglected to add the custom actions for the installer project. To find these right click project->\"view\"->\"custom actions\" under there it needs the primary output added to the folders.\n\n", "document_id": 734}]}, {"paragraphs": [{"qas": [{"question": "What are \"Content Files\" (in Visual Studio : Setup Project : File System", "id": 1054, "answers": [{"answer_id": 1050, "document_id": 635, "question_id": 1054, "text": "The build action property of the file will be labeled \"content\". Here is a link to more information about File Properties: https://docs.microsoft.com/en-us/previous-versions/visualstudio/visual-studio-2010/0c6xyb66(v=vs.100)?redirectedfrom=MSDN", "answer_start": 371, "answer_category": null}], "is_impossible": false}], "context": "In the context of a Visual Studio 2008 Setup Project, what are \"Content Files\". In other words, when creating a setup project and defining the File System settings and choosing: Add Project Output > Content Files, what files will be added?\nFor example, what has to be true about a file or its location for it to be considered a Content File for a given project's output?\nThe build action property of the file will be labeled \"content\". Here is a link to more information about File Properties: https://docs.microsoft.com/en-us/previous-versions/visualstudio/visual-studio-2010/0c6xyb66(v=vs.100)?redirectedfrom=MSDN\n", "document_id": 635}]}, {"paragraphs": [{"qas": [{"question": "when using XCode,which variable  provide s the toolset name of cmake", "id": 45, "answers": [{"answer_id": 47, "document_id": 55, "question_id": 45, "text": "CMAKE_XCODE_PLATFORM_TOOLSET ", "answer_start": 264, "answer_category": null}], "is_impossible": false}], "context": "\nBy default Xcode is allowed to select its own default toolchain. The CMAKE_GENERATOR_TOOLSET option may be set, perhaps via the cmake(1) -T option, to specify another toolset.\n\nThe toolset specifies the toolset name.  The selected toolset name is provided in the CMAKE_XCODE_PLATFORM_TOOLSET variable. The key=value pairs form a comma-separated list of options to specify generator-specific details of the toolset selection. Supported pairs are: buildsystem=<variant>Specify the buildsystem variant to use. See the CMAKE_XCODE_BUILD_SYSTEM variable for allowed values. For example, to select the original build system under Xcode 12, run cmake(1) with the option -T buildsystem=1.\n\nWhen using the Xcode generator with Xcode 6.1 or higher, one may enable the Swift language with the enable_languag() command or the project().\n\n\n\n\n\n\n\n\n", "document_id": 55}]}, {"paragraphs": [{"qas": [{"question": "Continuous Deployment with TeamCity", "id": 1266, "answers": [{"answer_id": 1258, "document_id": 837, "question_id": 1266, "text": "You can deploy directly from TeamCity using MS WebDeploy.", "answer_start": 617, "answer_category": null}], "is_impossible": false}], "context": "I recently set up a CI server in TeamCity and now want to take it to the next step, continuous deployment. Basically, we host a suite of restful services and about 3 web applications for each one of our customers. All customers get 3 environments QA, UAT and Prod. We want to be able to automatically deploy our builds once our tests pass. I'm not looking for custom scripting options to do this. I've seen plenty of those of SO. What we're looking for is a solutions like UDeploy but at a lower price point. Is anyone aware of alternatives to UDeploy? Or other Continuous Deployment plugins that work with TeamCity?\nYou can deploy directly from TeamCity using MS WebDeploy.\n", "document_id": 837}]}, {"paragraphs": [{"qas": [{"question": "How can I install the Spring Tool Suite in Ubuntu?", "id": 813, "answers": [{"answer_id": 808, "document_id": 495, "question_id": 813, "text": "(for 64 Bit)\nsudo tar -xvf spring-tool-suite-3.7.0.RELEASE-e4.5-linux-gtk-x86_64.tar.gz\n(or for 32 Bit)\nsudo tar -xvf spring-tool-suite-3.7.0.RELEASE-e4.5-linux-gtk.tar.gz", "answer_start": 138, "answer_category": null}], "is_impossible": false}], "context": "How do I install STS on Ubuntu? I already downloaded the compressed STS tar.gz file from spring.io. Extract tar.gz file wherever you want\n(for 64 Bit)\nsudo tar -xvf spring-tool-suite-3.7.0.RELEASE-e4.5-linux-gtk-x86_64.tar.gz\n(or for 32 Bit)\nsudo tar -xvf spring-tool-suite-3.7.0.RELEASE-e4.5-linux-gtk.tar.gz\nand you can start to use (/sts-bundle/sts-3.7.0.RELEASE/STS).\n", "document_id": 495}]}, {"paragraphs": [{"qas": [{"question": "sslerror using pip install to install tensorflow", "id": 1449, "answers": [{"answer_id": 1438, "document_id": 1022, "question_id": 1449, "text": "py -m pip install --upgrade tensorflow", "answer_start": 2811, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nwhile installing TF, exception appeared: \n\n\n  File\n  \"/usr/local/lib/python2.7/dist-packages/pip/_vendor/cachecontrol/adapter.py\",\n  line 46, in send\n  resp = super(CacheControlAdapter, self).send(request, **kw)   File \"/usr/local/lib/python2.7/dist-packages/pip/_vendor/requests/adapters.py\",\n  line 447, in send\n  raise SSLError(e, request=request) SSLError: (\"bad handshake: Error([('SSL routines', 'SSL3_GET_SERVER_CERTIFICATE', 'certificate\n  verify failed')],)\",)\n\n\ncommand: pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.6.0-cp27-none-linux_x86_64.whl\n\n\nI have no clue how to solve this. I was recently reinstalled pip, could reinstall cause it? \n    \n\nSSL error can be solved by bellow steps for sure. Just download the wheel on your own and pip install.\n\n# Ubuntu/Linux 64-bit, CPU only:\n\n$ wget https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.5.0-cp27-none-linux_x86_64.whl\n\n$ sudo pip install --upgrade tensorflow-0.5.0-cp27-none-linux_x86_64.whl\n\n# Ubuntu/Linux 64-bit, GPU enabled:\n\n$ wget https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.5.0-cp27-none-linux_x86_64.whl\n\n$ sudo pip install --upgrade tensorflow-0.5.0-cp27-none-linux_x86_64.whl\n    \n\nUse the following version of certifi\n\npip2 install 'certifi==2015.4.28' --force-reinstall\n\n\nafter that there will be no more SSL errors.\n\nSolution was found here:\nhttps://github.com/kennethreitz/requests/issues/3212\n    \n\nFor those working on macOS run from a terminal window..\n\n- /Applications/Python\\ 3.6/Install\\Certificates.command\n\n    \n\nThis command worked for me \n\npip3 install --trusted-host pypi.python.org  --upgrade http://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.0.0-py3-none-any.whl\n\n\nI made the request as http instead of https and I was able to bypass the ssl check.  Hope this helps.\n    \n\nI think you need some security certificates.\n\nPlease try the following command.\n\nsudo pip install requests[security]\n\n\nIf you get any error message, please uninstall and reinstall python-requests.\n\nsudo pip uninstall requests\nsudo apt-get install python-requests    \n\n\nI hoped it will give you the certificates you need.\nThanks.\n    \n\nI ran into this problem too, and in my case using curl to download manually didn't fix the problem. Curl reported this:\n\ncurl: (60) SSL certificate problem: certificate is not yet valid\n\n\nThe problem in my case turns out to have been my VM's clock not being in sync with the internet time servers. Resetting my VM's clock so the datetime was correct fixed the problem, and I was able to go right back to installing straight from pip.\n\nI've added this note to a related GitHub issue filed for TensorFlow. This Unix StackExchange question ultimately led me to the answer.\n    \n\nI used Anaconda to run \"py -m pip install --upgrade tensorflow\" and it worked.\n    \n\nTry to upgrade your pip version with another mirror.\npip install --upgrade pip -i https://mirrors.aliyun.com/pypi/simple/\n\n    ", "document_id": 1022}]}, {"paragraphs": [{"qas": [{"question": "\"Build Deployment Package\" VS2010 from script", "id": 1248, "answers": [{"answer_id": 1241, "document_id": 820, "question_id": 1248, "text": "First you need to set up your deployment package settings (if you already have run \"Build Deployment Package\" go to Step 2):\ngo in Project's Properties -> Package/Publish Web and specify the package location.\nSecond if you run this command:\nmsbuild /T:Package", "answer_start": 191, "answer_category": null}], "is_impossible": false}], "context": "How is it possible to build a web service deployment package from script.\nI can msbuild /target:rebuild /p:Configuration=Debug \".\\MyProject.sln\" but it does not build the deployment package.\nFirst you need to set up your deployment package settings (if you already have run \"Build Deployment Package\" go to Step 2):\ngo in Project's Properties -> Package/Publish Web and specify the package location.\nSecond if you run this command:\nmsbuild /T:Package\n", "document_id": 820}]}, {"paragraphs": [{"qas": [{"question": "problem installing rvm", "id": 1475, "answers": [{"answer_id": 1464, "document_id": 1049, "question_id": 1475, "text": "$HOME/.rvm/scripts/rvm\"\n    \n\nI just had a similar problem. \n\nIt turned out that many files in ~/.rvm/scripts/ and ~/.rvm/src/rvm/scripts/ which obviously should be executable did not have execute permissions. Running a script on both directories to set all files to executable solved that immediate problem. \n    \n\nI have got same problem after installation. Then I restarted terminal a", "answer_start": 3267, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI have executed the commands as prescribed in the instructions at the rvm website but things don't seem to work..\n\nFetching the code from the git repository runs smoothly but when I try to use\n\n rvm notes\n\n\nError:\n\n/usr/local/bin/rvm: line 73: /home/cody/.rvm/scripts/rvm: No such file or directory\n\n\nflashes in multiple lines and doesn't stop till I hit ctrl+C..\nI am running Ubuntu 8.04 and currently I am running ruby 1.9.2..\nSorry, if I am missing out any necessary information. Thanks in advance.\n    \n\nAck, I didn't mean to post this as a comment on the question. Anyway, if I had to guess, I'd say you installed rvm using sudo or as root. If that is the case, remove it and reinstall without sudo:\n\nsudo rm -rf $HOME/.rvm $HOME/.rvmrc /etc/rvmrc /etc/profile.d/rvm.sh \\\n  /usr/local/rvm /usr/local/bin/rvm\nsudo /usr/sbin/groupdel rvm # this might fail, it's not that important\n\n\nOpen new terminal window/tab and make sure rvm is removed:\n\nenv | grep rvm\n\n\nThe output should be empty, sometimes it's needed to relogin, after it's empty you can continue:\n\ncurl -sSL https://get.rvm.io | bash -s stable\n\n\nIt works perfectly fine installed for the local user.\n    \n\nOk, for anyone who tried to install RVM using sudo and is now pulling\ntheir hair out trying to get it to install in $HOME/.rvm, here's what\ndid it for me:\n\nWhen you installed RVM using sudo, it created a file /etc/rvmrc, which contains the following:  \n\numask g+w  \nexport rvm_path=\"/usr/local/rvm\"  \n\n\nThis makes all future attempts at installation (even when not run as sudo)\ninstall into /usr/local/rvm, which is NOT what you want for a single\nuser installation. So remove /etc/rvmrc and then you can run  \n\nbash &lt; &lt;(curl -s https://rvm.beginrescueend.com/install/rvm)  \n\n\nand it will install properly into $HOME/.rvm\n    \n\nDId you add this line to your ~/.bashrc?\n\n[[ -s \"$HOME/.rvm/scripts/rvm\" ]] &amp;&amp; . \"$HOME/.rvm/scripts/rvm\" # This loads RVM into a shell session.\n\n    \n\n\n  I have executed the commands as prescribed in the instructions at the rvm website. \n\n\nWHICH commands? There are several pages containing instructions to install RVM depending on whether you want a single-user \"sandbox\" or are installing system-wide for a multi-user system as the administrator.\n\nBecause you have RVM in /usr/local, I think you tried to do a system-wide install but didn't get it right. For 99% of us, that is the wrong installation method, and instead you should use the single-user installation, which is simple and puts everything in ~/.rvm.\n\nEither way, be sure to read the entire instructions. And, if doing a single-user install, finish the install with the \"Post Install\" modifications to ~/.bashrc or ~/.bash_profile for a single-user, then start a new terminal session.\n\nWhen using the single-user install NEVER use sudo to install gems to a RVM-managed Ruby, even though the instructions for a gem might say to. \n    \n\nLook at the section \"Troubleshooting your install\" here.  Since you are on Ubuntu, you probably need to make further mods to you .bashrc\n    \n\nin .bashrc have you changed the \n\n[ -z \"$PS1\" ] &amp;&amp; return\n\nto\n\nif [[ -n \"$PS1\" ]]; then\n\nand added this to the end of the file:\n\nfi\n\n[[ -s \"$HOME/.rvm/scripts/rvm\" ]] &amp;&amp; source \"$HOME/.rvm/scripts/rvm\"\n    \n\nI just had a similar problem. \n\nIt turned out that many files in ~/.rvm/scripts/ and ~/.rvm/src/rvm/scripts/ which obviously should be executable did not have execute permissions. Running a script on both directories to set all files to executable solved that immediate problem. \n    \n\nI have got same problem after installation. Then I restarted terminal and it started working poperly.\n    ", "document_id": 1049}]}, {"paragraphs": [{"qas": [{"question": "Deploy Qt5 QML application", "id": 1272, "answers": [{"answer_id": 1264, "document_id": 843, "question_id": 1272, "text": "If you use MinGW, then try to copy all folders from folders qml and plugins to directory with your program. Also copy libraries: icudt52.dll, icuin52.dll, icuuc52.dll, libgcc_s_dw2-1.dll, libstdc++-6.dll, libwinpthread-1.dll, Qt5Core.dll, Qt5Gui.dll, Qt5Network.dll, Qt5Qml.dll, Qt5Quick.dll, Qt5Svg.dll, Qt5Widgets.dll from bin.", "answer_start": 873, "answer_category": null}], "is_impossible": false}], "context": "To be sure no development library was installed in the system, I've set up a virtual machine with a pure Windows XP installation. Then, I've followed instructions as described here and copied all Qt5*.dll into the program directory, as well as platforms/qwindows.dll and icu*52.dll. Dependency Walker confirmed that no broken dependencies were left, i.e. everything should have been correctly set up.\nHowever, for some reasons, when I run my app I see nothing. Neither a window, nor an error message. Running from console also gives me no error. Despite this, I can see my app running in the Task manager, like it is running in background. Running the app on the development machine goes without problem: the app correctly starts and I can see its windows.\nWhat am I doing wrong? How can I deploy a QML app to be sure it will work on any other - non development - machine?\nIf you use MinGW, then try to copy all folders from folders qml and plugins to directory with your program. Also copy libraries: icudt52.dll, icuin52.dll, icuuc52.dll, libgcc_s_dw2-1.dll, libstdc++-6.dll, libwinpthread-1.dll, Qt5Core.dll, Qt5Gui.dll, Qt5Network.dll, Qt5Qml.dll, Qt5Quick.dll, Qt5Svg.dll, Qt5Widgets.dll from bin.\n", "document_id": 843}]}, {"paragraphs": [{"qas": [{"question": "Maven doesn't recognize sibling modules when running mvn dependency:tree", "id": 1877, "answers": [{"answer_id": 1863, "document_id": 1448, "question_id": 1877, "text": "You can work around this by mvn installing, as previously suggested, or doing something less onerous that invokes the reactor, such as\nmvn compile dependency:tree", "answer_start": 809, "answer_category": null}], "is_impossible": false}], "context": "I'm trying to set up a multi-module Maven project, and the inter-module dependencies are apparently not being set up correctly.\nin the parent POM (which has a packaging-type pom) and then subdirectories commons/ and storage/ which define JAR poms with the same name.\nTo clarify, what I am looking for here is this: I don't want to have to install module X to build module Y which depends on X, given that both are modules referenced from the same parent POM. This makes intuitive sense to me that if I have two things in the same source tree, I shouldn't have to install intermediate products to continue the build. Hopefully my thinking makes some sense here...\nAs discussed in this maven mailing list thread, the dependency:tree goal by itself will look things up in the repository rather than the reactor.\nYou can work around this by mvn installing, as previously suggested, or doing something less onerous that invokes the reactor, such as\nmvn compile dependency:tree\n", "document_id": 1448}]}, {"paragraphs": [{"qas": [{"question": "Capistrano deployment problems", "id": 480, "answers": [{"answer_id": 487, "document_id": 211, "question_id": 480, "text": "It occurs that config/deploy.rb has lock '3.1.0'.\nIt was enough to change it to '3.2.0' and now it's working.", "answer_start": 112, "answer_category": null}], "is_impossible": false}], "context": "When I type cap production deploy.\nI get Capfile locked at 3.1.0, but 3.2.0 is loaded.\nWhat to do in that case?\nIt occurs that config/deploy.rb has lock '3.1.0'.\nIt was enough to change it to '3.2.0' and now it's working.\n", "document_id": 211}]}, {"paragraphs": [{"qas": [{"question": "How to ForwardAgent yes using fabric?", "id": 521, "answers": [{"answer_id": 523, "document_id": 246, "question_id": 521, "text": "Since version 1.4 fabric has environment option that enables agent forwarding.\nenv.forward_agent = True.", "answer_start": 204, "answer_category": null}], "is_impossible": false}], "context": "I am successfully run()ning commands on remote server with my private key pair.\nHowever, I'd like to do git clone ssh://private/repo on remote server using my local key (or using local ssh agent I'm in).\nSince version 1.4 fabric has environment option that enables agent forwarding.\nenv.forward_agent = True.\n", "document_id": 246}]}, {"paragraphs": [{"qas": [{"question": "How to deploy android application to a device?", "id": 494, "answers": [{"answer_id": 498, "document_id": 222, "question_id": 494, "text": "There are multiple ways:\nIf you don't use eclipse, you can use adb tool. adb -d install PATH_TO_YOUR_APK_FILE\nIf you use eclipse, you can click run application in eclipse's launch menu. If this doesn't work, make sure you have \"USB Debugging Mode\" checked on your android phone. It's in the application menu.\nYou can export your package and sign it! And then browse to it to install.", "answer_start": 158, "answer_category": null}], "is_impossible": false}], "context": "I developed an application. Now I want to deploy it on real device. Can any body please tell me the steps and the requirements to deploy it on a real device.\nThere are multiple ways:\nIf you don't use eclipse, you can use adb tool. adb -d install PATH_TO_YOUR_APK_FILE\nIf you use eclipse, you can click run application in eclipse's launch menu. If this doesn't work, make sure you have \"USB Debugging Mode\" checked on your android phone. It's in the application menu.\nYou can export your package and sign it! And then browse to it to install.\n", "document_id": 222}]}, {"paragraphs": [{"qas": [{"question": "Using conda install within a python script", "id": 1800, "answers": [{"answer_id": 1786, "document_id": 1372, "question_id": 1800, "text": "You can use conda.cli.main. For example, this installs numpy:\nimport conda.cli\nconda.cli.main('conda', 'install',  '-y', 'numpy')\nUse the -y argument to avoid interactive questions:\n-y, --yes Do not ask for confirmation.", "answer_start": 464, "answer_category": null}], "is_impossible": false}], "context": "According to this answer you can import pip from within a Python script and use it to install a module. Is it possible to do this with conda install?\nThe conda documentation only shows examples from the command line but I'm looking for code that can be executed from within a Python script.\nYes, I could execute shell commands from within the script but I am trying to avoid this as it is basically assuming that conda cannot be imported and its functions called.\nYou can use conda.cli.main. For example, this installs numpy:\nimport conda.cli\nconda.cli.main('conda', 'install',  '-y', 'numpy')\nUse the -y argument to avoid interactive questions:\n-y, --yes Do not ask for confirmation.\n", "document_id": 1372}]}, {"paragraphs": [{"qas": [{"question": "How to install Python packages from the tar.gz file without using pip install", "id": 665, "answers": [{"answer_id": 670, "document_id": 358, "question_id": 665, "text": "pip install my-tarball-file-name.tar.gz", "answer_start": 731, "answer_category": null}], "is_impossible": false}], "context": "Long story short my work computer has network constraints which means trying to use pip install in cmd just leads to timing out/not finding package errors.\nFor example; when I try to pip install seaborn:  \nInstead I have tried to download the tar.gz file of the packages I want, however, I do not know how to install them. I've extracted the files from the tar.gz file and there is a \"setup\" file within but it's not doing much for me.\nIf someone could explain how to install python packages in this manner without using pip install on windows that would be amazing.\nYou can install a tarball without extracting it first. Just navigate to the directory containing your .tar.gz file from your command prompt and enter this command:\npip install my-tarball-file-name.tar.gz\nI am running python 3.4.3 and this works for me. I can't tell if this would work on other versions of python though.\n", "document_id": 358}]}, {"paragraphs": [{"qas": [{"question": "installing several applications at once from inno setup", "id": 1361, "answers": [{"answer_id": 1350, "document_id": 929, "question_id": 1361, "text": "An Inno Setup can include other setups and extract/run them on any condition, including dependancies (not) existing, user prompts and [Components]/[Tasks].\n\nYou can use a normal [Files] entry to extract it into {tmp} and a [Run] entry to run it.", "answer_start": 789, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI'm pretty new to Inno Setup. I created a fairly simple installer (and it wasn't hard at all), that consisted of one application. I wanted to know if Inno Setup is a suitable choice for creating an installer, that would consist of several applications, that have their own installers (as far as I understand, these installers could be run from Inno Setup). And is there an option (probably related to writing some Delphi code), that would let user choosing what apps need to be installed (with checkboxes or something like that). I understand that it's not really a concrete question (though a code sample or a full answer would be appreciated), I just don't want to spend too much time setting this tool (though I liked Inno Setup), if it doesn't meet my needs.\n    \n\nAn Inno Setup can include other setups and extract/run them on any condition, including dependancies (not) existing, user prompts and [Components]/[Tasks].\n\nYou can use a normal [Files] entry to extract it into {tmp} and a [Run] entry to run it.\n\nIf the other setups are external to the Inno setup, skip the [Files] entry and use {src}\\BlahSetup.exe for the [Run] entry.\n    ", "document_id": 929}]}, {"paragraphs": [{"qas": [{"question": "reinstalling java 7 jdk on osx", "id": 1985, "answers": [{"answer_id": 1971, "document_id": 1570, "question_id": 1985, "text": ", but in the end installing the Java for OSX from Apple's website fixed it.\n\nhttp://support.apple.com/kb/DL1572?viewlocale=en_US\n    \n\nFor others hitting this issue, you might try installing the latest jdk from Apple:\n\nhttps://stackoverflow.com/a/26813731/1", "answer_start": 2120, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI've tried to install Java 7 (aka 1.7) on OSX 10.7.5 (Lion).\n\nUnfortunately this seems to have mucked up Java all together and I'm not sure how to resolve the problem. Currently when I try to run Eclipse I get the error: \n\n\n  The JVM shared library \"/System/Library/Frameworks/JavaVM.framework\" does not contain the JNI_CreateJavaVM symbol.\n\n\nPrograms such as Minecraft and PhpStorm will crash completely with no errors.\n\nRunning 'java -version' in terminal gives me: \n\njava version \"1.6.0_37\"\nJava(TM) SE Runtime Environment (build 1.6.0_37-b06-434-11M3909)\nJava HotSpot(TM) 64-Bit Server VM (build 20.12-b01-434, mixed mode)\n\n\nWhich I understand is to be expected with a straight install of Java 7. Unfortunately Utilities -&gt; Java Preferences has been removed in OSX 10.7.5, and trying to load the System Preference -&gt; Java pane doesn't work (it tells me that it opens in a new window but doesn't, when the button appears to reload it, clicking it doesn't do anything).\n\nI've tried reinstalling various versions of JRE and JDK to no avail.\n\nIs there anyway to bin the lot and start again or am I missing something really obvious?\n    \n\nFind out where the installation directory of your Java 7 is on your Mac OS X. Set the JAVA_HOME shell variable to that directory (directory must contain a bin and lib subdirectory). Edit the path to export PATH=\"${JAVA_HOME}/bin:${PATH}\". Try running java -version again.\n\nIf all else fails, then perhaps you will have to reinstall your OS in order to bring back Mac Java defaults and just reinstall Java 7.\n\nI had this similar problem on Mountain Lion (10.8): after installing the Java 7 SDK, I deleted the Apple-default Java 6 JRE. This messed up my computer, especially Eclipse. The issue is related to the fact that Apple delivered the official Java JRE to Mac OS X, until Java 7 came out and Oracle became the official supplier for that version. After I reinstalled the OS, I was able to get the prompt window that asked me about downloading a Java runtime. This will reinstall the Apple Java 6 JRE.\n    \n\nI had the same issue. Tried several solutions, but in the end installing the Java for OSX from Apple's website fixed it.\n\nhttp://support.apple.com/kb/DL1572?viewlocale=en_US\n    \n\nFor others hitting this issue, you might try installing the latest jdk from Apple:\n\nhttps://stackoverflow.com/a/26813731/1636818\n    ", "document_id": 1570}]}, {"paragraphs": [{"qas": [{"question": "What platforms do Bash support?", "id": 57, "answers": [{"answer_id": 60, "document_id": 63, "question_id": 57, "text": "The distribution supports the\nGNU operating systems, nearly every version of Unix, and several\nnon-Unix systems such as BeOS and Interix.\nOther independent ports exist for\nMS-DOS, OS/2, and Windows platforms.", "answer_start": 247, "answer_category": null}], "is_impossible": false}], "context": "\n\n\n\n\n\nInstalling Bash\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNext: Reporting Bugs, Previous: Using History Interactively, Up: Top \u00a0 [Contents][Index]\n\n\n10 Installing Bash\nThis chapter provides basic instructions for installing Bash on\nthe various supported platforms.  The distribution supports the\nGNU operating systems, nearly every version of Unix, and several\nnon-Unix systems such as BeOS and Interix.\nOther independent ports exist for\nMS-DOS, OS/2, and Windows platforms.\n\n\n\u2022 Basic Installation\u00a0\u00a0Installation instructions.\n\n\u2022 Compilers and Options\u00a0\u00a0How to set special options for various\nsystems.\n\n\u2022 Compiling For Multiple Architectures\u00a0\u00a0How to compile Bash for more\nthan one kind of system from\nthe same source tree.\n\n\u2022 Installation Names\u00a0\u00a0How to set the various paths used by the installation.\n\n\u2022 Specifying the System Type\u00a0\u00a0How to configure Bash for a particular system.\n\n\u2022 Sharing Defaults\u00a0\u00a0How to share default configuration values among GNU\nprograms.\n\n\u2022 Operation Controls\u00a0\u00a0Options recognized by the configuration program.\n\n\u2022 Optional Features\u00a0\u00a0How to enable and disable optional features when\nbuilding Bash.\n\n\n\n\n", "document_id": 63}]}, {"paragraphs": [{"qas": [{"question": "Cannot install cocoa pods after uninstalling, results in error", "id": 1538, "answers": [{"answer_id": 1527, "document_id": 1115, "question_id": 1538, "text": "As rootless does not affect /usr/local/bin, the following succeeds:\nsudo gem install -n /usr/local/bin cocoapods", "answer_start": 182, "answer_category": null}], "is_impossible": false}], "context": "I removed cocoa pods because it claimed it had installed, but kept saying the command pod wasn't found afterward. When trying to reinstall cocoapods (sudo gem install cocoa pods -v)\nAs rootless does not affect /usr/local/bin, the following succeeds:\nsudo gem install -n /usr/local/bin cocoapods\n", "document_id": 1115}]}, {"paragraphs": [{"qas": [{"question": "Failure [INSTALL_FAILED_ALREADY_EXISTS] when I tried to update my application", "id": 1554, "answers": [{"answer_id": 1543, "document_id": 1131, "question_id": 1554, "text": "If you install the application on your device via adb install you should look for the reinstall option which should be -r. So if you do adb install -r you should be able to install without uninstalling before.", "answer_start": 133, "answer_category": null}], "is_impossible": false}], "context": "when I tried to update my applcation with new version that has same signature as previous one, shows above error.\nWhat I am missing?\nIf you install the application on your device via adb install you should look for the reinstall option which should be -r. So if you do adb install -r you should be able to install without uninstalling before.\n", "document_id": 1131}]}, {"paragraphs": [{"qas": [{"question": "How is Capistrano related to Rake?", "id": 1260, "answers": [{"answer_id": 1252, "document_id": 831, "question_id": 1260, "text": "Capistrano v1 and v2 had no dependencies on rake. It was written from scratch as a DSL for handling remote servers. It's evident that some aspects of capistrano were influenced by rake, but Jamis Buck felt it was necessary to make capistrano stand on its own. Capistrano tasks behave slightly differently than rake tasks and their hookable nature separates them from rake tasks.", "answer_start": 336, "answer_category": null}], "is_impossible": false}], "context": "I'm starting to read up on Capistrano after using Rake tasks to deploy apps for a long time. It's really striking how similar it is to Rake. A lot of parallel commands (like cap -T) and a lot of identical concepts (namespaces, tasks).\nDoes anyone know the history behind that? Is Capistrano an extension of Rake, or built on top of it?\nCapistrano v1 and v2 had no dependencies on rake. It was written from scratch as a DSL for handling remote servers. It's evident that some aspects of capistrano were influenced by rake, but Jamis Buck felt it was necessary to make capistrano stand on its own. Capistrano tasks behave slightly differently than rake tasks and their hookable nature separates them from rake tasks.\n", "document_id": 831}]}, {"paragraphs": [{"qas": [{"question": "creating an installer", "id": 1976, "answers": [{"answer_id": 1962, "document_id": 1561, "question_id": 1976, "text": "Now I solved the problem with a new approach and I've made a demo that handles most of the common features an installer need to do.\n\nThe Demo is entirely written in Delphi 7 ( 32 bits ), and is available to all at github. Hope it helps.\n\nhttps://github.com/kimherrero/DelphiAppSetupDemo", "answer_start": 1860, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI was just trying to make an installer. What is the best way to handle storing files in the exe file and then extracting them to the installation directory?\n\nFor example, I have my installer.exe file, I want it to contain the following:\n\n\nMain executable file\n3 x *.db files \n\n\nUpon selecting the install directory and clicking install, I wish for the contained files to be extracted to the chosen installation directory, however I can not find out how to store the files that I need extracting within the installer.\n    \n\nInno Setup is interesting. This is a huge project, with a lot of features (including scripting language and plugins), and is coded in Delphi.\n\nBut take a look at NSIS installer. It is not written in Delphi, but it is much lighter. Creating a simple .ini file and you've your full installer made. See for instance how it was easy to create an installer for our small Secure NotePad tool - very similar to your need.\n\nI use to create custom installers in Delphi code. In some cases, if you do not have to use a lot of features but need to reuse some existing code, it could make sense.\n\nFor this, I use two of our Open Source units:\n\n\nOur PasZip unit is able to create a stand-alone unzipper, directly from the exe - see the sample code;\nOur LVCL classes which are able to create very small executables.\n\n\nLast advice: do not pack your exe with upx or other packers, since it tends to create false positives associated to Delphi executables with some anti virus softwares.\n    \n\nWe use Setup Factory. It's simple to learn and use, and integrates with our automated build process using Final Builder.\n\nSetup Factory\n    \n\nI had an installer made in Delphi 7, but because of the elevated privileges needed to copy files on Program Files, the installed App was launched elevated too. That behaviour is not what I wanted.\n\nNow I solved the problem with a new approach and I've made a demo that handles most of the common features an installer need to do.\n\nThe Demo is entirely written in Delphi 7 ( 32 bits ), and is available to all at github. Hope it helps.\n\nhttps://github.com/kimherrero/DelphiAppSetupDemo\n    \n\nAll installers (Inno, Setup Factory, NSIS installer) are showing issues about false positive virus.\nTo fix this, it's necessary to use a sign tools to create a key for your software, but it is payed.\n    ", "document_id": 1561}]}, {"paragraphs": [{"qas": [{"question": "How can I deploy/push only a subdirectory of my git repo to Heroku?", "id": 1276, "answers": [{"answer_id": 1268, "document_id": 847, "question_id": 1276, "text": "There's an even easier way via git-subtree. Assuming you want to push your folder 'output' as the root to Heroku, you can do:\ngit subtree push --prefix output heroku master", "answer_start": 498, "answer_category": null}], "is_impossible": false}], "context": "I have a project that uses Serve and is version controlled using Git. Serve creates an output folder with static files that I want to deploy to Heroku.\nI don't want to deploy the Serve project itself since the Heroku Cedar stack doesn't seem too fond of it, but most importantly I want to take advantage of Heroku's great support for static websites.\nIs there a way to deploy a subfolder to a git remote? Should I create a Git repo in the output folder (that sounds wrong) and push that to Heroku?\nThere's an even easier way via git-subtree. Assuming you want to push your folder 'output' as the root to Heroku, you can do:\ngit subtree push --prefix output heroku master\n", "document_id": 847}]}, {"paragraphs": [{"qas": [{"question": "how to detect older java versions?", "id": 145, "answers": [{"answer_id": 153, "document_id": 91, "question_id": 145, "text": " Starting with Java 8 Update 20 (8u20), on Windows systems, the Java Uninstall Tool is integrated with the installer to provide an option to remove older versions of Java from the system. The change is applicable to 32 bit and 64 bit Windows platforms.\n", "answer_start": 1933, "answer_category": null}], "is_impossible": false}, {"question": "How do I manually download and install Java for my Windows computer?", "id": 146, "answers": [{"answer_id": 154, "document_id": 91, "question_id": 146, "text": "Double-click on the saved file to start the installation process.\nThe installation process starts. Click the Install button to accept the license terms and to continue with the installation.\n\nOracle has partnered with companies that offer various products. The installer may present you with option to install these programs when you install Java. After ensuring that the desired programs are selected, click the Next button to continue the installation.\nA few brief dialogs confirm the last steps of the installation process; click Close on the last dialog. This will complete Java installation process.", "answer_start": 1270, "answer_category": null}], "is_impossible": false}], "context": "This article applies to:\nPlatform(s): Windows 2008 Server, Windows 7, Windows 8, Windows XP, Windows Server 2012, Windows Vista, Windows 10\nJava version(s): 7.0, 8.0\nThe procedure to install Java broadly consists of:\nDownload and Install\nTest Installation\n\u00bb Windows System Requirements\n\nNote: Installing Java requires that you can gain administrator access to Windows on your computer.\n\nDownload and Install\nIt is recommended, before you proceed with online installation you may want to disable your Internet firewall. In some cases the default firewall settings are set to reject all automatic or online installations such as the Java online installation. If the firewall is not configured appropriately it may stall the download/install operation of Java under certain conditions. Refer to your specific Internet firewall manual for instructions on how to disable your Internet Firewall.\n\nGo to the Manual download page\nClick on Windows Online\nThe File Download dialog box appears prompting you to run or save the download file\nTo run the installer, click Run.\nTo save the file for later installation, click Save.\nChoose the folder location and save the file to your local system.\nTip: Save the file to a known location on your computer, for example, to your desktop.\nDouble-click on the saved file to start the installation process.\nThe installation process starts. Click the Install button to accept the license terms and to continue with the installation.\n\nOracle has partnered with companies that offer various products. The installer may present you with option to install these programs when you install Java. After ensuring that the desired programs are selected, click the Next button to continue the installation.\nA few brief dialogs confirm the last steps of the installation process; click Close on the last dialog. This will complete Java installation process.\n\nInfo iconDetect older versions (8u20 and later versions). Starting with Java 8 Update 20 (8u20), on Windows systems, the Java Uninstall Tool is integrated with the installer to provide an option to remove older versions of Java from the system. The change is applicable to 32 bit and 64 bit Windows platforms.\n\n\nNotifications about disabled Java and restoring prompts\nThe installer notifies you if Java content is disabled in web browsers, and provides instructions for enabling it. If you previously chose to hide some of the security prompts for applets and Java Web Start applications, the installer provides an option for restoring the prompts. The installer may ask you to reboot your computer if you chose not to restart an internet browser when it prompted you to do so.\n\nTest Installation\nTo test that Java is installed and working properly on your computer, run this test applet.\n\nNOTE: You may need to restart (close and re-open) your browser to enable the Java installation in your browser.", "document_id": 91}]}, {"paragraphs": [{"qas": [{"question": "Environment-specific configuration for a Spring-based web application?", "id": 1283, "answers": [{"answer_id": 1275, "document_id": 854, "question_id": 1283, "text": "Don't add logic to your code to test which environment you're running in - that is a recipe for disaster (or at least burning a lot of midnight oil down the road).", "answer_start": 197, "answer_category": null}], "is_impossible": false}], "context": "How can I know the deployment environment of a web application, e.g. whether it is local, dev, qa or prod, etc. Is there any way I can determine this in spring application context file at runtime?\nDon't add logic to your code to test which environment you're running in - that is a recipe for disaster (or at least burning a lot of midnight oil down the road).\n", "document_id": 854}]}, {"paragraphs": [{"qas": [{"question": "Backup strategy for Django", "id": 470, "answers": [{"answer_id": 479, "document_id": 203, "question_id": 470, "text": "Source code: use source control such as svn or git. This means that you will usually have: dev, deploy and repository backups for code (specially in a drsc).", "answer_start": 291, "answer_category": null}], "is_impossible": false}], "context": "I recently deployed a couple of web applications built using django (on webfaction). These would be some of the first projects of this scale that i am working on, so I wanted to know what an effective backup strategy was for maintaining backups both on webfaction and an alternate location.\nSource code: use source control such as svn or git. This means that you will usually have: dev, deploy and repository backups for code (specially in a drsc).\n", "document_id": 203}]}, {"paragraphs": [{"qas": [{"question": "Dependency graph of Visual Studio projects", "id": 1825, "answers": [{"answer_id": 1811, "document_id": 1396, "question_id": 1825, "text": "Have you tried NDepend? It'll shows you the dependencies and you can also analyze the usability of your classes and methods.", "answer_start": 353, "answer_category": null}], "is_impossible": false}], "context": "I'm currently migrating a big solution (~70 projects) from VS 2005 + .NET 2.0 to VS 2008 + .NET 3.5. Currently I have VS 2008 + .NET 2.0.\nThe problem is that I need to move projects one by one to new .NET framework ensuring that no .NET 2.0 project references .NET 3.5 project. Is there any tool that would give me a nice graph of project dependencies?\nHave you tried NDepend? It'll shows you the dependencies and you can also analyze the usability of your classes and methods.\n", "document_id": 1396}]}, {"paragraphs": [{"qas": [{"question": "how to determine if vsto 2010 runtime is installed for x64?", "id": 1503, "answers": [{"answer_id": 1492, "document_id": 1082, "question_id": 1503, "text": "HKLM\\SOFTWARE\\Wow6432Node\\Microsoft\\VSTO Runtime Setup\\v4 -&gt; Install\n  HKLM\\SOFTWARE\\Wow6432Node\\Microsoft\\VSTO Runtime Setup\\v4 -&gt; Produ", "answer_start": 3876, "answer_category": null}], "is_impossible": false}, {"question": "how to determine if vsto 2010 runtime is installed for x86?", "id": 1504, "answers": [{"answer_id": 1493, "document_id": 1082, "question_id": 1504, "text": " \n  \n  HKLM\\SOFTWARE\\Microsoft\\VSTO Runtime Setup -&gt; Inst", "answer_start": 4044, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nIt was easy to check if VSTO 2005 SE was installed by just calling MsiGetProductInfo() with the product code {388E4B09-3E71-4649-8921-F44A3A2954A7}, as listed in MSDN. \n\nWhat is the product code for the VSTO 2010/4.0 runtime?  Or is there a better way to determine if it's already installed?  This is for our installation process.\n\nAlso, I am trying to figure out the same for Windows Imaging Component.\n    \n\nUnfortunately the answers here so far do not quite cover all the bases.\n\nProduct Code\n\nThis does not appear to be reliable - we're looking for a minimum version, not a specific version. Though the product code is in theory only supposed to change for major version increments, the version of VSTO on my machine - 10.0.40303 - has a product code of {A0FE0292-D3BE-3447-80F2-72E032A54875}. This suggests that Microsoft isn't necessarily keeping them stable, so I'd suggest this is not a good option.\n\nFile version\n\nAnother option may be to check for the presence / version of the VSTO assemblies themselves, which may typically be in %PROGRAM FILES%\\Common Files\\Microsoft Shared\\VSTO\\10.0. However I'd say this directory is not guaranteed - the actual directory appears to be specified in the registry, but obviously this solution is now no better than just getting the version from the registry directly...\n\nRegistry\n\nSo going by the registry is probably the only option left.\n\nUnfortunately, the VSTO runtime version can appear in any one of 4 registry locations:\n\n\nHKLM\\SOFTWARE\\Microsoft\\VSTO Runtime Setup\\v4 (32-bit, VSTO installed from Office 2010 installation)\nHKLM\\SOFTWARE\\Microsoft\\VSTO Runtime Setup\\v4R (32-bit, VSTO installed from redistributable)\nHKLM\\SOFTWARE\\Wow6432Node\\Microsoft\\VSTO Runtime Setup\\v4 (64-bit, VSTO installed from Office 2010 installation)\nHKLM\\SOFTWARE\\Wow6432Node\\Microsoft\\VSTO Runtime Setup\\v4R (64-bit, VSTO installed from redistributable)\n\n\nNote: I don't have a definitive source for this - I'm cobbling together bits of information from, for example, this blog post by Wouter van Vugt and this SO answer. There may be even more to it than that.\n\nIn addition, there may be minimum version requirements, though I suspect that in practice this is only going to affect people running pre-release versions of Office 2010:\n\n\n  The Visual Studio 2010 Tools for Office runtime also ships with\n  Microsoft Office 2010. However at the time of Office 2010 RTM, the\n  runtime with Office only supports Office solutions that target the\n  .NET Framework 3.5. If your solution targets the .NET Framework 3.5,\n  it can run either if Office 2010 is installed or if the Visual Studio\n  2010 Tools for Office Runtime redistributable is installed. If your\n  Office solutions target the .NET Framework 4, you must redistribute\n  the Visual Studio 2010 Tools for Office runtime (citation).\n\n    \n\nThe easiest way is to check the registry.\n\nHKLM\\Microsoft\\vsto runtime setup\\v4\\Install \n\nHKLM\\Microsoft\\vsto runtime setup\\v4R\\VSTORFeature_CLR40 (this is for the 4.0 Office extensions)\n    \n\nThe safest and cleanest method is still checking the product codes, here they are:\n\nFor VSTO 2010 x86, version 10.0.31124:  {41A01180-D9FD-3428-9FD6-749F4C637CBF}\n\nFor VSTO 2010 x64, version 10.0.31124:  {C3600AE6-93A0-3DB7-B7AA-45BD58F133B5}\n\nI got them by extraction the contents of the following packages with 7-Zip and analyzing the MSIs with Orca:\n\nhttp://download.microsoft.com/download/F/3/9/F395E3C2-28A0-4F0D-9E20-FF4D1ADB08D8/vstor40_x86.exe\n\nhttp://download.microsoft.com/download/F/3/9/F395E3C2-28A0-4F0D-9E20-FF4D1ADB08D8/vstor40_x64.exe\n    \n\nThey keys vary by the OS you are installing to. I painstakingly installed the standalone vsto and office 2010 and 2013 .exe's in x86 and x64. \nIn order to use the registry to check if vsto is installed, you need to verify the existence of the following keys:\n\n\n  for x64:\n  \n  \n  HKLM\\SOFTWARE\\Wow6432Node\\Microsoft\\VSTO Runtime Setup\\v4 -&gt; Install\n  HKLM\\SOFTWARE\\Wow6432Node\\Microsoft\\VSTO Runtime Setup\\v4 -&gt; ProductCode\n  \n  \n  for x86:\n  \n  \n  HKLM\\SOFTWARE\\Microsoft\\VSTO Runtime Setup -&gt; InstallerPath\n  \n\n\nEdit: What I actually ended up doing, was verify that the v4 folder exists. \n    \n\nI needed a way of detecting this when deploying Vstor as an application in SCCM 2012. I used the following PowerShell script to retrieve the info from WMI. \n\nIf the version is 10.0.50908, output is generated. SCCM considers detection to be successful if the detection script returns any value.\n\n$VstorVersion = Get-WmiObject -Query \"select ProductVersion from SMS_InstalledSoftware where ARPDisplayName = 'Microsoft Visual Studio 2010 Tools for Office Runtime (x64)'\" -NameSpace \"root\\cimv2\\sms\"\nif ($VstorVersion.ProductVersion -ge '10.0.50908') {Write-Host \"Found\"}\n\n    \n\nHere code.\nIn public method we determine is VSTO installed from Office or VSTO runtime package.\nIn private method, check if version is equals or bigger than version VSTO2010\n\n\n\n\n\npublic static bool CheckVSTO2010 ( ) {\n\n        string regFragment = IntPtr.Size == 8 ? \"\\\\Wow6432Node\\\\\" : \"\\\\\";\n\n        string regVSTO = string.Format( @\"SOFTWARE{0}Microsoft\\VSTO Runtime Setup\\\", regFragment );\n\n        return CheckVSTOVersion( regVSTO + \"v4\\\\\" ) || CheckVSTOVersion( regVSTO + \"v4R\\\\\" );\n\n    }\n\n\n\n\n\n    private static bool CheckVSTOVersion ( string keyPath ) {\n\n        using (var key = Registry.LocalMachine.OpenSubKey( keyPath )) {\n\n            //Not installed\n\n            if (key == null) {\n\n                return false;\n\n            }\n\n            var releaseKey = key.GetValue( \"Version\" );\n\n            if (releaseKey != null &amp;&amp; !string.IsNullOrEmpty( releaseKey.ToString() )) {\n\n                var version = new Version( releaseKey.ToString() );\n\n                return version.Major &gt;= 10 &amp;&amp; version.Build &gt;= 40820;\n\n            }\n\n        }\n\n        return false;\n\n    }\n\n\n\n\n\n\n    ", "document_id": 1082}]}, {"paragraphs": [{"qas": [{"question": "Find unused npm packages in package.json", "id": 1836, "answers": [{"answer_id": 1822, "document_id": 1407, "question_id": 1836, "text": "You can use an npm module called depcheck (requires at least version 10 of Node):\nhttps://www.npmjs.com/package/depcheck", "answer_start": 350, "answer_category": null}], "is_impossible": false}], "context": "Is there a way to determine if you have packages in your package.json file that are no longer needed?\nFor instance, when trying out a package and later commenting or deleting code, but forgetting to uninstall it, I end up with a couple packages that could be deleted.\nWhat would be an efficient way to determine if a package could safely be deleted?\nYou can use an npm module called depcheck (requires at least version 10 of Node):\nhttps://www.npmjs.com/package/depcheck\n", "document_id": 1407}]}, {"paragraphs": [{"qas": [{"question": "Mercurial stuck \"waiting for lock\"", "id": 1313, "answers": [{"answer_id": 1303, "document_id": 882, "question_id": 1313, "text": "You should delete the repository file: .hg/wlock  it may be in .hg/store/lock.", "answer_start": 323, "answer_category": null}], "is_impossible": false}], "context": "Got a bluescreen in windows while cloning a mercurial repository.\nAfter reboot, I now get this message for almost all hg commands:\nc:\\src\\>hg commit\nwaiting for lock on repository c:\\src\\McVrsServer held by '\\x00\\x00\\x00\\x00\\x00\\\nx00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00'\ninterrupted!\nGoogle is no help.\nYou should delete the repository file: .hg/wlock  it may be in .hg/store/lock.\n", "document_id": 882}]}, {"paragraphs": [{"qas": [{"question": "Which maven dependencies to include for spring 3.0?", "id": 1900, "answers": [{"answer_id": 1887, "document_id": 1471, "question_id": 1900, "text": "There was a really nice post on the Spring Blog from Keith Donald detailing howto Obtain Spring 3 Aritfacts with Maven, with comments detailing when you'd need each of the dependencies.", "answer_start": 310, "answer_category": null}], "is_impossible": false}], "context": "I am trying to do my first project with Spring 3.0 (and maven). I have been using Spring 2.5 (and primer versions) in quite some projects. Nevertheless I am kinda confused, what modules I have to define as dependencies in my pom.xml. I just want to use the core container functions (beans, core, context, el).\nThere was a really nice post on the Spring Blog from Keith Donald detailing howto Obtain Spring 3 Aritfacts with Maven, with comments detailing when you'd need each of the dependencies.\n", "document_id": 1471}]}, {"paragraphs": [{"qas": [{"question": "How do I set up linkage between Docker containers so that restarting won't break it?", "id": 331, "answers": [{"answer_id": 339, "document_id": 143, "question_id": 331, "text": "Links are for a specific container, not based on the name of a container. So the moment you remove a container, the link is disconnected and the new container (even with the same name) will not automatically take its place.", "answer_start": 160, "answer_category": null}], "is_impossible": false}], "context": "I have a few Docker containers.Since Nginx needs to connect to the web application servers inside web app 1 and 2, and the web apps need to talk to PostgreSQL.\nLinks are for a specific container, not based on the name of a container. So the moment you remove a container, the link is disconnected and the new container (even with the same name) will not automatically take its place.The new networking feature allows you to connect to containers by their name, so if you create a new network, any container connected to that network can reach other containers by their name. \n", "document_id": 143}]}, {"paragraphs": [{"qas": [{"question": "mint genymotion installation bug on Ubuntu 15?\n", "id": 1520, "answers": [{"answer_id": 1509, "document_id": 1097, "question_id": 1520, "text": "this library with apt: sudo apt-get install ", "answer_start": 7049, "answer_category": null}], "is_impossible": false}, {"question": "mint genymotion installation bug on Ubuntu 14?", "id": 1521, "answers": [{"answer_id": 1510, "document_id": 1097, "question_id": 1521, "text": "kage doesn't exist. So I am again in a dea", "answer_start": 7136, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI'm newbie with Mint. I installed Genymotion emulator from the .bin file in /home/user/Android directory. And when I'm trying to execute Genymotion I receive such message:\n\n\n  /Android/genymotion $ ./genymotion\n  \n  ./genymotion: error while loading shared libraries:\n  libdouble-conversion.so.1: cannot open shared object file: No such\n  file or directory\n\n\nAs I understood from quick search I should link this conversion library. But I'm aware to do something wrong ;) Could someone advice proper way for doing this. Preferably with some explanation.\n\nUPDATE\n\nAfter I used @Atheror suggestion I got another message:\n\n\n  ./genymotion \n  \n  ./genymotion: /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version\n  `CXXABI_1.3.8' not found (required by\n  /opt/genymobile/genymotion/libQt5Core.so.5)\n  \n  ./genymotion: /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version\n  `GLIBCXX_3.4.20' not found (required by\n  /opt/genymobile/genymotion/libQt5WebKit.so.5)\n  \n  ./genymotion: /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version\n  `CXXABI_1.3.8' not found (required by\n  /opt/genymobile/genymotion/libicui18n.so.52)\n  \n  ./genymotion: /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version\n  `CXXABI_1.3.8' not found (required by\n  /opt/genymobile/genymotion/libicuuc.so.52)\n  \n  ./genymotion: /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version\n  `CXXABI_1.3.8' not found (required by\n  /opt/genymobile/genymotion/libicuuc.so.52)\n\n\nUPDATE\n\nAfter some additional search I tried to add missing libraries. After this, attempt to run genymotion leads to such record in genymotion.log:\n\n\n  [Genymotion] [Fatal] This application failed to start because it\n  could not find or load the Qt platform plugin \"xcb\".\n  \n  Available platform plugins are: eglfs, kms, linuxfb, minimal,\n  minimalegl, offscreen, xcb.\n\n\nSo xcb plugin is available. After another quick run in web I found suggestion to check dependencies by using ldd command and got:\n\nplatforms # ldd   libqxcb.so\n\nlinux-vdso.so.1 =&gt;  (0x00007ffff97ec000)\nlibX11-xcb.so.1 =&gt; /usr/lib/x86_64-linux-gnu/libX11-xcb.so.1 (0x00007f6b14c5d000)\nlibXi.so.6 =&gt; /usr/lib/x86_64-linux-gnu/libXi.so.6 (0x00007f6b14a4d000)\nlibxcb-render-util.so.0 =&gt; /usr/lib/x86_64-linux-gnu/libxcb-render-util.so.0 (0x00007f6b14849000)\nlibSM.so.6 =&gt; /usr/lib/x86_64-linux-gnu/libSM.so.6 (0x00007f6b14641000)\nlibICE.so.6 =&gt; /usr/lib/x86_64-linux-gnu/libICE.so.6 (0x00007f6b14425000)\nlibdl.so.2 =&gt; /lib/x86_64-linux-gnu/libdl.so.2 (0x00007f6b14221000)\nlibxcb-glx.so.0 =&gt; /usr/lib/x86_64-linux-gnu/libxcb-glx.so.0 (0x00007f6b1400a000)\nlibxcb-render.so.0 =&gt; /usr/lib/x86_64-linux-gnu/libxcb-render.so.0 (0x00007f6b13e01000)\nlibxcb.so.1 =&gt; /usr/lib/x86_64-linux-gnu/libxcb.so.1 (0x00007f6b13be2000)\nlibxcb-image.so.0 =&gt; /usr/lib/x86_64-linux-gnu/libxcb-image.so.0 (0x00007f6b139dd000)\nlibxcb-icccm.so.4 =&gt; /usr/lib/x86_64-linux-gnu/libxcb-icccm.so.4 (0x00007f6b137d8000)\nlibxcb-sync.so.1 =&gt; /usr/lib/x86_64-linux-gnu/libxcb-sync.so.1 (0x00007f6b135d2000)\nlibxcb-xfixes.so.0 =&gt; /usr/lib/x86_64-linux-gnu/libxcb-xfixes.so.0 (0x00007f6b133cb000)\nlibxcb-shm.so.0 =&gt; /usr/lib/x86_64-linux-gnu/libxcb-shm.so.0 (0x00007f6b131c8000)\nlibxcb-randr.so.0 =&gt; /usr/lib/x86_64-linux-gnu/libxcb-randr.so.0 (0x00007f6b12fbb000)\nlibxcb-shape.so.0 =&gt; /usr/lib/x86_64-linux-gnu/libxcb-shape.so.0 (0x00007f6b12db7000)\nlibxcb-keysyms.so.1 =&gt; /usr/lib/x86_64-linux-gnu/libxcb-keysyms.so.1 (0x00007f6b12bb4000)\nlibxcb-xkb.so.1 =&gt; /usr/lib/x86_64-linux-gnu/libxcb-xkb.so.1 (0x00007f6b1299a000)\nlibxkbcommon-x11.so.0 =&gt; /usr/lib/x86_64-linux-gnu/libxkbcommon-x11.so.0 (0x00007f6b12792000)\nlibxkbcommon.so.0 =&gt; /usr/lib/x86_64-linux-gnu/libxkbcommon.so.0 (0x00007f6b12558000)\nlibfontconfig.so.1 =&gt; /usr/lib/x86_64-linux-gnu/libfontconfig.so.1 (0x00007f6b1231c000)\nlibfreetype.so.6 =&gt; /usr/lib/x86_64-linux-gnu/libfreetype.so.6 (0x00007f6b12079000)\nlibQt5DBus.so.5 =&gt; /usr/lib/x86_64-linux-gnu/libQt5DBus.so.5 (0x00007f6b11dfa000)\nlibglib-2.0.so.0 =&gt; /lib/x86_64-linux-gnu/libglib-2.0.so.0 (0x00007f6b11af2000)\nlibXrender.so.1 =&gt; /usr/lib/x86_64-linux-gnu/libXrender.so.1 (0x00007f6b118e8000)\nlibX11.so.6 =&gt; /usr/lib/x86_64-linux-gnu/libX11.so.6 (0x00007f6b115b3000)\nlibQt5Gui.so.5 =&gt; /usr/lib/x86_64-linux-gnu/libQt5Gui.so.5 (0x00007f6b10f66000)\nlibQt5Core.so.5 =&gt; /usr/lib/x86_64-linux-gnu/libQt5Core.so.5 (0x00007f6b108c0000)\nlibGL.so.1 =&gt; /usr/lib/x86_64-linux-gnu/mesa/libGL.so.1 (0x00007f6b1062e000)\nlibpthread.so.0 =&gt; /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f6b10410000)\nlibstdc++.so.6 =&gt; /usr/lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007f6b10103000)\nlibm.so.6 =&gt; /lib/x86_64-linux-gnu/libm.so.6 (0x00007f6b0fdfd000)\nlibc.so.6 =&gt; /lib/x86_64-linux-gnu/libc.so.6 (0x00007f6b0fa38000)\nlibXext.so.6 =&gt; /usr/lib/x86_64-linux-gnu/libXext.so.6 (0x00007f6b0f826000)\nlibuuid.so.1 =&gt; /lib/x86_64-linux-gnu/libuuid.so.1 (0x00007f6b0f621000)\n/lib64/ld-linux-x86-64.so.2 (0x00007f6b14e5f000)\nlibXau.so.6 =&gt; /usr/lib/x86_64-linux-gnu/libXau.so.6 (0x00007f6b0f41d000)\nlibXdmcp.so.6 =&gt; /usr/lib/x86_64-linux-gnu/libXdmcp.so.6 (0x00007f6b0f217000)\nlibxcb-util.so.0 =&gt; /usr/lib/x86_64-linux-gnu/libxcb-util.so.0 (0x00007f6b0f010000)\nlibexpat.so.1 =&gt; /lib/x86_64-linux-gnu/libexpat.so.1 (0x00007f6b0ede6000)\nlibz.so.1 =&gt; /lib/x86_64-linux-gnu/libz.so.1 (0x00007f6b0ebcd000)\nlibpng12.so.0 =&gt; /lib/x86_64-linux-gnu/libpng12.so.0 (0x00007f6b0e9a7000)\nlibdbus-1.so.3 =&gt; /lib/x86_64-linux-gnu/libdbus-1.so.3 (0x00007f6b0e762000)\nlibpcre.so.3 =&gt; /lib/x86_64-linux-gnu/libpcre.so.3 (0x00007f6b0e524000)\nlibharfbuzz.so.0 =&gt; /usr/lib/x86_64-linux-gnu/libharfbuzz.so.0 (0x00007f6b0e2cf000)\nlibicui18n.so.52 =&gt; /usr/lib/x86_64-linux-gnu/libicui18n.so.52 (0x00007f6b0dec8000)\nlibicuuc.so.52 =&gt; /usr/lib/x86_64-linux-gnu/libicuuc.so.52 (0x00007f6b0db4f000)\nlibrt.so.1 =&gt; /lib/x86_64-linux-gnu/librt.so.1 (0x00007f6b0d947000)\nlibgcc_s.so.1 =&gt; /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007f6b0d730000)\nlibglapi.so.0 =&gt; /usr/lib/x86_64-linux-gnu/libglapi.so.0 (0x00007f6b0d506000)\nlibXdamage.so.1 =&gt; /usr/lib/x86_64-linux-gnu/libXdamage.so.1 (0x00007f6b0d303000)\nlibXfixes.so.3 =&gt; /usr/lib/x86_64-linux-gnu/libXfixes.so.3 (0x00007f6b0d0fd000)\nlibxcb-dri2.so.0 =&gt; /usr/lib/x86_64-linux-gnu/libxcb-dri2.so.0 (0x00007f6b0cef8000)\nlibxcb-dri3.so.0 =&gt; /usr/lib/x86_64-linux-gnu/libxcb-dri3.so.0 (0x00007f6b0ccf5000)\nlibxcb-present.so.0 =&gt; /usr/lib/x86_64-linux-gnu/libxcb-present.so.0 (0x00007f6b0caf2000)\nlibxshmfence.so.1 =&gt; /usr/lib/x86_64-linux-gnu/libxshmfence.so.1 (0x00007f6b0c8f0000)\nlibXxf86vm.so.1 =&gt; /usr/lib/x86_64-linux-gnu/libXxf86vm.so.1 (0x00007f6b0c6ea000)\nlibdrm.so.2 =&gt; /usr/lib/x86_64-linux-gnu/libdrm.so.2 (0x00007f6b0c4dd000)\nlibgraphite2.so.3 =&gt; /usr/lib/x86_64-linux-gnu/libgraphite2.so.3 (0x00007f6b0c2c1000)\nlibicudata.so.52 =&gt; /usr/lib/x86_64-linux-gnu/libicudata.so.52 (0x00007f6b0aa54000)\n\n\nSo I assumed that problem in linux-vdso.so.1 and tried to solve this by installing this library with apt: sudo apt-get install linux-vdso.so.1\nbut got error that this package doesn't exist. So I am again in a deadlock:(\n    \n\nSolved for me on Ubuntu 15.10 with:\n\nsudo apt-get install libdouble-conversion1v5\n\n\non Ubuntu 14.04 and MINT it should be:\n\nsudo apt-get install libdouble-conversion1\n\n\nHope it helps.\n    \n\nI found the solution to this issue on Linux Mint 17, installing genymotion 2.7.2 here: How to fix: [program name] /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version CXXABI_1.3.8' not found (required by [program name])\n\nI just need to do:\n\nsudo add-apt-repository ppa:ubuntu-toolchain-r/test\nsudo apt-get update\nsudo apt-get install gcc-4.9 g++-4.9\n\n    \n\nThe answer, based on comment of @Atheror:\n\nIt may be a problem of your Linux OS and Genymotion compatibility. Check version of your Linux using command in terminal:\n\ninxi -S\n\n\nor\n\ncat /etc/*-release\n\n\nAnd try to install the previous version of Genymotion.\nFor my Linux Mint 17 (based on Ubuntu 14.04) installing and runing 'Genymotion for Ubuntu 14.10 and older' gives no errors.\n    \n\nI solve this in xubuntu 16.04 using:\n\nsudo apt-get install libdouble-conversion1v5\n\n\nAnd running ./genymotion again.\n    \n\nRun following commands in terminal (ALT+CTRL+T)\n\nLD_LIBRARY_PATH=/usr/local/lib64/:$LD_LIBRARY_PATH\nexport LD_LIBRARY_PATH\n\nsudo add-apt-repository ppa:ubuntu-toolchain-r/test\nsudo apt-get update\n\nsudo apt-get install gcc-4.9 g++-4.9\n\nafter that run the genymotion. that will work perfectly\n    ", "document_id": 1097}]}, {"paragraphs": [{"qas": [{"question": "What is Angular?", "id": 19, "answers": [{"answer_id": 20, "document_id": 40, "question_id": 19, "text": "Angular is a development platform, built on TypeScript.", "answer_start": 5282, "answer_category": null}], "is_impossible": false}, {"question": "What does Angular include", "id": 21, "answers": [{"answer_id": 22, "document_id": 40, "question_id": 21, "text": "Angular includes:\n\nA component-based framework for building scalable web applications\nA collection of well-integrated libraries that cover a wide variety of features, including routing, forms management, client-server communication, and more\nA suite of developer tools to help you develop, build, test, and update your code", "answer_start": 5353, "answer_category": null}], "is_impossible": false}, {"question": "What are the advatanges of Angular?", "id": 22, "answers": [{"answer_id": 23, "document_id": 40, "question_id": 22, "text": "With Angular, you're taking advantage of a platform that can scale from single-developer projects to enterprise-level applications. Angular is designed to make updating as straightforward as possible, so take advantage of the latest developments with a minimum of effort. ", "answer_start": 5678, "answer_category": null}], "is_impossible": false}], "context": "What is Angular?\nCookies concent notice This site uses cookies from Google to deliver its services and to analyze traffic.  Learn more  OK, got it Skip to main content\n\n\nHelp Angular by taking a 1 minute survey!Go to survey\n\n\n\n\ndark_mode\n\n\n\n\n\nAbout Angular\n\nFeaturesResourcesEventsBlogIntroductionGetting Started\n\nWhat is Angular?Try it\n\nGetting startedAdding navigationManaging DataUsing Forms for User InputDeploying an applicationSetupUnderstanding Angular\n\nComponents\n\nOverviewComponent LifecycleView EncapsulationComponent InteractionComponent StylesSharing data between child and parent directives and componentsContent ProjectionDynamic ComponentsAngular ElementsTemplates\n\nIntroductionText interpolationTemplate statementsPipesProperty bindingAttribute, class, and style bindingsEvent bindingTwo-way bindingTemplate reference variablesSVG as templatesDirectives\n\nBuilt-in directivesAttribute DirectivesStructural DirectivesDependency Injection\n\nAngular Dependency InjectionDI ProvidersDeveloper Guides\n\nRouting & Navigation\n\nOverviewCommon routing tasksTutorial: Routing in Single-page ApplicationsTutorial: Creating custom route matchesTutorial: Adding routing to Tour of HeroesRouter referenceForms\n\nIntroductionReactive FormsValidate form inputBuilding Dynamic FormsHTTP ClientTesting\n\nIntro to TestingCode CoverageTesting ServicesBasics of Testing ComponentsComponent Testing ScenariosTesting Attribute DirectivesTesting PipesDebugging TestsTesting Utility APIsInternationalization\n\nOverviewCommon Internationalization tasks\n\nCommon Internationalization tasksAdd the localize packageRefer to locales by IDFormat data based on localePrepare templates for translationsWork with translation filesMerge translations into the appDeploy multiple localesExample Angular applicationOptional Internationalization practices\n\nOptional Internationalization practicesSet the source locale manuallyImport global variants of the locale dataManage marked text with custom IDsAnimations\n\nIntroductionTransition and TriggersComplex SequencesReusable AnimationsRoute Transition AnimationsService Workers & PWA\n\nIntroductionGetting StartedApp ShellService Worker CommunicationService Worker NotificationsService Worker in ProductionService Worker ConfigurationWeb WorkersServer-side RenderingPrerenderingBest Practices\n\nSecurityAccessibilityKeeping Up-to-DateProperty Binding Best PracticesLazy Loading Feature ModulesLightweight Injection Tokens for LibrariesAngular Tools\n\nDev Workflow\n\nDeploying applicationsAOT Compiler\n\nAhead-of-Time CompilationAngular Compiler OptionsAOT Metadata ErrorsTemplate Type-checkingBuilding & ServingCLI BuildersLanguage ServiceDevToolsSchematics\n\nSchematics OverviewAuthoring SchematicsSchematics for LibrariesTutorials\n\nTutorial: Tour of Heroes\n\nIntroductionCreate a Project1. The Hero Editor2. Display a List3. Create a Feature Component4. Add Services5. Add Navigation6. Get Data from a ServerBuilding a Template-driven FormAngular Libraries\n\nLibraries OverviewUsing Published LibrariesCreating LibrariesRelease Information\n\nRelease PracticesRoadmapBrowser SupportUpdating to Version 12\n\nOverviewIvy Compatibility GuideMigration: Legacy Localization IDsDeprecationsAngular IvyUpgrading from AngularJS\n\nUpgrading InstructionsSetup for Upgrading from AngularJSUpgrading for PerformanceAngularJS-Angular ConceptsReference\n\nConceptual Reference\n\nAngular Concepts\n\nIntro to Basic ConceptsIntro to ModulesIntro to ComponentsIntro to Services and DINext StepsBinding syntaxHow event binding worksTemplate variablesWorkspace and project structure\n\nProject File StructureWorkspace Configurationnpm DependenciesTypeScript ConfigurationNgModules\n\nNgModules IntroductionJS Modules vs NgModulesLaunching Apps with a Root ModuleFrequently Used NgModulesTypes of Feature ModulesEntry ComponentsFeature ModulesProviding DependenciesSingleton ServicesSharing NgModulesNgModule APINgModule FAQsObservables & RxJS\n\nObservables OverviewThe RxJS LibraryObservables in AngularPractical UsageCompare to Other TechniquesDependency injection\n\nHierarchical InjectorsDI in ActionCLI Command Reference\n\nOverviewUsage Analyticsng addng analyticsng buildng configng deployng docng e2eng extract-i18nng generateng helpng lintng newng runng serveng testng updateng versionAPI ReferenceError Reference\n\nNG0100: Expression Changed After CheckedNG0200: Circular Dependency in DING0201: No Provider FoundNG0300: Selector CollisionNG0301: Export Not FoundNG0302: Pipe Not FoundNG1001: Argument Not LiteralNG2003: Missing TokenNG2009: Invalid Shadow DOM selectorNG3003: Import Cycle DetectedNG6999: Invalid metadataNG8001: Invalid ElementNG8002: Invalid AttributeNG8003: Missing Reference TargetExample applicationsAngular GlossaryAngular Style and Usage\n\nQuick ReferenceCoding Style GuideContent Contributor's Guide\n\nOverviewReviewing contentUpdating search keywordsUpdating content using GitHub UIDocumentation Style GuideAngular doc localization guidelinesstable (v12.2.8)\n\nmode_edit\n\n\nWhat is Angular? Contents\n\nAngular applications: The essentialsComponentsTemplatesDependency injectionAngular CLIFirst-party librariesNext steps\nThis topic can help you understand Angular: what Angular is, what advantages it provides, and what you might expect as you start to build your applications.\nAngular is a development platform, built on TypeScript. As a platform, Angular includes:\n\nA component-based framework for building scalable web applications\nA collection of well-integrated libraries that cover a wide variety of features, including routing, forms management, client-server communication, and more\nA suite of developer tools to help you develop, build, test, and update your code\n\nWith Angular, you're taking advantage of a platform that can scale from single-developer projects to enterprise-level applications. Angular is designed to make updating as straightforward as possible, so take advantage of the latest developments with a minimum of effort. Best of all, the Angular ecosystem consists of a diverse group of over 1.7 million developers, library authors, and content creators.\n\nSee the live example / download example for a working example containing the code snippets in this guide.\n\n\nAngular applications: The essentials\nThis section explains the core ideas behind Angular. Understanding these ideas can help you design and build your applications more effectively.\n\nComponents\nComponents are the building blocks that compose an application. A component includes a TypeScript class with a @Component() decorator, an HTML template, and styles. The @Component() decorator specifies the following Angular-specific information:\n\nA CSS selector that defines how the component is used in a template. HTML elements in your template that match this selector become instances of the component.\nAn HTML template that instructs Angular how to render the component.\nAn optional set of CSS styles that define the appearance of the template's HTML elements.\n\nThe following is a minimal Angular component.\n\ncontent_copy\n\nimport { Component } from '@angular/core';\n\n@Component({\nselector: 'hello-world',\ntemplate: `\n<h2>Hello World</h2>\n<p>This is my first component!</p>\n`\n})\nexport class HelloWorldComponent {\n// The code in this class drives the component's behavior.\n}\n\nTo use this component, you write the following in a template:\n\ncontent_copy\n\n<hello-world></hello-world>\n\nWhen Angular renders this component, the resulting DOM looks like this:\n\ncontent_copy\n\n<hello-world>\n<h2>Hello World</h2>\n<p>This is my first component!</p>\n</hello-world>\n\nAngular's component model offers strong encapsulation and an intuitive application structure. Components also make your application painless to unit test and can improve the overall readability of your code.\nFor more information on what to do with components, see the Components section.\n\nTemplates\nEvery component has an HTML template that declares how that component renders. You define this template either inline or by file path.\nAngular extends HTML with additional syntax that lets you insert dynamic values from your component. Angular automatically updates the rendered DOM when your component\u2019s state changes. One application of this feature is inserting dynamic text, as shown in the following example.\n\ncontent_copy\n\n<p>{{ message }}</p>\n\nThe value for message comes from the component class:\n\ncontent_copy\n\nimport { Component } from '@angular/core';\n\n@Component ({\nselector: 'hello-world-interpolation',\ntemplateUrl: './hello-world-interpolation.component.html'\n})\nexport class HelloWorldInterpolationComponent {\nmessage = 'Hello, World!';\n}\n\nWhen the application loads the component and its template, the user sees the following:\n\ncontent_copy\n\n<p>Hello, World!</p>\n\nNotice the use of double curly braces--they instruct Angular to interpolate the contents within them.\nAngular also supports property bindings, to help you set values for properties and attributes of HTML elements and pass values to your application's presentation logic.\n\ncontent_copy\n\n<p\n[id]=\"sayHelloId\"\n[style.color]=\"fontColor\">\nYou can set my color in the component!\n</p>\n\nNotice the use of the square brackets--that syntax indicates that you're binding the property or attribute to a value in the component class.\nDeclare event listeners to listen for and respond to user actions such as keystrokes, mouse movements, clicks, and touches. You declare an event listener by specifying the event name in parentheses:\n\ncontent_copy\n\n<button\n[disabled]=\"canClick\"\n(click)=\"sayMessage()\">\nTrigger alert message\n</button>\n\nThe preceding example calls a method, which is defined in the component class:\n\ncontent_copy\n\nsayMessage() {\nalert(this.message);\n}\n\nThe following is a combined example of Interpolation, Property Binding and Event Binding within an Angular template:\nhello-world-bindings.component.tshello-world-bindings.component.html\ncontent_copy\n\nimport { Component } from '@angular/core';\u00a0@Component ({  selector: 'hello-world-bindings',  templateUrl: './hello-world-bindings.component.html'})export class HelloWorldBindingsComponent {  fontColor = 'blue';  sayHelloId = 1;  canClick = false;  message = 'Hello, World';\u00a0  sayMessage() {    alert(this.message);  }\u00a0}\n\nAdd additional functionality to your templates through the use of directives. The most popular directives in Angular are *ngIf and *ngFor. Use directives to perform a variety of tasks, such as dynamically modifying the DOM structure. And create your own custom directives to create great user experiences.\nThe following code is an example of the *ngIf directive.\nhello-world-ngif.component.tshello-world-ngif.component.html\ncontent_copy\n\nimport { Component } from '@angular/core';\u00a0@Component({  selector: 'hello-world-ngif',  templateUrl: './hello-world-ngif.component.html'})export class HelloWorldNgIfComponent {  message = 'I\\'m read only!';  canEdit = false;\u00a0  onEditClick() {    this.canEdit = !this.canEdit;    if (this.canEdit) {      this.message = 'You can edit me!';    } else {      this.message = 'I\\'m read only!';    }  }}\n\nAngular's declarative templates let you cleanly separate your application's logic from its presentation. Templates are based on standard HTML, for ease in building, maintaining, and updating.\nFor more information on templates, see the Templates section.\n\nDependency injection\nDependency injection lets you declare the dependencies of your TypeScript classes without taking care of their instantiation. Instead, Angular handles the instantiation for you. This design pattern lets you write more testable and flexible code. Even though understanding dependency injection is not critical to start using Angular, we strongly recommend it as a best practice and many aspects of Angular take advantage of it to some degree.\nTo illustrate how dependency injection works, consider the following example. The first file, logger.service.ts, defines a Logger class. This class contains a writeCount function that logs a number to the console.\n\ncontent_copy\n\nimport { Injectable } from '@angular/core';\n\n@Injectable({providedIn: 'root'})\nexport class Logger {\nwriteCount(count: number) {\nconsole.warn(count);\n}\n}\n\nNext, the hello-world-di.component.ts file defines an Angular component. This component contains a button that uses the writeCount function of the Logger class. To access that function, the Logger service is injected into the HelloWorldDI class by adding private logger: Logger to the constructor.\n\ncontent_copy\n\nimport { Component } from '@angular/core';\nimport { Logger } from '../logger.service';\n\n@Component({\nselector: 'hello-world-di',\ntemplateUrl: './hello-world-di.component.html'\n})\nexport class HelloWorldDependencyInjectionComponent  {\ncount = 0;\n\nconstructor(private logger: Logger) { }\n\nonLogMe() {\nthis.logger.writeCount(this.count);\nthis.count++;\n}\n}\n\nFor more information about dependency injection and Angular, see the Dependency injection in Angular section.\n\nAngular CLI\nThe Angular CLI is the fastest, straightforward, and recommended way to develop Angular applications. The Angular CLI makes a number of tasks trouble-free. Here are some examples:\n\n\nng build\nCompiles an Angular app into an output directory.\n\n\nng serve\nBuilds and serves your application, rebuilding on file changes.\n\n\nng generate\nGenerates or modifies files based on a schematic.\n\n\nng test\nRuns unit tests on a given project.\n\n\nng e2e\nBuilds and serves an Angular application, then runs end-to-end tests.\n\n\nYou'll find the Angular CLI a valuable tool for building out your applications.\nFor more information about the Angular CLI, see the CLI Reference section.\n\nFirst-party libraries\nThe section, Angular applications: The essentials, provides a brief overview of a couple of the key architectural elements you'll use when building Angular applications. But the many benefits of Angular really become apparent when your application grows and you want to add additional functions such as site navigation or user input. Use the Angular platform to incorporate one of the many first-party libraries that Angular provides.\nSome of the libraries available to you include:\n\n\nAngular Router\nAdvanced client-side navigation and routing based on Angular components. Supports lazy-loading, nested routes, custom path matching, and more.\n\n\nAngular Forms\nUniform system for form participation and validation.\n\nAngular HttpClient\nRobust HTTP client that can power more advanced client-server communication.\n\n\nAngular Animations\nRich system for driving animations based on application state.\n\n\nAngular PWA\nTools for building Progressive Web Applications (PWAs) including a service worker and Web app manifest.\n\n\nAngular Schematics\nAutomated scaffolding, refactoring, and update tools that simplify development at large scale.\n\n\nThese libraries expand your application's functionality while also letting you focus more on the features that make your application unique. Add these libraries knowing that they're designed to integrate seamlessly into and update simultaneously with the Angular framework.\nThese libraries are only required if and when they can help you add functionality to your applications or solve a particular problem.\nNext steps\nThis topic is intended to give you a brief overview of what Angular is, the advantages it provides, and what to expect as you start to build your applications.\nTo see Angular in action, see our Getting Started tutorial. This tutorial uses stackblitz.com, for you to explore a working example of Angular without any installation requirements.\nTo explore Angular's capabilities further, we recommend reading through the sections, Understanding Angular and Developer Guides.\nLast reviewed on Tue Sep 14 2021\n\n\nResourcesAboutResource ListingPress KitBlogUsage AnalyticsHelpStack OverflowJoin DiscordGitterReport IssuesCode of ConductCommunityEventsMeetupsTwitterGitHubContributeLanguagesEspa\u00f1ol\u7b80\u4f53\u4e2d\u6587\u7248\u6b63\u9ad4\u4e2d\u6587\u7248\u65e5\u672c\u8a9e\u7248\ud55c\uad6d\uc5b4Complete language list Super-powered by Google \u00a92010-2021.\nCode licensed under an MIT-style License. Documentation licensed under CC BY 4.0.\nVersion 12.2.9-local+sha.f45c692dd9.\n\n\n&amp;amp;lt;div class=\"background-sky hero\"&amp;amp;gt;&amp;amp;lt;/div&amp;amp;gt;\n&amp;amp;lt;section id=\"intro\" style=\"text-shadow: 1px 1px #1976d2;\"&amp;amp;gt;\n&amp;amp;lt;div class=\"hero-logo\"&amp;amp;gt;&amp;amp;lt;/div&amp;amp;gt;\n&amp;amp;lt;div class=\"homepage-container\"&amp;amp;gt;\n&amp;amp;lt;div class=\"hero-headline\"&amp;amp;gt;The modern web&amp;amp;lt;br&amp;amp;gt;developer's platform&amp;amp;lt;/div&amp;amp;gt;\n&amp;amp;lt;/div&amp;amp;gt;\n&amp;amp;lt;/section&amp;amp;gt;\n&amp;amp;lt;h2 style=\"color: red; margin-top: 40px; position: relative; text-align: center; text-shadow: 1px 1px #fafafa; border-top: none;\"&amp;amp;gt;\n&amp;amp;lt;b&amp;amp;gt;&amp;amp;lt;i&amp;amp;gt;This website requires JavaScript.&amp;amp;lt;/i&amp;amp;gt;&amp;amp;lt;/b&amp;amp;gt;\n&amp;amp;lt;/h2&amp;amp;gt;\n\n\n\n\n", "document_id": 40}]}, {"paragraphs": [{"qas": [{"question": "Capistrano + Git : repository local to production server", "id": 1049, "answers": [{"answer_id": 1044, "document_id": 630, "question_id": 1049, "text": "This should be set to the URL you use to access the repository from your development computer/laptop.\nset :repository, \"file:///srv/git/myapp.git\"\nset :local_repository, \"nameOfHostFromSSHConfig:/srv/git/myapp.git\"", "answer_start": 274, "answer_category": null}], "is_impossible": false}], "context": "I am trying to do 'deploy:cold' for my app. The git repo is local to my deployment server (i.e. I only have one server for everything and I don't host my code on github).\nJust ran into the same problem. The key is to not use deploy_via copy but rather set :local_repository\nThis should be set to the URL you use to access the repository from your development computer/laptop.\nset :repository, \"file:///srv/git/myapp.git\"\nset :local_repository, \"nameOfHostFromSSHConfig:/srv/git/myapp.git\"\n", "document_id": 630}]}, {"paragraphs": [{"qas": [{"question": "Deploying with Git/Github", "id": 1071, "answers": [{"answer_id": 1064, "document_id": 649, "question_id": 1071, "text": "You should look for http://beanstalkapp.com seems the best choice. It has automatic deployment feature and supports Git.", "answer_start": 501, "answer_category": null}], "is_impossible": false}], "context": "We are trying to setup an automated deployment environemt with Git/Github. We have 3 different environments; local, test and live. When we add a new feature on local, we first upload files to test server to test the newly created feature. If everything is OK, we than upload all files to live server. But this \"uploading\" process is not a perfect solution, as we sometimes forget to upload some files. Btw we also have mobile app on iPhone and Android, so mobile may be the fourth environment for us.\nYou should look for http://beanstalkapp.com seems the best choice. It has automatic deployment feature and supports Git.\n", "document_id": 649}]}, {"paragraphs": [{"qas": [{"question": "What is python support community?", "id": 229, "answers": [{"answer_id": 237, "document_id": 120, "question_id": 229, "text": " Python has an active supporting community of contributors and users that also make their software available for other Python developers to use under open source license terms.\n", "answer_start": 103, "answer_category": null}], "is_impossible": false}, {"question": "What is python virtual environment?", "id": 230, "answers": [{"answer_id": 238, "document_id": 120, "question_id": 230, "text": "A virtual environment is a semi-isolated Python environment that allows packages to be installed for use by a particular application, rather than being installed system wide.", "answer_start": 1098, "answer_category": null}], "is_impossible": false}, {"question": "What is vene in python?", "id": 231, "answers": [{"answer_id": 239, "document_id": 120, "question_id": 231, "text": "venv is the standard tool for creating virtual environments, and has been part of Python since Python 3.3. Starting with Python 3.4, it defaults to installing pip into all created virtual environments.", "answer_start": 1274, "answer_category": null}], "is_impossible": false}, {"question": "What is Python Packaging Authority?", "id": 232, "answers": [{"answer_id": 240, "document_id": 120, "question_id": 232, "text": "the Python Packaging Authority is the group of developers and documentation authors responsible for the maintenance and evolution of the standard packaging tools and the associated metadata and file format standards. They maintain a variety of tools, documentation, and issue trackers on both GitHub and Bitbucket.", "answer_start": 1862, "answer_category": null}], "is_impossible": false}, {"question": "What is Python Package Index?", "id": 233, "answers": [{"answer_id": 241, "document_id": 120, "question_id": 233, "text": "The Python Package Index is a public repository of open source licensed packages made available for use by other Python users.", "answer_start": 1734, "answer_category": null}], "is_impossible": false}, {"question": "What is python virtualenv?\n", "id": 234, "answers": [{"answer_id": 242, "document_id": 120, "question_id": 234, "text": "virtualenv is a third party alternative (and predecessor) to venv. It allows virtual environments to be used on versions of Python prior to 3.4, which either don\u2019t provide venv at all, or aren\u2019t able to automatically install pip into created environments", "answer_start": 1477, "answer_category": null}], "is_impossible": false}, {"question": "What is distutils in python?", "id": 235, "answers": [{"answer_id": 243, "document_id": 120, "question_id": 235, "text": "distutils is the original build and distribution system first added to the Python standard library in 1998. ", "answer_start": 2178, "answer_category": null}], "is_impossible": false}, {"question": "How to create python virtual environment\uff1f", "id": 236, "answers": [{"answer_id": 244, "document_id": 120, "question_id": 236, "text": "Creation of virtual environments is done through the venv module.", "answer_start": 4031, "answer_category": null}], "is_impossible": false}, {"question": "How can i install pip in versions of Python prior to Python 3.4?", "id": 237, "answers": [{"answer_id": 245, "document_id": 120, "question_id": 237, "text": "Python only started bundling pip with Python 3.4. For earlier versions, pip needs to be \u201cbootstrapped\u201d as described in the Python Packaging User Guide.", "answer_start": 4388, "answer_category": null}], "is_impossible": false}, {"question": "How can i install packages just for current user?", "id": 238, "answers": [{"answer_id": 246, "document_id": 120, "question_id": 238, "text": "Passing the --user option to python -m pip install will install a package just for the current user, rather than for all users of the system.", "answer_start": 4662, "answer_category": null}], "is_impossible": false}, {"question": "How can I install scientific python packages?", "id": 239, "answers": [{"answer_id": 247, "document_id": 120, "question_id": 239, "text": "A number of scientific Python packages have complex binary dependencies, and aren\u2019t currently easy to install using pip directly. At this point in time, it will often be easier for users to install these packages by other means rather than attempting to install them with pip.", "answer_start": 4843, "answer_category": null}], "is_impossible": false}, {"question": "work with multiple versions of Python installed in parallel on Macos?", "id": 240, "answers": [{"answer_id": 248, "document_id": 120, "question_id": 240, "text": "On Linux, macOS, and other POSIX systems, use the versioned Python commands in combination with the -m switch to run the appropriate copy of pip:\n\npython2   -m pip install SomePackage  # default Python 2\npython2.7 -m pip install SomePackage  # specifically Python 2.7\npython3   -m pip install SomePackage  # default Python 3\npython3.4 -m pip install SomePackage  # specifically Python 3.4\nAppropriately versioned pip commands may also be available.", "answer_start": 5253, "answer_category": null}], "is_impossible": false}, {"question": "work with multiple versions of Python installed in parallel on windows?", "id": 241, "answers": [{"answer_id": 249, "document_id": 120, "question_id": 241, "text": "On Windows, use the py Python launcher in combination with the -m switch:\n\npy -2   -m pip install SomePackage  # default Python 2\npy -2.7 -m pip install SomePackage  # specifically Python 2.7\npy -3   -m pip install SomePackage  # default Python 3\npy -3.4 -m pip install SomePackage  # specifically Python 3.4", "answer_start": 5703, "answer_category": null}], "is_impossible": false}], "context": "Installing Python Modules\nEmail\ndistutils-sig@python.org\n\nAs a popular open source development project, Python has an active supporting community of contributors and users that also make their software available for other Python developers to use under open source license terms.\n\nThis allows Python users to share and collaborate effectively, benefiting from the solutions others have already created to common (and sometimes even rare!) problems, as well as potentially contributing their own solutions to the common pool.\n\nThis guide covers the installation part of the process. For a guide to creating and sharing your own Python projects, refer to the distribution guide.\n\nNote For corporate and other institutional users, be aware that many organisations have their own policies around using and contributing to open source software. Please take such policies into account when making use of the distribution and installation tools provided with Python.\nKey terms\npip is the preferred installer program. Starting with Python 3.4, it is included by default with the Python binary installers.\n\nA virtual environment is a semi-isolated Python environment that allows packages to be installed for use by a particular application, rather than being installed system wide.\n\nvenv is the standard tool for creating virtual environments, and has been part of Python since Python 3.3. Starting with Python 3.4, it defaults to installing pip into all created virtual environments.\n\nvirtualenv is a third party alternative (and predecessor) to venv. It allows virtual environments to be used on versions of Python prior to 3.4, which either don\u2019t provide venv at all, or aren\u2019t able to automatically install pip into created environments.\n\nThe Python Package Index is a public repository of open source licensed packages made available for use by other Python users.\n\nthe Python Packaging Authority is the group of developers and documentation authors responsible for the maintenance and evolution of the standard packaging tools and the associated metadata and file format standards. They maintain a variety of tools, documentation, and issue trackers on both GitHub and Bitbucket.\n\ndistutils is the original build and distribution system first added to the Python standard library in 1998. While direct use of distutils is being phased out, it still laid the foundation for the current packaging and distribution infrastructure, and it not only remains part of the standard library, but its name lives on in other ways (such as the name of the mailing list used to coordinate Python packaging standards development).\n\nChanged in version 3.5: The use of venv is now recommended for creating virtual environments.\n\nSee also Python Packaging User Guide: Creating and using virtual environments\nBasic usage\nThe standard packaging tools are all designed to be used from the command line.\n\nThe following command will install the latest version of a module and its dependencies from the Python Package Index:\n\npython -m pip install SomePackage\nNote For POSIX users (including macOS and Linux users), the examples in this guide assume the use of a virtual environment.\nFor Windows users, the examples in this guide assume that the option to adjust the system PATH environment variable was selected when installing Python.\n\nIt\u2019s also possible to specify an exact or minimum version directly on the command line. When using comparator operators such as >, < or some other special character which get interpreted by shell, the package name and the version should be enclosed within double quotes:\n\npython -m pip install SomePackage==1.0.4    # specific version\npython -m pip install \"SomePackage>=1.0.4\"  # minimum version\nNormally, if a suitable module is already installed, attempting to install it again will have no effect. Upgrading existing modules must be requested explicitly:\n\npython -m pip install --upgrade SomePackage\nMore information and resources regarding pip and its capabilities can be found in the Python Packaging User Guide.\n\nCreation of virtual environments is done through the venv module. Installing packages into an active virtual environment uses the commands shown above.\n\nSee also Python Packaging User Guide: Installing Python Distribution Packages\nHow do I \u2026?\nThese are quick answers or links for some common tasks.\n\n\u2026 install pip in versions of Python prior to Python 3.4?\nPython only started bundling pip with Python 3.4. For earlier versions, pip needs to be \u201cbootstrapped\u201d as described in the Python Packaging User Guide.\n\nSee also Python Packaging User Guide: Requirements for Installing Packages\n\u2026 install packages just for the current user?\nPassing the --user option to python -m pip install will install a package just for the current user, rather than for all users of the system.\n\n\u2026 install scientific Python packages?\nA number of scientific Python packages have complex binary dependencies, and aren\u2019t currently easy to install using pip directly. At this point in time, it will often be easier for users to install these packages by other means rather than attempting to install them with pip.\n\nSee also Python Packaging User Guide: Installing Scientific Packages\n\u2026 work with multiple versions of Python installed in parallel?\nOn Linux, macOS, and other POSIX systems, use the versioned Python commands in combination with the -m switch to run the appropriate copy of pip:\n\npython2   -m pip install SomePackage  # default Python 2\npython2.7 -m pip install SomePackage  # specifically Python 2.7\npython3   -m pip install SomePackage  # default Python 3\npython3.4 -m pip install SomePackage  # specifically Python 3.4\nAppropriately versioned pip commands may also be available.\n\nOn Windows, use the py Python launcher in combination with the -m switch:\n\npy -2   -m pip install SomePackage  # default Python 2\npy -2.7 -m pip install SomePackage  # specifically Python 2.7\npy -3   -m pip install SomePackage  # default Python 3\npy -3.4 -m pip install SomePackage  # specifically Python 3.4\nCommon installation issues\nInstalling into the system Python on Linux\nOn Linux systems, a Python installation will typically be included as part of the distribution. Installing into this Python installation requires root access to the system, and may interfere with the operation of the system package manager and other components of the system if a component is unexpectedly upgraded using pip.\n\nOn such systems, it is often better to use a virtual environment or a per-user installation when installing packages with pip.\n\nPip not installed\nIt is possible that pip does not get installed by default. One potential fix is:\n\npython -m ensurepip --default-pip\nThere are also additional resources for installing pip.\n\nInstalling binary extensions\nPython has typically relied heavily on source based distribution, with end users being expected to compile extension modules from source as part of the installation process.\n\nWith the introduction of support for the binary wheel format, and the ability to publish wheels for at least Windows and macOS through the Python Package Index, this problem is expected to diminish over time, as users are more regularly able to install pre-built extensions rather than needing to build them themselves.\n\nSome of the solutions for installing scientific software that are not yet available as pre-built wheel files may also help with obtaining other binary extensions without needing to build them locally.\n\nSee also Python Packaging User Guide: Binary Extensions", "document_id": 120}]}, {"paragraphs": [{"qas": [{"question": "Build and Deploy a Web Application with TFS 2015 Build", "id": 1247, "answers": [{"answer_id": 1240, "document_id": 819, "question_id": 1247, "text": "You can use \"Visual Studio Build\" step and as Arguments for MSBuild use following line:\n/p:DeployOnBuild=True /p:PublishProfile=$(DeploymentConfiguration)", "answer_start": 674, "answer_category": null}], "is_impossible": false}], "context": "We have just installed TFS 2015 (Update 1) on-premise and are trying to create a Continuous Integration/Build system using the new TFS Build system. The build works fine, and gives me a green light, but when I look at the default build it has only built the binaries from the bin directory, and there seems to be no easy way to deploy the app on-premise to a local server.\nThere are two deploy options for a filesystem copy, and a powershell script, and it would certainly be easy enough to use them to copy files to a new server, but since the build only built the binaries, I don't see a tool to gather up the Web artifacts (cshtml, images, scripts, css, etc..) for this.\nYou can use \"Visual Studio Build\" step and as Arguments for MSBuild use following line:\n/p:DeployOnBuild=True /p:PublishProfile=$(DeploymentConfiguration)\n", "document_id": 819}]}, {"paragraphs": [{"qas": [{"question": "NSIS - How to include all folders from source to Installer", "id": 1230, "answers": [{"answer_id": 1223, "document_id": 806, "question_id": 1230, "text": "So you would use something like this:\nFile /r \"c:\\MyProject\\MyApp\\*\"", "answer_start": 398, "answer_category": null}], "is_impossible": false}], "context": "I have an application which consists one .exe, many .dlls and a few folders.\nI use NSIS to create an installer. It works but when I install the software, I don't see all the folders inside my application. What do I have to do to bundle all the folders within my application into the installer?\nThe documentation tells us that the /r argument of the File command includes all sub folders and files. So you would use something like this:\nFile /r \"c:\\MyProject\\MyApp\\*\"\nThe relevant section of the documentation can be found here: http://nsis.sourceforge.net/Docs/Chapter4.html#file\n", "document_id": 806}]}, {"paragraphs": [{"qas": [{"question": "no acceptable c compiler found when installing apc on Debian?", "id": 1455, "answers": [{"answer_id": 1444, "document_id": 1028, "question_id": 1455, "text": "sudo apt-get install build-essential", "answer_start": 2403, "answer_category": null}], "is_impossible": false}, {"question": "no acceptable c compiler found when installing apc on Ubuntu?", "id": 1456, "answers": [{"answer_id": 1445, "document_id": 1028, "question_id": 1456, "text": "sudo apt-get install build-essential", "answer_start": 2403, "answer_category": null}], "is_impossible": false}, {"question": "no acceptable c compiler found when installing apc on Redhat?", "id": 1457, "answers": [{"answer_id": 1446, "document_id": 1028, "question_id": 1457, "text": "sudo yum groupinstall \"Development Tools\"", "answer_start": 2467, "answer_category": null}], "is_impossible": false}, {"question": "no acceptable c compiler found when installing apc on Centos?", "id": 1458, "answers": [{"answer_id": 1447, "document_id": 1028, "question_id": 1458, "text": "sudo yum groupinstall \"Development Tools\"", "answer_start": 2467, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nWhat do I need to do to get APC to work correctly, it seems I dont have a c compiler in the correct place or something similar to that? Also once it is install how do I verify that it is running correctly?\n\n[root@ec2-user]# pecl install apc\ndownloading APC-3.1.9.tgz ...\nStarting to download APC-3.1.9.tgz (155,540 bytes)\n.................................done: 155,540 bytes\n54 source files, building\nrunning: phpize\nConfiguring for:\nPHP Api Version:         20090626\nZend Module Api No:      20090626\nZend Extension Api No:   220090626\nconfig.m4:180: warning: AC_CACHE_VAL(PHP_APC_GCC_ATOMICS, ...): suspicious cache-id,                     must contain _cv_ to be cached\n../../lib/autoconf/general.m4:1974: AC_CACHE_VAL is expanded from...\n../../lib/autoconf/general.m4:1994: AC_CACHE_CHECK is expanded from...\nconfig.m4:180: the top level\nconfig.m4:180: warning: AC_CACHE_VAL(PHP_APC_GCC_ATOMICS, ...): suspicious cache-id,     must contain _cv_ to be cached\n../../lib/autoconf/general.m4:1974: AC_CACHE_VAL is expanded from...\n../../lib/autoconf/general.m4:1994: AC_CACHE_CHECK is expanded from...\nconfig.m4:180: the top level\nEnable internal debugging in APC [no] : no\nEnable per request file info about files used from the APC cache [no] : no\nEnable spin locks (EXPERIMENTAL) [no] : no\nEnable memory protection (EXPERIMENTAL) [no] : no\nEnable pthread mutexes (default) [yes] : yes\nEnable pthread read/write locks (EXPERIMENTAL) [no] : no\nbuilding in /var/tmp/pear-build-root/APC-3.1.9\nrunning: /var/tmp/APC/configure --enable-apc-debug=no --enable-apc-filehits=no --enable-    apc-spinlocks=no --enable-apc-memprotect=no --enable-apc-pthreadmutex=yes --enable-apc-    pthreadrwlocks=no\nchecking for grep that handles long lines and -e... /bin/grep\nchecking for egrep... /bin/grep -E\nchecking for a sed that does not truncate output... /bin/sed\nchecking for cc... no\nchecking for gcc... no\nconfigure: error: in `/var/tmp/pear-build-root/APC-3.1.9':\nconfigure: error: no acceptable C compiler found in $PATH\nSee `config.log' for more details.\nERROR: `/var/tmp/APC/configure --enable-apc-debug=no --enable-apc-filehits=no --enable-apc-    spinlocks=no --enable-apc-memprotect=no --enable-apc-pthreadmutex=yes --enable-apc-    pthreadrwlocks=no' failed\n[root@ec2-user]#\n\n    \n\nYour system is missing a C compiler (or less likely, it cannot be found).\n\nLikely you just need to do:\n\nsudo apt-get install build-essential\n\n\nfor Debian or Ubuntu or\n\nsudo yum groupinstall \"Development Tools\"\n\n\nfor Red Hat / CentOS.\n\nBy the way, your system's package manager can likely install APC.\n    \n\nLooks like gcc isn't installed. Depending on your release of Linux, the way to install gcc differs. I'm sure if you post which release you're using, we'll be able to help you.\n    \n\nI solved this via the following:\n\n#rpm -qa | grep gcc\n\n# yum install gcc glibc glibc-common gd gd-devel -y\n\n    \n\nOn our CentOS 6 box I used yum install php-pecl-apc - I'd done some other yum installs first such as glib, gcc. But at least you know yum will handle the dependencies properly.\nNot actually seen a particular speed increase with it, but I assume this is where tuning comes in.\n    ", "document_id": 1028}]}, {"paragraphs": [{"qas": [{"question": "Why can't I find ansible when I install it using setup.py?", "id": 1810, "answers": [{"answer_id": 1795, "document_id": 1381, "question_id": 1810, "text": "When you invoke ansible from the shell, bash will search in your $PATH for a file named ansible that is executable. This may not be the only issue, but this is the immediate cause for the error you're seeing. The .egg file itself is not an executable, it's just a file used for distributing the code.", "answer_start": 765, "answer_category": null}], "is_impossible": false}], "context": "Because I had some trouble with Ansible (I'm on mac) which seemed to be fixed in the latest dev version today I uninstalled ansible through pip (sudo pip uninstall ansible) and reinstalled the latest dev version from the github repo using the classic setup.py method, which seemed to end successfully (full output here.\nI checked where it is installed. From the full output I linked to above I found that it is installed in /usr/local/lib/python2.7/site-packages, and indeed in there I find an egg:\nSo I guess the problem is that no symlink is created to the ansible package in /usr/local/bin/. But I'm unsure how I could create such a symlink and why it wouldn't appear in the first place.\nDoes anybody know how I can move forward from here? All tips are welcome!\nWhen you invoke ansible from the shell, bash will search in your $PATH for a file named ansible that is executable. This may not be the only issue, but this is the immediate cause for the error you're seeing. The .egg file itself is not an executable, it's just a file used for distributing the code.\n", "document_id": 1381}]}, {"paragraphs": [{"qas": [{"question": "Determine installed PowerShell version", "id": 1607, "answers": [{"answer_id": 1594, "document_id": 1181, "question_id": 1607, "text": "You can use $PSVersionTable.PSVersion to determine the engine version. If the variable does not exist, it is safe to assume the engine is version 1.0.", "answer_start": 113, "answer_category": null}], "is_impossible": false}], "context": "How can I determine what version of PowerShell is installed on a computer, and indeed if it is installed at all?\nYou can use $PSVersionTable.PSVersion to determine the engine version. If the variable does not exist, it is safe to assume the engine is version 1.0.\n", "document_id": 1181}]}, {"paragraphs": [{"qas": [{"question": "How to install InfluxDB in Windows", "id": 1760, "answers": [{"answer_id": 1747, "document_id": 1332, "question_id": 1760, "text": "The current 0.9 branch of influxdb is pure go and can be compiled on Windows with the following commands:\ncd %GOPATH%/src/github.com/influxdb\ngo get -u -f ./...\ngo build ./...", "answer_start": 202, "answer_category": null}], "is_impossible": false}], "context": "I am new to InfluxDB. I could not find any details about installing InfluxDB on Windows. Is there any way to install it on a Windows machine or do I need to use a Linux server for development purposes?\nThe current 0.9 branch of influxdb is pure go and can be compiled on Windows with the following commands:\ncd %GOPATH%/src/github.com/influxdb\ngo get -u -f ./...\ngo build ./...\n", "document_id": 1332}]}, {"paragraphs": [{"qas": [{"question": "WiX ServiceControl Stop a service on uninstall, but don't start it on install", "id": 1815, "answers": [{"answer_id": 1800, "document_id": 1386, "question_id": 1815, "text": "As per the documentation, the start attribute is optional, so simply omit it entirely.\n <ServiceControl Id=\"StartService\"\n    Stop=\"both\"\n    Remove=\"uninstall\"\n    Name=\"Remec.AteService\"\n    Wait=\"yes\" />", "answer_start": 240, "answer_category": null}], "is_impossible": false}], "context": "I need the service to stop and be removed on its uninstall, but I don't want it to start on install. The problem is, the start attribute on the ServiceControl element does not provide an option to disable starting. Or am I just missing it?\nAs per the documentation, the start attribute is optional, so simply omit it entirely.\n <ServiceControl Id=\"StartService\"\n    Stop=\"both\"\n    Remove=\"uninstall\"\n    Name=\"Remec.AteService\"\n    Wait=\"yes\" />\n", "document_id": 1386}]}, {"paragraphs": [{"qas": [{"question": "python-dev installation error: ImportError: No module named apt_pkg", "id": 882, "answers": [{"answer_id": 877, "document_id": 561, "question_id": 882, "text": "/usr/lib/python3/dist-packages# cp apt_pkg.cpython-34m-i386-linux-gnu.so apt_pkg.so\n\nOr:\n\n/usr/lib/python3/dist-packages# cp apt_pkg.cpython-35m-x86_64-linux-gnu.so apt_pkg.so", "answer_start": 2924, "answer_category": null}], "is_impossible": false}, {"question": "python-dev installation error: ImportError: No module named apt_pkg in", "id": 883, "answers": [{"answer_id": 878, "document_id": 561, "question_id": 883, "text": "1) cd /usr/lib/python3/dist-packages/\n\n2) sudo ln -s apt_pkg.cpython-36m-x86_64-linux-gnu.so apt_pkg.so\n", "answer_start": 4265, "answer_category": null}], "is_impossible": false}, {"question": "occur python-dev installation error: ImportError: No module named apt_pkg", "id": 884, "answers": [{"answer_id": 879, "document_id": 561, "question_id": 884, "text": "When in docker: RUN ln -s /usr/lib/python3/dist-packages/apt_pkg.cpython-36m-x86_64-linux-gnu.so /usr/lib/python3/dist-packages/apt_pkg.so", "answer_start": 4692, "answer_category": null}], "is_impossible": false}, {"question": "get python-dev installation error: ImportError: No module named apt_pkg", "id": 885, "answers": [{"answer_id": 880, "document_id": 561, "question_id": 885, "text": "apt-get install python-apt", "answer_start": 5308, "answer_category": null}], "is_impossible": false}, {"question": "found python-dev installation error: ImportError: No module named apt_pkg", "id": 886, "answers": [{"answer_id": 881, "document_id": 561, "question_id": 886, "text": "remove it using apt-get remove --purge python-apt and install it again", "answer_start": 5450, "answer_category": null}], "is_impossible": false}, {"question": "python-dev installation error: ImportError: No module named apt_pkg happened", "id": 887, "answers": [{"answer_id": 882, "document_id": 561, "question_id": 887, "text": "cd /usr/lib/python3/dist-packages\n\nsudo cp apt_pkg.cpython-36m-x86_64-linux-gnu.so apt_pkg.so\n", "answer_start": 8668, "answer_category": null}], "is_impossible": false}, {"question": "error: python-dev installation error: ImportError: No module named apt_pkg", "id": 888, "answers": [{"answer_id": 883, "document_id": 561, "question_id": 888, "text": "cd /usr/lib/python3/dist-packages\nsudo cp apt_pkg.cpython-36m-x86_64-linux-gnu.so apt_pkg.so", "answer_start": 9431, "answer_category": null}], "is_impossible": false}], "context": "I am Debian user, and I want to install python-dev, but when I run the code in the shell as a root:\n\n# aptitude install python-dev\nI get the following error:\n\nTraceback (most recent call last):       \n  File \"/usr/bin/apt-listchanges\", line 28, in <module>\n    import apt_pkg\nImportError: No module named apt_pkg\nWhat seems to be the problem and how can I resolve it?\n\npython\nlinux\ninstallation\ndebian\nShare\nImprove this question\nFollow\nedited May 8 '17 at 21:37\n\nMark Amery\n119k6363 gold badges370370 silver badges417417 bronze badges\nasked Dec 4 '12 at 17:07\n\nBelphegor\n3,8661111 gold badges3434 silver badges5757 bronze badges\n1\nlooks like your apt ist broken, what happens when you try apt-get install python-dev \u2013 \nPierre Geier\n Dec 5 '12 at 6:45 \nWhen I try with apt-get install python-dev it says that it is already installed: Reading package lists... Done Building dependency tree Reading state information... Done python-dev is already the newest version. 0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded. Any other suggestions? \u2013 \nBelphegor\n Dec 5 '12 at 9:14\nAdd a comment\n23 Answers\n\n164\n\nI met this problem when doing sudo apt-get update. My env is debian8, with python2.7 + 3.4(default) + 3.5.\n\nThe following code will only re-create a apt_pkg....so file for python 3.5\n\nsudo apt-get install python3-apt --reinstall\nThe following code solved my problem,\n\ncd /usr/lib/python3/dist-packages\nsudo ln -s apt_pkg.cpython-{your-version-number}-x86_64-linux-gnu.so apt_pkg.so\nReplace {your-version-number} appropriately.\n\nSo, obviously, python3-apt checks the highest python version, instead of the current python version in use.\n\nShare\nImprove this answer\nFollow\nedited Sep 23 at 13:29\n\nChris\n4,68433 gold badges2424 silver badges3232 bronze badges\nanswered Mar 26 '16 at 7:57\n\nzhazha\n2,21422 gold badges1111 silver badges77 bronze badges\n31\nsudo ln -s apt_pkg.cpython-{35m,34m}-x86_64-linux-gnu.so should be changed to sudo ln -s apt_pkg.cpython-{35m,34m}-x86_64-linux-gnu.so apt_pkg.so \u2013 \ndesaiankitb\n May 17 '18 at 11:56 \n40\nYou are amazing! For me, it was sudo ln -s apt_pkg.cpython-{35m,36m}-x86_64-linux-gnu.so for python3.6, and this horrific bug is now gone. \u2013 \nAlex Gurrola\n May 19 '18 at 21:38\n61\nAfter installing Python 3.7 next to the default 3.6 in Ubuntu 18.04 with sudo apt install python3.7 I got this apt_pkg error trying to run pip, so I needed to run cd /usr/lib/python3/dist-packages then sudo ln -s apt_pkg.cpython-{36m,37m}-x86_64-linux-gnu.so then sudo apt install python3-pip. \u2013 \nabulka\n Feb 28 '19 at 11:35\n3\nfor python 3.6, command will be sudo ln -s apt_pkg.cpython-{35m,36m}-x86_64-linux-gnu.so \u2013 \nStatguyUser\n Mar 11 '19 at 7:46\n2\nThose braces are not a property of the the ln command. They invoke shell brace expansion, see gnu.org/software/bash/manual/html_node/Brace-Expansion.html \u2013 \nCarlos A. Ibarra\n Aug 24 '20 at 16:03 \nShow 4 more comments\n\n\n128\n\nSolve it by this:\n\n/usr/lib/python3/dist-packages# cp apt_pkg.cpython-34m-i386-linux-gnu.so apt_pkg.so\n\nOr:\n\n/usr/lib/python3/dist-packages# cp apt_pkg.cpython-35m-x86_64-linux-gnu.so apt_pkg.so\n\nBasically, if you get a No such file or directory just ls to try to get the right name.\n\nShare\nImprove this answer\nFollow\nedited May 17 '18 at 11:46\n\nLarry\n5211 silver badge88 bronze badges\nanswered Jun 18 '17 at 6:02\n\nuser8178061\n1,28111 gold badge66 silver badges22 bronze badges\nWorks perfectly \u2013 \nSubhrajyoti Sen\n Jan 13 '19 at 19:16\n4\nOn Ubuntu 18.04, use this $ cd /usr/lib/python3/dist-packages $ sudo cp apt_pkg.cpython-36m-x86_64-linux-gnu.so apt_pkg.so \u2013 \nNaren Yellavula\n May 13 '19 at 14:31\nThis worked for me as well... I listed all the files on the /usr/lib/python3/dist-packages, and I saw that I didn't have of apt_pkg.cpython-34m-i386-linux-gnu.so or apt_pkg.cpython-3m-i386-linux-gnu.so, but I had apt_pkg.cpython-36m-i386-linux-gnu.so... I copied this file to apt_pkg.so and worked perfectly! Thanks! \u2013 \nLeonardo Isso\n Sep 25 '19 at 15:57\nThis worked for me as well. As @LeonardoIsso did I also listed all files to find the proper file name because i am on 32 bit Linux. \u2013 \nw3Develops\n Nov 21 '19 at 16:08 \nAdd a comment\n\n86\n\nThis happened to me on Ubuntu 18.04.2 after I tried to install Python3.7 from the deadsnakes repo.\n\nSolution was this\n\n1) cd /usr/lib/python3/dist-packages/\n\n2) sudo ln -s apt_pkg.cpython-36m-x86_64-linux-gnu.so apt_pkg.so\n\nShare\nImprove this answer\nFollow\nedited Sep 21 '19 at 21:16\n\nLoicM\n1,3221212 silver badges2727 bronze badges\nanswered Jul 22 '19 at 14:08\n\nMirror Mirage\n1,00755 silver badges44 bronze badges\n4\nJust wanted to confirm to others that this indeed worked for me. \u2013 \nAnthonyD973\n Oct 10 '19 at 17:51 \n2\nthat's crazy, thank you! When in docker: RUN ln -s /usr/lib/python3/dist-packages/apt_pkg.cpython-36m-x86_64-linux-gnu.so /usr/lib/python3/dist-packages/apt_pkg.so \u2013 \nloretoparisi\n Oct 24 '19 at 13:31\n2\nYep, that was the solution :) \u2013 \nszedjani\n Oct 25 '19 at 12:39\n1\nthis is the solution! After upgrading to python 3.7. \u2013 \nJOHN\n Mar 12 '20 at 4:01\n2\n18.04.04, upgrading to python 3.7 and this worked for me as well! Thanks!! \u2013 \nRJaus\n May 5 '20 at 11:48\nShow 3 more comments\n\n71\n\nMake sure you have a working python-apt package. You could try and remove and install that package again to fix the problem with apt_pkg.so not being located.\n\napt-get install python-apt\nShare\nImprove this answer\nFollow\nanswered Dec 5 '12 at 6:46\n\nArnestig\n2,1551616 silver badges2929 bronze badges\n19\nremove it using apt-get remove --purge python-apt and install it again \u2013 \nArnestig\n Dec 5 '12 at 9:20\n10\nBe aware of other dependencies. I removed the package (too) quickly in Ubuntu and lots of other dependencies were removed as well (e.g. ubuntu-desktop). It's my fault for not paying attention to the notes in the log, however. \u2013 \nAl R.\n Jan 29 '13 at 19:21 \n1\nNot really sure if it was due to python-apt. Look over at packages.ubuntu.com/lucid/ubuntu-desktop for dependencies to ubuntu-desktop. \u2013 \nArnestig\n Jan 29 '13 at 20:07\n3\n@Arnestig REMOVING python-apt sounds rather dangerous!! Learnt my lesson once and I shall not try it. \u2013 \nalvas\n Apr 11 '16 at 1:26\n1\nCompiling python worked for me, here is the link github.com/pypa/pip/issues/5367#issue-320060428 \u2013 \nAbhimanyu\n Aug 28 '19 at 11:17\nShow 4 more comments\n\n27\n\nThis error will often occur when a newer version of python has been installed alongside an older version e.g;\n\nUbuntu 18.04.1 ships with python version 3.6.6\nInstalled ppa:deadsnakes/python3.7.1 or alternative\nRun a command that uses the apt_pkg module and get an error such as;\n\n    from CommandNotFound.db.db import SqliteDatabase\nFile \"/usr/lib/python3/dist-packages/CommandNotFound/db/db.py\", line 5, in <module>\n    import apt_pkg\nWhen we install a non-distro python3 version with apt it will set a shared module directory to be that of python3 most usually it will be /usr/lib/python3.\n\nMost of the time this will be ok, but under some circumstances the different versions of python rely on different libraries or shared objects/libraries than the other python version does, so as other answers have pointed out we need to link the .SO to the correct python version. So if we have python3.6 installed on a 64bit system then the apt_pkg .SO link would be\n\nsudo ln -s apt_pkg.cpython-36m-x86_64-linux-gnu.so apt_pkg.so\nBut the problem lies in the fact that when we install a newer python version the link will update to point to the newest python version, which leads to the error of apt_pkg module not being found. By checking which version of python ships with your distro you can create the link as shown above. Or we use a method to offer the command a choice of python versions to link the .SO such as;\n\nsudo ln -s apt_pkg.cpython-{36m,35m,34m}-x86_64-linux-gnu.so apt_pkg.so\nBecause python will create this link to the newest installed python version we give the command the option to choose from 3 python versions, of which it will choose the highest version given.\n\nShare\nImprove this answer\nFollow\nedited May 21 '19 at 10:34\n\na.t.\n1,32211 gold badge1111 silver badges3838 bronze badges\nanswered Nov 5 '18 at 11:59\n\nJamie Lindsey\n8561010 silver badges2525 bronze badges\n2\nRunning 18.04.2, your first recommendation worked for me and I could finally run the sudo apt-get update with no errors. Your second recommendation return an error saying apt-get.so was not a folder. \u2013 \nCloseISQ\n Apr 28 '19 at 16:47 \nAdd a comment\n\n19\n\nThe solution of @user8178061 worked well but I did it with some modifications for my version wich is python3.7 with Ubuntu\n\nI replaced the apt_pkg.cpython-3m-i386-linux-gnu.so with apt_pkg.cpython-36m-x86_64-linux-gnu.so\n\nHere the two commands to execute:\n\ncd /usr/lib/python3/dist-packages\n\nsudo cp apt_pkg.cpython-36m-x86_64-linux-gnu.so apt_pkg.so\n\nShare\nImprove this answer\nFollow\nanswered Nov 28 '19 at 13:21\n\nHugo Sohm\n1,76522 gold badges1212 silver badges3131 bronze badges\n1\nThis fixed my problem. This solves the problem if it was caused by switching the Python version from 3.6 to 3.7. \u2013 \njumbot\n Dec 5 '19 at 18:39\n1\nThanks, had the same issue! Consider using sudo ln -s apt_pkg.cpython-36m-x86_64-linux-gnu.so apt_pkg.so so it's clear what apt_pkg.so is. \u2013 \nScipio\n Dec 17 '19 at 19:56\nWorked, but I had to change it to sudo cp apt_pkg.cpython-37m-x86_64-linux-gnu.so apt_pkg.so for some reason \u2013 \nDaniel\n Feb 25 '20 at 15:47\nAdd a comment\n\n19\n\nThis worked for me on after updating python3.7 on ubuntu18.04\n\ncd /usr/lib/python3/dist-packages\nsudo cp apt_pkg.cpython-36m-x86_64-linux-gnu.so apt_pkg.so", "document_id": 561}]}, {"paragraphs": [{"qas": [{"question": "'node' is not recognized as an internal or external command", "id": 722, "answers": [{"answer_id": 725, "document_id": 412, "question_id": 722, "text": "By default it adds the following to the path:\nC:\\Program Files\\nodejs\\\nThe ending \\ is unnecessary. Remove the \\ and everything will be beautiful again.", "answer_start": 89, "answer_category": null}], "is_impossible": false}], "context": "Nodejs's installation adds nodejs to the path in the environment properties incorrectly.\nBy default it adds the following to the path:\nC:\\Program Files\\nodejs\\\nThe ending \\ is unnecessary. Remove the \\ and everything will be beautiful again.\n\nI've been working with node.js v0.6.3, locally installed on Windows Vista at C:\\Program Files\\Nodejs. I recently upgraded to (by running the installer for) v0.6.6. I tried rebooting, removing node, reinstalling, reinstalling 0.6.3 - nothing seems to work. I just don't get why node fails to recognize system path, though node works from its base dir?\n", "document_id": 412}]}, {"paragraphs": [{"qas": [{"question": "When installing packages with Yarn, what does \"incorrect peer dependency\" mean?", "id": 1899, "answers": [{"answer_id": 1886, "document_id": 1470, "question_id": 1899, "text": "It is only a warning as it won't actually stop your code from running, It's just there to give you a heads up that there's something wrong with your dependencies.", "answer_start": 468, "answer_category": null}], "is_impossible": false}], "context": "I've just cloned a repo, which recommends the use of Yarn to install dependencies.\nI've looked online to find out exactly what \"has incorrect peer dependency\" means. But all I can find are reported issues on other repositories or questions about how to fix the problem.\nCan someone explain what this means and why it is only a warning, and not an error?\nAlso, is it something that I should try to address or report to the community behind the repo I have just cloned?\nIt is only a warning as it won't actually stop your code from running, It's just there to give you a heads up that there's something wrong with your dependencies.\n", "document_id": 1470}]}, {"paragraphs": [{"qas": [{"question": "Unable to install Pygame using pip", "id": 742, "answers": [{"answer_id": 743, "document_id": 430, "question_id": 742, "text": "sudo apt-get install mercurial\nsudo pip install hg+http://bitbucket.org/pygame/pygame", "answer_start": 368, "answer_category": null}], "is_impossible": false}], "context": "I'm trying to install Pygame. I am running Windows 7 with Enthought Python Distribution. I successfully installed pip, but when I try to install Pygame using pip, I get the following error:\n\"Could not install requirement Pygame because of HTTP error HTTP error 400: Bad request for URL ...\"\nI can't find anything about this issue with a Google search. Try doing this:\nsudo apt-get install mercurial\nsudo pip install hg+http://bitbucket.org/pygame/pygame\nFor me on Windows this was the easiest way to get pygame installed. Thanks! \n", "document_id": 430}]}, {"paragraphs": [{"qas": [{"question": "MANIFEST.in ignored on \"python setup.py install\" - no data files installed?", "id": 666, "answers": [{"answer_id": 671, "document_id": 359, "question_id": 666, "text": "MANIFEST.in tells Distutils what files to include in the source distribution but it does not directly affect what files are installed. For that you need to include the appropriate files in the setup.py file, generally either as package data or as additional files. ", "answer_start": 857, "answer_category": null}], "is_impossible": false}], "context": "When I run \"python setup.py install sdist\" I get a nice .tar.gz with a \"whyteboard-0.41\" root folder, with my locale/ images/ and whyteboard-help/ folders inside. This also has my whyteboard.py script that launches my program from inside the whyteboard source package. This mirrors the source of my program, is how everything should be, and is correct.\nHowever when I run \"python setup.py install\" none of my data files are written - only the \"whyteboard\" source package, and the whyteboard.py is placed in /usr/local/lib/python2.6/dist-packages/.\nIdeally, I'd like the same directory structure as what's been generated in the .tar.gz file to be created in dist-packages, as this is how my program expects to look for its resources.\nHow can I get \"install\" to create this directory structure? It seems to be ignoring my manifest file, as far as I can tell.\nMANIFEST.in tells Distutils what files to include in the source distribution but it does not directly affect what files are installed. For that you need to include the appropriate files in the setup.py file, generally either as package data or as additional files. The documentation linked in this answer gives you all the information you need about where data_files and package_data are installed. If these options aren't working for you, please update your question with the exact syntax you tried, the results, and what you expected\n", "document_id": 359}]}, {"paragraphs": [{"qas": [{"question": "What do you do to your JavaScript code before deployment?", "id": 1305, "answers": [{"answer_id": 1296, "document_id": 875, "question_id": 1305, "text": "Check out YUI Compressor its a console app that you can use to minify (strip out comments, whitespace etc..) and also obfuscate your javascript files.", "answer_start": 584, "answer_category": null}], "is_impossible": false}], "context": "Do you have a step in your deployment process that minifies JS? Do you have any sort of preprocessor for your JavaScript that allows you to leave in comments and console.logs and then have them automatically stripped out? Is your JavaScript machine generated by GWT or Script#? Do you use ANT or another tool to automate deployment?\nI see a lot of JavaScript that looks like it comes right out of the editor, complete with lots of white space and comments. How much of that is due to not caring about the state of the deployed code, and how much is due to the spirit of the open web?\nCheck out YUI Compressor its a console app that you can use to minify (strip out comments, whitespace etc..) and also obfuscate your javascript files.\n", "document_id": 875}]}, {"paragraphs": [{"qas": [{"question": "How to pass build artifact into another job in Jenkins", "id": 560, "answers": [{"answer_id": 563, "document_id": 285, "question_id": 560, "text": "There is just one workspace per project/job in Jenkins. The directories of builds contain just information about the builds and their results.\nThe root directories of both are specified in Manage Jenkins \u2192 Configure System \u2192 Advanced....\nTo deploy an artifact of a previous build you have to copy it to somewhere else in build master and access it there from deploy master later.", "answer_start": 242, "answer_category": null}], "is_impossible": false}], "context": "I have two jobs in Jenkins.I want to change this step from \"latest successful build\" to \"specified by a build parameter\" so that I can select a specific build when deploying without modifying the configuration of deploy master job each time.\nThere is just one workspace per project/job in Jenkins. The directories of builds contain just information about the builds and their results.\nThe root directories of both are specified in Manage Jenkins \u2192 Configure System \u2192 Advanced....\nTo deploy an artifact of a previous build you have to copy it to somewhere else in build master and access it there from deploy master later.\n", "document_id": 285}]}, {"paragraphs": [{"qas": [{"question": "error: --with-readline=yes (default) and headers/libs are not available", "id": 818, "answers": [{"answer_id": 813, "document_id": 500, "question_id": 818, "text": "You can install it with apt-get, aptitude, or the appropiate tool for your distribution. In Ubuntu:\naptitude install libreadline-dev.", "answer_start": 43, "answer_category": null}], "is_impossible": false}], "context": "I think you need the GNU readline package. You can install it with apt-get, aptitude, or the appropiate tool for your distribution. In Ubuntu:\naptitude install libreadline-dev.  think you need the GNU readline package. You can install it with apt-get, aptitude, or the appropiate tool for your distribution. In Ubuntu:\naptitude install libreadline-dev\n", "document_id": 500}]}, {"paragraphs": [{"qas": [{"question": "Using InstallUtil and silently setting a windows service logon username/password", "id": 792, "answers": [{"answer_id": 788, "document_id": 475, "question_id": 792, "text": "installUtil.exe /username=domain\\username /password=password /unattended C:\\My.exe", "answer_start": 272, "answer_category": null}], "is_impossible": false}], "context": "I need to use InstallUtil to install a C# windows service. I need to set the service logon credentials (username and password). All of this needs to be done silently. A much easier way than the posts above and with no extra code in your installer is to use the following:\ninstallUtil.exe /username=domain\\username /password=password /unattended C:\\My.exe\nJust ensure the account you use is valid. If not you will receive a \"No mapping between account names and security id's was done\" exception\n", "document_id": 475}]}, {"paragraphs": [{"qas": [{"question": "What will happen when require_valid_user is true?", "id": 200670, "answers": [{"answer_id": 239661, "document_id": 357785, "question_id": 200670, "text": "When this option is set to true, no requests are allowed from\nanonymous users. Everyone must be authenticated.", "answer_start": 4172, "answer_category": null}], "is_impossible": false}, {"question": "What is the secret token?", "id": 200672, "answers": [{"answer_id": 239688, "document_id": 357785, "question_id": 200672, "text": "The secret token is used for Proxy Authentication and for Cookie Authentication.", "answer_start": 7286, "answer_category": null}], "is_impossible": false}, {"question": "What does CouchDB need to start?", "id": 200669, "answers": [{"answer_id": 239618, "document_id": 357785, "question_id": 200669, "text": "CouchDB requires an admin account to start", "answer_start": 1381, "answer_category": null}], "is_impossible": false}, {"question": "What is auth_cache_size?", "id": 200671, "answers": [{"answer_id": 239678, "document_id": 357785, "question_id": 200671, "text": "Number of User Context Object to cache in memory, to reduce disk\nlookups.", "answer_start": 5312, "answer_category": null}], "is_impossible": false}, {"question": "Which option is responsible for expired time?", "id": 200673, "answers": [{"answer_id": 239702, "document_id": 357785, "question_id": 200673, "text": "timeout", "answer_start": 7434, "answer_category": null}], "is_impossible": false}], "context": "\n\n\n3.6. Authentication and Authorization\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApache CouchDB\u00ae\n\n\n\n\n\n\n\nstable\n\n\n\n\n\n\n\n\n\n\nTable of Contents\nUser Guides\n\n1. Introduction\n2. Replication\n3. Design Documents\n4. Best Practices\n\nAdministration Guides\n\n1. Installation\n2. Setup\n3. Configuration\n3.1. Introduction To Configuring\n3.2. Base Configuration\n3.3. Configuring Clustering\n3.4. couch_peruser\n3.5. CouchDB HTTP Server\n3.6. Authentication and Authorization\n3.6.1. Server Administrators\n3.6.2. Authentication Configuration\n\n\n3.7. Compaction\n3.8. Background Indexing\n3.9. IO Queue\n3.10. Logging\n3.11. Replicator\n3.12. Query Servers\n3.13. Miscellaneous Parameters\n3.14. Resharding\n\n\n4. Cluster Management\n5. Maintenance\n6. Fauxton\n7. Experimental Features\n\nReference Guides\n\n1. API Reference\n2. JSON Structure Reference\n3. Query Server\n4. Partitioned Databases\n\nOther\n\n1. Release Notes\n2. Security Issues / CVEs\n3. Reporting New Security Problems with Apache CouchDB\n4. License\n5. Contributing to this Documentation\n\nQuick Reference Guides\n\nAPI Quick Reference\nConfiguration Quick Reference\n\nMore Help\n\nCouchDB Homepage\nMailing Lists\nRealtime Chat\nIssue Tracker\nDownload Docs\n\n\n\n\n\n\n\n\nApache CouchDB\u00ae\n\n\n\n\n\nDocs \u00bb\n3. Configuration \u00bb\n3.6. Authentication and Authorization\n\nEdit on GitHub\n\n\n\n\n\n\n\n3.6. Authentication and Authorization\u00b6\n\n3.6.1. Server Administrators\u00b6\n\n\n[admins]\u00b6\n\n\nChanged in version 3.0.0: CouchDB requires an admin account to start. If an admin account has not\nbeen created, CouchDB will print an error message and terminate.\nCouchDB server administrators and passwords are not stored in the\n_users database, but in the last [admins] section that CouchDB\nfinds when loading its ini files. See :config:intro for details on config\nfile order and behaviour. This file (which could be something like\netc/local.ini or etc/local.d/10-admins.ini on a Debian/Ubuntu\nsystem installed from packages) should be appropriately secured and\nreadable only by system administrators:\n[admins]\n;admin = mysecretpassword\nadmin = -hashed-6d3c30241ba0aaa4e16c6ea99224f915687ed8cd,7f4a3e05e0cbc6f48a0035e3508eef90\narchitect = -pbkdf2-43ecbd256a70a3a2f7de40d2374b6c3002918834,921a12f74df0c1052b3e562a23cd227f,10000\n\n\nAdministrators can be added directly to the [admins] section, and when\nCouchDB is restarted, the passwords will be salted and encrypted. You may\nalso use the HTTP interface to create administrator accounts; this way,\nyou don\u2019t need to restart CouchDB, and there\u2019s no need to temporarily store\nor transmit passwords in plaintext. The HTTP\n/_node/{node-name}/_config/admins endpoint supports querying, deleting\nor creating new admin accounts:\nGET /_node/nonode@nohost/_config/admins HTTP/1.1\nAccept: application/json\nHost: localhost:5984\n\n\nHTTP/1.1 200 OK\nCache-Control: must-revalidate\nContent-Length: 196\nContent-Type: application/json\nDate: Fri, 30 Nov 2012 11:37:18 GMT\nServer: CouchDB (Erlang/OTP)\n\n\n{\n\"admin\": \"-hashed-6d3c30241ba0aaa4e16c6ea99224f915687ed8cd,7f4a3e05e0cbc6f48a0035e3508eef90\",\n\"architect\": \"-pbkdf2-43ecbd256a70a3a2f7de40d2374b6c3002918834,921a12f74df0c1052b3e562a23cd227f,10000\"\n}\n\n\nIf you already have a salted, encrypted password string (for example, from\nan old ini file, or from a different CouchDB server), then you can store\nthe \u201craw\u201d encrypted string, without having CouchDB doubly encrypt it.\nPUT /_node/nonode@nohost/_config/admins/architect?raw=true HTTP/1.1\nAccept: application/json\nContent-Type: application/json\nContent-Length: 89\nHost: localhost:5984\n\n\"-pbkdf2-43ecbd256a70a3a2f7de40d2374b6c3002918834,921a12f74df0c1052b3e562a23cd227f,10000\"\n\n\nHTTP/1.1 200 OK\nCache-Control: must-revalidate\nContent-Length: 89\nContent-Type: application/json\nDate: Fri, 30 Nov 2012 11:39:18 GMT\nServer: CouchDB (Erlang/OTP)\n\n\"-pbkdf2-43ecbd256a70a3a2f7de40d2374b6c3002918834,921a12f74df0c1052b3e562a23cd227f,10000\"\n\n\nFurther details are available in security, including configuring the work\nfactor for PBKDF2, and the algorithm itself at\nPBKDF2 (RFC-2898).\n\nChanged in version 1.4: PBKDF2 server-side hashed salted password support added, now as a\nsynchronous call for the _config/admins API.\n\n\n\n\n3.6.2. Authentication Configuration\u00b6\n\n\n[chttpd]\u00b6\n\n\nrequire_valid_user\u00b6\nWhen this option is set to true, no requests are allowed from\nanonymous users. Everyone must be authenticated.\n[chttpd]\nrequire_valid_user = false\n\n\n\n\n\nrequire_valid_user_except_for_up\u00b6\nWhen this option is set to true, no requests are allowed from\nanonymous users, except for the /_up endpoint. Everyone else must\nbe authenticated.\n[chttpd]\nrequire_valid_user_except_for_up = false\n\n\n\n\n\n\n[couch_httpd_auth]\u00b6\n\n\nallow_persistent_cookies\u00b6\nWhen set to true, CouchDB will set the Max-Age and Expires attributes\non the cookie, which causes user agents (like browsers) to preserve the cookie\nover restarts.\n[couch_httpd_auth]\nallow_persistent_cookies = true\n\n\n\n\n\ncookie_domain\u00b6\n\nNew in version 2.1.1.\n\nConfigures the domain attribute of the AuthSession cookie. By default the\ndomain attribute is empty, resulting in the cookie being set on CouchDB\u2019s domain.\n[couch_httpd_auth]\ncookie_domain = example.com\n\n\n\n\n\nsame_site\u00b6\n\nNew in version 3.0.0.\n\nWhen this option is set to a non-empty value, a SameSite attribute is added to\nthe AuthSession cookie. Valid values are none, lax or strict.:\n[couch_httpd_auth]\nsame_site = strict\n\n\n\n\n\nauth_cache_size\u00b6\nNumber of User Context Object to cache in memory, to reduce disk\nlookups.\n[couch_httpd_auth]\nauth_cache_size = 50\n\n\n\n\n\nauthentication_redirect\u00b6\nSpecifies the location for redirection on successful authentication if\na text/html response is accepted by the client (via an Accept\nheader).\n[couch_httpd_auth]\nauthentication_redirect = /_utils/session.html\n\n\n\n\n\niterations\u00b6\n\nNew in version 1.3.\n\nThe number of iterations for password hashing by the PBKDF2 algorithm.\nA higher  number provides better hash durability, but comes at a cost\nin performance for each request that requires authentication.\n[couch_httpd_auth]\niterations = 10000\n\n\n\n\n\nmin_iterations\u00b6\n\nNew in version 1.6.\n\nThe minimum number of iterations allowed for passwords hashed by the\nPBKDF2 algorithm. Any user with fewer iterations is forbidden.\n[couch_httpd_auth]\nmin_iterations = 100\n\n\n\n\n\nmax_iterations\u00b6\n\nNew in version 1.6.\n\nThe maximum number of iterations allowed for passwords hashed by the\nPBKDF2 algorithm. Any user with greater iterations is forbidden.\n[couch_httpd_auth]\nmax_iterations = 100000\n\n\n\n\n\nproxy_use_secret\u00b6\nWhen this option is set to true, the\ncouch_httpd_auth/secret option is required for\nProxy Authentication.\n[couch_httpd_auth]\nproxy_use_secret = false\n\n\n\n\n\npublic_fields\u00b6\n\nNew in version 1.4.\n\nA comma-separated list of field names in user documents (in\ncouchdb/users_db_suffix) that can be read by any\nuser. If unset or not specified, authenticated users can only retrieve\ntheir own document.\n[couch_httpd_auth]\npublic_fields = first_name, last_name, contacts, url\n\n\n\nNote\nUsing the public_fields whitelist for user document properties\nrequires setting the couch_httpd_auth/users_db_public\noption to true (the latter option has no other purpose):\n[couch_httpd_auth]\nusers_db_public = true\n\n\n\n\n\n\nrequire_valid_user\u00b6\nWhen this option is set to true, no requests are allowed from\nanonymous users. Everyone must be authenticated.\n[couch_httpd_auth]\nrequire_valid_user = false\n\n\n\n\n\nsecret\u00b6\nThe secret token is used for Proxy Authentication and for Cookie Authentication.\n[couch_httpd_auth]\nsecret = 92de07df7e7a3fe14808cef90a7cc0d91\n\n\n\n\n\ntimeout\u00b6\nNumber of seconds since the last request before sessions will be\nexpired.\n[couch_httpd_auth]\ntimeout = 600\n\n\n\n\n\nusers_db_public\u00b6\n\nNew in version 1.4.\n\nAllow all users to view user documents. By default, only admins may\nbrowse all users documents, while users may browse only their own\ndocument.\n[couch_httpd_auth]\nusers_db_public = false\n\n\n\n\n\nx_auth_roles\u00b6\nThe HTTP header name (X-Auth-CouchDB-Roles by default) that\ncontains the list of a user\u2019s roles, separated by a comma. Used for\nProxy Authentication.\n[couch_httpd_auth]\nx_auth_roles = X-Auth-CouchDB-Roles\n\n\n\n\n\nx_auth_token\u00b6\nThe HTTP header name (X-Auth-CouchDB-Token by default) containing\nthe token used to authenticate the authorization. This token is an\nHMAC-SHA1 created from the couch_httpd_auth/secret and\ncouch_httpd_auth/x_auth_username. The secret key should be\nthe same on the client and the CouchDB node. This token is optional if\nthe value of the couch_httpd_auth/proxy_use_secret option is\nnot true. Used for Proxy Authentication.\n[couch_httpd_auth]\nx_auth_token = X-Auth-CouchDB-Token\n\n\n\n\n\nx_auth_username\u00b6\nThe HTTP header name (X-Auth-CouchDB-UserName by default)\ncontaining the username. Used for Proxy Authentication.\n[couch_httpd_auth]\nx_auth_username = X-Auth-CouchDB-UserName\n\n\n\n\n\n\n[jwt_auth]\u00b6\n\n\nrequired_claims\u00b6\nThis parameter is a comma-separated list of additional mandatory JWT claims\nthat must be present in any presented JWT token. A\n:code 400:Bad Request is sent if any are missing.\n[jwt_auth]\nrequired_claims = exp,iat\n\n\n\n\n\n\n\n\n\n\nNext\nPrevious\n\n\n\n\n\u00a9 Copyright 2020, Apache Software Foundation. CouchDB\u00ae is a registered trademark of the Apache Software Foundation.\n\n\nRevision 3f39035f.\n\n\n\nBuilt with Sphinx using a theme provided by Read the Docs.\n\n\n\n\n\n\n\n\nRead the Docs\nv: stable\n\n\n\n\nVersions\nmaster\nlatest\nstable\n3.1.1\n2.3.1\n1.6.1\n\n\nDownloads\npdf\nhtml\nepub\n\n\nOn Read the Docs\n\nProject Home\n\n\nBuilds\n\n\n\nFree document hosting provided by Read the Docs.\n\n\n\n\n\n\n\n\n\n\n\n", "document_id": 357785}]}, {"paragraphs": [{"qas": [{"question": "E: Unable to locate package mongodb-org", "id": 1551, "answers": [{"answer_id": 1540, "document_id": 1128, "question_id": 1551, "text": "You should Install the default 2.X Ubuntu package might be your best bet with:\nsudo apt-get install -y mongodb", "answer_start": 125, "answer_category": null}], "is_impossible": false}], "context": "I am trying to download mongodb and I am following the steps on this link.\nWhy is this occurring and is there a work around?\nYou should Install the default 2.X Ubuntu package might be your best bet with:\nsudo apt-get install -y mongodb\n", "document_id": 1128}]}, {"paragraphs": [{"qas": [{"question": "Missing maven .m2 folder", "id": 856, "answers": [{"answer_id": 851, "document_id": 536, "question_id": 856, "text": "Unix/Mac OS X \u2013 ~/.m2/repository\nWindows \u2013 C:\\Users\\{your-username}\\.m2\\repository", "answer_start": 265, "answer_category": null}], "is_impossible": false}], "context": "Where is Maven local repository?\nauthor image\nBy mkyong | Last updated: November 14, 2018\n\nViewed: 734,463 (+679 pv/w)\n\n\nTags:maven | maven repository | maven-faq | maven-repo\nBy default, Maven local repository is defaulted to ${user.home}/.m2/repository folder :\n\nUnix/Mac OS X \u2013 ~/.m2/repository\nWindows \u2013 C:\\Users\\{your-username}\\.m2\\repository\nWhen we compile a Maven project, Maven will download all the project\u2019s dependency and plugin jars into the Maven local repository, save time for next compilation.\n\n1. Find Maven Local Repository\n1.1 If the default .m2 is unable to find, maybe someone changed the default path. Issue the following command to find out where is the Maven local repository:\n\n\nmvn help:evaluate -Dexpression=settings.localRepository\n1.2 Example :\n\nTerminal\n\nD:\\> mvn help:evaluate -Dexpression=settings.localRepository\n\n[INFO] Scanning for projects...\n[INFO]\n[INFO] ------------------< org.apache.maven:standalone-pom >-------------------\n[INFO] Building Maven Stub Project (No POM) 1\n[INFO] --------------------------------[ pom ]---------------------------------\n[INFO]\n[INFO] --- maven-help-plugin:3.1.0:evaluate (default-cli) @ standalone-pom ---\n[INFO] No artifact parameter specified, using 'org.apache.maven:standalone-pom:pom:1' as project.\n[INFO]\n\nC:\\opt\\maven-repository\n\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD SUCCESS\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time: 2.598 s\n[INFO] Finished at: 2018-10-24T16:44:18+08:00\n[INFO] ------------------------------------------------------------------------\nIn above output, The Maven local repository is relocated to C:\\opt\\maven-repository\n\n2. Update Maven Local Repository\n2.1 Find this file {MAVEN_HOME}\\conf\\settings.xml and update the localRepository.\n\n{MAVEN_HOME}\\conf\\settings.xml\n\n<settings>\n  <!-- localRepository\n   | The path to the local repository maven will use to store artifacts.\n   |\n   | Default: ~/.m2/repository\n  <localRepository>/path/to/local/repo</localRepository>\n  -->\n\n<localRepository>D:/maven_repo</localRepository>\nNote\nIssue mvn -version to find out where is Maven installed.\n2.2 Save the file, done, the Maven local repository is now changed to D:/maven_repo.\n\n\nReferences\nMaven Central Repository\nIntroduction to Repositories\n", "document_id": 536}]}, {"paragraphs": [{"qas": [{"question": "Installing Java 7 on Ubuntu", "id": 1592, "answers": [{"answer_id": 1581, "document_id": 1168, "question_id": 1592, "text": "It maybe work for you:\nsudo apt-get update\nsudo apt-get install openjdk-7-jdk", "answer_start": 406, "answer_category": null}], "is_impossible": false}], "context": "to install java I have always used the classic way from the terminal. I would like to install java manually. I placed the folder of the JDK on the desk and I set environment variables (PATH, CLASSPATH and JAVA_HOME).\nBut when I try to install eclipse or netbeans, the system warns by saying that there is no java installed on the machine.\nWhat is missing to compleatare manual installation? (Ubuntu 13.04)\nIt maybe work for you:\nsudo apt-get update\nsudo apt-get install openjdk-7-jdk\n", "document_id": 1168}]}, {"paragraphs": [{"qas": [{"question": ".NET Core - how does the 'dotnet publish' command work?", "id": 453, "answers": [{"answer_id": 462, "document_id": 186, "question_id": 453, "text": "You should use this command :dotnet publish -c Release -r win-x64 --output ./MyTargetFolder MySolution.sln", "answer_start": 295, "answer_category": null}], "is_impossible": false}], "context": "I have a solution with some projects targeting .NET Standard 2.0 and a console application project targeting .NET Core 2.1.\nI get the same using the \"dotnet build\" command. Now I need my console application's EXE file. So I use the \"dotnet publish -c Release -r win-x64 MySolution.sln\" command.\nYou should use this command :dotnet publish -c Release -r win-x64 --output ./MyTargetFolder MySolution.sln\n", "document_id": 186}]}, {"paragraphs": [{"qas": [{"question": "how to install a qt application on a customers system", "id": 1417, "answers": [{"answer_id": 1406, "document_id": 993, "question_id": 1417, "text": "1 - You can compile Qt statically. This will allow you to deploy you app without any qt dependencies.\n\n2 - You need to deploy your app with Qt librairy files you need (like qtcore.dll on Windows)\n", "answer_start": 1081, "answer_category": null}], "is_impossible": false}, {"question": "how to create a qt application installation on a MACOS?", "id": 1418, "answers": [{"answer_id": 1407, "document_id": 993, "question_id": 1418, "text": "For MacOSX you need to create a dmg image. This is very simple. Read the following web page for help : http://www.wikihow.com/Make-a-DMG-File-on-a-Mac. By using apple script you can customize dmg (like an Application folder link into the dmg).", "answer_start": 1473, "answer_category": null}], "is_impossible": false}, {"question": "how to create a qt application installation on a WINDOWS?", "id": 1419, "answers": [{"answer_id": 1408, "document_id": 993, "question_id": 1419, "text": "My preference for Win32 installer is NSIS.", "answer_start": 1718, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI've got a Qt app that I need installed on a customers computer, which I can't assume has Qt installed on it.  I'm on a Mac OSX and the computer I will be installing it on some Unix based system. I will be installing it myself so I don't need a GUI installation wizard or anything like that. Ideally I'd like to end up with a script or makefile, along with a folder of all the sources and necessary libraries, I just don't know where to start. References would be much appreciated, I haven't found anything useful after many google searches.\n\nMy question lies somewhere between these two:\nCan you create a setup.exe in qt to install your app on a client computer?\nCreate Linux install for Qt application?\n\nI don't need a full-blown install wizard (question 1), but I also won't have my machine at the installation site to just keep copying libraries until all dependencies are met (question 2). Basically I need to have everything on a CD ready to install when I get there.  Thanks in advance.\n    \n\nThere are two ways to install a Qt application on a system:\n\n1 - You can compile Qt statically. This will allow you to deploy you app without any qt dependencies.\n\n2 - You need to deploy your app with Qt librairy files you need (like qtcore.dll on Windows)\n\nYou will find all explications for each platform in the Qt documentation : http://qt-project.org/doc/qt-4.8/deployment.html.\n\nTo create installer you can use InstallJammer for Windows and Unix.\n\nFor MacOSX you need to create a dmg image. This is very simple. Read the following web page for help : http://www.wikihow.com/Make-a-DMG-File-on-a-Mac. By using apple script you can customize dmg (like an Application folder link into the dmg).\n\nMy preference for Win32 installer is NSIS.\n\nHope that helps!\n    \n\nNot sure why you want to avoid the install wizard. It can also help you create Uninstaller, desktop and start menu shortcuts, etc.  As mentioned in the posts you refer to, you could use BitRock InstallBuilder (Nokia uses it for Qt Creator)\n\nIf you do not want to use a wizard and don't want to compile statically, then you can bundle Qt libraries in the same folder as the app and setup a shell script that sets the LD_LIBRARY_PATH to that directory\n    ", "document_id": 993}]}, {"paragraphs": [{"qas": [{"question": "How to see dependency tree in sbt?", "id": 1866, "answers": [{"answer_id": 1852, "document_id": 1437, "question_id": 1866, "text": "If you want to actually view the library dependencies (as you would with Maven) rather than the task dependencies (which is what inspect tree displays), then you'll want to use the sbt-dependency-graph plugin.", "answer_start": 386, "answer_category": null}], "is_impossible": false}], "context": "I am trying to inspect the SBT dependency tree as described in the documentation:\nsbt inspect tree clean\nBut I get this error:\n[error] inspect usage:\n[error]   inspect [uses|tree|definitions] <key>   Prints the value for 'key', the defining scope, delegates, related definitions, and dependencies.\n[error]\n[error] inspect\n[error]        ^\nWhat is wrong? Why doesn't SBT build the tree?\nIf you want to actually view the library dependencies (as you would with Maven) rather than the task dependencies (which is what inspect tree displays), then you'll want to use the sbt-dependency-graph plugin.\n", "document_id": 1437}]}, {"paragraphs": [{"qas": [{"question": "packaging multiple rpms in one file", "id": 1410, "answers": [{"answer_id": 1399, "document_id": 984, "question_id": 1410, "text": "you don't have to repackage anything, especially not if your application uses some external libraries. You just have to mention in your recipe that your RPM (or DEB) depends on the other one. Both apt-get (for deb) and yum (for rpm) will check these dependencies and install them if needed.", "answer_start": 1638, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nIs it possible to paqckage multiple rpms into one file. I have got two bundles one of which requires that the other be installed. I would like to create a single installable out of them in such a way that this installer will first invoke pkg 1 and then install pkg 2. Is this possible? What about deb packages? Sorry if it a basic question. I have not worked with installers on Linux before. I have created Windows installers. There you can create two merge modules (.msm) and package them into a standalone installer (.msi) and specify the order of execution. I am looking for similar functionality on Linux.\n\nEDIT: I think the question was not clear enough. Let me try to rephrase it. I have a bunch of runtime libraries which is currently shipped as a standalone installer. Another team develops products which use these libraries at runtime. I now want to provide the libraries to the product team in some form (sub-package) which they can include with their installer and configure their installer to install my sub-package first. Both packages should be available locally to the end user. They are not available on any repository and cannot be pulled down from the net at install time. \n    \n\nTry packaging them into a self extraction bash script. You won't have to modify or aggregate the  libraries and rpms together. This should yield an executable file that can be included in another installation process. The last step of the extraction should be to call \"rpm -i\" with the rpm files as arguments.\n\nReference on building a self extracting bash script:\nhttp://www.linuxjournal.com/node/1005818\n    \n\nFirst : you don't have to repackage anything, especially not if your application uses some external libraries. You just have to mention in your recipe that your RPM (or DEB) depends on the other one. Both apt-get (for deb) and yum (for rpm) will check these dependencies and install them if needed.\n\nSee :\nhttp://rpm5.org/docs/api/dependencies.html (rpm) and http://www.linuxfordevices.com/c/a/Linux-For-Devices-Articles/How-to-make-deb-packages/ (deb)\n\n(These were just the first ones I found, you can find better resources out there :p).\n    ", "document_id": 984}]}, {"paragraphs": [{"qas": [{"question": "Get XamlParseException after deploying WPF project", "id": 1130, "answers": [{"answer_id": 1123, "document_id": 707, "question_id": 1130, "text": "CodeProject has a script to set the version number of an MSI file, which you could run in the pre-built step of the setup project. You find it here:\nhttp://www.codeproject.com/KB/install/NewSetupVersion.aspx\n", "answer_start": 577, "answer_category": null}], "is_impossible": false}], "context": "I have been trying to deploy my WPF app, I created a Setup Project using the Setup Wizard.The only Project Output I added was Primary. After building this and installing the program, as soon as i click the exe on my desktop i get a pop up that says \"'My Program' has stopped working\", so i click Debug the Program and i see some error.\nThis is normally caused by not having all dependencies copied to the output. As you say the error message is not very helpful, but I would check that your application has all the necessary dependencies available to resolve the parsed types.\nCodeProject has a script to set the version number of an MSI file, which you could run in the pre-built step of the setup project. You find it here:\nhttp://www.codeproject.com/KB/install/NewSetupVersion.aspx\n\n", "document_id": 707}]}, {"paragraphs": [{"qas": [{"question": "How to install lxml on Ubuntu", "id": 1585, "answers": [{"answer_id": 1574, "document_id": 1162, "question_id": 1585, "text": "Since you're on Ubuntu, don't bother with those source packages. Just install those development packages using apt-get.\napt-get install libxml2-dev libxslt1-dev python-dev\nIf you're happy with a possibly older version of lxml altogether though, you could try\napt-get install python-lxml", "answer_start": 256, "answer_category": null}], "is_impossible": false}], "context": "I'm having difficulty installing lxml with easy_install on Ubuntu 11.\nI've tried both versions 2.6.27 and 2.6.29 of libxml2 with no difference.\nLeaving no stone unturned, I have successfully done sudo apt-get install libxml2-dev, but this changes nothing.\nSince you're on Ubuntu, don't bother with those source packages. Just install those development packages using apt-get.\napt-get install libxml2-dev libxslt1-dev python-dev\nIf you're happy with a possibly older version of lxml altogether though, you could try\napt-get install python-lxml\n", "document_id": 1162}]}, {"paragraphs": [{"qas": [{"question": "which command can print the list of generators supported for the target platform in Cmake", "id": 36, "answers": [{"answer_id": 38, "document_id": 51, "question_id": 36, "text": "cpack --help", "answer_start": 700, "answer_category": null}], "is_impossible": false}, {"question": "which command can I use to set a CPack variable for CMake?", "id": 37, "answers": [{"answer_id": 39, "document_id": 51, "question_id": 37, "text": "-D <var>=<value>", "answer_start": 2097, "answer_category": null}], "is_impossible": false}, {"question": "which command can I use to Specify the project configuration for CMake?", "id": 38, "answers": [{"answer_id": 40, "document_id": 51, "question_id": 38, "text": "-C <configs>", "answer_start": 1660, "answer_category": null}], "is_impossible": false}, {"question": "which command can I use to Specify the configuration file read by cpack for cmake?", "id": 39, "answers": [{"answer_id": 41, "document_id": 51, "question_id": 39, "text": "--config <configFile>", "answer_start": 2213, "answer_category": null}], "is_impossible": false}, {"question": "How can I Run cpack with verbose output?", "id": 40, "answers": [{"answer_id": 42, "document_id": 51, "question_id": 40, "text": "--verbose, -V", "answer_start": 2384, "answer_category": null}], "is_impossible": false}, {"question": "How to Print help for one command and exit for cmake?", "id": 41, "answers": [{"answer_id": 43, "document_id": 51, "question_id": 41, "text": "--help-command", "answer_start": 4556, "answer_category": null}], "is_impossible": false}, {"question": "how to List commands with help available and exit for cmake?", "id": 42, "answers": [{"answer_id": 44, "document_id": 51, "question_id": 42, "text": "--help-command-list", "answer_start": 4757, "answer_category": null}], "is_impossible": false}], "context": "\n\n\n\n\n\n\ncpack(1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNavigation\n\n\nindex\n\nnext |\n\nprevious |\n\n\n\n\nCMake \u00bb\n\n\n3.21.3\nDocumentation \u00bb\n\ncpack(1)\n\n\n\n\n\n\n\ncpack(1)\u00b6\n\nSynopsis\u00b6\ncpack [<options>]\n\n\n\n\nDescription\u00b6\nThe cpack executable is the CMake packaging program.  It generates installers and source packages in a variety of formats. For each installer or package format, cpack has a specific backend, called \"generator\". A generator is responsible for generating the required inputs and invoking the specific package creation tools. These installer or package generators are not to be confused with the makefile generators of the cmake command. All supported generators are specified in the cpack-generators manual.  The command cpack --help prints a list of generators supported for the target platform.  Which of them are to be used can be selected through the CPACK_GENERATOR variable or through the command-line option -G. The cpack program is steered by a configuration file written in the CMake language. Unless chosen differently through the command-line option --config, the file CPackConfig.cmake in the current directory is used. In the standard CMake workflow, the file CPackConfig.cmake is generated by the cmake executable, provided the CPack module is included by the project's CMakeLists.txt file.\n\n\nOptions\u00b6\n\n-G <generators><generators> is a semicolon-separated list of generator names.  cpack will iterate through this list and produce package(s) in that generator's format according to the details provided in the CPackConfig.cmake configuration file.  If this option is not given,the CPACK_GENERATOR variable determines the default set of generators that will be used.\n\n-C <configs>Specify the project configuration(s) to be packaged (e.g. Debug, Release, etc.), where <configs> is a semicolon-separated list. When the CMake project uses a multi-configuration generator such as Xcode or Visual Studio, this option is needed to tell cpack which built executables to include in the package. The user is responsible for ensuring that the configuration(s) listed have already been built before invoking cpack.\n\n-D <var>=<value>Set a CPack variable.  This will override any value set for <var> in the input file read by cpack.\n\n--config <configFile>Specify the configuration file read by cpack to provide the packaging details.  By default, CPackConfig.cmake in the current directory will be used.\n\n--verbose, -VRun cpack with verbose output.  This can be used to show more details from the package generation tools and is suitable for project developers.\n\n--debugRun cpack with debug output.  This option is intended mainly for the developers of cpack itself and is not normally needed by project developers.\n\n--tracePut the underlying cmake scripts in trace mode.\n\n--trace-expandPut the underlying cmake scripts in expanded trace mode.\n\n-P <packageName>Override/define the value of the CPACK_PACKAGE_NAME variable used for packaging.  Any value set for this variable in the CPackConfig.cmake file will then be ignored.\n\n-R <packageVersion>Override/define the value of the CPACK_PACKAGE_VERSION variable used for packaging.  It will override a value set in the CPackConfig.cmake file or one automatically computed from CPACK_PACKAGE_VERSION_MAJOR, CPACK_PACKAGE_VERSION_MINOR and CPACK_PACKAGE_VERSION_PATCH.\n\n-B <packageDirectory>Override/define CPACK_PACKAGE_DIRECTORY, which controls the directory where CPack will perform its packaging work.  The resultant package(s) will be created at this location by default and a _CPack_Packages subdirectory will also be created below this directory to use as a working area during package creation.\n\n--vendor <vendorName>Override/define CPACK_PACKAGE_VENDOR.\n\n\n\n--help,-help,-usage,-h,-H,/?Print usage information and exit. Usage describes the basic command line interface and its options.\n\n--version,-version,/V [<f>]Show program name/version banner and exit. If a file is specified, the version is written into it. The help is printed to a named <f>ile if given.\n\n--help-full [<f>]Print all help manuals and exit. All manuals are printed in a human-readable text format. The help is printed to a named <f>ile if given.\n\n--help-manual <man> [<f>]Print one help manual and exit. The specified manual is printed in a human-readable text format. The help is printed to a named <f>ile if given.\n\n--help-manual-list [<f>]List help manuals available and exit. The list contains all manuals for which help may be obtained by using the --help-manual option followed by a manual name. The help is printed to a named <f>ile if given.\n\n--help-command <cmd> [<f>]Print help for one command and exit. The cmake-commands(7) manual entry for <cmd> is printed in a human-readable text format. The help is printed to a named <f>ile if given.\n\n--help-command-list [<f>]List commands with help available and exit.The list contains all commands for which help may be obtained by using the --help-command option followed by a command name. The help is printed to a named <f>ile if given.\n\n--help-commands [<f>]Print cmake-commands manual and exit. The cmake-commands(7) manual is printed in a human-readable text format. The help is printed to a named <f>ile if given.\n\n--help-module <mod> [<f>]Print help for one module and exit. The cmake-modules(7) manual entry for <mod> is printed in a human-readable text format. The help is printed to a named <f>ile if given.\n\n--help-module-list [<f>]List modules with help available and exit. The list contains all modules for which help may be obtained by using the --help-module option followed by a module name. The help is printed to a named <f>ile if given.\n\n--help-modules [<f>]Print cmake-modules manual and exit.The cmake-modules(7) manual is printed in a human-readable text format. The help is printed to a named <f>ile if given.\n\n--help-policy <cmp> [<f>]Print help for one policy and exit. The cmake-policies(7) manual entry for <cmp> is\nprinted in a human-readable text format. The help is printed to a named <f>ile if given.\n\n--help-policy-list [<f>]List policies with help available and exit. The list contains all policies for which help may be obtained by using the --help-policy option followed by a policy name. The help is printed to a named <f>ile if given.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNavigation\n\n\nindex\n\nnext |\n\nprevious |\n\n\n\n\nCMake \u00bb\n\n\n3.21.3\nDocumentation \u00bb\n\ncpack(1)\n\n\n\n\u00a9 Copyright 2000-2021 Kitware, Inc. and Contributors.\nCreated using Sphinx 4.0.1.\n\n\n\n\n\n", "document_id": 51}]}, {"paragraphs": [{"qas": [{"question": "setting up slime on macosx", "id": 1396, "answers": [{"answer_id": 1385, "document_id": 968, "question_id": 1396, "text": "\n\n\nThe best way to install SLIME itself is probably via the Quicklisp's SLIME hel", "answer_start": 1609, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI've been dancing around LISP for decades, but now have decided to get serious. I'm going through the online version of Practical Common LISP.\n\nThis is my setup:\n\nMacOSX 10.7.8\nXcode 4.5.2\nSBCL 1.0.55.0-abb03f9\nEmacs 24.2.1  (x86_64-apple-darwin, NS apple-appkit-1038.36)\nSLIME 1.6\n\nI tried to follow the instructions listed in the link:\n\nhttp://emacs-sbcl-slime.blogspot.com/2010/11/sbcl-emacs-slime-macosx.html\n\n\u2026but the problem is that on the MacOSX platform, nothing seems to be located where it should.\n\nSBCL was installed using its own script\u2026it is working.\n\nI setup the SBCL_HOME env var as instructed.\n\nEmacs was installed by dmg from this link:  \n\nhttp://emacs-sbcl-slime.blogspot.com/2010/11/sbcl-emacs-slime-macosx.html\n\n\u2026and is running.\n\nSLIME, however (which was download via cvs to \u02dc/.emacs.d/slime) doesn't appear to be recognized. I can't get the \"CL-USER&gt;\" prompt described by the author.\n\nAny help would be greatly appreciated!\n    \n\n\nCopy the entire directory of slime to emacs/site-lisp\nEnsure your lisp is accesible from the terminal. Just type sbcl in Terminal. Lisp interpreter should start.\nput into your .emacs file something like (setq inferior-lisp-program \"sbcl\")\n\n\nIt should work then.\n    \n\nI'd suggest just getting Emacs Prelude, which ships with a good Common Lisp + SLIME configuration be default. It's preconfigured for Clozure CL on OSX (given that it's arguably the most popular Common Lisp distribution on OSX), but a simple line of Emacs Lisp in your Prelude personal config can switch SLIME's default Lisp to SBCL:\n\n(setq slime-default-lisp 'sbcl)\n\n\nThe best way to install SLIME itself is probably via the Quicklisp's SLIME helper.\n    \n\nHave you considered using the excellent LispBox bundle that takes care of all the setup for you? It's super easy: http://common-lisp.net/project/lispbox/\n\nIt includes emacs, SLIME, the Clozure Common Lisp environment, QuickLisp and everything preconfigured.\n    ", "document_id": 968}]}, {"paragraphs": [{"qas": [{"question": "The target \"_CopyBinDeployableAssemblies\" does not exist in the project ", "id": 580, "answers": [{"answer_id": 586, "document_id": 305, "question_id": 580, "text": "if you have a depdendency on the above targets, which you can find out by looking for the following <Import> in your project files:\n<Import Project=\"$(MSBuildExtensionsPath)\\Microsoft\\VisualStudio\\v10.0\\Web\\Microsoft.Web.Publishing.targets\"", "answer_start": 430, "answer_category": null}], "is_impossible": false}], "context": "I installed Visual Studio 2010 SP1 the other day and now when i try to \"publish\" my apps i get this error on ALL of my projects.\nI have no idea what the problem is here and i find nothing on the all mighty Google and that makes me a bit nervous.\nCan anyone point me in the right direction here or maybe somebody else has the same problem? Because it happens in all of my projects so i'm kind of amazed im alone with this problem.\nif you have a depdendency on the above targets, which you can find out by looking for the following <Import> in your project files:\n<Import Project=\"$(MSBuildExtensionsPath)\\Microsoft\\VisualStudio\\v10.0\\Web\\Microsoft.Web.Publishing.targets\"\n", "document_id": 305}]}, {"paragraphs": [{"qas": [{"question": "How to bundle Java application for Mac/Windows?", "id": 1753, "answers": [{"answer_id": 1740, "document_id": 1325, "question_id": 1753, "text": "You should check out Package Maker for Mac and Advanced Installer for Windows.", "answer_start": 635, "answer_category": null}], "is_impossible": false}], "context": "I have a Java .jar application that I want to distribute to my clients who are on Macs or Windows. I want to use a tool that takes my jar file and wraps it in a .dmg and a .exe wrapper for Macs and Windows respectively that does this when run:\nChecks if JRE is installed; if not, it installs JRE6 from Oracle. Else, it updates installed JRE to latest 1.6.x version.\nCreates a short cut link in Start Menu (in Windows) or the Applications folder (in MacOSX) to my wrapped application and lets my application to run using the above JRE\nSupports easy \"uninstall application\" for Windows. For Mac, simply drag the .app to Trash to delete.\nYou should check out Package Maker for Mac and Advanced Installer for Windows.\n", "document_id": 1325}]}, {"paragraphs": [{"qas": [{"question": "How to deploy a java applet for today's browsers (applet, embed, object)?", "id": 493, "answers": [{"answer_id": 497, "document_id": 221, "question_id": 493, "text": "If you can target Java 6 update 10 or better, you can simplify your life. http://java.sun.com/javase/6/docs/technotes/guides/jweb/deployment_advice.html#deployingApplets.", "answer_start": 153, "answer_category": null}], "is_impossible": false}], "context": "How do i deploy a java applet for modern browsers? I know there are somehow 3 possibilities but nobody tells me which one to prefer and how to use them.\nIf you can target Java 6 update 10 or better, you can simplify your life. http://java.sun.com/javase/6/docs/technotes/guides/jweb/deployment_advice.html#deployingApplets.\n", "document_id": 221}]}, {"paragraphs": [{"qas": [{"question": "How do I install a script to run anywhere from the command line?", "id": 1181, "answers": [{"answer_id": 1174, "document_id": 757, "question_id": 1181, "text": "The best place to put things like this is /usr/local/bin.\nThis is the normal place to put custom installed binaries, and should be early in your PATH.\nSimply copy the script there (probably using sudo), and it should work for any use", "answer_start": 213, "answer_category": null}], "is_impossible": false}], "context": "anywhere in the system and it will run? Can this be implemented for all users on the system, or must it be redone for each one? Do I simply place the script in a specific directory, or are other things necessary? The best place to put things like this is /usr/local/bin.\nThis is the normal place to put custom installed binaries, and should be early in your PATH.\nSimply copy the script there (probably using sudo), and it should work for any use\n", "document_id": 757}]}, {"paragraphs": [{"qas": [{"question": "Atlassian Bamboo with Django & Python - Possible?", "id": 463, "answers": [{"answer_id": 472, "document_id": 196, "question_id": 463, "text": "You may have to massage to output of the Django test runner into traditional JUnit XML output, so that Bamboo can give you pretty graphs on how many tests passed. Look at this post about using xmlrunner.py to get Python working with Hudson. Also take a look at NoseXUnit.", "answer_start": 295, "answer_category": null}], "is_impossible": false}], "context": "At my company, we currently use Atlassian Bamboo for our continuous integration tool. We currently use Java for all of our projects, so it works great.\nHowever, we are considering using a Django + Python for one of our new applications. I was wondering if it is possible to use Bamboo for this.\nYou may have to massage to output of the Django test runner into traditional JUnit XML output, so that Bamboo can give you pretty graphs on how many tests passed. Look at this post about using xmlrunner.py to get Python working with Hudson. Also take a look at NoseXUnit.\n", "document_id": 196}]}, {"paragraphs": [{"qas": [{"question": "Can't run Curl command inside my Docker Container", "id": 671, "answers": [{"answer_id": 676, "document_id": 364, "question_id": 671, "text": " you have to install it with :apt-get update; apt-get install curl", "answer_start": 540, "answer_category": null}], "is_impossible": false}], "context": "I created a docker container from my OS X VM Docker host. I created it using the run command and created the container based off the ubuntu:xenial image off docker hub.\nI'm now connected to my container after it's created and logged in as root and at the command prompt inside my container.\nI tried to install homebrew and for some reason, I can't run the command to install Homebrew: when I run that I get a bash:\ncurl: command not found\nNot sure why I'm not able to use curl here inside my container.curl: command not found is a big hint, you have to install it with :apt-get update; apt-get install curl. Ran into this same issue while using the CURL command inside my Dockerfile. As Gilles pointed out, we have to install curl first\n", "document_id": 364}]}, {"paragraphs": [{"qas": [{"question": "cannot install aptana studio 3 6 on windows", "id": 1502, "answers": [{"answer_id": 1491, "document_id": 1081, "question_id": 1502, "text": "\n    \n\nRight click the installer and choose \"Run as administrator\". I suspect it needs administrator account to download and install Node JS during instal", "answer_start": 5592, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI'd like to use Aptana Studio for Rails development under Windows. I currently have different dev tools &amp; ide's up and running (like git/ruby/jdk) and I'd like to install Aptana Studio as well but I can't. After downloading and running installer, it starts properly and after I choose destination dir, it starts downloading prequisities. I have several problems with it:\n\n\nAfter a while, it tries to install node.js (well, I am not sure why, but let it be) and sometimes it installs it ok, but sometimes it just fails with no error, telling me only that aptana couldn't install prequisities.\nIf by a chance node.js is installed, it tries to install msysgit (again, I don't know why, because I have git installed and added to PATH). When installing msysgit is done, all i get is info that \"installing msysgit failed\" and all I can do is click finish in installer which is telling me that prequisities couldn't be installed.\n\n\nI am using Win7 x64, ruby 1.9.3p-545, msysgit 1.8.4, jdk 1.7. Does anyone had simmilar problem and succesfully installed this ide? \n    \n\nInstalling Aptana Studio in passive mode bypasses the installation of Git for Windows and Node.js.\n\n\n\nAptana_Studio_3_Setup_3.6.1 /passive /norestart\n\n\n(I am unsure whether Aptana Studio will work properly without those \"prerequisites\", but it appears to.)\n\nIf you want a global installation in a specific directory, the command line is\n\nAptana_Studio_3_Setup_3.6.1.exe /passive /norestart ALLUSERS=1 APPDIR=c:\\apps\\AptanaStudio\n\n    \n\nI had the same problem. I solved by installing NodeJS from this link: http://go.aptana.com/installer_nodejs_windows\nand Git latest version from https://git-scm.com/downloads.\n\nFinally I was able to run the Aptana installer with no problems.\n    \n\nI have some issue, the fix is:\n\n\nUninstall any nodejs version.\nInstall https://nodejs.org/dist/v0.10.36/x64/node-v0.10.36-x64.msi.\nInstall Aptana.\nCode...\n\n\ngreetings!\n    \n\nMost of us will run into this problem. This can be solved by installing NodeJS from http://go.aptana.com/installer_nodejs_windows. My Aptana installation installed the GIT automatically but if you still find trouble then I would suggest to install the GIT latest version yourself and rerun the Aptana installation.\n    \n\nInstall NODE.JS on windows before installing aptana\n\nTry the following link\nhttp://blueashes.com/2011/web-development/install-nodejs-on-windows/\n    \n\nIf your issue is related to a failed Windows install,  and you are receiving a message related to installer _jsnode_windows.msi CRC error:\n\n\n  Aptana Studio (Aptana Studio 3, build: 3.6.1.201410201044) currently\n  requires Nodejs 0.5.XX-0.11.xx\n\n\nEven though the current release of nodejs seems to be 5.X.X .   Apparently there will be a new release in Nov 2015 that corrects this defect.\n\nPre-installing x0.10.36-x64 allowed me to proceed with a successful install.   If version numbers can be believed,   this seems to be an ancient release of nodejs,   but hey - I saw a very impressive demo of Aptana Studio and really wanted to install it. :-)\n\nI also pre-installed GIT for windows,   but I'm not sure if that was necessary or not.\n    \n\nSimply, create the folder you want to extract Aptana Studio to, and then use this command:\n\nAptana_Studio_3_Setup_3.6.1.exe /extract:\"folder\"\n\n    \n\nIt seems having msysgit (Git for Windows) installed is causing the problem.\n\nIn most cases you'll have a pretty recent version of Git for Windows installed.\nCited from https://code.google.com/p/tortoisegit/:\n\n\n  There was a security issue in Git, see here. Git for Windows &lt; 1.9.5 is affected - so you should update, TortoiseGit itself is not affected (using the default configuration; only if libgit2 is manually enabled for checkout/fetching). TortoiseGit 1.8.13.0 includes all fixes.\n\n\nBut it seems Aptana Studio Installer won't accept any pre-installed version of Git for Windows!\n\nWhat you need to do:  \n\n\nUninstall Git for Windows.  \nInstall Aptana Studio.\nApanta Studio 3.6.1 will install Git for Windows 1.8.4-preview20130916.  \nDownload latest version of Git for Windows from http://msysgit.github.io/.  \nInstall latest version of Git for Windows.\nThe outdated Git for Windows 1.8.4-preview20130916 will be updated to recent version.\n\n\nThat's it !!!\n    \n\nI had this issue and it was because of limited internet connection to source. You can use a proxy (VPN) but the better solution is download manually NodeJs from the source https://nodejs.org/download/ and Git, too. \n\nafter installation manually, aptana will check if they installed or not.\n    \n\nI had a problem installing Aptana 3.6.1 on my windows machine. It always shows \"fail to locate node.js installer missing windows msi\" No matter how many times I tried to.\n\nHere is how I solved the problem:\n\n\nInstall Git: This link is for windows and will automatically have the file ready to dowload \nhttps://git-scm.com/download/win\nInstead of installing the latest version, I chose a little earlier version (also 3) but it's 3.2.2 instead of 3.61 (the current one on the Aptana page) :\nhttp://www.filehorse.com/download-aptana/11489/\n\n    \n\nYou have to use portable git not installer.Extract the folder to Program Files and rename the folder name from PortableGit to Git.\n    \n\nHad the same error initially. so.. pre-installed git and node.js and later installed Aptana, which installed perfectly. Rajeeva.\n    \n\nI'm using Win 8.1, and have installed Aptana easily, and use for PHP programming without any issues.\nAnd just experimented with Ruble enhancement to the UI.\nAll seems to work just fine.\n\n3.6.0.201407100658\n    \n\nRight click the installer and choose \"Run as administrator\". I suspect it needs administrator account to download and install Node JS during installation.\n    ", "document_id": 1081}]}, {"paragraphs": [{"qas": [{"question": "how do i resolve a permission error installing a custom built gem", "id": 1426, "answers": [{"answer_id": 1415, "document_id": 1000, "question_id": 1426, "text": "changing the ~/.gem permissions:\n\nsudo chown user ~/.gem -R", "answer_start": 2183, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI'm attempting to build my first ruby gem and all seemed to go well until I attempted to install the gem. I'm using RVM so no need for 'sudo gem install' here.\n\nFirst I attempted to do the following:\n\njim:~/Desktop/spectest \\ [git:master]  \n\u2192 rake manifest\n(in /Users/jim/Desktop/spectest)\nCleaning\n- pkg\nrm -rf pkg\nBuilding Manifest\n  Manifest\n  README\n  Rakefile\n  bin/buildcss\n  ...\n\njim:~/Desktop/spectest \\ [git:master]  \n\u2192 rake install\n(in /Users/jim/Desktop/spectest)\nCleaning\nGemspec generated\nmkdir -p pkg\nmkdir -p pkg/css-reader-0.1.0\nrm -f pkg/css-reader-0.1.0/Manifest\n...\ncd pkg\ntar zcvf css-reader-0.1.0.tar.gz css-reader-0.1.0\na css-reader-0.1.0\na css-reader-0.1.0/bin\n...\ncd -\nWARNING:  description and summary are identical\n  Successfully built RubyGem\n  Name: css-reader\n  Version: 0.1.0\n  File: css-reader-0.1.0.gem\nmv css-reader-0.1.0.gem pkg/css-reader-0.1.0.gem\nPrivate key not found; gem will not be signed.\nTargeting \"ruby\" platform.\nPassword:\nSorry, try again.\nPassword:\nERROR:  While executing gem ... (Gem::InstallError)\n    cannot uninstall, check `gem list -d css-reader`\nSuccessfully installed css-reader-0.1.0\n1 gem installed\nInstalling ri documentation for css-reader-0.1.0...\nUpdating class cache with 3288 classes...\nInstalling RDoc documentation for css-reader-0.1.0...\n\n\nOk - so I wasn't certain why I got the Gem::InstallError here. Is there a log ruby gems keeps that I can find more details regarding the error? Either way I attempted to try installing the .gem package directly:\n\ngem install pkg/css-reader-0.1.0.gem\nERROR:  While executing gem ... (Errno::EACCES)\n    Permission denied - /Users/jim/.rvm/gems/ruby-1.9.2-preview1/gems/css-reader-0.1.0/Manifest\n\n\nAnd I get a bit more feedback. This time it looks like a permissions error with the Manifest file. So I attempted to chmod 777 the Manifest file and repeat the process all over again but I keep getting the error. Seems like a novice mistake somewhere on my behalf. Any advice is appreciated!\n    \n\nIt is possible at some stage that you had something run with sudo which messed up your permisisons. Have you tried chown -R jim ~/.rvm\n    \n\nI solved the problem changing the ~/.gem permissions:\n\nsudo chown user ~/.gem -R\n\n    \n\nright on. solved a similar problem. i had ~/.gem directory locked down for some reason while switching between sudo and such. Thanks!\n    ", "document_id": 1000}]}, {"paragraphs": [{"qas": [{"question": "How to add man and zip to \"git bash\" installation on Windows?", "id": 857, "answers": [{"answer_id": 852, "document_id": 537, "question_id": 857, "text": "Gow (Gnu On Windows) is the lightweight alternative to Cygwin. It uses a convenient NSIS installer that installs over 100 extremely useful open source UNIX applications compiled as native win32 binaries. ", "answer_start": 95, "answer_category": null}], "is_impossible": false}], "context": "Gow - The lightweight alternative to Cygwin\nDownload Installer | Home Page | FAQ\n\nIntroduction\nGow (Gnu On Windows) is the lightweight alternative to Cygwin. It uses a convenient NSIS installer that installs over 100 extremely useful open source UNIX applications compiled as native win32 binaries. It is designed to be as small as possible, about 18 MB, as opposed to Cygwin which can run well over 100 MB depending upon options.\n\nHere are a couple quotes from happy Gow users:\n\n\"Gow is one of the few things that makes Windows bearable/usable\"\n\n\"I use Gow constantly. It's awesome.\"\n\n\"I just wanted to let you know that the GOW Suite is simply great - it is far lighter than the Cygwin tool, and is extremely useful. \"\n\nFeatures and Benefits\nUltra light: Small, light subset (about 18 MB) of very useful UNIX binaries that do not have decent installers (until now!).\nShell window from any directory: Adds a Windows Explorer shell window (screenshot) so that you can right-click on any directory and open a command (cmd.exe) window from that directory.\nSimple install/remove: Easy to install and remove, all files contained in a single directory in a standard C:\\Program Files path.\nIncluded in PATH: All binaries are conveniently installed into the Windows PATH so they are accessible from a command-line window.\nStable binaries: All commands are stable and tested.\nWin32 Utilities Overview\nBelow are just a few of the 100+ applications found in Gow.\n\nShell scripting: bash, zsh\nCompression: gzip, zip, bzip2, compress\nSSH: putty, psftp, pscp, pageant, plink\nDownload/upload: cURL, wget\nFTP: NcFTP\nEditing: vim, nano\nText search/view: grep, agrep, less, cat, tail, head\nFile system: mv, cp, du, ls, pwd, rmdir, whereis\nDevelopment: make, diff, diff3, sleep, cvs, dos2unix, unix2dos\nOther Links\nRelease Notes\nContributing\nExecutables list\nUnix command reference\nHistory\nGow was used by Vuzit LLC for years on production hardware as well as all development machines. In order to share the great product with the world as well as improve the quality it was released as an open source on July 14, 2010. This isn't the only open source project associated by Vuzit. The company has released several open source products that work with the DocuPub platform: VuzitJava, VuzitRuby, VuzitPHP and Vuzit.Net.\n\nLicense\nAll source code is released under the open source MIT license which allows you to use it in any proprietary product. All binaries are licensed under various licenses, mainly GPL.\n\nFeedback\nPlease submit feedback via the Gow Issues forum.\n\nOther Similar Projects\nGNU utilities for Win32: Quite literally the grand-daddy of Unix utilities projects. It hasn't been updated since 2003. Gow originally borrowed from these utilities but has updated many of them and added all of the installer goodness that you've come to expect.\nGnuWin: An excellent project with lots of utilities. GnuWin might be for you if you just want one or two utilities rather than the Cygwin-\"light\" suite that Gow provides.\nMSYS: From MinGW, well known as the basis for msysgit (in fact, if you have msysgit on your machine, then you already have these utilities).\nMSYS2: MSYS2 is software distribution and a building platform for Windows. It provides a Unix-like environment, a command-line interface and a software repository making it easier to install, use, build and port software on Windows.\nUWIN: Software package created by David Korn at AT&T which allows programs written for the operating system Unix be built and run on Microsoft Windows\nbabun: a cygwin-based shell with oh-my-zsh, package manager and plugin architecture.", "document_id": 537}]}, {"paragraphs": [{"qas": [{"question": "Configuration With Same Name Already Exists", "id": 1668, "answers": [{"answer_id": 1655, "document_id": 1241, "question_id": 1668, "text": "Make sure you're using the drop down list from the grid (not the one at the top of the dialog), and do not check the \"Create new solution configurations\" checkbox when adding your new project configuration", "answer_start": 1020, "answer_category": null}], "is_impossible": false}], "context": "I have a solution with 10+ projects (VS2010 SP1). I have the following configurations defined in the solution:\n\u2022\tDebug\n\u2022\tDebug-QA\n\u2022\tRelease-UAT\n\u2022\tRelease-Production\nThis allows me to easily setup specific settings for each deployment scenario. However, for some reason I can't get things setup as I'd like. Please see this screenshot:\n \nNotice the highlighted projects/configurations. I am unable to create a \"Debug-QA\" configuration for these projects (by selecting <New> in the cell for that particular project). When I try to add a new \"Debug-QA\" configuration to the DataUtility project, for instance, Visual Studio yells at me:\nThis configuration could not be created because a solution configuration of the same name already exists.\nI know it does! I'm trying to add the configuration to the project! What am I missing here? I want all projects to have all 5 configuration. I have the same problem when trying to match up (create) platforms (for instance, adding an \"Any CPU\" platform to the DataUtility project).\nMake sure you're using the drop down list from the grid (not the one at the top of the dialog), and do not check the \"Create new solution configurations\" checkbox when adding your new project configuration.\n", "document_id": 1241}]}, {"paragraphs": [{"qas": [{"question": "How to prevent \"This program might not have installed correctly\" messages on Vista", "id": 804, "answers": [{"answer_id": 799, "document_id": 486, "question_id": 804, "text": "You need to add some information into the AppCompat section of the registry.\nSee this link and look for \"How to disable a Program Compatibility Assistant warning\"", "answer_start": 202, "answer_category": null}], "is_impossible": false}], "context": "I have a product setup executable that copies some files to the user's hard drive. It's not a typical installer in the normal sense (it doesn't add anything to the Start Menu or Program Files folders).\nYou need to add some information into the AppCompat section of the registry.\nSee this link and look for \"How to disable a Program Compatibility Assistant warning\".\nIs there a function I need to call from the exe or registry entry to set, to indicate to the operating system that the program installed correctly (or to at least supress this message)?\n", "document_id": 486}]}, {"paragraphs": [{"qas": [{"question": "Android Studio 3.6.1 | Error: \"This project uses AndroidX dependencies\"", "id": 1819, "answers": [{"answer_id": 1804, "document_id": 1390, "question_id": 1819, "text": "To enable jetifier, add those two lines to your gradle.properties file:\nandroid.useAndroidX=true\nandroid.enableJetifier=true", "answer_start": 150, "answer_category": null}], "is_impossible": false}], "context": "I have updated my Android Studio and the targetSdkVersion (build.gradle) to 29. I migrated to AndroidX. Now the Gradle build gives me these 2 errors.\nTo enable jetifier, add those two lines to your gradle.properties file:\nandroid.useAndroidX=true\nandroid.enableJetifier=true\n", "document_id": 1390}]}, {"paragraphs": [{"qas": [{"question": "How to install CLang using precompiled binaries?", "id": 1759, "answers": [{"answer_id": 1746, "document_id": 1331, "question_id": 1759, "text": "You can follow the same step as mentioned in https://askubuntu.com/questions/89615/how-do-i-install-llvm-clang-3-0", "answer_start": 577, "answer_category": null}], "is_impossible": false}], "context": "How do I install CLang on Ubuntu, using precompiled binaries of CLang that I downloaded?\nHere's how I downloaded CLang: \"LLVM Download Page\" -> \"Download LLVM 3.2\" -> \"Clang Binaries for Ubuntu-12.04/x86_64\" ( http://llvm.org/releases/3.2/clang+llvm-3.2-x86_64-linux-ubuntu-12.04.tar.gz .)\nThen, I expanded the archive into a folder on my Ubuntu 12.04 LTS 64-bit machine.\nQuestion: What do I do next? Do I have to copy these into some folders myself, and if so, which ones exactly? Most instructions I found online are for building CLang from source, which doesn't apply here.\nYou can follow the same step as mentioned in https://askubuntu.com/questions/89615/how-do-i-install-llvm-clang-3-0\n", "document_id": 1331}]}, {"paragraphs": [{"qas": [{"question": "Why won't you switch to Python 3.x?", "id": 1127, "answers": [{"answer_id": 1120, "document_id": 704, "question_id": 1127, "text": "The django third-party packages and extensions used in many Django projects are in various stages of Python 3 compatibility implementation. More details can be found in: https://www.djangopackages.com/python3/.", "answer_start": 583, "answer_category": null}], "is_impossible": false}], "context": "I ask this for deployable reasons. As in, if I write a solution in python, I feel bound to write to 2.x due to the lack of adoption of python 3. This is a major daily concern of mine, and I want to figure out what's going on.\nFor many of the python-based questions here, people are giving solutions that simply do not work in python 3.x.\nSo, I ask the question: What has prevented you from switching to python 3.x in your personal or work environment? The conversion script exists, and yet programmers (who are usually the earliest adopters of everything) seem to refuse to upgrade.\nThe django third-party packages and extensions used in many Django projects are in various stages of Python 3 compatibility implementation. More details can be found in: https://www.djangopackages.com/python3/.\n", "document_id": 704}]}, {"paragraphs": [{"qas": [{"question": "CondaValueError: Malformed version string '~': invalid character(s)", "id": 1735, "answers": [{"answer_id": 1722, "document_id": 1307, "question_id": 1735, "text": "This looks like it was fixed with Conda 4.6.0. Upgrading your Conda should resolve the issue.\nconda upgrade -n base conda", "answer_start": 194, "answer_category": null}], "is_impossible": false}], "context": "I'm getting a Malformed version string error with my conda. I have no idea how to debug this or how to check it.\nCan anyone help? GitHub has talked about the issue but I haven't seen any fixes.\nThis looks like it was fixed with Conda 4.6.0. Upgrading your Conda should resolve the issue.\nconda upgrade -n base conda\n", "document_id": 1307}]}, {"paragraphs": [{"qas": [{"question": "How to install a Mac application using Terminal", "id": 1758, "answers": [{"answer_id": 1745, "document_id": 1330, "question_id": 1758, "text": "Do you have any spaces in your package path? You should wrap it up in double quotes to be safe, otherwise it can be taken as two separate arguments\nsudo installer -store -pkg \"/User/MyName/Desktop/helloWorld.", "answer_start": 504, "answer_category": null}], "is_impossible": false}], "context": "Apple suggests that prior to submitting to the Mac application store, the installation process for Macs be tested using the command\nsudo installer -store -pkg path-to-package -target /\nI saved the application package to the desktop and then in the terminal I sent the command\nsudo installer -store -pkg /User/MyName/Desktop/helloWorld.pk\nI am very new to the Terminal. What does this mean and how do I fix it so that I can install the application as suggested by Apple?\nProbably not exactly your issue..\nDo you have any spaces in your package path? You should wrap it up in double quotes to be safe, otherwise it can be taken as two separate arguments\nsudo installer -store -pkg \"/User/MyName/Desktop/helloWorld.\n", "document_id": 1330}]}, {"paragraphs": [{"qas": [{"question": "What flags do you set for your GFORTRAN debugger/compiler to catch faulty code?", "id": 1719, "answers": [{"answer_id": 1707, "document_id": 1292, "question_id": 1719, "text": "For debugging I use: -O2 -fimplicit-none -Wall -Wline-truncation -Wcharacter-truncation -Wsurprising -Waliasing -Wimplicit-interface -Wunused-parameter -fwhole-file -fcheck=all -std=f2008 -pedantic -fbacktrace. For ones not already explained, check the gfortran manual. -fcheck=all includes -fcheck=bounds.\nFor production I use: -O3 -march=native -fimplicit-none -Wall -Wline-truncation -fwhole-file -std=f2008.", "answer_start": 558, "answer_category": null}], "is_impossible": false}], "context": "I think I won't find that in any textbook, because answering this takes experience. I am currently in the stage of testing/validating my code / hunting bugs to get it into production state and any errors would lead to many people suffering e.g. the dark side.\n\u2022\tWhat kind of flags do you set when you compile your program for Fortran for debugging purposes?\n\u2022\tWhat kind of flags do you set for the production system?\n\u2022\tWhat do you do before you deploy?\nThe production version uses ifort as a compiler, yet I do my testing with gfortran. Am I doing it wrong?\nFor debugging I use: -O2 -fimplicit-none -Wall -Wline-truncation -Wcharacter-truncation -Wsurprising -Waliasing -Wimplicit-interface -Wunused-parameter -fwhole-file -fcheck=all -std=f2008 -pedantic -fbacktrace. For ones not already explained, check the gfortran manual. -fcheck=all includes -fcheck=bounds.\nFor production I use: -O3 -march=native -fimplicit-none -Wall -Wline-truncation -fwhole-file -std=f2008. \n\n", "document_id": 1292}]}, {"paragraphs": [{"qas": [{"question": "Where is the Windows Service located?", "id": 990, "answers": [{"answer_id": 985, "document_id": 610, "question_id": 990, "text": "LoaderService.cs", "answer_start": 6170, "answer_category": null}], "is_impossible": false}, {"question": "How to get called when the LoaderService is started?", "id": 991, "answers": [{"answer_id": 986, "document_id": 610, "question_id": 991, "text": "protected override void OnStart(string[] args)\n{\n    // the name of the application to launch\n    String applicationName = \"cmd.exe\";\n\n    // launch the application\n    ApplicationLoader.PROCESS_INFORMATION procInfo;\n    ApplicationLoader.StartProcessAndBypassUAC(applicationName, out procInfo);\n}", "answer_start": 6299, "answer_category": null}], "is_impossible": false}, {"question": "How to acquire the process handle?", "id": 992, "answers": [{"answer_id": 987, "document_id": 610, "question_id": 992, "text": "hProcess = OpenProcess(MAXIMUM_ALLOWED, false, winlogonPid);", "answer_start": 8223, "answer_category": null}], "is_impossible": false}, {"question": "How to acquire the access token of the winlogon process?", "id": 993, "answers": [{"answer_id": 988, "document_id": 610, "question_id": 993, "text": "if (!OpenProcessToken(hProcess, TOKEN_DUPLICATE, ref hPToken))\n{\n    CloseHandle(hProcess);\n    return false;\n}", "answer_start": 8519, "answer_category": null}], "is_impossible": false}, {"question": " configure LoaderService to launch the command prompt for every User on the computer the first time that they log on?", "id": 994, "answers": [{"answer_id": 989, "document_id": 610, "question_id": 994, "text": "The solution is in the OnStart function: either wire-up a Timer, or spawn a Thread that runs in an infinite loop every so many seconds (Thread.Sleep(1000) can be used to control how often the Thread runs). We can use a List<int> object to keep track of all the Session IDs that we have already launched our process in. Every time our Thread executes, we check to see if the Session ID has changed. The currently active Session ID can be retrieved by calling WTSGetActiveConsoleSessionId(). If the Session ID has changed, we check to see if we have launched our process into that Session. If we have not, then we call StartProcessAndBypassUAC and add the Session ID to the List<int> object.", "answer_start": 17405, "answer_category": null}], "is_impossible": false}, {"question": "What does loaderServiceProcessInstaller do?", "id": 995, "answers": [{"answer_id": 990, "document_id": 610, "question_id": 995, "text": "The loaderServiceProcessInstaller control allows us to specify the account under which the LoaderService will run", "answer_start": 12975, "answer_category": null}], "is_impossible": false}, {"question": "How to add an installer?", "id": 996, "answers": [{"answer_id": 991, "document_id": 610, "question_id": 996, "text": "open up the LoaderService.cs designer. Then, right-click, and select Add Installer:\n\nAdd Installer To LoaderService", "answer_start": 12573, "answer_category": null}], "is_impossible": false}, {"question": "how to bypass Vista UAC as well as how to correctly launch an interactive process from a Windows?", "id": 997, "answers": [{"answer_id": 992, "document_id": 610, "question_id": 997, "text": "First, we are going to create a Windows Service that runs under the System account. This service will be responsible for spawning an interactive process within the currently active User\u2019s Session. This newly created process will display a UI and run with full admin rights. When the first User logs on to the computer, this service will be started and will be running in Session0; however the process that this service spawns will be running on the desktop of the currently logged on User. We will refer to this service as the LoaderService.\n\nNext, the winlogon.exe process is responsible for managing User login and logout procedures. We know that every User who logs on to the computer will have a unique Session ID and a corresponding winlogon.exe process associated with their Session. Now, we mentioned above, the LoaderService runs under the System account. We also confirmed that each winlogon.exe process on the computer runs under the System account. Because the System account is the owner of both the LoaderService and the winlogon.exe processes, our LoaderService can copy the access token (and Session ID) of the winlogon.exe process and then call the Win32 API function CreateProcessAsUser to launch a process into the currently active Session of the logged on User. Since the Session ID located within the access token of the copied winlogon.exe process is greater than 0, we can launch an interactive process using that token.", "answer_start": 4635, "answer_category": null}], "is_impossible": false}, {"question": "What is physical console?", "id": 998, "answers": [{"answer_id": 993, "document_id": 610, "question_id": 998, "text": " The Physical Console consists of the monitor, keyboard, and mouse", "answer_start": 1490, "answer_category": null}], "is_impossible": false}, {"question": "What is the sessions in vista?", "id": 999, "answers": [{"answer_id": 994, "document_id": 610, "question_id": 999, "text": "When you log on, the system assigns you a unique Session ID", "answer_start": 1009, "answer_category": null}], "is_impossible": false}], "context": "Subverting Vista UAC in Both 32 and 64 bit Architectures\n\nPero Mati\u0107\nRate me:\n\n\n\n\n\n\n4.96/5 (143 votes)\n22 Apr 2009\nCPOL\n14 min read\nThis article illustrates how to bypass Vista UAC as well as how to correctly launch an interactive process from a Windows Service.\nDownload source - 15.8 KB\nIntroduction\nThe purpose of this article is to illustrate how to correctly launch an interactive process from a service in Windows Vista, and also to demonstrate how to launch that process with full Administrator privileges. An interactive process is one that is capable of displaying a UI on the desktop.\n\nThe article shows how to create a service called LoaderService that serves as an application loader and whose purpose is to launch, at boot time, a command prompt that runs as an Administrator. The article closes with a section discussing how the code could be extended for more practical purposes.\n\nSessions in Vista\nLet\u2019s start from the beginning\u2026 you have just booted up your computer and are about to log on. When you log on, the system assigns you a unique Session ID. In Windows Vista, the first User to log on to the computer is assigned a Session ID of 1 by the OS. The next User to log on will be assigned a Session ID of 2. And so on and so forth. You can view the Session ID assigned to each logged on User from the Users tab in Task Manager:\n\nTask Manager - Users\n\nNotice, I indicated that the User named Pero is in control of the Console. In this case, I mean the Physical Console. The Physical Console consists of the monitor, keyboard, and mouse. Since Pero is in control of the keyboard, monitor, and mouse, he is considered the currently active User. However, since Users can be impersonated, it is more appropriate to reference the currently active Session rather than the currently active User. The Win32 API contains a function called WTSGetActiveConsoleSessionId() which returns the Session ID of the User currently in control of the Physical Console. If we were to call that method right now, it would return a value of 1 because that is the Session ID of the User Pero.\n\nThere exists a special Session in Vista that has a Session ID of 0. This is commonly referenced as Session0. All Windows Services run within Session0, and Session0 is non-interactive. Non-interactive means that UI applications cannot be launched; however, there is a way around this by activating the Interactive Services Detection Service (ISDS). This not a very elegant solution, and will not be covered in this article. There is a quick 5 minute Channel 9 video that demonstrates the ISDS for those interested. This article assumes the absence of the ISDS. Now, because Session0 is not a User Session, it does not have access to the video driver, and therefore any attempts to render graphics will fail. Session0 isolation is a security feature added in Vista to isolate system processes and services from potentially malicious user applications.\n\nThis is where things get interesting. The reason for this isolation is because the System account (or System User) has elevated privileges that allow it to run unhindered by the restrictions of Vista UAC. If everything were running under the System account, Vista UAC might as well be turned off.\n\nNow, I know what you\u2019re thinking, \u201cIf Windows Services run in Session0, and Session0 cannot start processes that have a UI, then how can our loader service spawn a new process that not only has a UI, but that also runs within the currently logged on User\u2019s Session?\u201d Take a look at this screenshot from the Processes tab in Task Manager, and pay particular attention to the winlogon.exe processes:\n\nTask Manager - Processes\n\nNotice there are two winlogon.exe processes, and the User who owns both of those processes is the System User. The System User is a highly privileged User unhindered by the Vista UAC that we were talking about earlier. Also, notice the Session IDs that indicate within which Sessions the winlogon.exe processes are running. If you remember from earlier, Session ID 1 refers to the User Pero\u2019s Session, and Session ID 2 refers to the User Sienna\u2019s Session. This means that there is a winlogon.exe process running under the System account within Pero\u2019s Session. It also means that there is a winlogon.exe process running under the System account within Sienna\u2019s Session. This is the appropriate time to mention that any Session with an ID greater than 0 is capable of spawning an interactive process, which is a process capable of displaying a UI.\n\nThe solution may not be totally clear yet, but it will be shortly, as now it is time to discuss our strategy!\n\nOur Strategy\nFirst, we are going to create a Windows Service that runs under the System account. This service will be responsible for spawning an interactive process within the currently active User\u2019s Session. This newly created process will display a UI and run with full admin rights. When the first User logs on to the computer, this service will be started and will be running in Session0; however the process that this service spawns will be running on the desktop of the currently logged on User. We will refer to this service as the LoaderService.\n\nNext, the winlogon.exe process is responsible for managing User login and logout procedures. We know that every User who logs on to the computer will have a unique Session ID and a corresponding winlogon.exe process associated with their Session. Now, we mentioned above, the LoaderService runs under the System account. We also confirmed that each winlogon.exe process on the computer runs under the System account. Because the System account is the owner of both the LoaderService and the winlogon.exe processes, our LoaderService can copy the access token (and Session ID) of the winlogon.exe process and then call the Win32 API function CreateProcessAsUser to launch a process into the currently active Session of the logged on User. Since the Session ID located within the access token of the copied winlogon.exe process is greater than 0, we can launch an interactive process using that token.\n\nNow for the fun stuff\u2026 the code!\n\nThe Code\nThe Windows Service is located in a file called LoaderService.cs within the Toolkit project. Below is the code that gets called when the LoaderService is started:\n\nC#\nCopy Code\nprotected override void OnStart(string[] args)\n{\n    // the name of the application to launch\n    String applicationName = \"cmd.exe\";\n\n    // launch the application\n    ApplicationLoader.PROCESS_INFORMATION procInfo;\n    ApplicationLoader.StartProcessAndBypassUAC(applicationName, out procInfo);\n}\nThe code above calls the StartProcessAndBypassUAC(...) function which will launch a command prompt (with full admin rights) as part of a newly created process. Information about the newly created process will get stored into the variable procInfo.\n\nThe code for StartProcessAndBypassUAC(...) is located in the file ApplicationLoader.cs. Let\u2019s dissect that function to examine how a service running in Session0 will load a process into the currently logged on User\u2019s Session. To begin, we will obtain the Session ID of the currently logged on User. This is achieved by making a call to the Win32 API function WTSGetActiveConsoleSessionId().\n\nC#\nCopy Code\n// obtain the currently active session id; every logged on \n// User in the system has a unique session id\nuint dwSessionId = WTSGetActiveConsoleSessionId();\nNext, we will obtain the Process ID (PID) of the winlogon.exe process for the currently active Session. Remember, there are two Sessions currently running, and if we copy the access token of the wrong one, we could end up launching our new process on another User\u2019s desktop.\n\nC#\nCopy Code\n// obtain the process id of the winlogon process that \n// is running within the currently active session\nProcess[] processes = Process.GetProcessesByName(\"winlogon\");\nforeach (Process p in processes)\n{\n    if ((uint)p.SessionId == dwSessionId)\n    {\n        winlogonPid = (uint)p.Id;\n    }\n}\nNow that we have obtained the PID of the winlogon.exe process, we can use that information to obtain its process handle. To do so, we make a Win32 API call to OpenProcess(...):\n\nC#\nCopy Code\n// obtain a handle to the winlogon process\nhProcess = OpenProcess(MAXIMUM_ALLOWED, false, winlogonPid);\nHaving acquired the process handle, we can make a Win32 API call to OpenProcessToken(...) to obtain a handle to the access token of the winlogon.exe process:\n\nC#\nCopy Code\n// obtain a handle to the access token of the winlogon process\nif (!OpenProcessToken(hProcess, TOKEN_DUPLICATE, ref hPToken))\n{\n    CloseHandle(hProcess);\n    return false;\n}\nWith a handle to the access token, we can proceed to call the Win32 API function DuplicateTokenEx(...) which will duplicate the access token:\n\nC#\nCopy Code\n// Security attibute structure used in DuplicateTokenEx and CreateProcessAsUser\n// I would prefer to not have to use a security attribute variable and to just \n// simply pass null and inherit (by default) the security attributes\n// of the existing token. However, in C# structures are value types and therefore\n// cannot be assigned the null value.\nSECURITY_ATTRIBUTES sa = new SECURITY_ATTRIBUTES();\nsa.Length = Marshal.SizeOf(sa);\n\n// copy the access token of the winlogon process; \n// the newly created token will be a primary token\nif (!DuplicateTokenEx(hPToken, MAXIMUM_ALLOWED, ref sa, \n        (int)SECURITY_IMPERSONATION_LEVEL.SecurityIdentification, \n        (int)TOKEN_TYPE.TokenPrimary, ref hUserTokenDup))\n{\n    CloseHandle(hProcess);\n    CloseHandle(hPToken);\n    return false;\n}\nThere are many advantages to duplicating an access token. Most notable in our case is that we have a new copy of a primary access token which also contains within it the associated logon Session of that copied token. If you refer to the Task Manager screenshot above that shows the two winlogon.exe processes, you will notice that the duplicated Session ID will be 1, which is the Session ID of the currently logged on User, Pero. We can now call the Win32 API function CreateProcessAsUser to spawn a new process within the Session of the currently logged on User; in this case, the process will spawn in the Session of the User Pero. To summarize, the code below runs in Session0, but will launch a new process in Session 1:\n\nC#\nCopy Code\nSTARTUPINFO si = new STARTUPINFO();\nsi.cb = (int)Marshal.SizeOf(si);\n\n// interactive window station parameter; basically this indicates \n// that the process created can display a GUI on the desktop\nsi.lpDesktop = @\"winsta0\\default\";\n\n// flags that specify the priority and creation method of the process\nint dwCreationFlags = NORMAL_PRIORITY_CLASS | CREATE_NEW_CONSOLE;\n\n// create a new process in the current User's logon session\nbool result = CreateProcessAsUser(hUserTokenDup,  // client's access token\n                                null,             // file to execute\n                                applicationName,  // command line\n                                ref sa,           // pointer to process SECURITY_ATTRIBUTES\n                                ref sa,           // pointer to thread SECURITY_ATTRIBUTES\n                                false,            // handles are not inheritable\n                                dwCreationFlags,  // creation flags\n                                IntPtr.Zero,      // pointer to new environment block \n                                null,             // name of current directory \n                                ref si,           // pointer to STARTUPINFO structure\n                                out procInfo      // receives information about new process\n                                );\nThe above code will launch a command prompt that is running as an Administrator under the System account. I\u2019d like to comment on the parameter @\"winsta0\\default\". This is a hard-coded String that Microsoft arbitrarily chose to indicate to the OS that the process we are about to spawn in CreateProcessAsUser should have full access to the interactive windowstation and desktop, which basically means it is allowed to displayed UI elements on the desktop.\n\nThat\u2019s all there is to the code. Now, let\u2019s discuss how to deploy this service using an MSI, and how to configure it to launch automatically when the computer boots up!\n\nDeploying the Code\nThe most efficient way to deploy our code is to create an MSI installer for it. However, we have to first perform a couple of tasks to prepare our service for installation. To begin, we need to add an installer for our LoaderService. To add an installer, open up the LoaderService.cs designer. Then, right-click, and select Add Installer:\n\nAdd Installer To LoaderService\n\nThe above action adds a new class to the project called ProjectInstaller. This class inherits from the Installer class. There are two components visible on the designer of ProjectInstaller.cs that I have renamed for clarity to loaderServiceProcessInstaller and loaderServiceInstaller. The loaderServiceProcessInstaller control allows us to specify the account under which the LoaderService will run. This account has been set to System:\n\nProjectInstaller Screenshot\n\nNow, we are ready to add a Setup project. The primary output of the Setup project is set to the Toolkit project, which contains our LoaderService. This step is fairly trivial, and I will not be going through the details of adding it. However, I would like to comment that we need to hook up the ProjectInstaller class to this MSI. If we do not, then the contents of the Toolkit project will be deployed, but the LoaderService will not get registered as a Windows Service. To add a custom action, right-click on the Setup project and go to View > Custom Actions. From here, you can add a custom action. Specifying the custom action as the primary output from Toolkit is enough to hint to it that there is a custom installer, in our case ProjectInstaller, that needs to be run. Remember, ProjectInstaller is the installer class actually responsible for registering the service with Windows:\n\nLoader Service Setup\n\nNow, it's time to run the code and see the fruits of our labor!\n\nRunning the Code\nTo verify the code is working as expected, we will build the MSI and install it. When you install the MSI, you will notice a UAC prompt asking you to confirm the install. A good Marine friend of mine once told me the Marines have a saying, \"Once a Marine, always a Marine.\" In hacking and computer security, that would translate to, \"Once an Administrator, always an Administrator.\" This is the one and only time a User who installs your project will be presented with a UAC pop-up. Since most MSIs need Administrator privileges to install, this should come as no shock to the User.\n\nAfter installation, you will notice the service has been registered to start automatically (by the ProjectInstaller); however, this will only happen on the next reboot. You can also start it manually. This article assumes you have chosen to reboot. Notice, when the computer reboots, you are displayed with a command prompt that is running as an Administrator:\n\nCommand Prompt\n\nFrom here, you can type in regedit, gpedit.msc, or whatever command you like, and it will bypass the Vista UAC prompt. What\u2019s more is that the currently logged in User need not even be an Administrator to take advantage of this command prompt. The reason being is that the command prompt is running under the System account. This can be seen from the Task Manager in the screenshot below. Also take note of the Session ID:\n\nTask Manager : cmd\n\nBut, what about our LoaderService? Where is it, and under which Session is it running? Let\u2019s take another look at Task Manager to figure this out:\n\nTask Manager : LoaderService\n\nWe have just successfully bypassed Vista UAC and illustrated how to correctly spawn an interactive process from a Windows Service. But, there is more we can do!\n\nBeyond the Fundamentals\nThe topics discussed in this section are not contained in the downloadable code. The reason being to keep the example solution as simple as possible. The ideas below are meant to illustrate how the example code can be extended to support different types of scenarios.\n\nA Generic Solution\nMost Windows Services are started when the first User logs on to the computer, and they are launched in Session0. The way our code is currently written is such that only the first User who logs on to the computer will have the command prompt launched in their Session. The reason for this is that our LoaderService spawns the process from its OnStart function. The OnStart function executes only once, and that is when the service is first started. Since the first User to log on to the system has the net effect of starting all services in Session0, he is the User whose Session ID will be retrieved in the OnStart function when it calls StartProcessAndBypassUAC. The OnStart function in LoaderService.cs has been repeated below for clarity:\n\nC#\nCopy Code\nprotected override void OnStart(string[] args)\n{\n    // the name of the application to launch\n    String applicationName = \"cmd.exe\";\n\n    // launch the application\n    ApplicationLoader.PROCESS_INFORMATION procInfo;\n    ApplicationLoader.StartProcessAndBypassUAC(applicationName, out procInfo);\n}\nSo then, how can we configure our LoaderService to launch the command prompt for every User on the computer the first time that they log on? The solution is in the OnStart function: either wire-up a Timer, or spawn a Thread that runs in an infinite loop every so many seconds (Thread.Sleep(1000) can be used to control how often the Thread runs). We can use a List<int> object to keep track of all the Session IDs that we have already launched our process in. Every time our Thread executes, we check to see if the Session ID has changed. The currently active Session ID can be retrieved by calling WTSGetActiveConsoleSessionId(). If the Session ID has changed, we check to see if we have launched our process into that Session. If we have not, then we call StartProcessAndBypassUAC and add the Session ID to the List<int> object.\n\nLaunching an Application On Demand\nIt may be the case that you do not want your application to start immediately when Users log on to the computer. You may have an application that should only be loaded when the User chooses to run it. In addition, you may want this application and functionality to be available to every User on the system. The question then is how can our LoaderService accommodate this while still bypassing Vista UAC?\n\nBefore we begin this discussion, let\u2019s quickly talk about how UAC applies with regard to file and folder access. Vista supports the notion of Special Folders. There are several Special Folders in Vista, but the one we are going to focus on is the Documents folder. In .NET, you can query Special Folder locations by calling Environment.GetFolderPath(...).\n\nIf you spend enough time on your computer, you may have noticed that you can freely create, modify, and delete files located in your Documents folder without any interference from UAC. However, if you navigate to another User\u2019s Documents folder, you will be greeted with a UAC prompt asking for an Administrator\u2019s permission to touch the folder. You may have also noticed that there is a public Documents folder shared and accessible by all Users. In Vista, the path to this Special Folder is C:\\Users\\Public\\Documents. Any User on the system can freely create, modify, and delete files located here without any interference from UAC.\n\nNow, we are able to craft a solution! We can modify the OnStart function of our LoaderService to start an instance of the FileSystemWatcher class, and configure it to watch for changes to the public Documents folder, which all Users have access to. We will have to create a new Console application to communicate with the LoaderService via text files (do not confuse this Console application with the Console Session). The code for the Console application will look like the following:\n\nC#\nCopy Code\nstatic void Main(string[] args)\n{\n    string filename = @\"C:\\Users\\Public\\Documents\\appToLoad.txt\";\n    using (StreamWriter sw = new StreamWriter(filename, false))\n    {\n        sw.WriteLine(\"SessionID=\" + WTSGetActiveConsoleSessionId());\n        sw.WriteLine(\"ApplicationToLoad=cmd.exe\");\n        sw.Close();\n    }\n}\nUpon seeing the file appToLoad.txt, the LoaderService would parse the file and launch a command prompt in the currently active Session. At this point, we have successfully illustrated how to use a user application to communicate with a service, and how to have it launch an application for us with full Administrator rights and also bypassing Vista UAC in the process.", "document_id": 610}]}, {"paragraphs": [{"qas": [{"question": "What does Harvest Tool do?", "id": 1235, "answers": [{"answer_id": 1228, "document_id": 811, "question_id": 1235, "text": "Generates WiX authoring from various input formats.\n\nEvery time heat is run it regenerates the output file and any changes are lost.", "answer_start": 66, "answer_category": null}], "is_impossible": false}, {"question": "How to generate the wxs source files ?", "id": 1236, "answers": [{"answer_id": 1229, "document_id": 811, "question_id": 1236, "text": "heat file \".\\My Files\\File.dll\" -ag -template:fragment -out file.wxs", "answer_start": 4024, "answer_category": null}], "is_impossible": false}, {"question": "How to generate the wxs source directory?", "id": 1237, "answers": [{"answer_id": 1230, "document_id": 811, "question_id": 1237, "text": "heat dir \".\\My Files\" -gg -sfrag -template:fragment -out directory.wxs", "answer_start": 3785, "answer_category": null}], "is_impossible": false}, {"question": "How to harverst performance counters?", "id": 1238, "answers": [{"answer_id": 1231, "document_id": 811, "question_id": 1238, "text": "heat perf \"My Category\" -out perf.wxs", "answer_start": 4984, "answer_category": null}], "is_impossible": false}, {"question": "suppress VB6 COM component?", "id": 1239, "answers": [{"answer_id": 1232, "document_id": 811, "question_id": 1239, "text": "heat file \".\\My Files\\VB6File.dll\" -ag -template:fragment -svb6 -out vb6file.wxs", "answer_start": 4720, "answer_category": null}], "is_impossible": false}], "context": "WIX TOOLSET\nNEWS\nBUGS\nDOCUMENTATION\nDOWNLOADS\nHarvest Tool (Heat)\nGenerates WiX authoring from various input formats.\n\nEvery time heat is run it regenerates the output file and any changes are lost.\n\nUsage Information\nheat.exe [-?] harvestType <harvester arguments> -out sourceFile.wxs\nHeat supports the harvesting types:\n\nHarvest Type\n\nMeaning\n\ndir \n\nHarvest a directory.\n\nfile\n\nHarvest a file.\n\nproject\n\nHarvest outputs of a Visual Studio project.\n\nwebsite\n\nHarvest an IIS web site.\n\nperf\n\nHarvest performance counters from a category.\n\nreg\n\nHarvest registy information from a reg file..\n\nHeat supports the following command line parameters:\n\nSwitch\n\nMeaning\n\n-ag\n\nAuto generate component guids at compile time, e.g. set Guid=\"*\".\n\n-cg <ComponentGroupName>\n\nComponent group name (cannot contain spaces e.g -cg MyComponentGroup).\n\n-configuration\n\nConfiguration to set when harvesting the project.\n\n-directoryid\n\nOverridden directory id for generated directory elements.\n\n-dr <DirectoryName>\n\nDirectory reference to root directories (cannot contains spaces e.g. -dr MyAppDirRef).\n\n-ext <extension>\n\nExtension assembly or \"class, assembly\".\n\n-generate\n\nSpecify what elements to generate, one of: components, container, payloadgroup, layout (default is components).\n\n-gg\n\nGenerate guids now. All components are given a guid when heat is run.\n\n-g1\n\nGenerate component guids without curly braces.\n\n-ke\n\nKeep empty directories.\n\n-nologo\n\nSkip printing heat logo information.\n\n-out\n\nSpecify output file (default: write to current directory).\n\n-platform\n\nPlatform to set when harvesting the project.\n\n-pog:<group>\n\nSpecify output group of Visual Studio project, one of: Binaries, Symbols, Documents, Satellites, Sources, Content.\n\nBinaries - primary output of the project, e.g. the assembly exe or dll.\nSymbols - debug symbol files, e.g. pdb.\nDocuments - documentation files.\nSatellites - the localized resource assemblies.\nSources - source files.\nContent - content files.\nThis option may be repeated for multiple output groups; e.g. -pog:Binaries -pog:Content.\n\n-projectname\n\nOverridden project name to use in variables.\n\n-scom\n\nSuppress COM elements.\n\n-sfrag\n\nSuppress generation of fragments for directories and components.\n\n-srd\n\nSuppress harvesting the root directory as an element.\n\n-sreg\n\nSuppress registry harvesting.\n\n-suid\n\nSuppress unique identifiers for files, components, & directories.\n\n-svb6\n\nSuppress VB6 COM registration entries. When registering a COM component created in VB6 it adds registry entries that are part of the VB6 runtime component. This flag is recommend for VB6 components to avoid breaking the VB6 runtime on uninstall.\n\nThe following values are excluded:\n- CLSID\\{D5DE8D20-5BB8-11D1-A1E3-00A0C90F2731}\n- Typelib\\{EA544A21-C82D-11D1-A3E4-00A0C90AEA82}\n- Typelib\\{000204EF-0000-0000-C000-000000000046}\n- Any Interfaces that reference these two type libraries\n\n-sw<N>\n\nSuppress all warnings or a specific message ID, e.g. -sw1011 -sw1012.\n\n-swall\n\nSuppress all warnings (deprecated).\n\n-t <xsl>\n\nTransform harvested output with XSL file.\n\n-indent <n>\n\nIndentation multiple (overrides default of 4).\n\n-template <template>\n\nUse template, one of: fragment, module, product.\nDefault: fragment.\n\n-v\n\nVerbose output.\n\n-var <VariableName>\n\nSubstitute File/@Source=\"SourceDir\" with a preprocessor or a wix variable (e.g. -var var.MySource will become File/@Source=\"$(var.MySource)\\myfile.txt\" and -var wix.MySource will become File/@Source=\"!(wix.MySource)\\myfile.txt\".\n\n-wixvar\n\nGenerate binder variables instead of preprocessor variables.\n\n-wx[N]\n\nTreat all warnings or a specific message ID as an error. e.g. -wx1011 -wx1012.\n\n-wxall\n\nTreat all warnings as errors (deprecated).\n\n-? | -help\n\n Display heat help information.\n\nCommand line examples\nHarvest a directory\nheat dir \".\\My Files\" -gg -sfrag -template:fragment -out directory.wxs\nThis will harvest the sub folder \"My Files\" as a single fragment to the file directory.wxs. It will generate guids for all the files as they are found.\n\nHarvest a file\nheat file \".\\My Files\\File.dll\" -ag -template:fragment -out file.wxs\nThis will harvest the file \"File.dll\" as a single fragment to the file file.wxs. The component guid will be set to \"*\".\n\nHarvest a Visual Studio project\nheat project \"MyProject.csproj\" -pog:Binaries -ag -template:fragment -out project.wxs\nThis will harvest the binary output files from the Visual Studio project \"MyProject.csproj\" as a single fragment to the file project.wxs. The component guid will be set to \"*\".\n\nHarvest a Website\nheat website \"Default Web Site\" -template:fragment -out website.wxs\nThis will harvest the website \"Default Web Site\" as a single fragment to the file website.wxs.\n\nHarvest a VB6 COM component\nheat file \".\\My Files\\VB6File.dll\" -ag -template:fragment -svb6 -out vb6file.wxs\nThis will harvest the VB6 COM component \"VB6File.dll\"as a single fragment to the file vb6file.wxs and suppress the VB6 runtime specific registy entries.\n\nHarvest performance counters\nheat perf \"My Category\" -out perf.wxs\nThis will harvest all the performance counters from the category \"My Category\".\n\nHarvest a registry file\nheat reg registry.reg -out reg.wxs\nThis will harvest all the registry information from the file registry.reg. The registry file can be either a standard \"Windows Registry Editor Version 5.00\" reigstry file or a legacy Win9.x/NT4 (REGEDIT4) reigstry file.\n\nHosting sponsored by FireGiant.\n\nFireGiant\n  \nMember of the .NET Foundation.\n\n.NET Foundation", "document_id": 811}]}, {"paragraphs": [{"qas": [{"question": "Using npm to install or update required packages just like bundler for rubygems", "id": 1893, "answers": [{"answer_id": 1880, "document_id": 1464, "question_id": 1893, "text": "You should do following steps:\n1.Put a package.json file in the root of your project\n2.List your deps in that file\n{ \"name\" : \"my-project\"\n, \"version\" : \"1.0.0\"\n, \"dependencies\" : { \"express\" : \"1.0.0\" } }\n3.npm install Since you're calling this with no args, and not in global mode, it'll just install all your deps locally.\n4.require(\"express\") and be happy.", "answer_start": 587, "answer_category": null}], "is_impossible": false}], "context": "I love Bundler, it's great at dependency management. I love npm, installing node packages is easy! I have a nodejs app and would love to be able to specify my apps dependencies and easily install / update them wherever I deploy my app. This isn't a library I'm releasing, it's a full fledged web-app.\nI'm aware of the npm bundle command, but that just seems to simply override the directory where packages are installed.\nInstalls rails v3.0.3 and any other required gems on the host machine only if it doesn't already exist\n> bundle install\nHow can I achieve something similar with npm?\nYou should do following steps:\n1.Put a package.json file in the root of your project\n2.List your deps in that file\n{ \"name\" : \"my-project\"\n, \"version\" : \"1.0.0\"\n, \"dependencies\" : { \"express\" : \"1.0.0\" } }\n3.npm install Since you're calling this with no args, and not in global mode, it'll just install all your deps locally.\n4.require(\"express\") and be happy.\n", "document_id": 1464}]}, {"paragraphs": [{"qas": [{"question": "chrome extension id how to find it on Linux?", "id": 1936, "answers": [{"answer_id": 1923, "document_id": 1514, "question_id": 1936, "text": "\nFor Linux: $HOME/.config/google-chrome/Default/Preferences (json file) under [\"", "answer_start": 1734, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nHow can I find out what the chrome extension id is for an extension?\n    \n\nUse the chrome.runtime.id property from the chrome.runtime API.\n    \n\nYou get an extension ID when you upload your extension to Google Web Store. Ie. Adblock has URL https://chrome.google.com/webstore/detail/cfhdojbkjhnklbpkdaibdccddilifddb and the last part of this URL is its extension ID cfhdojbkjhnklbpkdaibdccddilifddb.\n\n\n\nIf you wish to read installed extension IDs from your extension, check out the managment module. chrome.management.getAll allows to fetch information about all installed extensions.\n    \n\nIf you just need to do it one-off, navigate to chrome://extensions.  Enable Developer Mode at upper right. The ID will be shown in the box for each extension.\n\nOr, if you're working on developing a userscript or extension, purposefully throw an error.  Look in the javascript console, and the ID will be there, on the right side of the console, in the line describing the error.\n\nLastly, you can look in your chrome extensions directory; it stores extensions in directories named by the ID.  This is the worst choice, as you'd have extension IDs, and have to read each manifest.json to figure out which ID was the right one.  But if you just installed something, you can also just sort by creation date, and the newest extension directory will be the ID you want.\n    \n\nAs Alex Gray points out in a comment above, \"all of the corresponding IDs are actually on the extensions page within the browser\".\n\nHowever, you must click the Developer Mode checkbox at top of Extensions page to see them.\n    \n\nExtension IDs can be found in:\n\nchrome://extensions (Chrome_Hotdog &gt;&gt; More_tools &gt;&gt; Extensions) Developer mode.\n\nFor Linux: $HOME/.config/google-chrome/Default/Preferences (json file) under [\"extensions\"].\n    \n\nAll extension ID are listed here:\nchrome://system\n\n\n    \n\nThis is the way to create desktop app for gmail with brave browser.\n\"C:\\Program Files\\BraveSoftware\\Brave-Browser\\Application\\chrome_proxy.exe\"  --profile-directory=Default --app-id=kmhopmchchfpfdcdjodmpfaaphdclmlj\n    ", "document_id": 1514}]}, {"paragraphs": [{"qas": [{"question": "mysql 5 7 19 not installing giving same error", "id": 1484, "answers": [{"answer_id": 1473, "document_id": 1060, "question_id": 1484, "text": " I have downloaded MySql Version 5.6 instead of 5.7, then its gets installed like a charm.", "answer_start": 2872, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI have been trying to install MYSQL 5.7.19 but it gives me the following error while installing the \"MYSQL SERVER 5.7.19\" although the rest of the things are installed perfectly.\n\n1: Action 14:02:10: INSTALL. \n1: 1: MySQL Server 5.7 2: {EC09D203-422B-4C9F-B623-230EF57EE709} \n1: Action 14:02:10: FindRelatedProducts. Searching for related applications\n1: Action 14:02:10: AppSearch. Searching for installed applications\n1: Action 14:02:10: LaunchConditions. Evaluating launch conditions\n1: This application requires Visual Studio 2013 Redistributable. Please install the Redistributable then run this installer again.\n1: 1: MySQL Server 5.7 2: {EC09D203-422B-4C9F-B623-230EF57EE709} 3: 3 \n1: The action 'Install' for product 'MySQL Server 5.7.19' failed.\n\n\nAfter this, I installed the \"VISUAL C++ Redistributable 2013\" as I could not find the \"Visual Studio 2013 Redistributable\". First thing I want to ask is are these two the same? If not then where to find the \"Visual Studio 2013 Redistributable\". After installing the \"VISUAL C++ Redistributable 2013\", I still get the same error. Another thing is, if I don't install 'MySQL Server 5.7.19' and let it fail, will my MySQL not work properly?\n    \n\nThis happened to me as well today actually.\nWhat you need to do is to install the Visual C++ Redistributable Packages for Visual Studio 2013, doesn't matter if you have installed the ones of 2015 for some reason. Install both x86 and x64 versions. Here's the  link to the installers. Hope it works for you, it did for me.\n    \n\nSimple workaround (there's a bug in msi install sequence file):\n\n\ndownload and install the server through the web installer and let it failed\nopen C:\\ProgramData\\MySQL\\MySQL Installer for Windows\\Product Cache\\mysql-5.7.20-winx64.exe or download msi installer from CDN (you can take the correct url from the installer logs)\ninstall the mysql-5.7.20-winx64.exe but exclude \"Server Data Files\"\nre-run the web installer and click \"Quick Action / Reconfigure\" and configure you mysql instance\nadd the required packages\n\n    \n\nI had the same problem with MySQL Server 5.7.19 x64.\n\nI installed Visual Studio 2013 Redistributable x64 but it does not works, so you have to install Update for Visual C++ 2013 and Visual C++ Redistributable Package\nwich can be found here https://support.microsoft.com/en-us/help/3179560/update-for-visual-c-2013-and-visual-c-redistributable-package.\n\nWith this solution you can install v5.7 instead of v5.6.\n\nCredits goes for Jafeth Carrillo Salas, https://forums.mysql.com/profile.php?169,11205311\n    \n\nThis seems duplicate of MySql 5.7 installer fails to detect VS 2013 redistributable.\nI had a same issue, I tried multiple times, even removed multiple version of the Visual Studio Redistribute 2013, but MySql V5.7 didn't get install. I tried both x86 and 64 bit, but result was same.\n\nSolution: I have downloaded MySql Version 5.6 instead of 5.7, then its gets installed like a charm.\n\nNote:\n\nSeems like there must be some issue with the MySql V 5.7 installer.\n    \n\nSimple workaround (there's a bug in msi install sequence file):\n\ndownload and install the server through the web installer and let it failed\nopen C:\\ProgramData\\MySQL\\MySQL Installer for Windows\\Product Cache\\mysql-5.7.20-winx64.exe or download msi installer from CDN (you can take the correct url from the installer logs)\ninstall the mysql-5.7.20-winx64.exe but exclude \"Server Data Files\"\nre-run the web installer and click \"Quick Action / Reconfigure\" and configure you mysql instance\nadd the required packages\n\nThanks. It works for me.!!!\n    \n\nYou can find answers to your question at the install instructions at mysql.com. To install the server you need the Visual Studio C++ 2013 Redistributable, which you can find at Microsoft.\n\nIf your MySQL will work properly, if you don't install the server depends on your needs. If you just connect to another server somewhere else I don't see any problems.\n    \n\nRemove mysql server and install Visual C++ Redistributable Packages 32bit and 64bit both version after that resolve the issue.\n\nhttps://www.microsoft.com/en-in/download/confirmation.aspx?id=40784\nvcredist_x64.exe\nvcredist_x86.exe\n    ", "document_id": 1060}]}, {"paragraphs": [{"qas": [{"question": "setup failed visual studio 2010 pro installation", "id": 1514, "answers": [{"answer_id": 1503, "document_id": 1090, "question_id": 1514, "text": " and install\n    \n\nYou might have missing ", "answer_start": 6079, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nTrying to install VS2010Pro on Windows 7 (64bit) and i also checked all system requirements for the same.\n\nERROR:\n\n\n\nVIEW ERROR LOG:\n\n[05/15/12,10:48:22] Microsoft Visual Studio 2010 64bit Prerequisites (x64): [2] Error: Installation failed for component Microsoft Visual Studio 2010 64bit Prerequisites (x64). MSI returned error code 1603\n[05/15/12,10:48:22] VS70pgui: [2] DepCheck indicates Microsoft Visual Studio 2010 64bit Prerequisites (x64) is not installed.\n[05/15/12,10:48:22] VS70pgui: [2] DepCheck indicates Microsoft Visual F# 2.0 Runtime was not attempted to be installed.\n[05/15/12,10:48:22] VS70pgui: [2] DepCheck indicates TFS Object Model (x64) was not attempted to be installed.\n[05/15/12,10:48:22] VS70pgui: [2] DepCheck indicates .NET Framework 4 Multi-Targeting Pack was not attempted to be installed.\n[05/15/12,10:48:22] VS70pgui: [2] DepCheck indicates Microsoft Visual Studio 2010 Professional - ENU was not attempted to be installed.\n[05/15/12,10:48:22] VS70pgui: [2] DepCheck indicates Microsoft Web Deployment Tool (x64) was not attempted to be installed.\n[05/15/12,10:48:22] VS70pgui: [2] DepCheck indicates Microsoft ASP.NET MVC 2 - Visual Studio 2010 Tools was not attempted to be installed.\n[05/15/12,10:48:22] VS70pgui: [2] DepCheck indicates Microsoft ASP.NET MVC 2 was not attempted to be installed.\n[05/15/12,10:48:22] VS70pgui: [2] DepCheck indicates Microsoft Silverlight 3 SDK was not attempted to be installed.\n[05/15/12,10:48:22] VS70pgui: [2] DepCheck indicates Microsoft Visual Studio 2010 Tools for Office Runtime (x64) was not attempted to be installed.\n[05/15/12,10:48:22] VS70pgui: [2] DepCheck indicates Microsoft Office Developer Tools (x64) was not attempted to be installed.\n[05/15/12,10:48:22] VS70pgui: [2] DepCheck indicates Dotfuscator Software Services - Community Edition was not attempted to be installed.\n[05/15/12,10:48:22] VS70pgui: [2] DepCheck indicates Crystal Reports templates for Visual Studio 2010 was not attempted to be installed.\n[05/15/12,10:48:22] VS70pgui: [2] DepCheck indicates Microsoft SQL Server Compact 3.5 SP2 (x64) ENU was not attempted to be installed.\n[05/15/12,10:48:22] VS70pgui: [2] DepCheck indicates Visual Studio 2010 Tools for SQL Server Compact 3.5 SP2 ENU was not attempted to be installed.\n[05/15/12,10:48:22] VS70pgui: [2] DepCheck indicates Microsoft Sync Framework Runtime v1.0 (x64) ENU was not attempted to be installed.\n[05/15/12,10:48:22] VS70pgui: [2] DepCheck indicates Microsoft Sync Services for ADO.NET v2.0 (x64) ENU was not attempted to be installed.\n[05/15/12,10:48:22] VS70pgui: [2] DepCheck indicates Microsoft Sync Framework Services v1.0 (x64) ENU was not attempted to be installed.\n[05/15/12,10:48:22] VS70pgui: [2] DepCheck indicates Microsoft Sync Framework SDK v1.0 (x64) ENU was not attempted to be installed.\n[05/15/12,10:48:22] VS70pgui: [2] DepCheck indicates VC 10.0 Designtime (x64) was not attempted to be installed.\n[05/15/12,10:48:22] VS70pgui: [2] DepCheck indicates Microsoft SQL Publishing Wizard 1.4 was not attempted to be installed.\n[05/15/12,10:48:22] VS70pgui: [2] DepCheck indicates Microsoft SQL Server System CLR Types was not attempted to be installed.\n[05/15/12,10:48:22] VS70pgui: [2] DepCheck indicates Microsoft SQL Server 2008 R2 Management Objects was not attempted to be installed.\n[05/15/12,10:48:22] VS70pgui: [2] DepCheck indicates Microsoft SQL Server 2008 R2 Management Objects (x64) was not attempted to be installed.\n[05/15/12,10:48:22] VS70pgui: [2] DepCheck indicates Microsoft SQL Server 2008 Express Service Pack 1 (x64) was not attempted to be installed.\n[05/15/12,10:48:22] VS70pgui: [2] DepCheck indicates Microsoft SQL Server 2008 R2 Data-Tier Application Framework was not attempted to be installed.\n[05/15/12,10:48:22] VS70pgui: [2] DepCheck indicates Microsoft SQL Server 2008 R2 Data-Tier Application Project was not attempted to be installed.\n\n\nREF LINKS\n1. MSDN\n2. BLOG (tried)\n\nhave also asked question in MSDN forum.\n\nDo i have any better solutions to trouble shoot this problem other than re installing the OS ?\n    \n\nAfter 2days of work around, Solution is finally here.....Phew!!!!!\n\nHere we go, \n\nAs error indicated VS2010 was getting stuck while installing Microsoft Visual Studio 2010 64bit Prerequisites (x64)\n\nError: Installation failed for component Microsoft Visual Studio 2010 64bit Prerequisites (x64). MSI returned error code 1603\n\n\nSo i searched for the corresponding directory and ran .exe manually\n\nVS2010Pro\\adminpoint\\WCU\\64bitPrereq\\x64\n\n\nran VS_Prerequisites_x64_enu &amp; got one more ERROR\n\nerror 25541 failed to open xml file C:\\Windows\\Microsoft.NET\\Framework64\\v2.0.50727\\CONFIG\\machine.config, system error -2147024786\n\n\nLooked for machine.config in indicated path, but I couldn't find machine.config in \n\nC:\\Windows\\Microsoft.NET\\Framework64\\v2.0.50727\\CONFIG\n\n\nSo i copied machine.config.default on to desktop and renamed it to machine.config and added it to  C:\\Windows\\Microsoft.NET\\Framework64\\v2.0.50727\\CONFIG then ran VS_Prerequisites_x64_enu &amp; then VS2010 installer.\n\nVS_Prerequisites_x64_enu ---&gt; (finally) VS2010 installer\n\n\nNote: by doing this i tricked installer &amp; installation went smooth. \n\nMore details HERE\n    \n\nError code 1603: solution can be found at this msdn link http://social.msdn.microsoft.com/Forums/eu/vssetup/thread/3f07772c-4187-4be7-a390-d775f8eb5a50\n\nIf its not solved please try to post it here in VisualStudio Setup &amp; Installation forum. Hope it helps\n    \n\nHad the same issue today and fixed it by reinstalling the .NET frameworks 4.5 and 4.0\n\nhttp://www.microsoft.com/en-us/download/details.aspx?id=30653\n\nhttp://www.microsoft.com/en-us/download/details.aspx?id=17851\n    \n\nthe problem is with the software you downloaded , its not downloaded completely, thats why that error , \ni also experienced the same , but i resolved it by redownloading the software,\n\nafter downloading check that the size of downloaded and the mentioned size in the website are same , if not its not downloaded completely, again download it and install\n    \n\nYou might have missing machine.config file.\n\nLocation: C:\\Windows\\Microsoft.NET\\Framework\\v2.0.50727\\CONFIG\n    \n\nI had a similar problem installing visual studio 2010 professional on a Windows 8.1 machine. \nI tried to install Visual Studio from a local directory, but this failed.\nAlthough all the files from the DVD looked exact the same as on the local directory I recopied all the files again and this time I had success with installing.\n\nI also followed these steps found here to install without typing in the Product Key and your custom settings. And adding SP1 with the installation as well. \nIt didn't solved the problem, but I installed the software with unattended setup.\n    \n\nI had a corrupted image that I downloaded and got the same result. My advice is to run the installer of a component individually; corrupted setups should fail immediately.\n    ", "document_id": 1090}]}, {"paragraphs": [{"qas": [{"question": ".NET Framework 4 installation in silent mode", "id": 1729, "answers": [{"answer_id": 1717, "document_id": 1302, "question_id": 1729, "text": "You should do the trick.\ndotnetfx40x86.exe /q", "answer_start": 227, "answer_category": null}], "is_impossible": false}], "context": "Can anyone explain how to make silent (without any user interface) install of .NET 4? It looks like .NET installer ignores any switches from this article and show interface always. .NET Framework 4 installer is packed by NSIS.\nYou should do the trick.\ndotnetfx40x86.exe /q\n", "document_id": 1302}]}, {"paragraphs": [{"qas": [{"question": "Can't install RMagick 2.13.1. Can't find MagickWand.h.", "id": 1539, "answers": [{"answer_id": 1528, "document_id": 1116, "question_id": 1539, "text": "If you're on Ubuntu, installing this package is what fixed it for me:\nsudo apt-get install libmagickwand-dev", "answer_start": 425, "answer_category": null}], "is_impossible": false}], "context": "When I try do install rmagick I get the following error message:\nCan't install RMagick 2.13.1. Can't find MagickWand.h.\n*** extconf.rb failed ***\nCould not create Makefile due to some reason, probably lack of\nnecessary libraries and/or headers.  Check the mkmf.log file for more\ndetails.  You may need configuration options.\nI'm on Mac OSX 10.6.8, ruby 1.9.2p290, rvm 1.10.2.\nCan anyone help me please to solve this problem.\nIf you're on Ubuntu, installing this package is what fixed it for me:\nsudo apt-get install libmagickwand-dev\n", "document_id": 1116}]}, {"paragraphs": [{"qas": [{"question": "How can I find out if I have Xcode commandline tools installed?", "id": 1560, "answers": [{"answer_id": 1549, "document_id": 1137, "question_id": 1560, "text": "This will give you the xcode version, run it via Terminal command\n/usr/bin/xcodebuild -version", "answer_start": 284, "answer_category": null}], "is_impossible": false}], "context": "I need to use gdb.\nSo I figure I need Xcode commandline tools.\nXcode is not currently available from the Software Update server\nmy current problem exactly. Comment on that question says \"you can get this error if you have them already\"\nBut how do I check whether I have them already?\nThis will give you the xcode version, run it via Terminal command\n/usr/bin/xcodebuild -version\n", "document_id": 1137}]}, {"paragraphs": [{"qas": [{"question": "Create a directly-executable cross-platform GUI app using Python", "id": 1648, "answers": [{"answer_id": 1636, "document_id": 1222, "question_id": 1648, "text": "An alternative tool to py2exe is bbfreeze which generates executables for windows and linux. It's newer than py2exe and handles eggs quite well. I've found it magically works better without configuration for a wide variety of applications", "answer_start": 344, "answer_category": null}], "is_impossible": false}], "context": "Python works on multiple platforms and can be used for desktop and web applications, thus I conclude that there is some way to compile it into an executable for Mac, Windows and Linux.\nThe problem being I have no idea where to start or how to write a GUI with it, can anybody shed some light on this and point me in the right direction please?\nAn alternative tool to py2exe is bbfreeze which generates executables for windows and linux. It's newer than py2exe and handles eggs quite well. I've found it magically works better without configuration for a wide variety of applications.\n", "document_id": 1222}]}, {"paragraphs": [{"qas": [{"question": "How to update an assembly for a running c# process (AKA hot deploy)?", "id": 1250, "answers": [{"answer_id": 1243, "document_id": 822, "question_id": 1250, "text": "It is easy to do. You can rename the file, Windows has a lock on the handle, not the directory entry for the file. Now you can just copy the update without problems. All that's left to do is to get rid of the renamed file after your app starts up again. If necessary.", "answer_start": 368, "answer_category": null}], "is_impossible": false}], "context": "I have a .exe assembly and a set of referenced assemblies that I would like to be able to deploy updated code for. I don't need to actually change the running code, but the next time I launch the executable I would want it to pick up the updated code. Currently when I attempt to copy over the running file I get an error about the .exe being used by another process.\nIt is easy to do. You can rename the file, Windows has a lock on the handle, not the directory entry for the file. Now you can just copy the update without problems. All that's left to do is to get rid of the renamed file after your app starts up again. If necessary.\n", "document_id": 822}]}, {"paragraphs": [{"qas": [{"question": "How do I prevent Maven from downloading artifacts every time?", "id": 1850, "answers": [{"answer_id": 1836, "document_id": 1421, "question_id": 1850, "text": "You may control the update frequency by configuring repositories in the $USER_HOME/.m2/settings.xml file. Specifically, change the updatePolicy to a value that results in less frequent updates.\nThis Stackoverflow answer has more detail.", "answer_start": 406, "answer_category": null}], "is_impossible": false}], "context": "I\u2019m using Maven 3.1.1. In one of my projects, I reference another one of my projects.\nThe above is dependent on a couple other of my projects. However, when I run \u201cmvn clean install,\u201d Maven attempts to download these artifacts instead of just using what\u2019s in my local repository. How do I get Maven to only download things if they do not exist in my local repository? Here\u2019s the output of what I\u2019m seeing.\nYou may control the update frequency by configuring repositories in the $USER_HOME/.m2/settings.xml file. Specifically, change the updatePolicy to a value that results in less frequent updates.\nThis Stackoverflow answer has more detail.\n", "document_id": 1421}]}, {"paragraphs": [{"qas": [{"question": "How to display ClickOnce Version number on Windows Forms", "id": 1700, "answers": [{"answer_id": 1688, "document_id": 1273, "question_id": 1700, "text": " I would simply make the assembly version of the main assembly the same as the CLickOnce version every time you put out a new version. Then when it runs as a non-clickonce application, just use Reflection to pick up the assembly version", "answer_start": 628, "answer_category": null}], "is_impossible": false}], "context": "I have a windows forms application that is deployed to two different locations.\n\u2022\tIntranet - ClickOnce\n\u2022\tInternet - Installed on a citrix farm through Windows installer\nI display ClickOnce version number for click-once deployed versionApplicationDeployment.IsNetworkDeployed.\nif (ApplicationDeployment.IsNetworkDeployed)\n        return ApplicationDeployment.CurrentDeployment.CurrentVersion;\nBut for the non-click application, I am not sure how to retrieve clickonce version unless I hardcode the version number in assembly info.\nIs there an automatic way of retrieve ClickOnce version number for non-clickonce deployed version? I would simply make the assembly version of the main assembly the same as the CLickOnce version every time you put out a new version. Then when it runs as a non-clickonce application, just use Reflection to pick up the assembly version.\n", "document_id": 1273}]}, {"paragraphs": [{"qas": [{"question": "How do I detect what .NET Framework versions and service packs are installed?", "id": 1545, "answers": [{"answer_id": 1534, "document_id": 1122, "question_id": 1545, "text": "There is an official Microsoft answer to this question you can see the following knowledge base article:\nArticle ID: 318785 - Last Review: November 7, 2008 - Revision: 20.1 How to determine which versions of the .NET Framework are installed and whether service packs have been applied", "answer_start": 319, "answer_category": null}], "is_impossible": false}], "context": "A similar question was asked here, but it was specific to .NET 3.5. Specifically, I'm looking for the following:\nWhat is the correct way to determine which .NET Framework versions and service packs are installed?\nIs there a list of registry keys that can be used?\nAre there any dependencies between Framework versions?\nThere is an official Microsoft answer to this question you can see the following knowledge base article:\nArticle ID: 318785 - Last Review: November 7, 2008 - Revision: 20.1 How to determine which versions of the .NET Framework are installed and whether service packs have been applied\n", "document_id": 1122}]}, {"paragraphs": [{"qas": [{"question": "How to make a .NET Windows Service start right after the installation?", "id": 864, "answers": [{"answer_id": 859, "document_id": 544, "question_id": 864, "text": " This class is called by the install utility when installing a service application.\nOverride the OnAfterInstall event to use a ServiceController class to start the service.", "answer_start": 168, "answer_category": null}], "is_impossible": false}], "context": "ServiceInstaller Class\nDefinition\nNamespace:\nSystem.ServiceProcess\nAssembly:\nSystem.ServiceProcess.dll\nInstalls a class that extends ServiceBase to implement a service. This class is called by the install utility when installing a service application.\nOverride the OnAfterInstall event to use a ServiceController class to start the service.\nC#\n\nCopy\npublic class ServiceInstaller : System.Configuration.Install.ComponentInstaller\nInheritance\nObject\nMarshalByRefObject\nComponent\nInstaller\nComponentInstaller\nServiceInstaller\nExamples\nThe following example creates a project installer, called MyProjectInstaller, which inherits from Installer. It is assumed there is a service executable that contains two services, \"Hello-World Service 1\" and \"Hello-World Service 2\". Within the constructor for MyProjectInstaller (which would be called by the install utility), ServiceInstaller objects are created for each of these services, and a ServiceProcessInstaller is created for the executable. For the install utility to recognize MyProjectInstaller as a valid installer, the RunInstallerAttribute attribute is set to true.\n\nOptional properties are set on the process installer and the service installers before the installers are added to the Installers collection. When the install utility accesses MyProjectInstaller, the objects added to the Installers collection through a call to InstallerCollection.Add will be installed in turn. During the process, the installer maintains state information indicating which objects have been installed, so each can be backed out in turn, if an installation failure occurs.\n\nNormally, you would not create an instance of your project installer class explicitly. You would create it and add the RunInstallerAttribute attribute to the syntax, but it is the install utility that actually calls, and therefore instantiates, the class.\n\nC#\n\nCopy\nusing System;\nusing System.Collections;\nusing System.Configuration.Install;\nusing System.ServiceProcess;\nusing System.ComponentModel;\n\n[RunInstaller(true)]\npublic class MyProjectInstaller : Installer\n{\n    private ServiceInstaller serviceInstaller1;\n    private ServiceInstaller serviceInstaller2;\n    private ServiceProcessInstaller processInstaller;\n\n    public MyProjectInstaller()\n    {\n        // Instantiate installers for process and services.\n        processInstaller = new ServiceProcessInstaller();\n        serviceInstaller1 = new ServiceInstaller();\n        serviceInstaller2 = new ServiceInstaller();\n\n        // The services run under the system account.\n        processInstaller.Account = ServiceAccount.LocalSystem;\n\n        // The services are started manually.\n        serviceInstaller1.StartType = ServiceStartMode.Manual;\n        serviceInstaller2.StartType = ServiceStartMode.Manual;\n\n        // ServiceName must equal those on ServiceBase derived classes.\n        serviceInstaller1.ServiceName = \"Hello-World Service 1\";\n        serviceInstaller2.ServiceName = \"Hello-World Service 2\";\n\n        // Add installers to collection. Order is not important.\n        Installers.Add(serviceInstaller1);\n        Installers.Add(serviceInstaller2);\n        Installers.Add(processInstaller);\n    }\n\n    public static void Main()\n    {\n        Console.WriteLine(\"Usage: InstallUtil.exe [<service>.exe]\");\n    }\n}\nRemarks\nThe ServiceInstaller does work specific to the service with which it is associated. It is used by the installation utility to write registry values associated with the service to a subkey within the HKEY_LOCAL_MACHINE\\System\\CurrentControlSet\\Services registry key. The service is identified by its ServiceName within this subkey. The subkey also includes the name of the executable or .dll to which the service belongs.\n\nTo install a service, create a project installer class that inherits from the Installer class, and set the RunInstallerAttribute attribute on the class to true. Within your project, create one ServiceProcessInstaller instance per service application, and one ServiceInstaller instance for each service in the application. Within your project installer class constructor, set the installation properties for the service using the ServiceProcessInstaller and ServiceInstaller instances, and add the instances to the Installers collection.\n\n Note\n\nIt is recommended that you use the constructor for adding installer instances; however, if you need to add to the Installers collection in the Install method, be sure to perform the same additions to the collection in the Uninstall method.\n\nFor all classes deriving from the Installer class, the state of the Installers collection must be the same in the Install and Uninstall methods. However, you can avoid the maintenance of the collection across the Install and Uninstall methods if you add installer instances to the Installers collection in your custom installer class constructor.When the install utility is called, it looks for the RunInstallerAttribute attribute. If the attribute is true, the utility installs all the services that were added to the Installers collection that were associated with your project installer. If RunInstallerAttribute is false or does not exist, the install utility ignores the project installer.\n\nThe ServiceProcessInstaller associated with your project installation class installs information common to all ServiceInstaller instances in the project. If this service has anything that separates it from the other services in the installation project, that service-specific information is installed by this method.\n\n Note\n\nIt is crucial that the ServiceName be identical to the ServiceBase.ServiceName of the class you derived from ServiceBase. Normally, the value of the ServiceBase.ServiceName property for the service is set within the Main() function of the service application's executable. The Service Control Manager uses the ServiceInstaller.ServiceName property to locate the service within this executable.\n\nYou can modify other properties on the ServiceInstaller either before or after adding it to the Installers collection of your project installer. For example, a service's StartType may be set to start the service automatically at reboot or require a user to start the service manually.\n\nNormally, you will not call the methods on ServiceInstaller within your code; they are generally called only by the install utility. The install utility automatically calls the ServiceProcessInstaller.Install and ServiceInstaller.Install methods during the installation process. It backs out failures, if necessary, by calling Rollback (or ServiceInstaller.Rollback) on all previously installed components.\n\nThe installation utility calls Uninstall to remove the object.\n\nAn application's install routine maintains information automatically about the components already installed, using the project installer's Installer.Context. This state information is continuously updated as the ServiceProcessInstaller instance, and each ServiceInstaller instance is installed by the utility. It is usually unnecessary for your code to modify state information explicitly.\n\nWhen the installation is performed, it automatically creates an EventLogInstaller to install the event log source associated with the ServiceBase derived class. The Log property for this source is set by the ServiceInstaller constructor to the computer's Application log. When you set the ServiceName of the ServiceInstaller (which should be identical to the ServiceBase.ServiceName of the service), the Source is automatically set to the same value. In an installation failure, the source's installation is rolled-back along with previously installed services.\n\nThe Uninstall method tries to stop the service if it is running. Whether this succeeds or not, Uninstall undoes the changes made by Install. If a new source was created for event logging, the source is deleted.", "document_id": 544}]}, {"paragraphs": [{"qas": [{"question": "how to compile ruby with rvm on a low memory system", "id": 1997, "answers": [{"answer_id": 1983, "document_id": 1583, "question_id": 1997, "text": "Creating a 512MB swap-file solved the problem. Here are the steps:\n\nsudo mkdir -p /var/cache/swap/\nsudo dd if=/dev/zero of=/var/cache/swap/swap0 bs=1M count=512\nsudo chmod 0600 /var/cache/swap/swap0\nsudo mkswap /var/cache/swap/swap0 \nsudo swapon /var/cache/swap/swap0", "answer_start": 1672, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nrvm install 1.9.3\n\n\nleads to the error in the make.log:\n\n...\ncompiling ./enc/trans/emoji_sjis_docomo.c\ncompiling ./enc/trans/emoji_sjis_kddi.c\ngcc: internal compiler error: Killed (program cc1)\ngcc: internal compiler error: Killed (program cc1)\ngcc: internal compiler error: Killed (program cc1)\nPlease submit a full bug report,\nwith preprocessed source if appropriate.\n...\n\n\ndmesg shows\n\n[180031.341709] send sigkill to 3705 (cc1), adj 0, size 3394\n\n\nfree shows at some point running configure process:\n\n             total       used       free     shared    buffers     cached\nMem:        241668     238676       2992          0         92       2020\n-/+ buffers/cache:     236564       5104\nSwap:       262140     262140          0\n\n\nSo I assume that 256MB RAM and 256MB Swap is not enough to compile Ruby on it.\n\nI read that it should be possible using some parameters for gcc, see:\nhttp://hostingfu.com/article/compiling-with-gcc-on-low-memory-vps\n\nBut \n\n  rvm install 1.9.3 --with-CFLAGS=\"$CFLAGS --param ggc-min-expand=0 --param ggc-min-heapsize=8192\"\n\n\nDoes not work to give the flags to gcc, log is still the same for the flags:\n\ncommand(2): __rvm_make -j4\n        CC = gcc\n        LD = ld\n        LDSHARED = gcc -shared\n        CFLAGS = -O3 -ggdb -Wall -Wextra -Wno-unused-parameter -Wno-parentheses -Wno-long-long -Wno-missing-fiel$\n        XCFLAGS = -include ruby/config.h -include ruby/missing.h -fvisibility=hidden -DRUBY_EXPORT\n        CPPFLAGS =   -I. -I.ext/include/x86_64-linux -I./include -I.\n        DLDFLAGS = -Wl,-soname,libruby.so.1.9\n        SOLIBS = -lpthread -lrt -ldl -lcrypt -lm\n\n\nHow to compile ruby on that machine?\n    \n\nCreating a 512MB swap-file solved the problem. Here are the steps:\n\nsudo mkdir -p /var/cache/swap/\nsudo dd if=/dev/zero of=/var/cache/swap/swap0 bs=1M count=512\nsudo chmod 0600 /var/cache/swap/swap0\nsudo mkswap /var/cache/swap/swap0 \nsudo swapon /var/cache/swap/swap0\n\n\nThe swap file is not used after a restart. It can be integrated in /etc/fstab to use it after restart:\n\n /var/cache/swap/swap0    none    swap    sw      0 0\n\n\nThe above steps to create a swap-file I found here (in German): http://wiki.ubuntuusers.de/Swap#Swap-als-Datei - licence for the above content: http://creativecommons.org/licenses/by-nc-sa/2.0/de/deed.en (Attribution-NonCommercial-ShareAlike 2.0 Germany (CC BY-NC-SA 2.0 DE))\n    ", "document_id": 1583}]}, {"paragraphs": [{"qas": [{"question": "System Python conflict between Anaconda and existing Python installation", "id": 1797, "answers": [{"answer_id": 1783, "document_id": 1369, "question_id": 1797, "text": "The solution is simply to uninstall python (for example, run the original python installer and select the uninstall option). The python key in the windows registry will be removed (which is what unregister means in this context).", "answer_start": 823, "answer_category": null}], "is_impossible": false}], "context": "I've been going with a basic Python3.4 install that I've been installing many modules into for over the past month but have reached a point where pip is coming up short and I'm going to just install the full Anaconda on my system to go deeper into bokeh-server stuff.\nDidn't find much documentation on this subject, and I'm not really sure how to \"unregister\" that installation of Python apart from uninstalling it entirely from Windows which I imagine would accomplish such a thing. Is this basically telling me to check how my Python Launcher for Windows is setup after the Anaconda installation? I'm completely unfamiliar with this notion of python system registration? Is that just a round about warning about which python version takes precedence on the system path, or which installation holds the file associations?\nThe solution is simply to uninstall python (for example, run the original python installer and select the uninstall option). The python key in the windows registry will be removed (which is what unregister means in this context).\n", "document_id": 1369}]}, {"paragraphs": [{"qas": [{"question": "WARNING The requested profile \"projectname\" could not be activated because it does not exist", "id": 1118, "answers": [{"answer_id": 1111, "document_id": 695, "question_id": 1118, "text": "This happens when you have the following in your settings.xml (in your .m2 directory), and no profile with the id projectname.\n<activeProfiles>\n  <activeProfile>projectname</activeProfile>\n</activeProfiles>", "answer_start": 301, "answer_category": null}], "is_impossible": false}], "context": "I get this WARNING when I try to build my maven project. I have searched google but with no luck. This is really annoying since I wanna release my project but it wont work with this warning (I think). The build is successful, but when I try to deploy the war file it doesen't work (no error message).\nThis happens when you have the following in your settings.xml (in your .m2 directory), and no profile with the id projectname.\n<activeProfiles>\n  <activeProfile>projectname</activeProfile>\n</activeProfiles>\n", "document_id": 695}]}, {"paragraphs": [{"qas": [{"question": "Is there a source code level debugger with breakpoints, single-stepping, etc.?", "id": 246, "answers": [{"answer_id": 254, "document_id": 122, "question_id": 246, "text": "The pdb module is a simple but adequate console-mode debugger for Python. It is part of the standard Python library, and is documented in the Library Reference Manual. ", "answer_start": 4008, "answer_category": null}], "is_impossible": false}, {"question": "What is PythonWin?", "id": 247, "answers": [{"answer_id": 255, "document_id": 122, "question_id": 247, "text": "PythonWin is a Python IDE that includes a GUI debugger based on pdb. The PythonWin debugger colors breakpoints and has quite a few cool features such as debugging non-PythonWin programs. PythonWin is available as part of pywin32 project and as a part of the ActivePython distribution.\n", "answer_start": 4427, "answer_category": null}], "is_impossible": false}, {"question": "What is eric?", "id": 249, "answers": [{"answer_id": 257, "document_id": 122, "question_id": 249, "text": "Eric is an IDE built on PyQt and the Scintilla editing component.", "answer_start": 4713, "answer_category": null}], "is_impossible": false}, {"question": "What is trepan3k?", "id": 250, "answers": [{"answer_id": 258, "document_id": 122, "question_id": 250, "text": "trepan3k is a gdb-like debugger.", "answer_start": 4780, "answer_category": null}], "is_impossible": false}, {"question": "What is visual studio code?", "id": 251, "answers": [{"answer_id": 259, "document_id": 122, "question_id": 251, "text": "Visual Studio Code is an IDE with debugging tools that integrates with version-control software.", "answer_start": 4814, "answer_category": null}], "is_impossible": false}, {"question": "Do python ide contain a debug ide?", "id": 252, "answers": [{"answer_id": 260, "document_id": 122, "question_id": 252, "text": "\nThere are a number of commercial Python IDEs that include graphical debuggers. They include:\n\nWing IDE\n\nKomodo IDE\n\nPyCharm\n", "answer_start": 4911, "answer_category": null}], "is_impossible": false}, {"question": "Are there tools to help find bugs or perform static analysis?", "id": 253, "answers": [{"answer_id": 261, "document_id": 122, "question_id": 253, "text": "Pylint and Pyflakes do basic checking that will help you catch bugs sooner.", "answer_start": 5105, "answer_category": null}], "is_impossible": false}, {"question": "How can I create a stand-alone binary from a Python script?", "id": 254, "answers": [{"answer_id": 264, "document_id": 122, "question_id": 254, "text": "You don\u2019t need the ability to compile Python to C code if all you want is a stand-alone program that users can download and run without having to install the Python distribution first. There are a number of tools that determine the set of modules required by a program and bind these modules together with a Python binary to produce a single executable.\n\nOne is to use the freeze tool, which is included in the Python source tree as Tools/freeze. It converts Python byte code to C arrays; a C compiler you can embed all your modules into a new program, which is then linked with the standard Python modules.", "answer_start": 5339, "answer_category": null}], "is_impossible": false}, {"question": "help with the creation of console and GUI executables", "id": 256, "answers": [{"answer_id": 265, "document_id": 122, "question_id": 256, "text": "The following packages can help with the creation of console and GUI executables:\n\nNuitka (Cross-platform)\n\nPyInstaller (Cross-platform)\n\nPyOxidizer (Cross-platform)\n\ncx_Freeze (Cross-platform)\n\npy2app (macOS only)\n\npy2exe (Windows only)", "answer_start": 6579, "answer_category": null}], "is_impossible": false}, {"question": "Are there coding standards or a style guide for Python programs?", "id": 257, "answers": [{"answer_id": 266, "document_id": 122, "question_id": 257, "text": "Yes. The coding style required for standard library modules is documented as PEP 8.", "answer_start": 6883, "answer_category": null}], "is_impossible": false}, {"question": "Why am I getting an UnboundLocalError when the variable has a value?", "id": 258, "answers": [{"answer_id": 273, "document_id": 122, "question_id": 258, "text": "The canonical way to share information across modules within a single program is to create a special module (often called config or cfg). Just import the config module in all modules of your application; the module then becomes available as a global name. Because there is only one instance of each module, any changes made to the module object get reflected everywhere. ", "answer_start": 10905, "answer_category": null}], "is_impossible": false}, {"question": "How do I share global variables across modules?", "id": 262, "answers": [{"answer_id": 271, "document_id": 122, "question_id": 262, "text": "The canonical way to share information across modules within a single program is to create a special module (often called config or cfg). Just import the config module in all modules of your application; the module then becomes available as a global name. Because there is only one instance of each module, any changes made to the module object get reflected everywhere. For example:\n", "answer_start": 10905, "answer_category": null}], "is_impossible": false}, {"question": "How do I share global variables across modules in python?", "id": 264, "answers": [{"answer_id": 274, "document_id": 122, "question_id": 264, "text": "The canonical way to share information across modules within a single program is to create a special module (often called config or cfg). Just import the config module in all modules of your application; the module then becomes available as a global name. Because there is only one instance of each module, any changes made to the module object get reflected everywhere.", "answer_start": 10905, "answer_category": null}], "is_impossible": false}, {"question": "What are the \u201cbest practices\u201d for using import in a module?", "id": 265, "answers": [{"answer_id": 275, "document_id": 122, "question_id": 265, "text": "In general, don\u2019t use from modulename import *. Doing so clutters the importer\u2019s namespace, and makes it much harder for linters to detect undefined names.", "answer_start": 11618, "answer_category": null}], "is_impossible": false}, {"question": "How do I convert a string to a number?", "id": 266, "answers": [{"answer_id": 276, "document_id": 122, "question_id": 266, "text": "For integers, use the built-in int() type constructor, e.g. int('144') == 144. Similarly, float() converts to floating-point, e.g. float('144') == 144.0.\n\nBy default, these interpret the number as decimal, so that int('0144') == 144 holds true, and int('0x144') raises ValueError. int(string, base) takes the base to convert from as a second optional argument, so int( '0x144', 16) == 324. If the base is specified as 0, the number is interpreted using Python\u2019s rules: a leading \u20180o\u2019 indicates octal, and \u20180x\u2019 indicates a hex number.\n\nDo not use the built-in function eval() if all you need is to convert strings to numbers. eval() will be significantly slower and it presents a security risk: someone could pass you a Python expression that might have unwanted side effects. For example, someone could pass __import__('os').system(\"rm -rf $HOME\") which would erase your home directory.", "answer_start": 30085, "answer_category": null}], "is_impossible": false}, {"question": "How do I convert a number to a string?", "id": 267, "answers": [{"answer_id": 277, "document_id": 122, "question_id": 267, "text": "To convert, e.g., the number 144 to the string \u2018144\u2019, use the built-in type constructor str(). If you want a hexadecimal or octal representation, use the built-in functions hex() or oct(). For fancy formatting, see the Formatted string literals and Format String Syntax sections, e.g. \"{:04d}\".format(144) yields '0144' and \"{:.3f}\".format(1.0/3.0) yields '0.333'.", "answer_start": 31208, "answer_category": null}], "is_impossible": false}, {"question": "How do I modify a string in place?", "id": 268, "answers": [{"answer_id": 278, "document_id": 122, "question_id": 268, "text": "You can\u2019t, because strings are immutable. In most situations, you should simply construct a new string from the various parts you want to assemble it from. However, if you need an object with the ability to modify in-place unicode data, try using an io.StringIO object or the array module:", "answer_start": 31609, "answer_category": null}], "is_impossible": false}, {"question": "How do I use strings to call functions/methods?", "id": 269, "answers": [{"answer_id": 279, "document_id": 122, "question_id": 269, "text": "The best is to use a dictionary that maps strings to functions. The primary advantage of this technique is that the strings do not need to match the names of the functions. This is also the primary technique used to emulate a case construct:", "answer_start": 32332, "answer_category": null}], "is_impossible": false}, {"question": "Is there an equivalent to Perl\u2019s chomp() for removing trailing newlines from strings?", "id": 270, "answers": [{"answer_id": 280, "document_id": 122, "question_id": 270, "text": "You can use S.rstrip(\"\\r\\n\") to remove all occurrences of any line terminator from the end of the string S without removing other trailing whitespace.", "answer_start": 33314, "answer_category": null}], "is_impossible": false}, {"question": "Is there a scanf() or sscanf() equivalent?", "id": 271, "answers": [{"answer_id": 281, "document_id": 122, "question_id": 271, "text": "Not as such.\n\nFor simple input parsing, the easiest approach is usually to split the line into whitespace-delimited words using the split() method of string objects and then convert decimal strings to numeric values using int() or float(). split() supports an optional \u201csep\u201d parameter which is useful if the line uses something other than whitespace as a separator.", "answer_start": 33876, "answer_category": null}], "is_impossible": false}, {"question": "My program is too slow. How do I speed it up?", "id": 272, "answers": [{"answer_id": 282, "document_id": 122, "question_id": 272, "text": "That\u2019s a tough one, in general. First, here are a list of things to remember before diving further:\n\nPerformance characteristics vary across Python implementations. This FAQ focuses on CPython.\n\nBehaviour can vary across operating systems, especially when talking about I/O or multi-threading.\n\nYou should always find the hot spots in your program before attempting to optimize any code (see the profile module).\n\nWriting benchmark scripts will allow you to iterate quickly when searching for improvements (see the timeit module).", "answer_start": 34517, "answer_category": null}], "is_impossible": false}, {"question": "What is the most efficient way to concatenate many strings together?", "id": 273, "answers": [{"answer_id": 283, "document_id": 122, "question_id": 273, "text": "str and bytes objects are immutable, therefore concatenating many strings together is inefficient as each concatenation creates a new object. In the general case, the total runtime cost is quadratic in the total string length.", "answer_start": 37030, "answer_category": null}], "is_impossible": false}, {"question": "What\u2019s a negative index?", "id": 279, "answers": [{"answer_id": 289, "document_id": 122, "question_id": 279, "text": "Python sequences are indexed with positive numbers and negative numbers. For positive numbers 0 is the first index 1 is the second index and so forth. For negative indices -1 is the last index and -2 is the penultimate (next to last) index and so forth. Think of seq[-n] as the same as seq[len(seq)-n].", "answer_start": 38466, "answer_category": null}], "is_impossible": false}, {"question": "How do I iterate over a sequence in reverse order?", "id": 280, "answers": [{"answer_id": 290, "document_id": 122, "question_id": 280, "text": "Use the reversed() built-in function:\n\nfor x in reversed(sequence):\n    ...  # do something with x ...\nThis won\u2019t touch your original sequence, but build a new copy with reversed order to iterate over.", "answer_start": 39005, "answer_category": null}], "is_impossible": false}, {"question": "How do you remove duplicates from a list?", "id": 282, "answers": [{"answer_id": 292, "document_id": 122, "question_id": 282, "text": "See the Python Cookbook for a long discussion of many ways to do this:\n\nhttps://code.activestate.com/recipes/52560/", "answer_start": 39250, "answer_category": null}], "is_impossible": false}, {"question": "How do you make an array in Python?", "id": 283, "answers": [{"answer_id": 293, "document_id": 122, "question_id": 283, "text": "The array module also provides methods for creating arrays of fixed types with compact representations, but they are slower to index than lists. Also note that NumPy and other third party packages define array-like structures with various characteristics as well.\n", "answer_start": 40610, "answer_category": null}], "is_impossible": false}, {"question": "How do I apply a method to a sequence of objects?", "id": 284, "answers": [{"answer_id": 294, "document_id": 122, "question_id": 284, "text": "Use a list comprehension:\n\nresult = [obj.method() for obj in mylist]", "answer_start": 42371, "answer_category": null}], "is_impossible": false}, {"question": "Why does a_tuple[i] += [\u2018item\u2019] raise an exception when the addition works?", "id": 285, "answers": [{"answer_id": 295, "document_id": 122, "question_id": 285, "text": "This is because of a combination of the fact that augmented assignment operators are assignment operators, and the difference between mutable and immutable objects in Python.", "answer_start": 42516, "answer_category": null}], "is_impossible": false}, {"question": "What is a class?", "id": 286, "answers": [{"answer_id": 296, "document_id": 122, "question_id": 286, "text": "A class is the particular object type created by executing a class statement. Class objects are used as templates to create instance objects, which embody both the data (attributes) and code (methods) specific to a datatype.", "answer_start": 46173, "answer_category": null}], "is_impossible": false}, {"question": "What is a method?", "id": 287, "answers": [{"answer_id": 297, "document_id": 122, "question_id": 287, "text": "A method is a function on some object x that you normally call as x.name(arguments...). ", "answer_start": 46836, "answer_category": null}], "is_impossible": false}, {"question": "What is self?", "id": 289, "answers": [{"answer_id": 299, "document_id": 122, "question_id": 289, "text": "Self is merely a conventional name for the first argument of a method. A method defined as meth(self, a, b, c) should be called as x.meth(a, b, c) for some instance x of the class in which the definition occurs; the called method will think it is called as meth(x, a, b, c).", "answer_start": 47075, "answer_category": null}], "is_impossible": false}, {"question": "How do I check if an object is an instance of a given class or of a subclass of it?", "id": 290, "answers": [{"answer_id": 300, "document_id": 122, "question_id": 290, "text": "Use the built-in function isinstance(obj, cls). You can check if an object is an instance of any of a number of classes by providing a tuple instead of a single class, e.g. isinstance(obj, (class1, class2, ...)), and can also check whether an object is one of Python\u2019s built-in types, e.g. isinstance(obj, str) or isinstance(obj, (int, float, complex)).", "answer_start": 47514, "answer_category": null}], "is_impossible": false}, {"question": "How can a subclass control what data is stored in an immutable instance?", "id": 303, "answers": [{"answer_id": 313, "document_id": 122, "question_id": 303, "text": "When subclassing an immutable type, override the __new__() method instead of the __init__() method. The latter only runs after an instance is created, which is too late to alter data in an immutable instance.", "answer_start": 60239, "answer_category": null}], "is_impossible": false}, {"question": "What is delegation?", "id": 291, "answers": [{"answer_id": 301, "document_id": 122, "question_id": 291, "text": "Delegation is an object oriented technique (also called a design pattern). Let\u2019s say you have an object x and want to change the behaviour of just one of its methods. You can create a new class that provides a new implementation of the method you\u2019re interested in changing and delegates all other methods to the corresponding method of x.", "answer_start": 49383, "answer_category": null}], "is_impossible": false}, {"question": "How can I organize my code to make it easier to change the base class?", "id": 292, "answers": [{"answer_id": 302, "document_id": 122, "question_id": 292, "text": "You could assign the base class to an alias and derive from the alias. Then all you have to change is the value assigned to the alias.", "answer_start": 51516, "answer_category": null}], "is_impossible": false}, {"question": "How do I create static class data and static class methods?", "id": 293, "answers": [{"answer_id": 303, "document_id": 122, "question_id": 293, "text": "Both static data and static methods (in the sense of C++ or Java) are supported in Python.\n", "answer_start": 51938, "answer_category": null}], "is_impossible": false}, {"question": "How do I get a list of all instances of a given class in python?", "id": 296, "answers": [{"answer_id": 306, "document_id": 122, "question_id": 296, "text": "Python does not keep track of all instances of a class (or of a built-in type). You can program the class\u2019s constructor to keep track of all instances by keeping a list of weak references to each instance.", "answer_start": 56441, "answer_category": null}], "is_impossible": false}, {"question": "Why does the result of id() appear to be not unique in python?", "id": 297, "answers": [{"answer_id": 307, "document_id": 122, "question_id": 297, "text": "You can program the class\u2019s constructor to keep track of all instances by keeping a list of weak references to each instance.", "answer_start": 56521, "answer_category": null}], "is_impossible": false}, {"question": "How do I get a list of all instances of a given class?", "id": 298, "answers": [{"answer_id": 308, "document_id": 122, "question_id": 298, "text": "You can program the class\u2019s constructor to keep track of all instances by keeping a list of weak references to each instance.", "answer_start": 56521, "answer_category": null}], "is_impossible": false}, {"question": "Why does the result of id() appear to be not unique?", "id": 300, "answers": [{"answer_id": 310, "document_id": 122, "question_id": 300, "text": "The id() builtin returns an integer that is guaranteed to be unique during the lifetime of the object.", "answer_start": 56701, "answer_category": null}], "is_impossible": false}, {"question": "When can I rely on identity tests with the is operator?", "id": 301, "answers": [{"answer_id": 311, "document_id": 122, "question_id": 301, "text": "The is operator tests for object identity. The test a is b is equivalent to id(a) == id(b).", "answer_start": 57436, "answer_category": null}], "is_impossible": false}, {"question": "How do I cache method calls?", "id": 305, "answers": [{"answer_id": 315, "document_id": 122, "question_id": 305, "text": "The two principal tools for caching methods are functools.cached_property() and functools.lru_cache(). The former stores results at the instance level and the latter at the class level.", "answer_start": 61427, "answer_category": null}], "is_impossible": false}, {"question": "How do I create a .pyc file?", "id": 306, "answers": [{"answer_id": 316, "document_id": 122, "question_id": 306, "text": "If you need to create a .pyc file for foo \u2013 that is, to create a .pyc file for a module that is not imported \u2013 you can, using the py_compile and compileall modules.", "answer_start": 65549, "answer_category": null}], "is_impossible": false}, {"question": "How do I find the current module name?", "id": 307, "answers": [{"answer_id": 317, "document_id": 122, "question_id": 307, "text": "A module can find out its own module name by looking at the predefined global variable __name__. ", "answer_start": 66369, "answer_category": null}], "is_impossible": false}, {"question": "When I edit an imported module and reimport it, the changes don\u2019t show up. Why does this happen?", "id": 310, "answers": [{"answer_id": 320, "document_id": 122, "question_id": 310, "text": "For reasons of efficiency as well as consistency, Python only reads the module file on the first time a module is imported.", "answer_start": 68794, "answer_category": null}], "is_impossible": false}, {"question": "__import__(\u2018x.y.z\u2019) returns \u2039module \u2018x\u2019>; how do I get z?", "id": 311, "answers": [{"answer_id": 321, "document_id": 122, "question_id": 311, "text": "Consider using the convenience function import_module() from importlib instead:", "answer_start": 68579, "answer_category": null}], "is_impossible": false}], "context": "Programming FAQ\nContents\n\nProgramming FAQ\n\nGeneral Questions\n\nIs there a source code level debugger with breakpoints, single-stepping, etc.?\n\nAre there tools to help find bugs or perform static analysis?\n\nHow can I create a stand-alone binary from a Python script?\n\nAre there coding standards or a style guide for Python programs?\n\nCore Language\n\nWhy am I getting an UnboundLocalError when the variable has a value?\n\nWhat are the rules for local and global variables in Python?\n\nWhy do lambdas defined in a loop with different values all return the same result?\n\nHow do I share global variables across modules?\n\nWhat are the \u201cbest practices\u201d for using import in a module?\n\nWhy are default values shared between objects?\n\nHow can I pass optional or keyword parameters from one function to another?\n\nWhat is the difference between arguments and parameters?\n\nWhy did changing list \u2018y\u2019 also change list \u2018x\u2019?\n\nHow do I write a function with output parameters (call by reference)?\n\nHow do you make a higher order function in Python?\n\nHow do I copy an object in Python?\n\nHow can I find the methods or attributes of an object?\n\nHow can my code discover the name of an object?\n\nWhat\u2019s up with the comma operator\u2019s precedence?\n\nIs there an equivalent of C\u2019s \u201c?:\u201d ternary operator?\n\nIs it possible to write obfuscated one-liners in Python?\n\nWhat does the slash(/) in the parameter list of a function mean?\n\nNumbers and strings\n\nHow do I specify hexadecimal and octal integers?\n\nWhy does -22 // 10 return -3?\n\nHow do I get int literal attribute instead of SyntaxError?\n\nHow do I convert a string to a number?\n\nHow do I convert a number to a string?\n\nHow do I modify a string in place?\n\nHow do I use strings to call functions/methods?\n\nIs there an equivalent to Perl\u2019s chomp() for removing trailing newlines from strings?\n\nIs there a scanf() or sscanf() equivalent?\n\nWhat does \u2018UnicodeDecodeError\u2019 or \u2018UnicodeEncodeError\u2019 error mean?\n\nPerformance\n\nMy program is too slow. How do I speed it up?\n\nWhat is the most efficient way to concatenate many strings together?\n\nSequences (Tuples/Lists)\n\nHow do I convert between tuples and lists?\n\nWhat\u2019s a negative index?\n\nHow do I iterate over a sequence in reverse order?\n\nHow do you remove duplicates from a list?\n\nHow do you remove multiple items from a list\n\nHow do you make an array in Python?\n\nHow do I create a multidimensional list?\n\nHow do I apply a method to a sequence of objects?\n\nWhy does a_tuple[i] += [\u2018item\u2019] raise an exception when the addition works?\n\nI want to do a complicated sort: can you do a Schwartzian Transform in Python?\n\nHow can I sort one list by values from another list?\n\nObjects\n\nWhat is a class?\n\nWhat is a method?\n\nWhat is self?\n\nHow do I check if an object is an instance of a given class or of a subclass of it?\n\nWhat is delegation?\n\nHow do I call a method defined in a base class from a derived class that extends it?\n\nHow can I organize my code to make it easier to change the base class?\n\nHow do I create static class data and static class methods?\n\nHow can I overload constructors (or methods) in Python?\n\nI try to use __spam and I get an error about _SomeClassName__spam.\n\nMy class defines __del__ but it is not called when I delete the object.\n\nHow do I get a list of all instances of a given class?\n\nWhy does the result of id() appear to be not unique?\n\nWhen can I rely on identity tests with the is operator?\n\nHow can a subclass control what data is stored in an immutable instance?\n\nHow do I cache method calls?\n\nModules\n\nHow do I create a .pyc file?\n\nHow do I find the current module name?\n\nHow can I have modules that mutually import each other?\n\n__import__(\u2018x.y.z\u2019) returns <module \u2018x\u2019>; how do I get z?\n\nWhen I edit an imported module and reimport it, the changes don\u2019t show up. Why does this happen?\n\nGeneral Questions\nIs there a source code level debugger with breakpoints, single-stepping, etc.?\nYes.\n\nSeveral debuggers for Python are described below, and the built-in function breakpoint() allows you to drop into any of them.\n\nThe pdb module is a simple but adequate console-mode debugger for Python. It is part of the standard Python library, and is documented in the Library Reference Manual. You can also write your own debugger by using the code for pdb as an example.\n\nThe IDLE interactive development environment, which is part of the standard Python distribution (normally available as Tools/scripts/idle), includes a graphical debugger.\n\nPythonWin is a Python IDE that includes a GUI debugger based on pdb. The PythonWin debugger colors breakpoints and has quite a few cool features such as debugging non-PythonWin programs. PythonWin is available as part of pywin32 project and as a part of the ActivePython distribution.\n\nEric is an IDE built on PyQt and the Scintilla editing component.\n\ntrepan3k is a gdb-like debugger.\n\nVisual Studio Code is an IDE with debugging tools that integrates with version-control software.\n\nThere are a number of commercial Python IDEs that include graphical debuggers. They include:\n\nWing IDE\n\nKomodo IDE\n\nPyCharm\n\nAre there tools to help find bugs or perform static analysis?\nYes.\n\nPylint and Pyflakes do basic checking that will help you catch bugs sooner.\n\nStatic type checkers such as Mypy, Pyre, and Pytype can check type hints in Python source code.\n\nHow can I create a stand-alone binary from a Python script?\nYou don\u2019t need the ability to compile Python to C code if all you want is a stand-alone program that users can download and run without having to install the Python distribution first. There are a number of tools that determine the set of modules required by a program and bind these modules together with a Python binary to produce a single executable.\n\nOne is to use the freeze tool, which is included in the Python source tree as Tools/freeze. It converts Python byte code to C arrays; a C compiler you can embed all your modules into a new program, which is then linked with the standard Python modules.\n\nIt works by scanning your source recursively for import statements (in both forms) and looking for the modules in the standard Python path as well as in the source directory (for built-in modules). It then turns the bytecode for modules written in Python into C code (array initializers that can be turned into code objects using the marshal module) and creates a custom-made config file that only contains those built-in modules which are actually used in the program. It then compiles the generated C code and links it with the rest of the Python interpreter to form a self-contained binary which acts exactly like your script.\n\nThe following packages can help with the creation of console and GUI executables:\n\nNuitka (Cross-platform)\n\nPyInstaller (Cross-platform)\n\nPyOxidizer (Cross-platform)\n\ncx_Freeze (Cross-platform)\n\npy2app (macOS only)\n\npy2exe (Windows only)\n\nAre there coding standards or a style guide for Python programs?\nYes. The coding style required for standard library modules is documented as PEP 8.\n\nCore Language\nWhy am I getting an UnboundLocalError when the variable has a value?\nIt can be a surprise to get the UnboundLocalError in previously working code when it is modified by adding an assignment statement somewhere in the body of a function.\n\nThis code:\n\n>>>\nx = 10\ndef bar():\n    print(x)\nbar()\n10\nworks, but this code:\n\n>>>\nx = 10\ndef foo():\n    print(x)\n    x += 1\nresults in an UnboundLocalError:\n\n>>>\nfoo()\nTraceback (most recent call last):\n  ...\nUnboundLocalError: local variable 'x' referenced before assignment\nThis is because when you make an assignment to a variable in a scope, that variable becomes local to that scope and shadows any similarly named variable in the outer scope. Since the last statement in foo assigns a new value to x, the compiler recognizes it as a local variable. Consequently when the earlier print(x) attempts to print the uninitialized local variable and an error results.\n\nIn the example above you can access the outer scope variable by declaring it global:\n\n>>>\nx = 10\ndef foobar():\n    global x\n    print(x)\n    x += 1\nfoobar()\n10\nThis explicit declaration is required in order to remind you that (unlike the superficially analogous situation with class and instance variables) you are actually modifying the value of the variable in the outer scope:\n\n>>>\nprint(x)\n11\nYou can do a similar thing in a nested scope using the nonlocal keyword:\n\n>>>\ndef foo():\n   x = 10\n   def bar():\n       nonlocal x\n       print(x)\n       x += 1\n   bar()\n   print(x)\nfoo()\n10\n11\nWhat are the rules for local and global variables in Python?\nIn Python, variables that are only referenced inside a function are implicitly global. If a variable is assigned a value anywhere within the function\u2019s body, it\u2019s assumed to be a local unless explicitly declared as global.\n\nThough a bit surprising at first, a moment\u2019s consideration explains this. On one hand, requiring global for assigned variables provides a bar against unintended side-effects. On the other hand, if global was required for all global references, you\u2019d be using global all the time. You\u2019d have to declare as global every reference to a built-in function or to a component of an imported module. This clutter would defeat the usefulness of the global declaration for identifying side-effects.\n\nWhy do lambdas defined in a loop with different values all return the same result?\nAssume you use a for loop to define a few different lambdas (or even plain functions), e.g.:\n\n>>>\n>>> squares = []\n>>> for x in range(5):\n...     squares.append(lambda: x**2)\nThis gives you a list that contains 5 lambdas that calculate x**2. You might expect that, when called, they would return, respectively, 0, 1, 4, 9, and 16. However, when you actually try you will see that they all return 16:\n\n>>>\n>>> squares[2]()\n16\n>>> squares[4]()\n16\nThis happens because x is not local to the lambdas, but is defined in the outer scope, and it is accessed when the lambda is called \u2014 not when it is defined. At the end of the loop, the value of x is 4, so all the functions now return 4**2, i.e. 16. You can also verify this by changing the value of x and see how the results of the lambdas change:\n\n>>>\n>>> x = 8\n>>> squares[2]()\n64\nIn order to avoid this, you need to save the values in variables local to the lambdas, so that they don\u2019t rely on the value of the global x:\n\n>>>\n>>> squares = []\n>>> for x in range(5):\n...     squares.append(lambda n=x: n**2)\nHere, n=x creates a new variable n local to the lambda and computed when the lambda is defined so that it has the same value that x had at that point in the loop. This means that the value of n will be 0 in the first lambda, 1 in the second, 2 in the third, and so on. Therefore each lambda will now return the correct result:\n\n>>>\n>>> squares[2]()\n4\n>>> squares[4]()\n16\nNote that this behaviour is not peculiar to lambdas, but applies to regular functions too.\n\nHow do I share global variables across modules?\nThe canonical way to share information across modules within a single program is to create a special module (often called config or cfg). Just import the config module in all modules of your application; the module then becomes available as a global name. Because there is only one instance of each module, any changes made to the module object get reflected everywhere. For example:\n\nconfig.py:\n\nx = 0   # Default value of the 'x' configuration setting\nmod.py:\n\nimport config\nconfig.x = 1\nmain.py:\n\nimport config\nimport mod\nprint(config.x)\nNote that using a module is also the basis for implementing the Singleton design pattern, for the same reason.\n\nWhat are the \u201cbest practices\u201d for using import in a module?\nIn general, don\u2019t use from modulename import *. Doing so clutters the importer\u2019s namespace, and makes it much harder for linters to detect undefined names.\n\nImport modules at the top of a file. Doing so makes it clear what other modules your code requires and avoids questions of whether the module name is in scope. Using one import per line makes it easy to add and delete module imports, but using multiple imports per line uses less screen space.\n\nIt\u2019s good practice if you import modules in the following order:\n\nstandard library modules \u2013 e.g. sys, os, getopt, re\n\nthird-party library modules (anything installed in Python\u2019s site-packages directory) \u2013 e.g. mx.DateTime, ZODB, PIL.Image, etc.\n\nlocally-developed modules\n\nIt is sometimes necessary to move imports to a function or class to avoid problems with circular imports. Gordon McMillan says:\n\nCircular imports are fine where both modules use the \u201cimport <module>\u201d form of import. They fail when the 2nd module wants to grab a name out of the first (\u201cfrom module import name\u201d) and the import is at the top level. That\u2019s because names in the 1st are not yet available, because the first module is busy importing the 2nd.\n\nIn this case, if the second module is only used in one function, then the import can easily be moved into that function. By the time the import is called, the first module will have finished initializing, and the second module can do its import.\n\nIt may also be necessary to move imports out of the top level of code if some of the modules are platform-specific. In that case, it may not even be possible to import all of the modules at the top of the file. In this case, importing the correct modules in the corresponding platform-specific code is a good option.\n\nOnly move imports into a local scope, such as inside a function definition, if it\u2019s necessary to solve a problem such as avoiding a circular import or are trying to reduce the initialization time of a module. This technique is especially helpful if many of the imports are unnecessary depending on how the program executes. You may also want to move imports into a function if the modules are only ever used in that function. Note that loading a module the first time may be expensive because of the one time initialization of the module, but loading a module multiple times is virtually free, costing only a couple of dictionary lookups. Even if the module name has gone out of scope, the module is probably available in sys.modules.\n\nWhy are default values shared between objects?\nThis type of bug commonly bites neophyte programmers. Consider this function:\n\ndef foo(mydict={}):  # Danger: shared reference to one dict for all calls\n    ... compute something ...\n    mydict[key] = value\n    return mydict\nThe first time you call this function, mydict contains a single item. The second time, mydict contains two items because when foo() begins executing, mydict starts out with an item already in it.\n\nIt is often expected that a function call creates new objects for default values. This is not what happens. Default values are created exactly once, when the function is defined. If that object is changed, like the dictionary in this example, subsequent calls to the function will refer to this changed object.\n\nBy definition, immutable objects such as numbers, strings, tuples, and None, are safe from change. Changes to mutable objects such as dictionaries, lists, and class instances can lead to confusion.\n\nBecause of this feature, it is good programming practice to not use mutable objects as default values. Instead, use None as the default value and inside the function, check if the parameter is None and create a new list/dictionary/whatever if it is. For example, don\u2019t write:\n\ndef foo(mydict={}):\n    ...\nbut:\n\ndef foo(mydict=None):\n    if mydict is None:\n        mydict = {}  # create a new dict for local namespace\nThis feature can be useful. When you have a function that\u2019s time-consuming to compute, a common technique is to cache the parameters and the resulting value of each call to the function, and return the cached value if the same value is requested again. This is called \u201cmemoizing\u201d, and can be implemented like this:\n\n# Callers can only provide two parameters and optionally pass _cache by keyword\ndef expensive(arg1, arg2, *, _cache={}):\n    if (arg1, arg2) in _cache:\n        return _cache[(arg1, arg2)]\n\n    # Calculate the value\n    result = ... expensive computation ...\n    _cache[(arg1, arg2)] = result           # Store result in the cache\n    return result\nYou could use a global variable containing a dictionary instead of the default value; it\u2019s a matter of taste.\n\nHow can I pass optional or keyword parameters from one function to another?\nCollect the arguments using the * and ** specifiers in the function\u2019s parameter list; this gives you the positional arguments as a tuple and the keyword arguments as a dictionary. You can then pass these arguments when calling another function by using * and **:\n\ndef f(x, *args, **kwargs):\n    ...\n    kwargs['width'] = '14.3c'\n    ...\n    g(x, *args, **kwargs)\nWhat is the difference between arguments and parameters?\nParameters are defined by the names that appear in a function definition, whereas arguments are the values actually passed to a function when calling it. Parameters define what types of arguments a function can accept. For example, given the function definition:\n\ndef func(foo, bar=None, **kwargs):\n    pass\nfoo, bar and kwargs are parameters of func. However, when calling func, for example:\n\nfunc(42, bar=314, extra=somevar)\nthe values 42, 314, and somevar are arguments.\n\nWhy did changing list \u2018y\u2019 also change list \u2018x\u2019?\nIf you wrote code like:\n\n>>>\n>>> x = []\n>>> y = x\n>>> y.append(10)\n>>> y\n[10]\n>>> x\n[10]\nyou might be wondering why appending an element to y changed x too.\n\nThere are two factors that produce this result:\n\nVariables are simply names that refer to objects. Doing y = x doesn\u2019t create a copy of the list \u2013 it creates a new variable y that refers to the same object x refers to. This means that there is only one object (the list), and both x and y refer to it.\n\nLists are mutable, which means that you can change their content.\n\nAfter the call to append(), the content of the mutable object has changed from [] to [10]. Since both the variables refer to the same object, using either name accesses the modified value [10].\n\nIf we instead assign an immutable object to x:\n\n>>>\n>>> x = 5  # ints are immutable\n>>> y = x\n>>> x = x + 1  # 5 can't be mutated, we are creating a new object here\n>>> x\n6\n>>> y\n5\nwe can see that in this case x and y are not equal anymore. This is because integers are immutable, and when we do x = x + 1 we are not mutating the int 5 by incrementing its value; instead, we are creating a new object (the int 6) and assigning it to x (that is, changing which object x refers to). After this assignment we have two objects (the ints 6 and 5) and two variables that refer to them (x now refers to 6 but y still refers to 5).\n\nSome operations (for example y.append(10) and y.sort()) mutate the object, whereas superficially similar operations (for example y = y + [10] and sorted(y)) create a new object. In general in Python (and in all cases in the standard library) a method that mutates an object will return None to help avoid getting the two types of operations confused. So if you mistakenly write y.sort() thinking it will give you a sorted copy of y, you\u2019ll instead end up with None, which will likely cause your program to generate an easily diagnosed error.\n\nHowever, there is one class of operations where the same operation sometimes has different behaviors with different types: the augmented assignment operators. For example, += mutates lists but not tuples or ints (a_list += [1, 2, 3] is equivalent to a_list.extend([1, 2, 3]) and mutates a_list, whereas some_tuple += (1, 2, 3) and some_int += 1 create new objects).\n\nIn other words:\n\nIf we have a mutable object (list, dict, set, etc.), we can use some specific operations to mutate it and all the variables that refer to it will see the change.\n\nIf we have an immutable object (str, int, tuple, etc.), all the variables that refer to it will always see the same value, but operations that transform that value into a new value always return a new object.\n\nIf you want to know if two variables refer to the same object or not, you can use the is operator, or the built-in function id().\n\nHow do I write a function with output parameters (call by reference)?\nRemember that arguments are passed by assignment in Python. Since assignment just creates references to objects, there\u2019s no alias between an argument name in the caller and callee, and so no call-by-reference per se. You can achieve the desired effect in a number of ways.\n\nBy returning a tuple of the results:\n\n>>>\n>>> def func1(a, b):\n...     a = 'new-value'        # a and b are local names\n...     b = b + 1              # assigned to new objects\n...     return a, b            # return new values\n...\n>>> x, y = 'old-value', 99\n>>> func1(x, y)\n('new-value', 100)\nThis is almost always the clearest solution.\n\nBy using global variables. This isn\u2019t thread-safe, and is not recommended.\n\nBy passing a mutable (changeable in-place) object:\n\n>>>\n>>> def func2(a):\n...     a[0] = 'new-value'     # 'a' references a mutable list\n...     a[1] = a[1] + 1        # changes a shared object\n...\n>>> args = ['old-value', 99]\n>>> func2(args)\n>>> args\n['new-value', 100]\nBy passing in a dictionary that gets mutated:\n\n>>>\n>>> def func3(args):\n...     args['a'] = 'new-value'     # args is a mutable dictionary\n...     args['b'] = args['b'] + 1   # change it in-place\n...\n>>> args = {'a': 'old-value', 'b': 99}\n>>> func3(args)\n>>> args\n{'a': 'new-value', 'b': 100}\nOr bundle up values in a class instance:\n\n>>>\n>>> class Namespace:\n...     def __init__(self, /, **args):\n...         for key, value in args.items():\n...             setattr(self, key, value)\n...\n>>> def func4(args):\n...     args.a = 'new-value'        # args is a mutable Namespace\n...     args.b = args.b + 1         # change object in-place\n...\n>>> args = Namespace(a='old-value', b=99)\n>>> func4(args)\n>>> vars(args)\n{'a': 'new-value', 'b': 100}\nThere\u2019s almost never a good reason to get this complicated.\n\nYour best choice is to return a tuple containing the multiple results.\n\nHow do you make a higher order function in Python?\nYou have two choices: you can use nested scopes or you can use callable objects. For example, suppose you wanted to define linear(a,b) which returns a function f(x) that computes the value a*x+b. Using nested scopes:\n\ndef linear(a, b):\n    def result(x):\n        return a * x + b\n    return result\nOr using a callable object:\n\nclass linear:\n\n    def __init__(self, a, b):\n        self.a, self.b = a, b\n\n    def __call__(self, x):\n        return self.a * x + self.b\nIn both cases,\n\ntaxes = linear(0.3, 2)\ngives a callable object where taxes(10e6) == 0.3 * 10e6 + 2.\n\nThe callable object approach has the disadvantage that it is a bit slower and results in slightly longer code. However, note that a collection of callables can share their signature via inheritance:\n\nclass exponential(linear):\n    # __init__ inherited\n    def __call__(self, x):\n        return self.a * (x ** self.b)\nObject can encapsulate state for several methods:\n\nclass counter:\n\n    value = 0\n\n    def set(self, x):\n        self.value = x\n\n    def up(self):\n        self.value = self.value + 1\n\n    def down(self):\n        self.value = self.value - 1\n\ncount = counter()\ninc, dec, reset = count.up, count.down, count.set\nHere inc(), dec() and reset() act like functions which share the same counting variable.\n\nHow do I copy an object in Python?\nIn general, try copy.copy() or copy.deepcopy() for the general case. Not all objects can be copied, but most can.\n\nSome objects can be copied more easily. Dictionaries have a copy() method:\n\nnewdict = olddict.copy()\nSequences can be copied by slicing:\n\nnew_l = l[:]\nHow can I find the methods or attributes of an object?\nFor an instance x of a user-defined class, dir(x) returns an alphabetized list of the names containing the instance attributes and methods and attributes defined by its class.\n\nHow can my code discover the name of an object?\nGenerally speaking, it can\u2019t, because objects don\u2019t really have names. Essentially, assignment always binds a name to a value; the same is true of def and class statements, but in that case the value is a callable. Consider the following code:\n\n>>>\n>>> class A:\n...     pass\n...\n>>> B = A\n>>> a = B()\n>>> b = a\n>>> print(b)\n<__main__.A object at 0x16D07CC>\n>>> print(a)\n<__main__.A object at 0x16D07CC>\nArguably the class has a name: even though it is bound to two names and invoked through the name B the created instance is still reported as an instance of class A. However, it is impossible to say whether the instance\u2019s name is a or b, since both names are bound to the same value.\n\nGenerally speaking it should not be necessary for your code to \u201cknow the names\u201d of particular values. Unless you are deliberately writing introspective programs, this is usually an indication that a change of approach might be beneficial.\n\nIn comp.lang.python, Fredrik Lundh once gave an excellent analogy in answer to this question:\n\nThe same way as you get the name of that cat you found on your porch: the cat (object) itself cannot tell you its name, and it doesn\u2019t really care \u2013 so the only way to find out what it\u2019s called is to ask all your neighbours (namespaces) if it\u2019s their cat (object)\u2026\n\n\u2026.and don\u2019t be surprised if you\u2019ll find that it\u2019s known by many names, or no name at all!\n\nWhat\u2019s up with the comma operator\u2019s precedence?\nComma is not an operator in Python. Consider this session:\n\n>>>\n>>> \"a\" in \"b\", \"a\"\n(False, 'a')\nSince the comma is not an operator, but a separator between expressions the above is evaluated as if you had entered:\n\n(\"a\" in \"b\"), \"a\"\nnot:\n\n\"a\" in (\"b\", \"a\")\nThe same is true of the various assignment operators (=, += etc). They are not truly operators but syntactic delimiters in assignment statements.\n\nIs there an equivalent of C\u2019s \u201c?:\u201d ternary operator?\nYes, there is. The syntax is as follows:\n\n[on_true] if [expression] else [on_false]\n\nx, y = 50, 25\nsmall = x if x < y else y\nBefore this syntax was introduced in Python 2.5, a common idiom was to use logical operators:\n\n[expression] and [on_true] or [on_false]\nHowever, this idiom is unsafe, as it can give wrong results when on_true has a false boolean value. Therefore, it is always better to use the ... if ... else ... form.\n\nIs it possible to write obfuscated one-liners in Python?\nYes. Usually this is done by nesting lambda within lambda. See the following three examples, due to Ulf Bartelt:\n\nfrom functools import reduce\n\n# Primes < 1000\nprint(list(filter(None,map(lambda y:y*reduce(lambda x,y:x*y!=0,\nmap(lambda x,y=y:y%x,range(2,int(pow(y,0.5)+1))),1),range(2,1000)))))\n\n# First 10 Fibonacci numbers\nprint(list(map(lambda x,f=lambda x,f:(f(x-1,f)+f(x-2,f)) if x>1 else 1:\nf(x,f), range(10))))\n\n# Mandelbrot set\nprint((lambda Ru,Ro,Iu,Io,IM,Sx,Sy:reduce(lambda x,y:x+y,map(lambda y,\nIu=Iu,Io=Io,Ru=Ru,Ro=Ro,Sy=Sy,L=lambda yc,Iu=Iu,Io=Io,Ru=Ru,Ro=Ro,i=IM,\nSx=Sx,Sy=Sy:reduce(lambda x,y:x+y,map(lambda x,xc=Ru,yc=yc,Ru=Ru,Ro=Ro,\ni=i,Sx=Sx,F=lambda xc,yc,x,y,k,f=lambda xc,yc,x,y,k,f:(k<=0)or (x*x+y*y\n>=4.0) or 1+f(xc,yc,x*x-y*y+xc,2.0*x*y+yc,k-1,f):f(xc,yc,x,y,k,f):chr(\n64+F(Ru+x*(Ro-Ru)/Sx,yc,0,0,i)),range(Sx))):L(Iu+y*(Io-Iu)/Sy),range(Sy\n))))(-2.1, 0.7, -1.2, 1.2, 30, 80, 24))\n#    \\___ ___/  \\___ ___/  |   |   |__ lines on screen\n#        V          V      |   |______ columns on screen\n#        |          |      |__________ maximum of \"iterations\"\n#        |          |_________________ range on y axis\n#        |____________________________ range on x axis\nDon\u2019t try this at home, kids!\n\nWhat does the slash(/) in the parameter list of a function mean?\nA slash in the argument list of a function denotes that the parameters prior to it are positional-only. Positional-only parameters are the ones without an externally-usable name. Upon calling a function that accepts positional-only parameters, arguments are mapped to parameters based solely on their position. For example, divmod() is a function that accepts positional-only parameters. Its documentation looks like this:\n\n>>>\n>>> help(divmod)\nHelp on built-in function divmod in module builtins:\n\ndivmod(x, y, /)\n    Return the tuple (x//y, x%y).  Invariant: div*y + mod == x.\nThe slash at the end of the parameter list means that both parameters are positional-only. Thus, calling divmod() with keyword arguments would lead to an error:\n\n>>>\n>>> divmod(x=3, y=4)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: divmod() takes no keyword arguments\nNumbers and strings\nHow do I specify hexadecimal and octal integers?\nTo specify an octal digit, precede the octal value with a zero, and then a lower or uppercase \u201co\u201d. For example, to set the variable \u201ca\u201d to the octal value \u201c10\u201d (8 in decimal), type:\n\n>>>\n>>> a = 0o10\n>>> a\n8\nHexadecimal is just as easy. Simply precede the hexadecimal number with a zero, and then a lower or uppercase \u201cx\u201d. Hexadecimal digits can be specified in lower or uppercase. For example, in the Python interpreter:\n\n>>>\n>>> a = 0xa5\n>>> a\n165\n>>> b = 0XB2\n>>> b\n178\nWhy does -22 // 10 return -3?\nIt\u2019s primarily driven by the desire that i % j have the same sign as j. If you want that, and also want:\n\ni == (i // j) * j + (i % j)\nthen integer division has to return the floor. C also requires that identity to hold, and then compilers that truncate i // j need to make i % j have the same sign as i.\n\nThere are few real use cases for i % j when j is negative. When j is positive, there are many, and in virtually all of them it\u2019s more useful for i % j to be >= 0. If the clock says 10 now, what did it say 200 hours ago? -190 % 12 == 2 is useful; -190 % 12 == -10 is a bug waiting to bite.\n\nHow do I get int literal attribute instead of SyntaxError?\nTrying to lookup an int literal attribute in the normal manner gives a syntax error because the period is seen as a decimal point:\n\n>>>\n>>> 1.__class__\n  File \"<stdin>\", line 1\n  1.__class__\n   ^\nSyntaxError: invalid decimal literal\nThe solution is to separate the literal from the period with either a space or parentheses.\n\n>>>\n1 .__class__\n<class 'int'>\n(1).__class__\n<class 'int'>\nHow do I convert a string to a number?\nFor integers, use the built-in int() type constructor, e.g. int('144') == 144. Similarly, float() converts to floating-point, e.g. float('144') == 144.0.\n\nBy default, these interpret the number as decimal, so that int('0144') == 144 holds true, and int('0x144') raises ValueError. int(string, base) takes the base to convert from as a second optional argument, so int( '0x144', 16) == 324. If the base is specified as 0, the number is interpreted using Python\u2019s rules: a leading \u20180o\u2019 indicates octal, and \u20180x\u2019 indicates a hex number.\n\nDo not use the built-in function eval() if all you need is to convert strings to numbers. eval() will be significantly slower and it presents a security risk: someone could pass you a Python expression that might have unwanted side effects. For example, someone could pass __import__('os').system(\"rm -rf $HOME\") which would erase your home directory.\n\neval() also has the effect of interpreting numbers as Python expressions, so that e.g. eval('09') gives a syntax error because Python does not allow leading \u20180\u2019 in a decimal number (except \u20180\u2019).\n\nHow do I convert a number to a string?\nTo convert, e.g., the number 144 to the string \u2018144\u2019, use the built-in type constructor str(). If you want a hexadecimal or octal representation, use the built-in functions hex() or oct(). For fancy formatting, see the Formatted string literals and Format String Syntax sections, e.g. \"{:04d}\".format(144) yields '0144' and \"{:.3f}\".format(1.0/3.0) yields '0.333'.\n\nHow do I modify a string in place?\nYou can\u2019t, because strings are immutable. In most situations, you should simply construct a new string from the various parts you want to assemble it from. However, if you need an object with the ability to modify in-place unicode data, try using an io.StringIO object or the array module:\n\n>>>\n>>> import io\n>>> s = \"Hello, world\"\n>>> sio = io.StringIO(s)\n>>> sio.getvalue()\n'Hello, world'\n>>> sio.seek(7)\n7\n>>> sio.write(\"there!\")\n6\n>>> sio.getvalue()\n'Hello, there!'\n\n>>> import array\n>>> a = array.array('u', s)\n>>> print(a)\narray('u', 'Hello, world')\n>>> a[0] = 'y'\n>>> print(a)\narray('u', 'yello, world')\n>>> a.tounicode()\n'yello, world'\nHow do I use strings to call functions/methods?\nThere are various techniques.\n\nThe best is to use a dictionary that maps strings to functions. The primary advantage of this technique is that the strings do not need to match the names of the functions. This is also the primary technique used to emulate a case construct:\n\ndef a():\n    pass\n\ndef b():\n    pass\n\ndispatch = {'go': a, 'stop': b}  # Note lack of parens for funcs\n\ndispatch[get_input()]()  # Note trailing parens to call function\nUse the built-in function getattr():\n\nimport foo\ngetattr(foo, 'bar')()\nNote that getattr() works on any object, including classes, class instances, modules, and so on.\n\nThis is used in several places in the standard library, like this:\n\nclass Foo:\n    def do_foo(self):\n        ...\n\n    def do_bar(self):\n        ...\n\nf = getattr(foo_instance, 'do_' + opname)\nf()\nUse locals() to resolve the function name:\n\ndef myFunc():\n    print(\"hello\")\n\nfname = \"myFunc\"\n\nf = locals()[fname]\nf()\nIs there an equivalent to Perl\u2019s chomp() for removing trailing newlines from strings?\nYou can use S.rstrip(\"\\r\\n\") to remove all occurrences of any line terminator from the end of the string S without removing other trailing whitespace. If the string S represents more than one line, with several empty lines at the end, the line terminators for all the blank lines will be removed:\n\n>>>\n>>> lines = (\"line 1 \\r\\n\"\n...          \"\\r\\n\"\n...          \"\\r\\n\")\n>>> lines.rstrip(\"\\n\\r\")\n'line 1 '\nSince this is typically only desired when reading text one line at a time, using S.rstrip() this way works well.\n\nIs there a scanf() or sscanf() equivalent?\nNot as such.\n\nFor simple input parsing, the easiest approach is usually to split the line into whitespace-delimited words using the split() method of string objects and then convert decimal strings to numeric values using int() or float(). split() supports an optional \u201csep\u201d parameter which is useful if the line uses something other than whitespace as a separator.\n\nFor more complicated input parsing, regular expressions are more powerful than C\u2019s sscanf() and better suited for the task.\n\nWhat does \u2018UnicodeDecodeError\u2019 or \u2018UnicodeEncodeError\u2019 error mean?\nSee the Unicode HOWTO.\n\nPerformance\nMy program is too slow. How do I speed it up?\nThat\u2019s a tough one, in general. First, here are a list of things to remember before diving further:\n\nPerformance characteristics vary across Python implementations. This FAQ focuses on CPython.\n\nBehaviour can vary across operating systems, especially when talking about I/O or multi-threading.\n\nYou should always find the hot spots in your program before attempting to optimize any code (see the profile module).\n\nWriting benchmark scripts will allow you to iterate quickly when searching for improvements (see the timeit module).\n\nIt is highly recommended to have good code coverage (through unit testing or any other technique) before potentially introducing regressions hidden in sophisticated optimizations.\n\nThat being said, there are many tricks to speed up Python code. Here are some general principles which go a long way towards reaching acceptable performance levels:\n\nMaking your algorithms faster (or changing to faster ones) can yield much larger benefits than trying to sprinkle micro-optimization tricks all over your code.\n\nUse the right data structures. Study documentation for the Built-in Types and the collections module.\n\nWhen the standard library provides a primitive for doing something, it is likely (although not guaranteed) to be faster than any alternative you may come up with. This is doubly true for primitives written in C, such as builtins and some extension types. For example, be sure to use either the list.sort() built-in method or the related sorted() function to do sorting (and see the Sorting HOW TO for examples of moderately advanced usage).\n\nAbstractions tend to create indirections and force the interpreter to work more. If the levels of indirection outweigh the amount of useful work done, your program will be slower. You should avoid excessive abstraction, especially under the form of tiny functions or methods (which are also often detrimental to readability).\n\nIf you have reached the limit of what pure Python can allow, there are tools to take you further away. For example, Cython can compile a slightly modified version of Python code into a C extension, and can be used on many different platforms. Cython can take advantage of compilation (and optional type annotations) to make your code significantly faster than when interpreted. If you are confident in your C programming skills, you can also write a C extension module yourself.\n\nSee also The wiki page devoted to performance tips.\nWhat is the most efficient way to concatenate many strings together?\nstr and bytes objects are immutable, therefore concatenating many strings together is inefficient as each concatenation creates a new object. In the general case, the total runtime cost is quadratic in the total string length.\n\nTo accumulate many str objects, the recommended idiom is to place them into a list and call str.join() at the end:\n\nchunks = []\nfor s in my_strings:\n    chunks.append(s)\nresult = ''.join(chunks)\n(another reasonably efficient idiom is to use io.StringIO)\n\nTo accumulate many bytes objects, the recommended idiom is to extend a bytearray object using in-place concatenation (the += operator):\n\nresult = bytearray()\nfor b in my_bytes_objects:\n    result += b\nSequences (Tuples/Lists)\nHow do I convert between tuples and lists?\nThe type constructor tuple(seq) converts any sequence (actually, any iterable) into a tuple with the same items in the same order.\n\nFor example, tuple([1, 2, 3]) yields (1, 2, 3) and tuple('abc') yields ('a', 'b', 'c'). If the argument is a tuple, it does not make a copy but returns the same object, so it is cheap to call tuple() when you aren\u2019t sure that an object is already a tuple.\n\nThe type constructor list(seq) converts any sequence or iterable into a list with the same items in the same order. For example, list((1, 2, 3)) yields [1, 2, 3] and list('abc') yields ['a', 'b', 'c']. If the argument is a list, it makes a copy just like seq[:] would.\n\nWhat\u2019s a negative index?\nPython sequences are indexed with positive numbers and negative numbers. For positive numbers 0 is the first index 1 is the second index and so forth. For negative indices -1 is the last index and -2 is the penultimate (next to last) index and so forth. Think of seq[-n] as the same as seq[len(seq)-n].\n\nUsing negative indices can be very convenient. For example S[:-1] is all of the string except for its last character, which is useful for removing the trailing newline from a string.\n\nHow do I iterate over a sequence in reverse order?\nUse the reversed() built-in function:\n\nfor x in reversed(sequence):\n    ...  # do something with x ...\nThis won\u2019t touch your original sequence, but build a new copy with reversed order to iterate over.\n\nHow do you remove duplicates from a list?\nSee the Python Cookbook for a long discussion of many ways to do this:\n\nhttps://code.activestate.com/recipes/52560/\n\nIf you don\u2019t mind reordering the list, sort it and then scan from the end of the list, deleting duplicates as you go:\n\nif mylist:\n    mylist.sort()\n    last = mylist[-1]\n    for i in range(len(mylist)-2, -1, -1):\n        if last == mylist[i]:\n            del mylist[i]\n        else:\n            last = mylist[i]\nIf all elements of the list may be used as set keys (i.e. they are all hashable) this is often faster\n\nmylist = list(set(mylist))\nThis converts the list into a set, thereby removing duplicates, and then back into a list.\n\nHow do you remove multiple items from a list\nAs with removing duplicates, explicitly iterating in reverse with a delete condition is one possibility. However, it is easier and faster to use slice replacement with an implicit or explicit forward iteration. Here are three variations.:\n\nmylist[:] = filter(keep_function, mylist)\nmylist[:] = (x for x in mylist if keep_condition)\nmylist[:] = [x for x in mylist if keep_condition]\nThe list comprehension may be fastest.\n\nHow do you make an array in Python?\nUse a list:\n\n[\"this\", 1, \"is\", \"an\", \"array\"]\nLists are equivalent to C or Pascal arrays in their time complexity; the primary difference is that a Python list can contain objects of many different types.\n\nThe array module also provides methods for creating arrays of fixed types with compact representations, but they are slower to index than lists. Also note that NumPy and other third party packages define array-like structures with various characteristics as well.\n\nTo get Lisp-style linked lists, you can emulate cons cells using tuples:\n\nlisp_list = (\"like\",  (\"this\",  (\"example\", None) ) )\nIf mutability is desired, you could use lists instead of tuples. Here the analogue of lisp car is lisp_list[0] and the analogue of cdr is lisp_list[1]. Only do this if you\u2019re sure you really need to, because it\u2019s usually a lot slower than using Python lists.\n\nHow do I create a multidimensional list?\nYou probably tried to make a multidimensional array like this:\n\n>>>\n>>> A = [[None] * 2] * 3\nThis looks correct if you print it:\n\n>>>\n>>> A\n[[None, None], [None, None], [None, None]]\nBut when you assign a value, it shows up in multiple places:\n\n>>>\n>>> A[0][0] = 5\n>>> A\n[[5, None], [5, None], [5, None]]\nThe reason is that replicating a list with * doesn\u2019t create copies, it only creates references to the existing objects. The *3 creates a list containing 3 references to the same list of length two. Changes to one row will show in all rows, which is almost certainly not what you want.\n\nThe suggested approach is to create a list of the desired length first and then fill in each element with a newly created list:\n\nA = [None] * 3\nfor i in range(3):\n    A[i] = [None] * 2\nThis generates a list containing 3 different lists of length two. You can also use a list comprehension:\n\nw, h = 2, 3\nA = [[None] * w for i in range(h)]\nOr, you can use an extension that provides a matrix datatype; NumPy is the best known.\n\nHow do I apply a method to a sequence of objects?\nUse a list comprehension:\n\nresult = [obj.method() for obj in mylist]\nWhy does a_tuple[i] += [\u2018item\u2019] raise an exception when the addition works?\nThis is because of a combination of the fact that augmented assignment operators are assignment operators, and the difference between mutable and immutable objects in Python.\n\nThis discussion applies in general when augmented assignment operators are applied to elements of a tuple that point to mutable objects, but we\u2019ll use a list and += as our exemplar.\n\nIf you wrote:\n\n>>>\n>>> a_tuple = (1, 2)\n>>> a_tuple[0] += 1\nTraceback (most recent call last):\n   ...\nTypeError: 'tuple' object does not support item assignment\nThe reason for the exception should be immediately clear: 1 is added to the object a_tuple[0] points to (1), producing the result object, 2, but when we attempt to assign the result of the computation, 2, to element 0 of the tuple, we get an error because we can\u2019t change what an element of a tuple points to.\n\nUnder the covers, what this augmented assignment statement is doing is approximately this:\n\n>>>\n>>> result = a_tuple[0] + 1\n>>> a_tuple[0] = result\nTraceback (most recent call last):\n  ...\nTypeError: 'tuple' object does not support item assignment\nIt is the assignment part of the operation that produces the error, since a tuple is immutable.\n\nWhen you write something like:\n\n>>>\n>>> a_tuple = (['foo'], 'bar')\n>>> a_tuple[0] += ['item']\nTraceback (most recent call last):\n  ...\nTypeError: 'tuple' object does not support item assignment\nThe exception is a bit more surprising, and even more surprising is the fact that even though there was an error, the append worked:\n\n>>>\n>>> a_tuple[0]\n['foo', 'item']\nTo see why this happens, you need to know that (a) if an object implements an __iadd__ magic method, it gets called when the += augmented assignment is executed, and its return value is what gets used in the assignment statement; and (b) for lists, __iadd__ is equivalent to calling extend on the list and returning the list. That\u2019s why we say that for lists, += is a \u201cshorthand\u201d for list.extend:\n\n>>>\n>>> a_list = []\n>>> a_list += [1]\n>>> a_list\n[1]\nThis is equivalent to:\n\n>>>\n>>> result = a_list.__iadd__([1])\n>>> a_list = result\nThe object pointed to by a_list has been mutated, and the pointer to the mutated object is assigned back to a_list. The end result of the assignment is a no-op, since it is a pointer to the same object that a_list was previously pointing to, but the assignment still happens.\n\nThus, in our tuple example what is happening is equivalent to:\n\n>>>\n>>> result = a_tuple[0].__iadd__(['item'])\n>>> a_tuple[0] = result\nTraceback (most recent call last):\n  ...\nTypeError: 'tuple' object does not support item assignment\nThe __iadd__ succeeds, and thus the list is extended, but even though result points to the same object that a_tuple[0] already points to, that final assignment still results in an error, because tuples are immutable.\n\nI want to do a complicated sort: can you do a Schwartzian Transform in Python?\nThe technique, attributed to Randal Schwartz of the Perl community, sorts the elements of a list by a metric which maps each element to its \u201csort value\u201d. In Python, use the key argument for the list.sort() method:\n\nIsorted = L[:]\nIsorted.sort(key=lambda s: int(s[10:15]))\nHow can I sort one list by values from another list?\nMerge them into an iterator of tuples, sort the resulting list, and then pick out the element you want.\n\n>>>\n>>> list1 = [\"what\", \"I'm\", \"sorting\", \"by\"]\n>>> list2 = [\"something\", \"else\", \"to\", \"sort\"]\n>>> pairs = zip(list1, list2)\n>>> pairs = sorted(pairs)\n>>> pairs\n[(\"I'm\", 'else'), ('by', 'sort'), ('sorting', 'to'), ('what', 'something')]\n>>> result = [x[1] for x in pairs]\n>>> result\n['else', 'sort', 'to', 'something']\nObjects\nWhat is a class?\nA class is the particular object type created by executing a class statement. Class objects are used as templates to create instance objects, which embody both the data (attributes) and code (methods) specific to a datatype.\n\nA class can be based on one or more other classes, called its base class(es). It then inherits the attributes and methods of its base classes. This allows an object model to be successively refined by inheritance. You might have a generic Mailbox class that provides basic accessor methods for a mailbox, and subclasses such as MboxMailbox, MaildirMailbox, OutlookMailbox that handle various specific mailbox formats.\n\nWhat is a method?\nA method is a function on some object x that you normally call as x.name(arguments...). Methods are defined as functions inside the class definition:\n\nclass C:\n    def meth(self, arg):\n        return arg * 2 + self.attribute\nWhat is self?\nSelf is merely a conventional name for the first argument of a method. A method defined as meth(self, a, b, c) should be called as x.meth(a, b, c) for some instance x of the class in which the definition occurs; the called method will think it is called as meth(x, a, b, c).\n\nSee also Why must \u2018self\u2019 be used explicitly in method definitions and calls?.\n\nHow do I check if an object is an instance of a given class or of a subclass of it?\nUse the built-in function isinstance(obj, cls). You can check if an object is an instance of any of a number of classes by providing a tuple instead of a single class, e.g. isinstance(obj, (class1, class2, ...)), and can also check whether an object is one of Python\u2019s built-in types, e.g. isinstance(obj, str) or isinstance(obj, (int, float, complex)).\n\nNote that isinstance() also checks for virtual inheritance from an abstract base class. So, the test will return True for a registered class even if hasn\u2019t directly or indirectly inherited from it. To test for \u201ctrue inheritance\u201d, scan the MRO of the class:\n\nfrom collections.abc import Mapping\n\nclass P:\n     pass\n\nclass C(P):\n    pass\n\nMapping.register(P)\n>>>\n>>> c = C()\n>>> isinstance(c, C)        # direct\nTrue\n>>> isinstance(c, P)        # indirect\nTrue\n>>> isinstance(c, Mapping)  # virtual\nTrue\n\n# Actual inheritance chain\n>>> type(c).__mro__\n(<class 'C'>, <class 'P'>, <class 'object'>)\n\n# Test for \"true inheritance\"\n>>> Mapping in type(c).__mro__\nFalse\nNote that most programs do not use isinstance() on user-defined classes very often. If you are developing the classes yourself, a more proper object-oriented style is to define methods on the classes that encapsulate a particular behaviour, instead of checking the object\u2019s class and doing a different thing based on what class it is. For example, if you have a function that does something:\n\ndef search(obj):\n    if isinstance(obj, Mailbox):\n        ...  # code to search a mailbox\n    elif isinstance(obj, Document):\n        ...  # code to search a document\n    elif ...\nA better approach is to define a search() method on all the classes and just call it:\n\nclass Mailbox:\n    def search(self):\n        ...  # code to search a mailbox\n\nclass Document:\n    def search(self):\n        ...  # code to search a document\n\nobj.search()\nWhat is delegation?\nDelegation is an object oriented technique (also called a design pattern). Let\u2019s say you have an object x and want to change the behaviour of just one of its methods. You can create a new class that provides a new implementation of the method you\u2019re interested in changing and delegates all other methods to the corresponding method of x.\n\nPython programmers can easily implement delegation. For example, the following class implements a class that behaves like a file but converts all written data to uppercase:\n\nclass UpperOut:\n\n    def __init__(self, outfile):\n        self._outfile = outfile\n\n    def write(self, s):\n        self._outfile.write(s.upper())\n\n    def __getattr__(self, name):\n        return getattr(self._outfile, name)\nHere the UpperOut class redefines the write() method to convert the argument string to uppercase before calling the underlying self._outfile.write() method. All other methods are delegated to the underlying self._outfile object. The delegation is accomplished via the __getattr__ method; consult the language reference for more information about controlling attribute access.\n\nNote that for more general cases delegation can get trickier. When attributes must be set as well as retrieved, the class must define a __setattr__() method too, and it must do so carefully. The basic implementation of __setattr__() is roughly equivalent to the following:\n\nclass X:\n    ...\n    def __setattr__(self, name, value):\n        self.__dict__[name] = value\n    ...\nMost __setattr__() implementations must modify self.__dict__ to store local state for self without causing an infinite recursion.\n\nHow do I call a method defined in a base class from a derived class that extends it?\nUse the built-in super() function:\n\nclass Derived(Base):\n    def meth(self):\n        super().meth()  # calls Base.meth\nIn the example, super() will automatically determine the instance from which it was called (the self value), look up the method resolution order (MRO) with type(self).__mro__, and return the next in line after Derived in the MRO: Base.\n\nHow can I organize my code to make it easier to change the base class?\nYou could assign the base class to an alias and derive from the alias. Then all you have to change is the value assigned to the alias. Incidentally, this trick is also handy if you want to decide dynamically (e.g. depending on availability of resources) which base class to use. Example:\n\nclass Base:\n    ...\n\nBaseAlias = Base\n\nclass Derived(BaseAlias):\n    ...\nHow do I create static class data and static class methods?\nBoth static data and static methods (in the sense of C++ or Java) are supported in Python.\n\nFor static data, simply define a class attribute. To assign a new value to the attribute, you have to explicitly use the class name in the assignment:\n\nclass C:\n    count = 0   # number of times C.__init__ called\n\n    def __init__(self):\n        C.count = C.count + 1\n\n    def getcount(self):\n        return C.count  # or return self.count\nc.count also refers to C.count for any c such that isinstance(c, C) holds, unless overridden by c itself or by some class on the base-class search path from c.__class__ back to C.\n\nCaution: within a method of C, an assignment like self.count = 42 creates a new and unrelated instance named \u201ccount\u201d in self\u2019s own dict. Rebinding of a class-static data name must always specify the class whether inside a method or not:\n\nC.count = 314\nStatic methods are possible:\n\nclass C:\n    @staticmethod\n    def static(arg1, arg2, arg3):\n        # No 'self' parameter!\n        ...\nHowever, a far more straightforward way to get the effect of a static method is via a simple module-level function:\n\ndef getcount():\n    return C.count\nIf your code is structured so as to define one class (or tightly related class hierarchy) per module, this supplies the desired encapsulation.\n\nHow can I overload constructors (or methods) in Python?\nThis answer actually applies to all methods, but the question usually comes up first in the context of constructors.\n\nIn C++ you\u2019d write\n\nclass C {\n    C() { cout << \"No arguments\\n\"; }\n    C(int i) { cout << \"Argument is \" << i << \"\\n\"; }\n}\nIn Python you have to write a single constructor that catches all cases using default arguments. For example:\n\nclass C:\n    def __init__(self, i=None):\n        if i is None:\n            print(\"No arguments\")\n        else:\n            print(\"Argument is\", i)\nThis is not entirely equivalent, but close enough in practice.\n\nYou could also try a variable-length argument list, e.g.\n\ndef __init__(self, *args):\n    ...\nThe same approach works for all method definitions.\n\nI try to use __spam and I get an error about _SomeClassName__spam.\nVariable names with double leading underscores are \u201cmangled\u201d to provide a simple but effective way to define class private variables. Any identifier of the form __spam (at least two leading underscores, at most one trailing underscore) is textually replaced with _classname__spam, where classname is the current class name with any leading underscores stripped.\n\nThis doesn\u2019t guarantee privacy: an outside user can still deliberately access the \u201c_classname__spam\u201d attribute, and private values are visible in the object\u2019s __dict__. Many Python programmers never bother to use private variable names at all.\n\nMy class defines __del__ but it is not called when I delete the object.\nThere are several possible reasons for this.\n\nThe del statement does not necessarily call __del__() \u2013 it simply decrements the object\u2019s reference count, and if this reaches zero __del__() is called.\n\nIf your data structures contain circular links (e.g. a tree where each child has a parent reference and each parent has a list of children) the reference counts will never go back to zero. Once in a while Python runs an algorithm to detect such cycles, but the garbage collector might run some time after the last reference to your data structure vanishes, so your __del__() method may be called at an inconvenient and random time. This is inconvenient if you\u2019re trying to reproduce a problem. Worse, the order in which object\u2019s __del__() methods are executed is arbitrary. You can run gc.collect() to force a collection, but there are pathological cases where objects will never be collected.\n\nDespite the cycle collector, it\u2019s still a good idea to define an explicit close() method on objects to be called whenever you\u2019re done with them. The close() method can then remove attributes that refer to subobjects. Don\u2019t call __del__() directly \u2013 __del__() should call close() and close() should make sure that it can be called more than once for the same object.\n\nAnother way to avoid cyclical references is to use the weakref module, which allows you to point to objects without incrementing their reference count. Tree data structures, for instance, should use weak references for their parent and sibling references (if they need them!).\n\nFinally, if your __del__() method raises an exception, a warning message is printed to sys.stderr.\n\nHow do I get a list of all instances of a given class?\nPython does not keep track of all instances of a class (or of a built-in type). You can program the class\u2019s constructor to keep track of all instances by keeping a list of weak references to each instance.\n\nWhy does the result of id() appear to be not unique?\nThe id() builtin returns an integer that is guaranteed to be unique during the lifetime of the object. Since in CPython, this is the object\u2019s memory address, it happens frequently that after an object is deleted from memory, the next freshly created object is allocated at the same position in memory. This is illustrated by this example:\n\n>>>\nid(1000) \n13901272\nid(2000) \n13901272\nThe two ids belong to different integer objects that are created before, and deleted immediately after execution of the id() call. To be sure that objects whose id you want to examine are still alive, create another reference to the object:\n\n>>>\na = 1000; b = 2000\nid(a) \n13901272\nid(b) \n13891296\nWhen can I rely on identity tests with the is operator?\nThe is operator tests for object identity. The test a is b is equivalent to id(a) == id(b).\n\nThe most important property of an identity test is that an object is always identical to itself, a is a always returns True. Identity tests are usually faster than equality tests. And unlike equality tests, identity tests are guaranteed to return a boolean True or False.\n\nHowever, identity tests can only be substituted for equality tests when object identity is assured. Generally, there are three circumstances where identity is guaranteed:\n\n1) Assignments create new names but do not change object identity. After the assignment new = old, it is guaranteed that new is old.\n\n2) Putting an object in a container that stores object references does not change object identity. After the list assignment s[0] = x, it is guaranteed that s[0] is x.\n\n3) If an object is a singleton, it means that only one instance of that object can exist. After the assignments a = None and b = None, it is guaranteed that a is b because None is a singleton.\n\nIn most other circumstances, identity tests are inadvisable and equality tests are preferred. In particular, identity tests should not be used to check constants such as int and str which aren\u2019t guaranteed to be singletons:\n\n>>>\n>>> a = 1000\n>>> b = 500\n>>> c = b + 500\n>>> a is c\nFalse\n\n>>> a = 'Python'\n>>> b = 'Py'\n>>> c = b + 'thon'\n>>> a is c\nFalse\nLikewise, new instances of mutable containers are never identical:\n\n>>>\n>>> a = []\n>>> b = []\n>>> a is b\nFalse\nIn the standard library code, you will see several common patterns for correctly using identity tests:\n\n1) As recommended by PEP 8, an identity test is the preferred way to check for None. This reads like plain English in code and avoids confusion with other objects that may have boolean values that evaluate to false.\n\n2) Detecting optional arguments can be tricky when None is a valid input value. In those situations, you can create an singleton sentinel object guaranteed to be distinct from other objects. For example, here is how to implement a method that behaves like dict.pop():\n\n_sentinel = object()\n\ndef pop(self, key, default=_sentinel):\n    if key in self:\n        value = self[key]\n        del self[key]\n        return value\n    if default is _sentinel:\n        raise KeyError(key)\n    return default\n3) Container implementations sometimes need to augment equality tests with identity tests. This prevents the code from being confused by objects such as float('NaN') that are not equal to themselves.\n\nFor example, here is the implementation of collections.abc.Sequence.__contains__():\n\ndef __contains__(self, value):\n    for v in self:\n        if v is value or v == value:\n            return True\n    return False\nHow can a subclass control what data is stored in an immutable instance?\nWhen subclassing an immutable type, override the __new__() method instead of the __init__() method. The latter only runs after an instance is created, which is too late to alter data in an immutable instance.\n\nAll of these immutable classes have a different signature than their parent class:\n\nfrom datetime import date\n\nclass FirstOfMonthDate(date):\n    \"Always choose the first day of the month\"\n    def __new__(cls, year, month, day):\n        return super().__new__(cls, year, month, 1)\n\nclass NamedInt(int):\n    \"Allow text names for some numbers\"\n    xlat = {'zero': 0, 'one': 1, 'ten': 10}\n    def __new__(cls, value):\n        value = cls.xlat.get(value, value)\n        return super().__new__(cls, value)\n\nclass TitleStr(str):\n    \"Convert str to name suitable for a URL path\"\n    def __new__(cls, s):\n        s = s.lower().replace(' ', '-')\n        s = ''.join([c for c in s if c.isalnum() or c == '-'])\n        return super().__new__(cls, s)\nThe classes can be used like this:\n\n>>>\n>>> FirstOfMonthDate(2012, 2, 14)\nFirstOfMonthDate(2012, 2, 1)\n>>> NamedInt('ten')\n10\n>>> NamedInt(20)\n20\n>>> TitleStr('Blog: Why Python Rocks')\n'blog-why-python-rocks'\nHow do I cache method calls?\nThe two principal tools for caching methods are functools.cached_property() and functools.lru_cache(). The former stores results at the instance level and the latter at the class level.\n\nThe cached_property approach only works with methods that do not take any arguments. It does not create a reference to the instance. The cached method result will be kept only as long as the instance is alive.\n\nThe advantage is that when an instance is not longer used, the cached method result will be released right away. The disadvantage is that if instances accumulate, so too will the accumulated method results. They can grow without bound.\n\nThe lru_cache approach works with methods that have hashable arguments. It creates a reference to the instance unless special efforts are made to pass in weak references.\n\nThe advantage of the least recently used algorithm is that the cache is bounded by the specified maxsize. The disadvantage is that instances are kept alive until they age out of the cache or until the cache is cleared.\n\nThis example shows the various techniques:\n\nclass Weather:\n    \"Lookup weather information on a government website\"\n\n    def __init__(self, station_id):\n        self._station_id = station_id\n        # The _station_id is private and immutable\n\n    def current_temperature(self):\n        \"Latest hourly observation\"\n        # Do not cache this because old results\n        # can be out of date.\n\n    @cached_property\n    def location(self):\n        \"Return the longitude/latitude coordinates of the station\"\n        # Result only depends on the station_id\n\n    @lru_cache(maxsize=20)\n    def historic_rainfall(self, date, units='mm'):\n        \"Rainfall on a given date\"\n        # Depends on the station_id, date, and units.\nThe above example assumes that the station_id never changes. If the relevant instance attributes are mutable, the cached_property approach can\u2019t be made to work because it cannot detect changes to the attributes.\n\nThe lru_cache approach can be made to work, but the class needs to define the __eq__ and __hash__ methods so the cache can detect relevant attribute updates:\n\nclass Weather:\n    \"Example with a mutable station identifier\"\n\n    def __init__(self, station_id):\n        self.station_id = station_id\n\n    def change_station(self, station_id):\n        self.station_id = station_id\n\n    def __eq__(self, other):\n        return self.station_id == other.station_id\n\n    def __hash__(self):\n        return hash(self.station_id)\n\n    @lru_cache(maxsize=20)\n    def historic_rainfall(self, date, units='cm'):\n        'Rainfall on a given date'\n        # Depends on the station_id, date, and units.\nModules\nHow do I create a .pyc file?\nWhen a module is imported for the first time (or when the source file has changed since the current compiled file was created) a .pyc file containing the compiled code should be created in a __pycache__ subdirectory of the directory containing the .py file. The .pyc file will have a filename that starts with the same name as the .py file, and ends with .pyc, with a middle component that depends on the particular python binary that created it. (See PEP 3147 for details.)\n\nOne reason that a .pyc file may not be created is a permissions problem with the directory containing the source file, meaning that the __pycache__ subdirectory cannot be created. This can happen, for example, if you develop as one user but run as another, such as if you are testing with a web server.\n\nUnless the PYTHONDONTWRITEBYTECODE environment variable is set, creation of a .pyc file is automatic if you\u2019re importing a module and Python has the ability (permissions, free space, etc\u2026) to create a __pycache__ subdirectory and write the compiled module to that subdirectory.\n\nRunning Python on a top level script is not considered an import and no .pyc will be created. For example, if you have a top-level module foo.py that imports another module xyz.py, when you run foo (by typing python foo.py as a shell command), a .pyc will be created for xyz because xyz is imported, but no .pyc file will be created for foo since foo.py isn\u2019t being imported.\n\nIf you need to create a .pyc file for foo \u2013 that is, to create a .pyc file for a module that is not imported \u2013 you can, using the py_compile and compileall modules.\n\nThe py_compile module can manually compile any module. One way is to use the compile() function in that module interactively:\n\n>>>\n>>> import py_compile\n>>> py_compile.compile('foo.py')                 \nThis will write the .pyc to a __pycache__ subdirectory in the same location as foo.py (or you can override that with the optional parameter cfile).\n\nYou can also automatically compile all files in a directory or directories using the compileall module. You can do it from the shell prompt by running compileall.py and providing the path of a directory containing Python files to compile:\n\npython -m compileall .\nHow do I find the current module name?\nA module can find out its own module name by looking at the predefined global variable __name__. If this has the value '__main__', the program is running as a script. Many modules that are usually used by importing them also provide a command-line interface or a self-test, and only execute this code after checking __name__:\n\ndef main():\n    print('Running test...')\n    ...\n\nif __name__ == '__main__':\n    main()\nHow can I have modules that mutually import each other?\nSuppose you have the following modules:\n\nfoo.py:\n\nfrom bar import bar_var\nfoo_var = 1\nbar.py:\n\nfrom foo import foo_var\nbar_var = 2\nThe problem is that the interpreter will perform the following steps:\n\nmain imports foo\n\nEmpty globals for foo are created\n\nfoo is compiled and starts executing\n\nfoo imports bar\n\nEmpty globals for bar are created\n\nbar is compiled and starts executing\n\nbar imports foo (which is a no-op since there already is a module named foo)\n\nThe import mechanism tries to read foo_var from foo globals, to set bar.foo_var = foo.foo_var\n\nThe last step fails, because Python isn\u2019t done with interpreting foo yet and the global symbol dictionary for foo is still empty.\n\nThe same thing happens when you use import foo, and then try to access foo.foo_var in global code.\n\nThere are (at least) three possible workarounds for this problem.\n\nGuido van Rossum recommends avoiding all uses of from <module> import ..., and placing all code inside functions. Initializations of global variables and class variables should use constants or built-in functions only. This means everything from an imported module is referenced as <module>.<name>.\n\nJim Roskind suggests performing steps in the following order in each module:\n\nexports (globals, functions, and classes that don\u2019t need imported base classes)\n\nimport statements\n\nactive code (including globals that are initialized from imported values).\n\nvan Rossum doesn\u2019t like this approach much because the imports appear in a strange place, but it does work.\n\nMatthias Urlichs recommends restructuring your code so that the recursive import is not necessary in the first place.\n\nThese solutions are not mutually exclusive.\n\n__import__(\u2018x.y.z\u2019) returns <module \u2018x\u2019>; how do I get z?\nConsider using the convenience function import_module() from importlib instead:\n\nz = importlib.import_module('x.y.z')\nWhen I edit an imported module and reimport it, the changes don\u2019t show up. Why does this happen?\nFor reasons of efficiency as well as consistency, Python only reads the module file on the first time a module is imported. If it didn\u2019t, in a program consisting of many modules where each one imports the same basic module, the basic module would be parsed and re-parsed many times. To force re-reading of a changed module, do this:\n\nimport importlib\nimport modname\nimportlib.reload(modname)\nWarning: this technique is not 100% fool-proof. In particular, modules containing statements like\n\nfrom modname import some_objects\nwill continue to work with the old version of the imported objects. If the module contains class definitions, existing class instances will not be updated to use the new class definition. This can result in the following paradoxical behaviour:\n\n>>>\n>>> import importlib\n>>> import cls\n>>> c = cls.C()                # Create an instance of C\n>>> importlib.reload(cls)\n<module 'cls' from 'cls.py'>\n>>> isinstance(c, cls.C)       # isinstance is false?!?\nFalse\nThe nature of the problem is made clear if you print out the \u201cidentity\u201d of the class objects:\n\n>>>\n>>> hex(id(c.__class__))\n'0x7352a0'\n>>> hex(id(cls.C))\n'0x4198d0'\n\u00a9 Copyright 2001-2021, Python Software Foundation.\nThis page is licensed under the Python Software Foundation License Version 2.\nExamples, recipes, and other code in the documentation are additionally licensed under the Zero Clause BSD License.\nSee History and License for more information.\n\nThe Python Software Foundation is a non-profit corporation. Please donate.\n\nLast updated on Oct 24, 2021. Found a bug?\nCreated using", "document_id": 122}]}, {"paragraphs": [{"qas": [{"question": "trouble installing scipy on windows", "id": 1463, "answers": [{"answer_id": 1452, "document_id": 1034, "question_id": 1463, "text": "ipy on Windows-\n\n1.Go to website sourceforge.net\n\n2.Click Files\n\n3.Downlaod the type you wan", "answer_start": 3238, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI have Python 2.7 and NumPy installed.  I have downloaded pre-built binaries for SciPy, but the install script fails with this error:\n\nBlas (http://www.netlib.org/blas/) libraries not found.\nDirectories to search for the libraries can be specified in the\nnumpy/distutils/site.cfg file (section [blas]) or by setting\nthe BLAS environment variable.\n\nI really don't know enough about this to fool with it.  I assumed it was a straightforward install process, but doesn't appear to be.  I googled for the BLAS environment variable, but couldn't find anything that seemed appropriate.  Any help is appreciated.\n\nMike\n\nEDIT: Nevermind, I found an unofficial installer exe.\n    \n\nTry installing using Scipy wheel file. Download it from here: http://www.lfd.uci.edu/~gohlke/pythonlibs/#scipy\n\nMake sure to download the one that's compatible with your Python version and your laptop bit. Then install it like this: pip install \"path\\to\\your\\wheel\\file\\scipy-0.18.1-cp27-cp27m-win_amd64.whl\"\n    \n\nTo install SciPy on Windows you have to have a fortran compiler installed.  The SciPy project recommends MinGW.  See Building and installing SciPy.  To install MinGW follow these instructions: HOWTO Install the MinGW (GCC) Compiler Suite.  Then before you run pip or easy_install to install SciPy make sure that you have MinGW added to your path.  See MinGW Installation Notes - Environmental Variables\n\nA side note, It would be easier to use either the Enthought Distribution (part of the initial install) or the Active State Distribution (through pypm 32-bit only) as they already have precomiled binary packages for SciPy.  Or, you could use the SciPy precompiled binary package installer for Windows.\n    \n\nHere I am going to share what I have done to install scipy.\n\n\n  MY PC Configuration is windows-7 64-bit &amp; python 2.7\n\n\n\nFirst  I download  the required packages form  http://www.lfd.uci.edu/~gohlke/pythonlibs/ (which version match your configuration EX: cp27==&gt;python2.7 &amp; cp36==&gt;3.6)\nSecond  I extract the file using 7zip (also can be used any zipper like winrar) \nThird I copy the scipy folder which I extracted and paste it into C:\\Python27\\Lib\\site-packages (or put it where the exact location is in your PC like ..\\..\\Lib\\site-packages)\n\n\n\n  NOTE: Have to install numpy first before installing scipy in this same way.\n\n    \n\nTo install Scipy on Windows requires a C compiler and the presence of 3rd party C libraries on the system which are difficult to install on Windows. However you can use a Wheel (.whl) file through your command prompt to install Scipy.\n\nI faced the same problem and this is what I did:\n\nGo to https://pypi.python.org/pypi/scipy and download the version of Scipy which is compatible with your system and the Python version you have installed\ne.g If you have 32-Bit Windows and Python 3.6 installed then you download the version with cp36 (version3.6) and Win-32.\n\nAfter downloading copy this file in the directory where you have installed Python either in the Scripts or Lib folder.\n\nNext use the command prompt to install it after changing to the directory where you have copied the file:\n\nC:\\....&gt; pip install scipy-1.0.0b1-cp36-cp36m-win32.whl\n\n    \n\nSteps to download scipy on Windows-\n\n1.Go to website sourceforge.net\n\n2.Click Files\n\n3.Downlaod the type you want\n\n4.Install it.\n\n\nVery easy and it worked for me.\n    ", "document_id": 1034}]}, {"paragraphs": [{"qas": [{"question": "why cant i import candlestick ohlc from mplfinance?", "id": 1358, "answers": [{"answer_id": 1347, "document_id": 926, "question_id": 1358, "text": "You do not have to import 'candlestick_ohlc' anymore.\n\n'mplfinance.plot()' defaults to ohlc style charts.", "answer_start": 697, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nSo I have been able to successfully install mplfinance with pip and when I import it alone I receive no error. Though when I do: from mplfinance import candlestick_ohlc I get the error ImportError: cannot import name 'candlestick_ohlc' from 'mplfinance' I have checked command prompt again, and it says it has successfully installed mplfinance. Why am I receiving this error?\n    \n\nSo from what I understand the Matplotlib for finance has changed so that:\n\nTo access the old API with the new mplfinance package installed, change statments\n\nfrom:\n\nfrom mpl_finance import \n\n\nto:\n\n    from mplfinance.original_flavor import candlestick_ohlc\n\n\nand then it should work fine.\n    \n\nYou do not have to import 'candlestick_ohlc' anymore.\n\n'mplfinance.plot()' defaults to ohlc style charts.\n\nThese links provide good examples.  The second one uses candlesticks.  You can change that arg.\n\nhttps://towardsdatascience.com/trading-toolbox-03-ohlc-charts-95b48bb9d748\n\nhttps://openwritings.net/pg/mplfinance/python-draw-candlestickohlc-using-new-mplfinance\n    ", "document_id": 926}]}, {"paragraphs": [{"qas": [{"question": "python installation compilation errors", "id": 1492, "answers": [{"answer_id": 1481, "document_id": 1070, "question_id": 1492, "text": "Run Visual Studio Installer.\nSelect Modify button.\nGo to \"Individual Components\" tab.\nScroll down to \"Compilers, build tools and runtimes\".\nTick \"Windows Universal CRT SDK\".\nInstall.", "answer_start": 3918, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI'm hoping someone can help me since I've been stuck on this for a while, and I'm not very familiar with compiling packages. Trying to install the following package: https://github.com/jhkorhonen/MOODS/wiki/Installation\n\nRunning Python 3.5 (Anaconda), Windows 10 64bit, Microsoft Visual Studio 2017 Community Edition. Here is what I did so far. \n\n\nError 1:cded to extracted package location, and ran python setup.py install --user but got the error that says:\n\nrunning install\nrunning build\nrunning build_py\nrunning build_ext\nbuilding 'MOODS._tools' extension\ncl.exe /c /nologo /Ox /W3 /GL /DNDEBUG /MD -Icore/ -IC:\\Users\\Wolf\\Anaconda3\\include -IC:\\Users\\Wolf\\Anaconda3\\include /EHsc /Tpcore/tools_wrap.cxx /Fobuild\\temp.win-amd64-3.5\\Release\\core/tools_wrap.obj -march=native -O3 -fPIC --std=c++11\nerror: command 'cl.exe' failed: No such file or directory\nSolution 1: Turns out C:\\Program Files (x86)\\Microsoft Visual Studio 14.0 does not have the \\VC folder it is looking for, but I did find it at C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\14.0\\VC\\bin, so I added that to PATH. \n\n\nThen another error:\n\n\nError 2: C:\\Program Files (x86)\\Microsoft Visual\nStudio\\Shared\\14.0\\VC\\bin\\cl.exe /c /nologo /Ox /W3 /GL /DNDEBUG /MD\n-Icore/ -IC:\\Users\\Wolf\\Anaconda3\\include -IC:\\Users\\Wolf\\Anaconda3\\include /EHsc /Tpcore/tools_wrap.cxx /Fobuild\\temp.win-amd64-3.5\\Release\\core/tools_wrap.obj -march=native\n-O3 -fPIC --std=c++11 cl : Command line warning D9002 : ignoring unknown option '-march=native' cl : Command line warning D9002 :\nignoring unknown option '-O3' cl : Command line warning D9002 :\nignoring unknown option '-fPIC' cl : Command line warning D9002 :\nignoring unknown option '--std=c++11' tools_wrap.cxx\nc:\\users\\wolf\\anaconda3\\include\\pyconfig.h(68): fatal error C1083:\nCannot open include file: 'io.h': No such file or directory error:\ncommand 'C:\\\\Program Files (x86)\\\\Microsoft Visual\nStudio\\\\Shared\\\\14.0\\\\VC\\\\bin\\\\cl.exe' failed with exit status 2\nSolution 2: So I added an environmental variable INCLUDE and set it\nto C:\\Program Files (x86)\\Windows\nKits\\10\\Include\\10.0.14393.0\\ucrt, which has io.h.\n\n\nHowever, yet another error: \n\nC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\14.0\\VC\\bin\\cl.exe /c /nologo /Ox /W3 /GL /DNDEBUG /MD -Icore/ -IC:\\Users\\Wolf\\Anaconda3\\include -IC:\\Users\\Wolf\\Anaconda3\\include \"-IC:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.14393.0\\ucrt\" /EHsc /Tpcore/tools_wrap.cxx /Fobuild\\temp.win-amd64-3.5\\Release\\core/tools_wrap.obj -march=native -O3 -fPIC --std=c++11\ncl : Command line warning D9002 : ignoring unknown option '-march=native'\ncl : Command line warning D9002 : ignoring unknown option '-O3'\ncl : Command line warning D9002 : ignoring unknown option '-fPIC'\ncl : Command line warning D9002 : ignoring unknown option '--std=c++11'\ntools_wrap.cxx\nC:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.14393.0\\ucrt\\corecrt.h(10): fatal error C1083: Cannot open include file: 'vcruntime.h': No such file or directory\nerror: command 'C:\\\\Program Files (x86)\\\\Microsoft Visual Studio\\\\Shared\\\\14.0\\\\VC\\\\bin\\\\cl.exe' failed with exit status 2\n\nI'm not sure how to solve this. It seems like adding things to PATH isn't helping a whole lot. Maybe it has to do with the introduction of Universal CRT? Should I just uninstall Visual Studio 2017 and use an older version?\n    \n\nI had very similiar issue running Python 3.5 (Anaconda), Windows 10 64bit, Microsoft Visual Studio 2017 Professional Edition. \n\nDid you try to enable a 64-Bit Visual C++ Toolset on the Command Line?\nTo do this, run     vcvars64.bat on your command line first.\nIn my case the localization is:\n\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Professional\\VC\\Auxiliary\\Build\n\n\nThis was sufficient for me and solved my problem.\n\nIn addition, I see some users have to install \"Windows Universal CRT SDK\" (I have it already). Check if you also have it:\n\n\nRun Visual Studio Installer.\nSelect Modify button.\nGo to \"Individual Components\" tab.\nScroll down to \"Compilers, build tools and runtimes\".\nTick \"Windows Universal CRT SDK\".\nInstall.\n\n\nPS: for convenience I recommend using powershell. A script for setting vcvars64.bat example from here:\n\npushd \"C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Professional\\VC\\Auxiliary\\Build\\\"\ncmd /c \"vcvars64.bat&amp;set\" |\nforeach {\n  if ($_ -match \"=\") {\n    $v = $_.split(\"=\"); set-item -force -path \"ENV:\\$($v[0])\"  -value \"$($v[1])\"\n  }\n}\npopd\nWrite-Host \"`nVisual Studio 2017 Command Prompt variables set.\" -ForegroundColor Yellow\n\n    \n\nyou can also download and install window 10 sdk independently, \n\nusing this link, hope it solves the issue.\n\n2nd try to use the the visual studio command propmpt e.g Vs2017 x64 Native Tools command prompt and then try the compilation process.\n    ", "document_id": 1070}]}, {"paragraphs": [{"qas": [{"question": "How to change the Windows service startup type in a WiX installer", "id": 1222, "answers": [{"answer_id": 1215, "document_id": 798, "question_id": 1222, "text": "Place a ServiceConfig element within the ServiceInstall element.\n<ServiceConfig DelayedAutoStart=\"yes\" OnInstall=\"yes\" OnReinstall =\"yes\" />", "answer_start": 766, "answer_category": null}], "is_impossible": false}], "context": "We need to modify the Startup type of our Windows service from \"Automatic\" to \"Automatic Delayed Start\". How do I do this?\nMy code is like this:\n<ServiceInstall\n    Id=\"WinServiceInstall\"\n    Name=\"ServiceManager\"\n    DisplayName=\"ServiceManager\"\n    Type=\"ownProcess\"\n    Start=\"auto\"\n    ErrorControl=\"normal\"\n    Vital ='yes'\n    Description ='Monitoring and running the jobs'\n    Account=\"[SERVICEACCOUNT]\"\n    Password=\"[SERVICEPASSWORD]\">\n    <util:ServiceConfig\n        FirstFailureActionType=\"restart\"\n        SecondFailureActionType=\"restart\"\n        ThirdFailureActionType =\"restart\"\n        cRestartServiceDelayInSeconds =\"10\" />\n</ServiceInstall>\nAnd how do I set the Restart service time? I would like to set Restart service after 2 minutes if failed.\n\nPlace a ServiceConfig element within the ServiceInstall element.\n<ServiceConfig DelayedAutoStart=\"yes\" OnInstall=\"yes\" OnReinstall =\"yes\" />\nI put this in the same component as the ServiceInstall, and everything seems to work fine. I imagine you could do the same thing for the service restart time.\n", "document_id": 798}]}, {"paragraphs": [{"qas": [{"question": "How do I update a Tomcat webapp without restarting the entire service?", "id": 334, "answers": [{"answer_id": 342, "document_id": 146, "question_id": 334, "text": "You can try to use Tomcat's Manager application. It allows you to undeploy / deploy war files with out shutting Tomcat down.", "answer_start": 146, "answer_category": null}], "is_impossible": false}], "context": "I'm new to Tomcat. We have a dev machine with about 5 apps running. Even though it's dev, it's used by our clients pretty heavily during testing. You can try to use Tomcat's Manager application. It allows you to undeploy / deploy war files with out shutting Tomcat down. If you don't want to use the Manager application, you can also delete the war file from the webapps directory, Tomcat will undeploy the application after a short period of time. You can then copy a war file back into the directory, and Tomcat will deploy the war file.If you are running Tomcat on Windows, you may need to configure your Context to not lock various files.", "document_id": 146}]}, {"paragraphs": [{"qas": [{"question": "how to set application installer icon in javafx", "id": 1501, "answers": [{"answer_id": 1490, "document_id": 1080, "question_id": 1501, "text": "\n\nUpdated:\n\nYou have to take a look at A guide to the Gradle JavaFX Plugin which describes the Javafx packages are complete with cross-platform flair-like start menu integration, dock and tray icons, menu-bar integration, and single click icons. For that you've to Sign your files in the output folder if you plan to distribute the application, stated here in 7.3.5 using s", "answer_start": 2085, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI'm using the JavaFX-Gradle-plugin to build the distribute-able binaries and the installer of a JavaFX application. When my application runs, I'm able to set the icon this way:\n\nstage.getIcons().add(new Image(this.getClass().getResourceAsStream(\"/isotype.png\")));\n\n\nwhich correctly sets the icon for the running application:\n\n\n\nas well as the task bar:\n\n\n\nBut how do I set the icons for the start menu:\n\n\n\nand potentially other places:\n\n\n    \n\nThere is a open pull request documenting this here\n\nIt says:\n\n\n  Customize Icons\n  \n  To customize the icons used in a native bundle, you have to provide the icons for the appropriate bundle.\n  The icons must follow the file name convention in order to get picked up.\n  \n  \n    Tip: Set the verbose setting to true, to have log which files are picked up from your deploy directory.\n  \n\n\nand for Microsoft Windows in particular:\n\n\n  Windows\n  \n  Icon location: src/main/deploy/windows\n  \n  For Windows you can provide two different icons.\n  \n  \n  application icon\n  setup icon - the icon of the installer\n  \n  \n  | Type              | Filename                  |\n  | :---------------- |:------------------------- |\n  | .exe icon         | \\&lt;appName&gt;.ico            |\n  | setup exe icon    | \\&lt;appName&gt;-setup-icon.bmp |\n\n\nDespite what it says there, the correct path is src/main/deploy/packages/windows as show in the adjusted-launcher-icon example.\n    \n\nMaybe the path of your image (\"/isotype.png\") is incorrect. Choose one method to give correct path from below options. If icon image is stored:\n\n\nIn a folder (e.g. images) then use this path \"/images/isotype.png\" as like:\n\nstage.getIcons().add(\n      new Image(this.getClass().getResourceAsStream(\"/images/isotype.png\")));\n\nIn package directory then use this path \"isotype.png\" as like:\n\nstage.getIcons().add(new Image(this.getClass().getResourceAsStream(\"isotype.png\")));\n\nIn a folder structure then use this path \"../images/isotype.png\" as like:\n\nstage.getIcons().add(\n      new Image(this.getClass().getResourceAsStream(\"../images/isotype.png\"\")));\n\n\n\n\n\nUpdated:\n\nYou have to take a look at A guide to the Gradle JavaFX Plugin which describes the Javafx packages are complete with cross-platform flair-like start menu integration, dock and tray icons, menu-bar integration, and single click icons. For that you've to Sign your files in the output folder if you plan to distribute the application, stated here in 7.3.5 using signtool.exe.\n\nNow you have to some (icons) configuration options inside of the build.gradle as:\n\njavafx {\n    appID 'SampleApp'\n    appName 'Sample Application'\n    mainClass 'com.example.sample.Main'\n\n    jvmArgs = ['-XX:+AggressiveOpts', '-XX:CompileThreshold=1']\n    systemProperties = [ 'prism.disableRegionCaching':'true' ]\n    arguments = ['-l', '--fast']\n\n    embedLauncher = false\n\n    // deploy/info attributes\n    category = 'Demos'\n    copyright = 'Copyright (c) 2013 Acme'\n    description = 'This is a sample configuration, it is not real.'\n    licenseType = 'Apache 2.0'\n    vendor = 'Acme'\n    installSystemWide = true\n    menu = true\n    shortcut = true\n\n    // app icons\n    icons {\n        shortcut = ['shortcut-16.png', 'shortcut-32.png', 'shortcut-128.png', 'shortcut-256.png', 'shortcut-16@2x.png', 'shortcut-32@2x.png', 'shortcut-128@2x.png']\n        volume = 'javafx-icon.png'\n        setup = 'javafx-icon.png'\n    }\n\n    // applet and webstart stuff\n    debugKey {\n        alias = 'debugKey'\n        //keyPass = 'password' // provide via command line\n        keyStore = file('~/keys/debug.jks')\n        //storePass = 'password'  // provide via command line\n    }\n    releaseKey {\n        alias = 'production'\n        //keyPass = 'password' // provide via command line\n        keyStore = file('/Volumes/ProdThumbDrive/production.jks')\n        //storePass = 'password'  // provide via command line\n    }\n    signingMode 'release'\n\n    width = 800\n    height = 600\n    embedJNLP = false\n    codebase = 'http://example.com/bogus/JNLP/Codebase'\n\n    // arbitrary jnlp icons\n    icon {\n        href = 'src/main/resources/javafx-icon.png'\n        kind = 'splash'\n        width = 128\n        height = 128\n    }\n    icon {\n        href = 'shortcut-32@2x.png'\n        kind = 'selected'\n        width = 16\n        height = 16\n        scale = 1\n    }\n}\n\n    \n\nThe general procedure how to do that is documented here:\nhttps://github.com/BilledTrain380/javafx-gradle-plugin/blob/648acafa7198e9bd7cf1a2ef933456ce5e0b65f9/README.md#customize-icons\nbut lately I had problems with the latest version of the packager (actually the ant tasks) to get that working. Something seems to be broken there because it works with older (Java 8) versions of the packager but not with the recent ones. I was able however to get it working by explicitly specifiying\n\n&lt;fx:bundleArgument arg=\"icon\" value=\"package/macosx/myapp.icns\"/&gt;\n\n\ninside the fx:deploy section. I don't know how do that in Gradle because I used ant but you should be able to find that out. In older versions of the packager this was not necessary.\n    \n\nif you are using ant build or artifact to build javafx application follow the post might help \n\nhttps://flaironix.com/2019/09/18/adding-custom-icon-for-javafx-application-exe-file-in-intelije/\n\nussing this option tag in artifact \n\n&lt;option name=\"icons\"&gt;\n    &lt;JavaFxApplicationIcons&gt;\n    &lt;option name=\"linuxIcon\" value=\"$PROJECT_DIR$/src/Controller/logo.png\" /&gt;\n    &lt;option name=\"windowsIcon\" value=\"$PROJECT_DIR$/src/Controller/logo.ico\"/&gt;\n   &lt;/JavaFxApplicationIcons&gt;\n&lt;/option&gt;\n\n    ", "document_id": 1080}]}, {"paragraphs": [{"qas": [{"question": "Getting error while trying to run this command \" pipenv install requests \" in mac OS", "id": 1742, "answers": [{"answer_id": 1729, "document_id": 1314, "question_id": 1742, "text": "you must add those lines to your ~/.zshrc\nexport LANG=\"en_US.UTF-8\"\nexport LC_ALL=\"en_US.UTF-8\"\nexport LC_CTYPE=\"en_US.UTF-8\"\nThen I reload the bash profile with: source ~/.zshrc", "answer_start": 395, "answer_category": null}], "is_impossible": false}], "context": "I am facing the following error:\nWarning: the environment variable LANG is not set!\nWe recommend setting this in ~/.profile (or equivalent) for proper expected behavior.\nCreating a virtualenv for this project\u2026\nUsing /usr/local/opt/python/bin/python3.6 (3.6.4) to create virtualenv\u2026\nTraceback (most recent call last):\nI tried setting the LANG in ~/.profile and ~/.bash_profile. Both didn't work.\nyou must add those lines to your ~/.zshrc\nexport LANG=\"en_US.UTF-8\"\nexport LC_ALL=\"en_US.UTF-8\"\nexport LC_CTYPE=\"en_US.UTF-8\"\nThen I reload the bash profile with: source ~/.zshrc\n", "document_id": 1314}]}, {"paragraphs": [{"qas": [{"question": "windows service wont automatically start after reboot", "id": 1416, "answers": [{"answer_id": 1405, "document_id": 992, "question_id": 1416, "text": " I set the W32 service to delay automatic and it solved the issue", "answer_start": 2154, "answer_category": null}], "is_impossible": false}], "context": "\n\n                    \n\n            \n\n        \n\n            \n\n                \n\n                    \n\n                        This question already has answers here:\n\n                        \n\n                    \n\n                \n\n            \n\n                    \n\n                        \"A timeout was reached while waiting for the service to connect\" error after rebooting\n\n                            \n\n                                (6 answers)\n\n                            \n\n                    \n\n                Closed 8 years ago.\n\n        \n\n\n\n\n\n    \n\n\n\nMy automatically starting windows service fails to start only on reboot.  I have a windows service created in C# and installed via a Wix created installer. The service is set up to start automatically. The service is installed and run under the NT AUTHORITY\\NETWORK SERVICE.  When the service is started, it first makes an external web services call. \n\nIn Windows 7 I can set the service to be  Automatic - Delayed start and the service will start on reboot no problem. However, this option is not available in Windows XP, and when set to Automatic start, the service fails due to \n\nA timeout was reached (30000 milliseconds) while waiting for the MyService service to connect.\n\nIf I try to start manually after the login process, the service starts fine, it is only when the service tries to auto start on reboot that there is an issue, leading me to believe there are dependency services that I need to add to my service for it to start correctly.  \n\nCan anyone point me to the correct dependencies or an alternative approach?\n    \n\nYou likely have a race condition with a dependency.  You could probably patch around this by configuring your service to have a dependency on another service ( say tcp/ip )  but what I'd really do is rewrite your service to not need to make this call during the criticial execution path of startup.  It should instead periodically attempt to make the webservice call at a later point and log useful messages or send messages to a taskbar utility or similar if there is a problem that needs to be addressed.\n    \n\nI had the same issue on 4 new servers.  I set the W32 service to delay automatic and it solved the issue.\n    ", "document_id": 992}]}, {"paragraphs": [{"qas": [{"question": "warnings and errors after trying to install flask 0 9", "id": 1496, "answers": [{"answer_id": 1485, "document_id": 1075, "question_id": 1496, "text": "all.\n\nUsing a virtual environment:\n\n$ virtualenv flask_env\n$ source flask_env/bin/activate\n(flask_env) $ pip install", "answer_start": 4437, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI'm trying ot install Flask, but I'm betting all these warnings and errors:\n\nalex@alex-K43U:~/flask$ pip install Flask\nDownloading/unpacking Flask\n  Downloading Flask-0.9.tar.gz (481Kb): 481Kb downloaded\n  Running setup.py egg_info for package Flask\n\n    warning: no files found matching '*' under directory 'tests'\n    warning: no previously-included files matching '*.pyc' found under directory 'docs'\n    warning: no previously-included files matching '*.pyo' found under directory 'docs'\n    warning: no previously-included files matching '*.pyc' found under directory 'tests'\n    warning: no previously-included files matching '*.pyo' found under directory 'tests'\n    warning: no previously-included files matching '*.pyc' found under directory 'examples'\n    warning: no previously-included files matching '*.pyo' found under directory 'examples'\n    no previously-included directories found matching 'docs/_build'\n    no previously-included directories found matching 'docs/_themes/.git'\nDownloading/unpacking Werkzeug&gt;=0.7 (from Flask)\n  Downloading Werkzeug-0.8.3.tar.gz (1.1Mb): 1.1Mb downloaded\n  Running setup.py egg_info for package Werkzeug\n\n    warning: no files found matching '*' under directory 'werkzeug/debug/templates'\n    warning: no files found matching '*' under directory 'tests'\n    warning: no previously-included files matching '*.pyc' found under directory 'docs'\n    warning: no previously-included files matching '*.pyo' found under directory 'docs'\n    warning: no previously-included files matching '*.pyc' found under directory 'tests'\n    warning: no previously-included files matching '*.pyo' found under directory 'tests'\n    warning: no previously-included files matching '*.pyc' found under directory 'examples'\n    warning: no previously-included files matching '*.pyo' found under directory 'examples'\n    no previously-included directories found matching 'docs/_build'\nDownloading/unpacking Jinja2&gt;=2.4 (from Flask)\n  Downloading Jinja2-2.6.tar.gz (389Kb): 389Kb downloaded\n  Running setup.py egg_info for package Jinja2\n\n    warning: no previously-included files matching '*' found under directory 'docs/_build'\n    warning: no previously-included files matching '*.pyc' found under directory 'jinja2'\n    warning: no previously-included files matching '*.pyc' found under directory 'docs'\n    warning: no previously-included files matching '*.pyo' found under directory 'jinja2'\n    warning: no previously-included files matching '*.pyo' found under directory 'docs'\nInstalling collected packages: Flask, Werkzeug, Jinja2\n  Running setup.py install for Flask\n\n    warning: no files found matching '*' under directory 'tests'\n    warning: no previously-included files matching '*.pyc' found under directory 'docs'\n    warning: no previously-included files matching '*.pyo' found under directory 'docs'\n    warning: no previously-included files matching '*.pyc' found under directory 'tests'\n    warning: no previously-included files matching '*.pyo' found under directory 'tests'\n    warning: no previously-included files matching '*.pyc' found under directory 'examples'\n    warning: no previously-included files matching '*.pyo' found under directory 'examples'\n    no previously-included directories found matching 'docs/_build'\n    no previously-included directories found matching 'docs/_themes/.git'\n    error: could not create '/usr/local/lib/python2.7/dist-packages/flask': Permission denied\n    Complete output from command /usr/bin/python -c \"import setuptools;__file__='/home/alex/flask/build/Flask/setup.py';exec(compile(open(__file__).read().replace('\\r\\n', '\\n'), __file__, 'exec'))\" install --single-version-externally-managed --record /tmp/pip-KD7NsY-record/install-record.txt:\n    running install\n\nrunning build\n\n(a lot of creating and building)\n\nerror: could not create '/usr/local/lib/python2.7/dist-packages/flask': Permission denied\n\n\nAny suggestions to solve this problem?\n\nI'm using ubuntu 11.10.\n    \n\nThe warnings you can safely ignore; however this error:\n\nerror: could not create '/usr/local/lib/python2.7/dist-packages/flask': Permission denied\n\nTells me that you are trying to install this in to your global system Python. Nothing wrong with that, but if you want to do that you need to run the command with elevated privileges (using sudo).\n\nIt is better to use a virtual environment so that you don't pollute the system-wide Python install.\n\nUsing a virtual environment:\n\n$ virtualenv flask_env\n$ source flask_env/bin/activate\n(flask_env) $ pip install Flask\n\n\nYou should probably install the virtualenv binaries first with sudo apt-get install python-virtualenv\n    \n\nAs for the warnings, they can sometimes be ignored. The only relevant line is the last one, which says the application does not have permission to create a directory in that folder.\n\nAdd sudo to your command to fix this.\n\nsudo pip install Flask\n\n\nAs a general rule, you don't want to install packages system wide. In the Python world the norm is using virtual env to create a local environment, and install packages into each of these. You can find some more information on virtualenv here.\n    ", "document_id": 1075}]}, {"paragraphs": [{"qas": [{"question": "Xcode 7: App installation failed: A valid provisioning profile for this executable was not found", "id": 694, "answers": [{"answer_id": 698, "document_id": 386, "question_id": 694, "text": "First, go to ~/Library/MobileDevice/Provisioning Profiles. Make sure Xcode isn't running. Then, delete all provisioning files (like xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx.mobileprovision).\nStart Xcode.\nYou will see something like \"Fix this issue\" in your Target's General tab. Click it.\nXcode will now load new provisioning profile.", "answer_start": 263, "answer_category": null}], "is_impossible": false}], "context": "I have already searched and almost implemented max solution but it's not installing any app even though if I am creating just sample single view app.App installation failed\nA valid provisioning profile for this executable was not found. I fixed this issue today.\nFirst, go to ~/Library/MobileDevice/Provisioning Profiles. Make sure Xcode isn't running. Then, delete all provisioning files (like xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx.mobileprovision).\nStart Xcode.\nYou will see something like \"Fix this issue\" in your Target's General tab. Click it.\nXcode will now load new provisioning profile.\nThat's it.  In my case, my provisioning profile had the wrong UDIDs but right machine count.\nI've registered machines, and Apple Developer Center listed right UDIDs for those machines.\nHowever, whenever I downloaded new provisioning profile, it contained wrong UDIDs.\n", "document_id": 386}]}, {"paragraphs": [{"qas": [{"question": "Install windows service without InstallUtil.exe", "id": 1154, "answers": [{"answer_id": 1147, "document_id": 731, "question_id": 1154, "text": "On your server, open a command prompt as administrator then:\nCD C:\\Windows\\Microsoft.NET\\Framework\\v4.0.version (insert your version)\n\ninstallutil \"C:\\Program Files\\YourWindowsService\\YourWindowsService.exe\" (insert your service name/location)", "answer_start": 545, "answer_category": null}], "is_impossible": false}], "context": "I'm trying to deploy a windows service but not quite sure how to do it right. I built it as a console app to start with, I've now turned it into a windows service project and just call my class from the OnStart method in the service.\nI now need to install this on a server which doesn't have Visual Studio on it, which if I've understood it correctly means I can't use the InstallUtil.exe and have to create an installer class instead. Is this correct?\nYou can still use installutil without visual studio, it is included with the .net framework\nOn your server, open a command prompt as administrator then:\nCD C:\\Windows\\Microsoft.NET\\Framework\\v4.0.version (insert your version)\n\ninstallutil \"C:\\Program Files\\YourWindowsService\\YourWindowsService.exe\" (insert your service name/location)\n", "document_id": 731}]}, {"paragraphs": [{"qas": [{"question": "Why updating of dependencies in composer is so slow?", "id": 1902, "answers": [{"answer_id": 1889, "document_id": 1473, "question_id": 1902, "text": "You can try to specify a version for each dependency in composer.json and use the option --prefer-dist when calling composer. It will download ZIP files from the repositories (if available) instead of the single files.\nphp composer.phar install --prefer-dist\n", "answer_start": 501, "answer_category": null}], "is_impossible": false}], "context": "I am using composer (http://getcomposer.org/) to manage installed bundles in the Symfony2 (symfony v 2.1.3). Version of the composer is de3188c.\nI have problem that when I add new bundle into the composer.json and execute it the time to show messages about Updating dependencies and next downloading them all is very low.\nI have this data in the composer.json (see below) and the executing time is approximately 20 MINUTES!!! The internet connection is fast enough I can download big files very fast.\nYou can try to specify a version for each dependency in composer.json and use the option --prefer-dist when calling composer. It will download ZIP files from the repositories (if available) instead of the single files.\nphp composer.phar install --prefer-dist\n\n", "document_id": 1473}]}, {"paragraphs": [{"qas": [{"question": "Python - manually install package using virtualenv", "id": 682, "answers": [{"answer_id": 687, "document_id": 375, "question_id": 682, "text": "use this (from your temporary folder)\n/home/username/virtualpy/bin/python setup.py install", "answer_start": 995, "answer_category": null}], "is_impossible": false}], "context": "So - does it matter where I unzip the python package/program to - or should I be logged in to the virtualenv first before unzipping? After I load up the virtualenv and I'm inside using it with the 'workon test' command, will any python package I install, regardless of the directory I find it, install itself into the proper virtualenv's site-packages folder?\nOption 1 is to unzip the python program into /home/username/tmp - then log into my virtualenv, navigate to that folder and run the setup.py program - assuming that the virtualenv will transfer all relevant files to it's own site-packages folder.\nOR scenario 2 is to unzip the files directly into site-packages, and run it from there (after logging in to the virtualenv), etc\nThank you for helping a Python clutz with this! I typically would extract the program to a temporary folder, then from that folder, run the setup.py using the direct path to the virtualenv python instance. eg if your virtualenv is in /home/username/virtualpy, use this (from your temporary folder)\n/home/username/virtualpy/bin/python setup.py install\nThis should install it to your virtualenv site package folder.\n", "document_id": 375}]}, {"paragraphs": [{"qas": [{"question": "Installing OpenSSH on the Alpine Docker Container", "id": 680, "answers": [{"answer_id": 685, "document_id": 373, "question_id": 680, "text": "Run apk update first\napk add --no-cache openssh", "answer_start": 179, "answer_category": null}], "is_impossible": false}], "context": "In the new version, If you want to install something without caching things locally, which is recommended for keeping your containers small, include the --no-cache flag. Example:\nRun apk update first\napk add --no-cache openssh\nThis is a small gain, it keeps you from having the common rm -rf /var/cache/apk/* at the end of your Dockerfile.\n", "document_id": 373}]}, {"paragraphs": [{"qas": [{"question": "How should I deal with \"package 'xxx' is not available (for R version x.y.z)\" warning?", "id": 1614, "answers": [{"answer_id": 1601, "document_id": 1188, "question_id": 1614, "text": "You need to do is just:\ninstall.packages('package-name',repos='http://cran.us.r-project.", "answer_start": 302, "answer_category": null}], "is_impossible": false}], "context": "I tried to install a package, using\ninstall.packages(\"foobarbaz\")\nbut received the warning\nWarning message:\npackage 'foobarbaz' is not available (for R version x.y.z)\nWhy doesn't R think that the package is available?\nThis solution might break R but here is an easiest solution that works 99% of time.\nYou need to do is just:\ninstall.packages('package-name',repos='http://cran.us.r-project.\n", "document_id": 1188}]}, {"paragraphs": [{"qas": [{"question": "What is cmake_install.cmake", "id": 1197, "answers": [{"answer_id": 1190, "document_id": 773, "question_id": 1197, "text": "the cmake_install.cmake contains the commands generated by install command from your CMakeLists.txt.\nYou can execute it by cmake -P cmake_install.cmake and it performs the installation of your project even on windows", "answer_start": 265, "answer_category": null}], "is_impossible": false}], "context": "I wrote a very simple HelloWorld.c program and ran Cmake. It created a cmake_install.cmake file in my build directory. Can somebody explain to me why CMake generated the file cmake_install.cmake? What is it's purpose and how can I use it? As previous answer tells, the cmake_install.cmake contains the commands generated by install command from your CMakeLists.txt.\nYou can execute it by cmake -P cmake_install.cmake and it performs the installation of your project even on windows.\n", "document_id": 773}]}, {"paragraphs": [{"qas": [{"question": "ServiceController:InvalidOperationException", "id": 967, "answers": [{"answer_id": 962, "document_id": 595, "question_id": 967, "text": "The service was not found.", "answer_start": 505, "answer_category": null}], "is_impossible": false}, {"question": "What does ServiceController.Start method do?", "id": 968, "answers": [{"answer_id": 963, "document_id": 595, "question_id": 968, "text": "Starts the service.\n", "answer_start": 161, "answer_category": null}], "is_impossible": false}], "context": "ServiceController.Start Method\nReference\nIs this page helpful?\nDefinition\nNamespace:\nSystem.ServiceProcess\nAssembly:\nSystem.ServiceProcess.ServiceController.dll\nStarts the service.\n\nOverloads\nOVERLOADS\nStart()\t\nStarts the service, passing no arguments.\n\nStart(String[])\t\nStarts a service, passing the specified arguments.\n\nStart()\nStarts the service, passing no arguments.\n\nC#\n\nCopy\npublic void Start ();\nExceptions\nWin32Exception\nAn error occurred when accessing a system API.\n\nInvalidOperationException\nThe service was not found.\n\nExamples\nThe following example uses the ServiceController class to check whether the Alerter service is stopped. If the service is stopped, the example starts the service and waits until the service status is set to Running.\n\nC#\n\nCopy\n\n// Check whether the Alerter service is started.\n\nServiceController sc  = new ServiceController();\nsc.ServiceName = \"Alerter\";\nConsole.WriteLine(\"The Alerter service status is currently set to {0}\",\n                   sc.Status.ToString());\n\nif (sc.Status == ServiceControllerStatus.Stopped)\n{\n   // Start the service if the current status is stopped.\n\n   Console.WriteLine(\"Starting the Alerter service...\");\n   try\n   {\n      // Start the service, and wait until its status is \"Running\".\n      sc.Start();\n      sc.WaitForStatus(ServiceControllerStatus.Running);\n\n      // Display the current service status.\n      Console.WriteLine(\"The Alerter service status is now set to {0}.\",\n                         sc.Status.ToString());\n   }\n   catch (InvalidOperationException)\n   {\n      Console.WriteLine(\"Could not start the Alerter service.\");\n   }\n}\nRemarks\nYou cannot call Stop for the service until the service controller status is Running.\n\nSee also\nStop()\nStatus\nApplies to\n.NET Core 1.1 and other versions\nStart(String[])\nStarts a service, passing the specified arguments.\n\nC#\n\nCopy\npublic void Start (string[] args);\nParameters\nargs\nString[]\nAn array of arguments to pass to the service when it starts.\n\nExceptions\nWin32Exception\nAn error occurred when accessing a system API.\n\nInvalidOperationException\nThe service cannot be started.\n\nArgumentNullException\nargs is null.\n\n-or-\n\nA member of the array is null.\n\nRemarks\nYou cannot call Stop for the service until the service controller status is Running.\n\nSee also\nStop()\nStatus\nApplies to\n.NET Core 1.1 and other versions\nRecommended content\nServiceController.Stop Method (System.ServiceProcess)\nStops this service and any services that are dependent on this service.\nServiceController Class (System.ServiceProcess)\nRepresents a Windows service and allows you to connect to a running or stopped service, manipulate it, or get information about it.\n", "document_id": 595}]}, {"paragraphs": [{"qas": [{"question": "What is the foolproof way to tell which version(s) of .NET are installed on a production Windows Server?", "id": 1675, "answers": [{"answer_id": 1662, "document_id": 1248, "question_id": 1675, "text": "You can programmatically check the registry and a few other things as per this blog entry.\nThe registry key to look at is\n[HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\NET Framework Setup\\ND", "answer_start": 2080, "answer_category": null}], "is_impossible": false}], "context": "This question is not so much programming related as it is deployment related.\nI find myself conversing a lot with the group in my company whose job it is to maintain our production Windows servers and deploy our code on them. For legal and compliance reasons, I do not have direct visibility or any control over the servers so the only way I can tell which version(s) of .NET are installed on any of them is through directions I give to that group.\nSo far, all of the methods I can think of to tell which version(s) are installed (check for Administrative Tools matching 1.1 or 2.0, check for the entries in the \"Add/Remove Programs\" list, check for the existence of the directories under c:\\Windows\\Microsoft.NET) are flawed (I've seen at least one machine with 2.0 but no 2.0 entries under Administrative Tools - and that method tells you nothing about 3.0+, the \"Add/Remove Programs\" list can get out of sync with reality, and the existence of the directories doesn't necessarily mean anything).\nGiven that I generally need to know these things are in place in advance (discovering that \"oops, this one doesn't have all the versions and service packs you need\" doesn't really work well with short maintenance windows) and I have to do the checking \"by proxy\" since I can't get on the servers directly, what's the foolproof way to tell which version(s) of .NET are installed on a production Windows Server? Preferably some intrinsic way to do so using what the framework installs since it will be quicker and not need some sort of utility to be loaded and also a method which will definitely fail if the frameworks are not properly installed but still have files in place (i.e., there's a directory and gacutil.exe is inded there but that version of the framework is not really \"installed\")\nEDIT: In the absence of a good foolproof intrinsic way to do this built into the Framework(s), does anyone know of a good, lightweight, no-install-required program that can find this out? I can imagine someone could easily write one but if one already exists, that would be even better.\nYou can programmatically check the registry and a few other things as per this blog entry.\nThe registry key to look at is\n[HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\NET Framework Setup\\ND\n", "document_id": 1248}]}, {"paragraphs": [{"qas": [{"question": "Jenkins: Multiple Git repositories for one project", "id": 1728, "answers": [{"answer_id": 1716, "document_id": 1301, "question_id": 1728, "text": "Just use Multiple SCMs under Source Code Management, add your repositories and then go to the Advanced section of each repository. Here you need to set Local subdirectory for repo (optional) and Unique SCM name (optional).", "answer_start": 447, "answer_category": null}], "is_impossible": false}], "context": "I want to build a project using two Git repositories. One of them contains of the source code, while the other has the build and deployment scripts.\nMy problem is that I need to have a repository for building and deployment of different parts of the project (big project, multiple repositories, same build and deployment scripts), but Jenkins does not seem to be able to handle this (or I don't know/didn't find how). es, Jenkins can handle this. Just use Multiple SCMs under Source Code Management, add your repositories and then go to the Advanced section of each repository. Here you need to set Local subdirectory for repo (optional) and Unique SCM name (optional).\nYour repository will be pulled to the Local subdirectory which you have set so then you can build them in any order you want.\nUpdating per harishs answer - you need to install Multiple SCMs Plugin in order to achieve this functionality.\n\n\n", "document_id": 1301}]}, {"paragraphs": [{"qas": [{"question": "another mysqld server running on port 3306 error", "id": 1935, "answers": [{"answer_id": 1922, "document_id": 1513, "question_id": 1935, "text": "use lsof -i TCP:3306 to check which program binds port 3306", "answer_start": 1692, "answer_category": null}], "is_impossible": false}], "context": "\n\n                    \n\n            \n\n        \n\n            \n\n                \n\n                    \n\n                        Closed. This question is off-topic. It is not currently accepting answers.\n\n                        \n\n                    \n\n                \n\n            \n\n        \n\n            \n\n        \n\n                \n\n                    \n\n                \n\n            \n\n                \n\n                    Want to improve this question? Update the question so it's on-topic for Stack Overflow.\n\n                \n\n                    Closed 9 years ago.\n\n\n\n            \n\n        \n\n            \n\n                    \n\n                        Improve this question\n\n                    \n\n            \n\n\n\n\n\n    \n\n\n\nI have installed Mysql 5.1 on Mac OS X 10.7 Lion. For some reason, though, when I try starting the server with the command \"mysqld\" I get an error in the log file which says:\n\n120328 21:32:40 [ERROR] Can't start server: Bind on TCP/IP port: Address already in use\n\n120328 21:32:40 [ERROR] Do you already have another mysqld server running on port: 3306 ?\n\n120328 21:32:40 [ERROR] Aborting\n\nIf I run \"netstat -nat | grep 3306\" in my terminal, I get the following:\ntcp4       0      0  *.3306                 .                    LISTEN\n\nUPDATE:\n\nSo here's the output for that. \nmysqld  24645 sb1752   12u  IPv4 0xffffff8010f6bde0      0t0  TCP *:mysql (LISTEN)\n\nThis is odd though! Because my mysql server is not started.\nWhen I type \"mysql\" in command line, it says \nERROR 2002 (HY000): Can't connect to local MySQL server through socket '/tmp/mysql.sock' (2)\n\nI did install other version of mysql and uninstalled earlier today. Any idea what to do here?\n    \n\nuse lsof -i TCP:3306 to check which program binds port 3306\n    \n\nYou could use netstat -lp | grep 3306 to find out what program is already listening on port 3306 (you should see PID/Program name in last column) and stop that (maybe mysql is already running?).\n\nAlternatively you could start the newly installed server on a different port. (edit my.cnf and change the default port there)\n    ", "document_id": 1513}]}, {"paragraphs": [{"qas": [{"question": "how to show a hyperlink in inno setup", "id": 1386, "answers": [{"answer_id": 1375, "document_id": 958, "question_id": 1386, "text": "on, but you could create a custom page instead which  checks whether the update has been installed, and otherwise prevents navigation to the n", "answer_start": 1519, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nI'm making a validation in my Inno Setup installer to check whether or not a Microsoft update is installed on the machine, if not, I'm showing a simple message box telling the user that the update is required, this is the message code:\n\nMsgBox(\n  'Your system requires an update supplied by Microsoft. ' +\n  'Please follow this link to install it: ' + \n  'http://www.microsoft.com/downloads/details.aspx?FamilyID=1B0BFB35-C252-43CC-8A2A-6A64D6AC4670&amp;displaylang=en',\n  mbInformation, MB_OK);\n\n\nI want to make the URL an hyperlink to the web page, but I haven't been able to figure it out how, it is possible in Inno Setup to add text as an hyperlink?\n\nThanks.\n    \n\nThe MsgBox() function in Inno Setup is a wrapper for the standard Windows MessageBox() function, which AFAIK doesn't support embedded links, so it's not possible to simply show the link there.\n\nWhat you could do however is to notify the user that the update is required, and ask them whether to open the link in the default browser. Something like:\n\nfunction InitializeSetup(): Boolean;\nvar\n  ErrCode: integer;\nbegin\n  if MsgBox('Your system requires an update supplied by Microsoft. Would you like to visit the download page now?', mbConfirmation, MB_YESNO) = IDYES\n  then begin\n    ShellExec('open', 'http://www.microsoft.com/downloads/details.aspx?FamilyID=1B0BFB35-C252-43CC-8A2A-6A64D6AC4670&amp;displaylang=en',\n      '', '', SW_SHOW, ewNoWait, ErrCode);\n  end;\n  Result := False;\nend;\n\n\nThis code will abort the installation, but you could create a custom page instead which  checks whether the update has been installed, and otherwise prevents navigation to the next page. This would only work if the update can be installed without a system restart, though.\n    ", "document_id": 958}]}, {"paragraphs": [{"qas": [{"question": "How to install Laravel via Laravel Installer on Windows?", "id": 718, "answers": [{"answer_id": 721, "document_id": 408, "question_id": 718, "text": "Install WAMP first.\n2.\tEnable OpenSSL and all required extensions in PHP.ini\n3.\tInstall Composer\n4.\tMake sure that your htppd.conf includes httpd-vhosts.conf\n5.\tInstall Laravel in specified folder\n6.\tEnable Laravel installer\n7.\tCreate Virtual Host\n8.\tUpdate Windows Hosts file", "answer_start": 21, "answer_category": null}], "is_impossible": false}], "context": "To do as follows:\n1.\tInstall WAMP first.\n2.\tEnable OpenSSL and all required extensions in PHP.ini\n3.\tInstall Composer\n4.\tMake sure that your htppd.conf includes httpd-vhosts.conf\n5.\tInstall Laravel in specified folder\n6.\tEnable Laravel installer\n7.\tCreate Virtual Host\n8.\tUpdate Windows Hosts file\nYou\u2019re done! Creating a new project with the Laravel Installer took only a couple of seconds. The former composer installation method needed ages compared to the Installer. That's the reason I posted this Q&A to provide the steps required to make it work for others using Windows, as at the time of writing the Laravel documentation does not provide windows specific instructions. \n", "document_id": 408}]}, {"paragraphs": [{"qas": [{"question": "application installation failed in android studio", "id": 1507, "answers": [{"answer_id": 1496, "document_id": 1085, "question_id": 1507, "text": "** For MAC OSX/Windows ****\nGoto *\n\nAndroid Studio - Preferences (In OSX)\nAndroid Studio - File -&gt; Settings (In Windows)\n\n\nin the menu bar then select\n\n\nBuild,Execution,Deployment - Instant run\n\n\nth", "answer_start": 2267, "answer_category": null}], "is_impossible": false}], "context": "\n\n                \n\nYesterday my app was running perfect from Android Studio but today when I started working on my app and running it i am getting error message continuously\n\n\n  Installation failed with message Failed to establish session.\n\n\nScreen :\n\n\n\nOn click OK getting error message \n\n\n  Session 'app':Error Installing APKs\n\n\nAnd App is not exist(already Uninstalled) in device. Please suggest me what to do ?\n    \n\nAgain in this issue also I found Instant Run buggy. When I disable the Instant run and run the app again App starts successfully installing in the Device without showing any error Window. \nI hope google will sort out these Issues with Instant run soon.\n\nSteps to disable Instant Run from Android Studio:\n\n\n  File &gt; Settings &gt; Build,Execution,Deployment &gt; Instant Run &gt; Un-check\n  (Enable Instant Run to hot swap code)\n\n    \n\nIf you use MIUI ROM\n\nGo to the developer option and in that disable MIUI optimization.You will be asked to reboot your phone.\nReboot it and then run the app.\n    \n\nYour APK file is missing . So , Clean Project &gt;&gt; Build APK &gt;&gt; Run the project .\n    \n\nFor those who uses Xiaomi phones, follow these steps:\n\n\nSettings-&gt; Additional Settings-&gt; Developer options\nTurn off MIUI Optimization and reboot your phone\nLast Disable verify app over USB\n\n\nYour device will respond properly.\n\nAlso enable install via USB\n    \n\nI had the same issue in Android studio 2.3 when I tried to test the app using Xiaomi's Mi5 and Mi4 phones. Disabling instant run didn't help me. So here is what I did.\n\nTurn Off MIUI optimization in the Developer Options in the phone.\n\n\n\n\n\n\nThen the device will be rebooted and then you'll be able to test the app over the phone.\n\n\nUsing this method you can still use instant run option in android studio. So this will fix your problem at least temporary. Hope that we'll be able to use MIUI optimization in the near future updates :)  \n    \n\nIn my case, it was because my emulator ran out of disk space.\n    \n\nIn my case, it was very a silly and funny mistake. I, accidentally without actually knowing, checked \"Android Debug Bridge, Use libUsb backend\". Actually it should stay unchecked..\n\nIn Mac Pro, Go Preferences - &gt; Build, Execution,Deployment -&gt; Debugger.\n    \n\n**** For MAC OSX/Windows ****\nGoto *\n\nAndroid Studio - Preferences (In OSX)\nAndroid Studio - File -&gt; Settings (In Windows)\n\n\nin the menu bar then select\n\n\nBuild,Execution,Deployment - Instant run\n\n\nthen uncheck it and rebuild it will works\n\n    \n\nFinally I've SOLVED it!\n\nBelow a temporary solution. Issue was reported to Google.\n\nFirst of all I found in Run log that Android Studion 2.3 tries to install app-debug.apk from many slices, like this:\n\n\n  $ adb install-multiple -r\n  E:\\Android_Projects\\ActivityLifecycle\\app\\build\\intermediates\\split-apk\\debug\\slices\\slice_1.apk\n  E:\\Android_Projects\\ActivityLifecycle\\app\\build\\intermediates\\split-apk\\debug\\dep\\dependencies.apk\n  E:\\Android_Projects\\ActivityLifecycle\\app\\build\\intermediates\\split-apk\\debug\\slices\\slice_0.apk\n  E:\\Android_Projects\\ActivityLifecycle\\app\\build\\intermediates\\split-apk\\debug\\slices\\slice_2.apk\n  E:\\Android_Projects\\ActivityLifecycle\\app\\build\\intermediates\\split-apk\\debug\\slices\\slice_9.apk\n  E:\\Android_Projects\\ActivityLifecycle\\app\\build\\intermediates\\split-apk\\debug\\slices\\slice_4.apk\n  E:\\Android_Projects\\ActivityLifecycle\\app\\build\\intermediates\\split-apk\\debug\\slices\\slice_3.apk\n  E:\\Android_Projects\\ActivityLifecycle\\app\\build\\intermediates\\split-apk\\debug\\slices\\slice_5.apk\n  E:\\Android_Projects\\ActivityLifecycle\\app\\build\\intermediates\\split-apk\\debug\\slices\\slice_8.apk\n  E:\\Android_Projects\\ActivityLifecycle\\app\\build\\intermediates\\split-apk\\debug\\slices\\slice_7.apk\n  E:\\Android_Projects\\ActivityLifecycle\\app\\build\\intermediates\\split-apk\\debug\\slices\\slice_6.apk\n  E:\\Android_Projects\\ActivityLifecycle\\app\\build\\outputs\\apk\\app-debug.apk\n\n\nThen I tried to install only app-debug.apk from command line by: \n\n\n  adb install -d E:\\Android_Projects\\ActivityLifecycle\\app\\build\\outputs\\apk\\app-debug.apk\n\n\nApp was installed successfully but was failed to run on my phone.\n\nAnd finally:\n\n\nI recompiled app-debug.apk from command line as:\n\n\n\n  gradlew.bat assembleDebug\n\n\n\nRepeat installation of app-debug.apk from command line and became happy:\n\n\n\n  adb install -rd E:\\Android_Projects\\ActivityLifecycle\\app\\build\\outputs\\apk\\app-debug.apk\n\n\nThis is definitely some gradle problem in AndroidStudio 2.3.\n    \n\nGo to Build --&gt; Clean Project --&gt; Run \n\nThats all it takes.\n    \n\nI had the same issue in MIUI. Enabling OEM unlocking worked for me without disabling MIUI optimization.\n\nBelow is a screenshot of my Redmi 3s prime developer options setting:\n\n\n    \n\nIn my own case, it was because my phone was out of space. For people that are facing this problem right now, if Clean Project + Build APKs does not work, check the available space on your phone or emulator.\n\nI hope this helps.. Merry coding!\n    \n\n\n  Just do the following step...\n\n\nBuild&gt;Clean Project\n\nafter that Run project again, this worked for me\n    \n\nI found the solution go to\n\nsettings&gt;build,execute,deployment&gt;instant run&gt;Enable instant run to hot swap code /resource change on deploy(unchecked this option)\n\n\nThis will work on 3.4 android studio too. thanks\n    \n\nTry disabling the Instant run in Settings.\n    \n\nEasily can be solved this problem.\n\nEx:- in Huawei GR3 mobile,\n\nGoto Setting in your mobile -&gt; Storage -&gt; Storage Cleaner\n    \n\nChange your applicationId in the android/app/build.gradle file.\n\nFor example:\n\n// Change this\napplicationId \"com.example.myAndroidApp\"\n//\n// to this\napplicationId \"com.example.somethingElse\"\n\n\nThen Sync your gradle then you can able to install your app,\nif the previous applicationId is your production id the again change it the previous one now the device will allow to install the app.\n\nHope this may help you....\n    \n\nThis is caused by \"instant run\" feature, you can disable it by:\n\nOpen the Settings or Preferences dialog. Navigate to Build, Execution, Deployment &gt; Instant Run.\n\nUncheck the box next to Enable Instant Run and u are ready to go.\n    \n\nI also had the problem after globally changing the project name, applicationid  and the folders containing the java files. \n\nDisabling Instant run helped, but was not a good option, so this helped:\n\n\nclose Android Studio \ndeleted those files and folders: rm -Rf .gradle .tags local.properties .idea/workspace.xml .idea/caches/* .idea/libraries app/build\nstart Android Studio and let it resync everything\npress run\n\n    ", "document_id": 1085}]}, {"paragraphs": [{"qas": [{"question": "How to download CA certificate?", "id": 917, "answers": [{"answer_id": 912, "document_id": 572, "question_id": 917, "text": "curl -k -o \"cacert-root.crt\"   \"https://www.cacert.org/certs/root_X0F.crt\"\ncurl -k -o \"cacert-class3.crt\" \"https://www.cacert.org/certs/class3_x14E228.crt\"", "answer_start": 2601, "answer_category": null}], "is_impossible": false}, {"question": "How to install trusted CA certificate on Android device?", "id": 918, "answers": [{"answer_id": 913, "document_id": 572, "question_id": 918, "text": " system trusted certificates are on the (read-only) system partition in the folder '/system/etc/security/' as individual files. ", "answer_start": 13953, "answer_category": null}], "is_impossible": false}], "context": " \nCAcert Wiki\n\u641c\u7d22\n  \n\u767b\u5f55\nFAQImportRootCert\nRecentChangesFindPageHelpContentsFAQ/ImportRootCert\n\u53ea\u8bfb\u7f51\u9875\u4fe1\u606f\u9644\u4ef6 \n\u66f4\u591a\u64cd\u4f5c\uff1a\n\n\u010desky | deutsch | english\n\n\nHow can I trust CAcert's root certificate?\nSee also:\n\nHow to install CAcert root certificates on Windows systems, and\n\nHow to import CAcert root certificates into browser clients.\n\nIn order to have your browser or system automatically trust all certificates signed by the CAcert Certificate Authority, you must instruct your platform or browser to trust the CAcert root certificate http://www.cacert.org/index.php?id=3.\n\nNote that for all systems, you will need to trust both the root certificate root_X0F.crt, as well as the class 3 certificate class3_x14E228.crt.\n\nSome of this information is already covered in the BrowserClients article, so also look there to see if it has the information you need.\n\nTrusting a new Certificate Authority is a process that varies from one platform to the next, so here are some of the ways to trust the CAcert root certificates. The instructions below will only outline how to trust one certificate, and just repeat the process to trust the second certificate.\n\nWARNING: Always double-check the fingerprint on the downloaded certificates before trusting them. If you don't, you could be trusting a maliciously modified root certificate.\n\n\u76ee\u5f55\n\nHow can I trust CAcert's root certificate?\nMac OS X\nUsing the Keychain GUI\nUsing the command line\n10.5 Leopard\nWindows\nWindows: cygwin environment\nWindowsMobile\nWindows Mobile 5\nPocketPC2002\nNotes\nLinux\nDebian\nInstall from CAcert site manually (recommended)\nInstall from unstable (sid via package management\nInstall from unstable (sid) manually\nKDE\nSymbian\nNokia E61\nJava\nAcrobat Reader\nAndroid Phones & Tablets\nCAcert user trusted certificates\nCAcert system trusted certificates (without lockscreen)\nCreating\nImporting\nVerifying\nReferences\nPalm Pre (webOS)\nHow can I be sure that it is authentic?\nFinding the correct fingerprints\n \nMac OS X\nThere are two ways to trust the CAcert root certificates: one from the command line, and one from the Keychain GUI. Each method requires that you use an account with administrative privileges.\n\nUsing the Keychain GUI\nDownload the desired certificate to your desktop from here.\n\nCAUTION: Verify the certificate fingerprints before proceeding!\n\nOpen the certificate file, either using Command-O or by double-clicking on the file.\nWhen Keychain appears, select the X509Anchors keychain.\n\nYou will be prompted to authenticate with your password to modify the system-wide X509Anchors keychain.\n\nUsing the command line\n\n# Download the certificates\ncurl -k -o \"cacert-root.crt\"   \"https://www.cacert.org/certs/root_X0F.crt\"\ncurl -k -o \"cacert-class3.crt\" \"https://www.cacert.org/certs/class3_x14E228.crt\"\n#\n# CAUTION: Verify the certificate fingerprints before proceeding!\n#\n# Import the certificates into the desired keychain\nsudo certtool i \"cacert-root.crt\"   k=/System/Library/Keychains/X509Anchors\nsudo certtool i \"cacert-class3.crt\" k=/System/Library/Keychains/X509Anchors\n# Clean up after ourselves\nrm \"cacert-root.crt\"\nrm \"cacert-class3.crt\"\n10.5 Leopard\nIf you're using 10.5 Leopard and try the certtool command above, you may see this error message:\n\n***************************************************************\n                         WARNING\n\nThe keychain you are accessing, X509Anchors, is no longer\nused by Mac OS X as the system root certificate store.\nPlease read the security man page for information on the\nadd-trusted-cert command. New system root certificates should\nbe added to the Admin Trust Settings domain and to the\nSystem keychain in /Library/Keychains.\n***************************************************************\n***Error adding certificate to keychain\nThe solution is to use the security command with add-trusted-cert instead:\n\nsudo /usr/bin/security add-trusted-cert -d -r trustRoot -k /Library/Keychains/System.keychain cacert-root.crt\nsudo /usr/bin/security add-trusted-cert -d -r trustRoot -k /Library/Keychains/System.keychain cacert-class3.crt\nWindows\nCovering all of the ways to import this certificate into Windows is beyond the scope of this article, and is already covered by How to import CAcert root certificates into browser clients.\n\nWindows: cygwin environment\nThere is no /etc/ssl; instead, you have to save it in /usr/ssl/certs, and under its special name.\n\nwget \"http://www.cacert.org/certs/root_X0F.crt\" -O /usr/ssl/certs/5ed36f99.0\nLocation found using \"strace wget https://somesite 2>&1 |grep ssl\", which obviously fails, but you see the attempt to read a cert at this location.\n\nWindowsMobile\nWindows Mobile 5\nOn WindowsMobile2005 you need to download the DER-Encoded certificate (pocketIE cannot save it, so you need to store it in a zip-file for download). Then you'll need to rename it to .cer . Only then you will be able to install it with a double-click.\n\n/!\\ OBSOLETE - not valid for the new roots, SHA256 signed! /!\\ You can also import new certificates using a CAB file in Windows Mobile. Generic instructions on how to make them can be found on the Windows Mobile blog. A prepared CAB file with both the Class 1 and Class 3 certificates can be found at http://jacob.steenhagen.us/CAcert.cab. This CAB, signed by Jacob Steenhagen's CAcert certificate, can simply be downloaded to your device and installed. You can verify the certificates are genuine by opening the CAB file and inspecting _setup.xml ensuring that the line before the <param/> (which contains the actual certificate) reads:\n\n\n   <characteristic type=\"135cec36f49cb8e93b1ab270cd80884676ce8f33\">\nfor class 1 and:\n\n\n   <characteristic type=\"db4c4269073fe9c2a37d890a5c1b18c4184e2a2d\">\nfor class 3. (Note: These should be verified against the Internet Explorer thumbprint at http://www.cacert.org/index.php?id=3). The previous thumbprints (MD-5) are valid for the old roots. Now you will find another thumbprints there, namely SHA1 and SHA256.\n\n/!\\ OBSOLETE - END /!\\\n\nPocketPC2002\nThere is a tool contained in the zip-file downloadable at http://support.microsoft.com/default.aspx?scid=kb;en-us;322956 (pocketIE on a wm2005-device could not display the html-page here but another browser might do.). This tool will only work with Self-Issued certificates.\n\nLoad the CA Cert into Internet Explorer as a trusted root.\nExport the certificate from IE as a DER encoded binary X.509 (.CER)\nUpload the file to your PocketPC\nIn the File Manager, locate the uploaded certificate and click on it to open and import it.\nAfter loading the certificate, warm-reset the Pocket PC to make it read the new root certificate.\nTo verify the certificate has been successfully imported into the Pocket PC device:\n\nIn the Settings menu, tap the \"System\" tab. Then tap \"Certificates\".\nTap on the \"Root\" tab. You should now see the new CA Cert root certificate that you added.\nNotes\n(Note that for wap1.x-gateways there is no way to host encrypted wap-pages if your provider's wap-gateway does not have the certificate because it's not end-to-end but decrypted on the gateway, not the device.)\n\nLinux\nHow your particular distribution will need to be modified to trust the CAcert root certificates will vary from one distribution to the next. However, there are some distributions about which we know some information, listed below.\n\nThe instructions for Red Hat 5+, Red Hat 4 and Fedora are topic of bug 1344: Wrong install instruction.\n\nKnoppix: CD versions newer than 3.8 have the certificates already.\n\nRed Hat 5+: wget -O - http://www.cacert.org/certs/root_X0F.txt >> /etc/pki/tls/certs/ca-bundle.crt (this will be overridden by updated openssl RPMs so it is likely not the best method)\n\nRed Hat 4: Change the above location of ca-bundle.crt to /usr/share/ssl/certs/ca-bundle.crt\n\nFedora: Copy the certificate to /etc/pki/ca-trust/source/anchors/ then run update-ca-trust extract\n\nThe Bug1344 text explains furthermore:\n\nRHEL 4 is deprecated and only supported under very special terms\n\nfor RHEL generation 7 the same instruction as for Fedora should be used\n\nThe correct call for Fedora is \"update-ca-trust\" instead of \"update-ca-trust extract\"\n\nOtherwise, you can obtain the certificates from the website as usual, from here.\n\n\nOn ubuntu : try\nsudo apt-get install ca-certificates\nwhen the package is installed, you can do :\ndpkg-query -L ca-certificates\nto have the list of the root certs\nDebian\nAs of March 2014, Debian no longer distributes CACert root certificates as part of Debian releases. Although a package is available in the unstable (sid) distribution (updated in 2019), it is inconvenient to use because you either have to check signatures manually or configure sid as package source.\n\nInstall from CAcert site manually (recommended)\nImport CAcert root certificates using the following (\"$\" not be entered - is a prompt):\n\n\n$ wget http://www.cacert.org/certs/root_X0F.crt http://www.cacert.org/certs/class3_x14E228.crt\n$ sudo cp root_X0F.crt /usr/local/share/ca-certificates/cacert-root.crt\n$ sudo cp class3_x14E228.crt /usr/local/share/ca-certificates/cacert-class3.crt\n$ sudo update-ca-certificates\nThis should output something like this:\n\n\n$ sudo update-ca-certificates\nUpdating certificates in /etc/ssl/certs... 2 added, 0 removed; done.\nRunning hooks in /etc/ca-certificates/update.d....\nAdding debian:cacert-class3.pem\nAdding debian:cacert-root.pem\ndone.\ndone.\nThen, you are done. See man update-ca-certificates and /usr/share/doc/ca-certificates/README.Debian from the ca-certificates package for more information.\n\nInstall from unstable (sid via package management\nYou need to add unstable (sid) as a package source in a way that all packages are not considered to resolve versions, except ca-cacert.\n\nEdit as root (so prepend sudo to editor command) file /etc/apt/preferences to contain\n\n# Passivate unstable (sid)\nPackage: *\nPin: release o=Debian,a=unstable\nPin-Priority: -10\n\n# Allow ca-cacert from unstable (sid) but still prefer target release \nPackage: ca-cacert\nPin: release o=Debian,a=unstable\nPin-Priority: 100\nEdit as root file /etc/apt/sources.list to contain (feel free to alter mirror according to preexisting entries)\ndeb http://ftp.de.debian.org/debian/ sid main # provides package ca-cacert\nUpdate package information and install package by running commands\nsudo apt update\nsudo apt-get install ca-cacert\nAdd trust to fresh certificates interactively running command\nsudo dpkg-reconfigure ca-certificates\n\nThe certicates to mark as trusted are listed as\n\nCAcert/class3_x14E228.crt\nCAcert/root_X0F.crt\nInstall from unstable (sid) manually\nWarning: Use this installation method only if you are prepared for cumbersome checking of signatures. The verified file can then be reused for later installs.\n\nSearch and download package from packages.debian.org with an expected name of the pattern ca-cacert_<version>_all.deb .\n\nManually check the signature of the downloaded file via Release.gpg as described in the Secure APT (Debian wiki)\n\nInstall package running command\nsudo dpkg -i <downloaded-package-file-name>\n\nAdd trust to fresh certificates interactively running command\nsudo dpkg-reconfigure ca-certificates\n\nKDE\nThe CAcert root certificate can be added to KDE's certificate store so that all KDE applications, including Konqueror, will trust certificates signed by it.\n\nDownload the certificate(s) in PEM or DER format.\nIn the KDE Control Center, under \"Security & Privacy > Crypto,\" go to the \"SSL Signers\" page, and click \"Import.\"\n\nChoose a certificate you downloaded.\nYou may be asked whether to make the certificate available to KMail as well. This is recommended.\nIMPORTANT! Find the certificate in the list (it may help to sort by \"Organizational Unit\" and then look for \"http://www.cacert.org\"), click on it, and verify that the MD5 digest shown at the bottom of the window matches the one shown on the download page.\n\nSymbian\nNokia E61\nDownload the root and class 3 certificates in der format\n\nCopy the certificates to the E61 (the E61 cannot read the files direct from the web)\nOpen each certificate in File Manager, and save the certificate. You will have to confirm this as the E61 believes that the certificates may be unsafe.\nJava\n\n$ keytool -keystore $/PATH/TO/CACERTS/KEYSTORE -storepass changeit -import -trustcacerts -v -alias cacertclass1 -file root_X0F.crt\n$ keytool -keystore $/PATH/TO/CACERTS/KEYSTORE -storepass changeit -import -trustcacerts -v -alias cacertclass3 -file class3_XOE.crt\nTypical locations of the cacerts keystore:\n\nLinux Ubuntu: /usr/lib/jvm/java-$VERSION/jre/lib/security/cacerts\nLinux SuSE: /usr/java/jre$VERSION/lib/security/cacerts\nExplanations:\n\nName of the keystore file is \"cacerts\", its password is \"changeit\" (a clue for you to change this password).\n$/PATH/TO/CACERTS/KEYSTORE = placeholder of the path to the file named \"cacerts\", including the filename itself.\n$VERSION = Java version, examples:\nLinux Ubuntu: \"7-openjdk-amd64\" - thus the whole path including the filename is: /usr/lib/jvm/java-7-openjdk-amd64/jre/lib/security/cacerts\nLinux SuSE: \"1.8.0_71\" - thus the whole path including the filename is: /usr/java/jre1.8.0_71/lib/security/cacerts\nAcrobat Reader\nSee also AdobeReader.\n\nProcedure for Acrobat 8:\n\nMenu Document -> Manage Trusted Identities...\n\nDisplay: Certificates\nButton \"Add COntacts...\"\n\"Browse\" to the root certificates\nEdit trust to your liking. \"Signatures and as trusted root\" is the essential thing, the others are optional.\nAndroid Phones & Tablets\nBefore Android version 4.0, with Android version Gingerbread & Froyo, there was a single read-only file ( /system/etc/security/cacerts.bks ) containing the trust store with all the CA ('system') certificates trusted by default on Android. Both system apps and all applications developed with the Android SDK use this. Use these instructions on installing CAcert certificates on Android Gingerbread, Froyo, ...\n\nStarting from Android 4.0 (Android ICS/'Ice Cream Sandwich', Android 4.3 'Jelly Bean' & Android 4.4 'KitKat'), system trusted certificates are on the (read-only) system partition in the folder '/system/etc/security/' as individual files. However, users can now easily add their own 'user' certificates which will be stored in '/data/misc/keychain/certs-added'.\n\nSystem-installed certificates can be managed on the Android device in the Settings -> Security -> Certificates -> 'System'-section, whereas the user trusted certificates are manged in the 'User'-section there. When using user trusted certificates, Android will force the user of the Android device to implement additional safety measures: the use of a PIN-code, a pattern-lock or a password to unlock the device are mandatory when user-supplied certificates are used.\n\nInstalling CAcert certificates as 'user trusted'-certificates is very easy. Installing new certificates as 'system trusted'-certificates requires more work (and requires root access), but it has the advantage of avoiding the Android lockscreen requirement.\n\nCAcert user trusted certificates\nDownload the certificate files ('root_X0F.crt' and 'class3_x14E228.crt') onto the internal flash storage (the '/sdcard' or any subfolder). Browse to this folder with the file manager and open 'root_X0F.crt'. Although there might not be an icon for certificates and the files will have a '?'-icon, files will be opened with the certificate manager, asking you for a name to describe the to-be-imported certificate. If it is the first user certificate you install, the Android Security Model forces you to use a lock-screen to unlock your device (see \"CAcert system trusted certificates\" if you really need to avoid this) Repeat with the 'class3_x14E228.crt' file. Check if both certificate files are installed correctly, Settings -> Security -> Certificates -> 'User'-section should now list the certificates you have just installed.\n\nCAcert system trusted certificates (without lockscreen)\nThe existing method of importing user certificates works fine, but it has the disadvantage that it requires a PIN / password lockscreen whenever user certificates are installed. By installing the CAcert certificates as system certificates, these files are better protected from tampering by malicious apps, and there is no lockscreen requirement (allows 'Slide to unlock' or no lock at all). You will need a rooted phone (or at least temporary root access), and a system with openssl software for creating the new certificates.\n\nThe next steps will show you how to create Android compatible certificate files from the original CAcert certificate files, how to install/import them on your android device, and how to verify everything is correctly installed.\n\nIt is possible, in Android OS version 4.4.2, to save certificates as user trusted ones (Android itself creates their correct names, derived from hashes), and then move them into the system trusted certs repository, using program for Android as Terminal, adb shell, or Ghost Commander. If you decide to follow this process, skip to the Importing paragraph replacing the source folder \"/sdcard\" used there, with the \"/data/misc/keychain/cacerts-added\" folder, where Android stores user trusted certificates. Do not copy CAcert roots, move them!\n\nCreating\nWe will create Android compatible certificate files from the original CAcert certificate files.\n\nBoth files are prepared and ready to download, with their hashes to check, here.\n\nGet the CAcert root certificates from the cacert.org website https://www.cacert.org/index.php?id=3 Download the root certificate PEM format (root_X0F.crt) and the Class 3 PKI key in PEM format (class3_x14E228.crt) Get the hash of the root_X0F.crt certificate:\n\nopenssl x509 -inform PEM -subject_hash_old -in root_X0F.crt | head -1\nThis shows you the hash, in the case of the CAcert PEM file 'root_X0F.crt' it is '5ed36f99' (note the use of '-subject_hash_old' instead of '-subject_hash', to get an openssl 0.9 compatible hash) We will use this hash value, append '.0' (dot zero) and use this as the filename for the resulting Android certificate:\n\ncat root_X0F.crt > 5ed36f99.0\nopenssl x509 -inform PEM -text -in root_X0F.crt -noout >> 5ed36f99.0\nRepeat these steps for the Class 3 PEM certificate file 'class3_x14E228.crt'. If things go well you will end up with the files 5ed36f99.0 and e5662767.0 (if you get the hash values 590d426f and 99d0fa06, you are not using the '-subject_hash_old' parameter to openssl).\n\nThe md5sum of the certificate files:\n\nmd5sum 5ed36f99.0\n6ecc343c22ba3ba6ef817f0d8bd744e1  5ed36f99.0\n\nmd5sum e5662767.0\ne2e7c5924103de7d2b93fef735176b45  e5662767.0\nThe sha1sum of the certificate files:\n\nsha1sum 5ed36f99.0\n8d9ca4e340ecf56911296b3c48b3a4969515b268  5ed36f99.0\n\nsha1sum e5662767.0\n915346ab8ea8a2a00e158afb8c03ce43c8745f16  e5662767.0\nThe sha256sum of the certificate files:\n\nsha256sum 5ed36f99.0\na04100c5026e41cf6d79a4653495258afc02f1819d742a3f8af848e052036196  5ed36f99.0\n\nsha256sum e5662767.0\n239e3845dde6dba0a63b5e17d7365c27f0af27b51da2bddf293c54a84fa7f181  e5662767.0\nImporting\nWe now have Android compatible certificate files, and we will import them into Android 'System' certificate store. It is necessary for you to gain the super-user rights to be able to write to / remove from / move between system subfolders. To achieve this, the Android system has to contain the \"su\" (super-user) program, which provides you with the super-user rights. Some phones' Android systems do not include this program. In such a case, you have to store all certificates added as the user ones.\n\nCopy the files to the /sdcard folder, either with any file manager or with adb push. Go into adb shell (adb shell from commandline), or open the 'terminal'-application on your android device. You will get a command prompt similar like shell@android:/ $ Gain superuser/root rights, neccessary to perform privileged actions:\n\nsu\nMake the /system folder writable (will return to read-only upon reboot):\n\nmount -o remount,rw /system\nCopy the new certificate files to the correct folder on your Android device:\n\ncp /sdcard/5ed36f99.0 /system/etc/security/cacerts/\ncp /sdcard/e5662767.0 /system/etc/security/cacerts/\nCorrect the file permissions to u=rw, g=r, o=r:\n\ncd /system/etc/security/cacerts/\nchmod 644 5ed36f99.0\nchmod 644 e5662767.0\nCheck if the files are ok:\n\nls -al -Z\nOmit '-Z' if you are using a version of Android without SElinux, it just shows some extra security settings which might be useful if you run into trouble.\n\nAmongst the other default android certificate files, you will see the two new files:\n\n-rw-r--r-- root     root              u:object_r:system_file:s0 5ed36f99.0\n-rw-r--r-- root     root              u:object_r:system_file:s0 e5662767.0\nThe certificates will be loaded upon the next boot of your device, so reboot your device:\n\nreboot\nVerifying\nTo verify certificates are installed correctly, go to Settings -> Security -> Certificates. It should list both \"CAcert Inc.\" and \"Root CA\" among the other certificates in the 'System' section. Make sure that these CAcert certificates are not also in the 'User' (user defined) section. From your android device, visit https://www.cacert.org. If you do not see a warning about missing or untrusted certificates, all went well.\n\nNote that some browsers might use their own certificate store instead of the Android one, you might need to import certificate files into those browsers as well.\n\nIf you are unable to disable the Android PIN/Pattern lock screen after installing the system certificates, you might need to \"Clear/delete credentials\" (in Settings -> Security) even though you have removed all user certificates.\n\nIf you run into problems, compare the md5 sum of the certificate files with the md5 values in this article, check the file permissions of the newly installed files. Make sure no user certificates are installed (Settings -> Security -> Clear certificates), and make sure you are using a browser app that uses the android certificate store and does not implement an own certificate store.\n\nIn the future, newer versions of openssl might be used on Android, if so, you might need to drop the \"_old\"-part of the \"-subject_hash_old\" openssl parameter.\n\nReferences\nCyanogenMod forum with identical article by Sebastiaan Giebels\n\nStackOverflow - user2708846 comment on how to create correct Android certificate files\n\nDescription of the ICS Trust Store Implementation by Nikolay Elenkov\n\nCyanogenmod wiki (old) - articke on adding a CA without requiring a PIN(makes the mistake of not using the certificate hash as filename)\n\nStackExchange - Install CA without having to activate screen lock\n\nPalm Pre (webOS)\nStarting with webOS 1.2, the proceeding for adding the root certificates to the Palm Pre is extremely simple and can be done entirely on the phone.\n\nVisit http://www.cacert.org/index.php?id=3 in the Pre's browser (http://www.cacert.org and click on Root Certificates)\n\nUnder Class 1, click the link for Root Certificate (PEM Format)\n\nA gray progress will appear at the bottom of the screen. Once the certificate is fully downloaded, an arrow will appear on the right side of the bar\nClick on the bar containing root_X0F.crt and the aforementioned arrow\n\nThe certificate manager will open giving you the ability to view detail and accept or reject the certificate\nUnfortunately, I don't see anything on that details screen to validate the fingerprint\nClick on the \"Trust Certificate\" button\nRepeat for Class 3, Intermediate Certificate (PEM Format)\n\nNote: prior to webOS 1.2 you had to copy the .crt files to the phone's memory using USB mode, load the certificate manager (Device Info; More Info (button at bottom); Certificate Manager (Preferences menu at top)), and import the certificates (icon in lower left of screen to browse for them).\n\nHow can I be sure that it is authentic?\nThere are many ways to ensure that you have an authentic, non-tampered copy of the root certificates, all of which boil down to having a trusted party verify the certificate fingerprints. In some cases, your system distribution is the trusted party, but you can also verify it for yourself.\n\nIf your system is mentioned above, you can follow those instructions to ensure you have a authentic copy of the CAcert root certificates.\nYou can manually download and verify the certificates from here.\n\nFinding the correct fingerprints\nCAcert is working to provide multiple places to verify the certificate fingerprints. The following are already known ways to find authentic copies of the CAcert root certificate fingerprints.\n\nYou can decrypt the GPG signed message from here and compare the certificate fingerprints contained in the message with those contained in your downloaded certificates.\n\nObtain a copy printed on the AssuranceForms; ask for one at the next event.\n\nFind them in the Impressum of Linux Magazin\n\nCAcert is currently working on providing fingerprints through these additional means:\n\nBusiness cards with the fingerprints printed on them.\nListing the fingerprints in additional magazines.\n\nCategoryCommunity\n\nCategoryConfiguration\n\nCategoryGuide\n\nCategorySoftware\n\nCategorySupport\n\nFAQ/ImportRootCert (2021-07-13 21:48:03\u7531AlesKastner\u7f16\u8f91)\n\u53ea\u8bfb\u7f51\u9875\u4fe1\u606f\u9644\u4ef6 \n\u66f4\u591a\u64cd\u4f5c\uff1a\nMoinMoin PoweredPython PoweredGPL licensedValid HTML 4.01", "document_id": 572}]}, {"paragraphs": [{"qas": [{"question": "Chrome extension id - how to find it", "id": 760, "answers": [{"answer_id": 760, "document_id": 447, "question_id": 760, "text": "Use the chrome.runtime.id property from the chrome.runtime API.", "answer_start": 69, "answer_category": null}], "is_impossible": false}], "context": "How can I find out what the chrome extension id is for an extension? Use the chrome.runtime.id property from the chrome.runtime API. You get an extension ID when you upload your extension to Google Web Store. Ie. Adblock has URL https://chrome.google.com/webstore/detail/cfhdojbkjhnklbpkdaibdccddilifddb and the last part of this URL is its extension ID cfhdojbkjhnklbpkdaibdccddilifddb.", "document_id": 447}]}]}